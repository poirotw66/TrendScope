 VITAL SHIRODKAR My name is Vithal Shirodkar. I am the vice president and general manager for Google Distributed Cloud. And I'll be talking about what's new with Google Distributed Cloud or GDC. So, one could argue that Gen.AI is the most transformative technology leap that humanity has ever experienced, even more than the internet and semiconductor eras. Generated AI's impact on productivity could add trillions of dollars in value to the global economy. There is so much innovation and new products being launched every day on the cloud. I hope you all had the opportunity to experience some of these products this week, as well as experience them in demos or breakout sessions, and got to hear from your other Google customers and peers on how they are transforming their business using AI. But there's still many of you who are operating outside the cloud and who are operating on-prem. Now, there are a lot of reasons why customers cannot run their workloads on public cloud. These could be latency requirements. So, there are customers who feel that the round-trip delay to the cloud and back is a lot for sort of real-time applications. Customers want to build and do things in milliseconds and, you know, it takes time to go to the nearest cloud region. You might have bandwidth constraints in your edge. So, we have customers who say that they get isolated from the internet for up to a day at a time. Or it could be that you're generating a lot of data and don't have that much of, you know, required bandwidth to the cloud and need to process data locally. There's also cases where you've got data residency, right? It could be customers whose data needs to be local due to maybe a state and city or local jurisdiction laws and you don't have access to a public cloud region. Or for customers who've got extremely sensitive data that cannot leave on-premise, they need these to be in isolated environments which are fully air-gapped. Gartner estimates that by 2027, about 50% of critical applications will continue to remain outside public cloud. Now, how do customers continue to adopt AI in an on-prem environment? We've heard that there are a lot of challenges. And from our customers, we've heard that it's expensive and time-intensive for four reasons. Data sovereignty and regulatory, right? So adopting latest AI technologies can be slow, even impossible, due to sort of stringent regulatory and data sovereignty requirements, leading to lost business opportunities and subpar customer expectations. Or, you know, managing the AI infrastructure on-prem. Deploying and managing AI hardware is complex, right? It takes a lot of effort. Going and deploying and developing sort of, you know, cross-platform AI software, again, is time-consuming. It can increase your operational burden and increase your complexity of your solution. And all this slows your time to market. And it also takes away valuable resources that you can apply to your core business. Then you've got inconsistent hybrid environments. So AI platforms that are in the cloud and that are on-prem typically could be different. That could lead to sort of, you know, more complexity in your application. And it hinders sort of application scale. And finally, access to frontier models, right? So today's customers are limited to open source models on-prem. And, you know, where the highly capable and next-generation models are being released on public cloud. And you don't have access to that. To solve for these, we have built Google Distributed Cloud. So GDC is a hardware and software product that brings the best of Google Cloud, including Gemini and AI-optimized infrastructure, directly to your data centers, retail locations, public sector locations, and factory floor. It includes fully managed and integrated cloud services, including Vertex AI, Gemini, databases, infrastructure services like network, compute storage, and other platform services. So we start with a very prescriptive hardware that you can purchase yourself or through us. And then on top of that, we layer a system layer that is built on Kubernetes and other open source systems that allows you to continue operating it without any vendor lock-in. And then we have an abstraction layer that enables us to get some GCP services like Vertex and Gemini to run on top of GDC. It comes in various form factors. You can start small. In a connected servers product, you can have a one-node or a three-node server. And in the air gap product, you start with a three-rack solution. And then you can expand that to tens of racks per zone and hundreds of racks in a region. Google distributed cloud comes in sort of three variants. So you have the GDC software only, which really brings Kubernetes on-prem. That runs on your hardware. Then you've got the fully air gap solution where the system is completely deployed and operated on-prem and is isolated from the internet completely. It's even sort of the initial deployment of the bootstrap is also done in an isolated manner. And then in the connected, where there is connectivity to the cloud, but your data and your services remain on-prem. And the connectivity is for management purposes. But you do have the ability to inspect what's going back and forth from the cloud using a boundary proxy. And then you can inspect and you can block any requests that are coming in. And the connected version can continue to operate even if it is isolated for up to seven days. So this week, we are announcing that Gemini will be available on Google distributed cloud, bringing Google's most capable models to on-premise environments, with public preview starting Q3 of 2025. So I hope you all have attended, you know, some of the Gemini sessions or gone and visited some of the demo and seen Gemini in action to experience its capabilities. But I'm going to talk a little bit about Gemini, right? So it is a multimodal model and it's able to process text, images and video all at once. Gemini models support more than 100 languages. It has more than 1 million context windows, depending on which model you choose, which means you can process, 1 million context means you can process something like 700,000, you know, lines, words, or 30,000 lines of code, or an entire hour of video in a single prompt. It comes in three variants. So you've got your Gemini Pro, which is our most powerful thinking model, with maximum response accuracy and state of art performance. And then you've got your Flash, and Flash is optimized for lower latency and enhanced performance. And it's really built to power agentic experiences. And then you've got Flashlight, which is optimized for low latency and cost. Customers can now get Gemini on-prem using GDC. So we have partnered with NVIDIA to bring our Gemini models to NVIDIA Blackwell systems. So when we launch Gemini on the cloud, it's built and it runs on TPUs. And we port that and run it. We're partnered with NVIDIA. We are now running it on GPUs. And you can purchase that through Google or your preferred channels, right? You can get Gemini on Edge GX or DGX servers. You can get them in the Edge 200 or B200 GPUs. And depending on your use case and the models that you want to run. We have two different approaches. So in the connected, the customer brings the hardware. So you procure the servers through your channels. And Google provides the software management and completely manages your host and Gemini models in the connected setting. So we have built a confidential compute layer. We start all the way from sort of secure and measured boot. And we build confidential layers and the OS and VMs and containers. And we do attestation to make sure that the models are secure and your data is secure. So your data and model prompts are secure and cannot be accessed. In the air gap deployments, Google brings the entire packaged offering to you with the hardware, software, and Gemini onto customers' premises in a fully air gap setting. So Google distributed cloud brings the best of Google to your on-prems, right? So data remains on-premise. So even all your model prompts, your inferencing, everything happens locally and does not travel to the cloud. Even in the connected more, there is no access to your data from GCP. And you can actually inspect that from the boundary proxy solution I talked about. In addition to bringing Gemini to Google distributed cloud, customers can benefit from the Vertex AI platform on GDC, which lets them accelerate the development and deployment of agent-tech applications on-premise. This entire complete AI platform includes pre-trained APIs. So we have ready-to-use, task-optimized, pre-trained APIs based on advanced Google models for translation, speech-to-text, optical character recognition. And these APIs offer advanced features such as customizable glossaries and in-place translation. Then you've got Gen AI building tools that allow you to use third-party models. So you can use third-party and open-source models using our model garden. You could do retrieval augmented generation to personalize and augment the AI model output, eliminating the need to do fine-tuning and additional retraining of the models. And you also have built-in embeddings and alloy DB vector database support for you to sort of run in your on-prem and use AI applications. So here is an example of a solution that's built on top of GDC. So imagine that you are a fraud analyst at a financial institution and you're working with highly sensitive data. Now this data cannot run on or cannot be moved to the public cloud due to the sensitivity and the large volume of the data. And so you need to work on it on-prem. Now you've got an ML pipeline that flags fraud, but, you know, confirming these transactions is complex. So we have a fraud analyst assistant that uses Gemini to answer questions and find patterns of fraud in your data. Running on Google distributed cloud, it analyzes data on-premise and it reduces the confirmation time and identifies previously hidden fraud patterns. We have this in our as demo in our hardware works. So if you have time today, please do visit our hardware works for a hands-on demo of the fraud analyst assistant. And you can also explore other applications that are running securely on GDC at the hardware works. Today we are also excited to announce that Google agent space search will be available on GDC with public preview starting in Q3 of 2025. So Google agent space search provides all enterprise knowledge workers with out of the box capabilities that unify access to all your data in a secure permissions aware manner. So enterprise are very eager to deploy Gen AI. But they also struggle to connect large volumes of siloed information across various repositories and formats such as images, PDFs, text, etc. Now, in many cases, these are sitting in different systems. You know, you have stuff in Jira. You've got it in Confluence. You've got it in SharePoint. And you've got to build search that actually goes and, you know, accesses them. You've got it in databases. And this hinders productivity and innovation. Employees waste time searching for information across all these different systems. And in many cases, there is also a lack of consistent access control on premise in on premise search. And that really exposes you to having privacy and leakage risks. At the same time, building an in-house search is difficult. It's costly and requires access to really scarce AI expertise. So with Google agent space search on GDC, you get access to company branded multimodal search agent. Right. It's a conversational search interface that can answer very complex questions. It's based on your company's information and your company's unique data acting as a central source of enterprise for a central source of truth for your entire organization. You it comes with prebuilt enterprise data connectors to index data from the most common data sources and data repositories. Like as I said, such as Confluence, Jira, ServiceNow, and SharePoint. And with many more connectors that are on the roadmap in the future. You've got robust access control list enforcement that helps ensure that search results are permission aware. So that people who get access to data through search actually have got permissions to view that. And maintaining the security and compliance for all your on prem data. And then Vertex AI is integrated out of the box with agent space, starting with search agents and with more prebuilt agents coming soon. And also having the ability for you to build your own agents. So now let's see a demo of agent space search on GDC. Agent space allows GDC companies to offer their employees robust enterprise search across a set of Google and 3P connectors. These users get the functionality of the agent space search tier, the advantage of on prem compatibility and full enterprise compliance. Let's see how it works. This is my company branded agent space landing page. From the set of sources made available to me by my administrator, I can modify the ones used for each search. With my sources set, I'll ask agent space to find emerging credit risks for our corporate portfolio. Agent space searches across data connectors showing ranked search results that can include people. It generates an AI answer and I can clearly see which specific sources were used. Now let's ask a follow up question. The context from my initial search is carried over into a conversation view. I get a multimodal response and can drill into the specific sources for each point. Let's ask another follow up. I want to know about support tickets related to these. Agent space understands my intent and shows me a list of tickets. Now I can open each one to learn more or ask agent space to summarize the top results. Yeah, this is pretty phenomenal. I've been playing around with this. We've got dog food at Google and I got access to this a few months ago. And, you know, asking it questions, it's got access to my data, my email, my drive, my calendar and all that stuff. And you just ask questions, hey, when am I going for dinner? It tells you you've got these meetings. This is where it is. It ends and, you know, go ask it to get data from public internet. So you can choose to say include internet access or you can choose to keep it local. So it's a pretty phenomenal experience. If you haven't seen it already, I urge you to go take a look at it in one of the demo sessions. So bringing all these things together, GDC brings AI to on premise. It's based on an AI optimized infrastructure with HGX and DGX servers running H200 or B200 GPUs. It's got a core set of IaaS services like VMs, Kubernetes, block and object storage. And it's got a set of core services like observability, HSM, marketplace, et cetera, for third party ISVs. It's got Vertex AI services with both the first party models as well as third party open source models available through the model garden. And a set of pre-built AI agents with the ability to build your own agents. With that, I'd like to invite Pan Yong on the stage, who is the chief information officer of our key partner, HTX, to share his GDC journey. Thanks, Peter. So in the next few minutes, I'd like to share where does GDC fit in the Singapore government's agency's cloud strategy in the age of AI. But before I get into the details, I thought it's kind of useful to explain to the audience who is HTX, what we do. We are a Singapore government agency. Our mission is to exponentially impact public safety and security through science and technology. Now, that sounds like a mouthful, so I'd like to maybe give you some practical examples on what exactly we're doing. So we have traveled into Singapore. Our mission is really to deploy technology such that when visitors arrive at the air bridge, you can essentially breeze through your clearance from the time you arrive at the air bridge to the time you take a taxi. Hopefully, you can achieve that within a matter of minutes. We apply facial recognition and biometric solutions to enable that kind of experience. We offer video emergency call services to our emergency call center such that our first responders can have visual insights of what's happening on the ground and be much more prepared than arrive on the ground. Now, the difference for people who are under duress, that could be a matter of life and death when the first responders are much prepared for the situation. We also deploy drones and unmanned technology to support our police force such that in the situation of a large event, we're able to isolate potential hotspots and the police can so-called bring things back to normalcy before things start to escalate. In fact, two weeks ago, last week, there was an earthquake in our neighboring country, Myanmar. What we did was actually deploy our cyborg. These are actually cockroaches with IoT sensors mounted on top. And they actually went out to support our firefighters in the situation of search and rescue. So that's who we are. And many of this exciting work and technology cannot be achieved without the cloud. And I'd like to share a bit where are we in terms of our cloud journey and how does GDC fuse that gap. We started our cloud journey back in 2019. Right, we started the commercial cloud. And the goal is really to replatform a whole bunch of on-premise system to give them the elasticity and the consumption based model in the cloud. We added additional security guardrails. In fact, in that journey, we learned that there's a lot of vulnerability we have in our system that deploy and plan on-prem. In fact, the cloud enables us to level up our security posture. Together with that, we apply a goal local strategy to ensure that the data sits in-country to make sure that it's safe and secure. With that experience, it was super useful for us when COVID hit us. We're able to leverage our cloud to be agile, to deploy a solution that can deal with, to solve some of those challenges we face during COVID. With the success in commercial cloud, we decided to move into a hybrid cloud strategy. The hybrid sovereign cloud is a special partnership with CSP. It's to build dedicated data halls or region in-country. Right. We added additional operational guardrails, including having clear personnel, having some controls in terms of data movement in and out of the data centers. Right. These are connected to a cloud region, but that setup ensures we have sovereignty to our data, that when there's a need for us to essentially gain physical control of those data, we have full access to that. Moving forward in 2025, we decided to partner with Google for GDC. And that's pretty much driven by the demand for AI, right? The demand for GPU to accelerate some of workload and to secure, as what Victor shared a bit earlier, to secure AI model that can deploy on-prem. What we learned in our journey in the commercial cloud is KA is the new cloud native. We are able to leverage on GDC to deploy solutions developed and build around open source Kubernetes technology. We all heard about all the AI solutions, Vertex, enterprise search, agent space. We're quite excited to be able to bring some of this SaaS technology that traditionally not available for on-prem stack into an on-premise environment. So that's our journey into the cloud. So I'm going to elaborate a bit more how AI is changing our lenses of looking at everything. Many of you here could have been in industry long enough as I do, whether it be the early days of the internet, whether it be the dot-com or the WET 2.0 or the mobile revolution. I think nothing has been what we are experiencing right now in terms of the phase change in AI. So it really changes the way we look at things. What ChatGPT has done is really opened the imagination of our stakeholders. They have pretty much democratized AI as a technology. Therefore, we have quite a tremendous pleasure for stakeholders to bring AI into operation needs. But the challenge for us is we still have lots of data that's hosted on-prem. And it's quite a challenge for us to move the data from the on-premise environment into the cloud. What we want to do instead is to bring AI into those workloads, bring AI models, AI technology, and OLM to those data in the on-premise environment. So demand of AI in the machine drives us to think otherwise. Balancing AI safety and security. Everything we see today with AI, they're all version 0.something, right? Whether it's in Lanchain, Lama Index, even a lot of models, they are really new. In fact, every single week we are experiencing a new model that's being deployed out of the market. How do we provide guardrails and secure those AI models? Giving us, having an on-premise implementation, allow us to sort of air-gap some of these vulnerabilities, reduce the attack surface that we're experiencing if things are deployed in the cloud. So balancing AI and AI safety and security is important for us so that we can enable AI innovation while making sure there's a level of guardrail we can put in place. GPU agility in innovation. We need to have GPU to drive innovation. And that's what we experienced today. And how do we leverage on GPU technology to support many of our workload. And one of the big challenges we face today is the way we design our data centers. Things have changed a lot. We used to build a single rack, which is 5 kilowatt, 10 kilowatt, 12 kilowatt. We think that's quite massive, but things have changed quite significantly. We're looking at minimum footprint of 20 kilowatt per rack and looking at data centers that from what we're covering built with around 3 megawatt to potentially building set data centers for 40 to 80 megawatt per data centers. LRM in product development. We also recognize that everything we do now is being transformed and being disrupted. Be it vibe coding by developers at the front end, using AI to fix those badly written vibe codes, or maybe they are not badly written, but essentially using AI to do code verification, to generate unit testing, and to validate what we do. How do we bring AI to be the heart of our software, moving away from what we traditionally call systems of record model to being AI powered as a solution. So the change in product development cycle has driven us to adopt AI. So that's really how we see GDC fits in our scheme of things, where agents, AI models, and agility of the cloud can be deployed in the on-premise environment. To closing off the last slide, I just want to quickly share about how everything has come together for us. If you look across the three different pillars, the first pillar is what we adopt, work quite closely with a lot of new cloud and SaaS provider to continue to adopt GPU as a service to train our models, to experiment with new AI models without compromising data security. We continue to work closely with a commercial cloud solution, because that provides us the biggest set of innovation and features capability, like for example GCP. Sovereign Cloud is a strategy we continue to deploy, sovereign hybrid cloud, that gives us maybe a reduced set of capability in terms of cloud services, but that gives us a particular security guardrails. An important strategy for us that we have moved forward is AI on-premise cloud. That requires us to build new sets of data centers, work with partners like Google for AirGap cloud. But what we see on top of those stacks is really deploying cross-cloud services, cross-cloud delivery platform, as well as harmonizing our DevsetOps and MLOps pipeline. So with that, that's my part of the presentation. Maybe I invite Vito back on stage. Yeah. Thank you, Pang Yong. Thank you very much for sharing your GDC journey and how you think about sovereignty and sovereign cloud. And at Google, we do give you lots of options and we believe, as a customer, you need to have a choice when it comes to sovereignty. So we've got sovereign controls in GCP in every region. You've got the ability to actually run workloads and have complete sovereign controls. Then we've got the option of sovereign cloud itself. So there's Google sovereign cloud. We've got an entire cloud in a couple of countries. And then now with GDC's AirGap, you can have a fully isolated cloud with both sovereignty of your data as well as sovereignty of operations in on-prem. So with that, I'm going to go into talking about Edge and talk about how GDC unlocks inferencing at the edge, helping customers to transform their business in retail and manufacturing environments. GDC connected servers come with NVIDIA L4 GPUs that enable retail and manufacturing customers to enable inference at the edge without infrastructure or model management capabilities. Narc چinter smacked by prolific andIMP urban, bank companies. Narcats divided enough distributors or providers, evolving emissions from the shareholders of India. And creating exciting deutschen products, , including marketing products,Hmm memes and amerif وأ. In which moment of line figures, it'schangeable. In fact, we've got to, to create pre-安WS and strengthening lessons. Far more and everything is trehyally cool. Thank you, our coven en絶 music Auntie. You're comfortable with味 and loving customers and insights to improve worker productivity and effectiveness. And GDC provides a complete unified edge platform. You can run multiple diverse applications on a single consistent infrastructure reducing device sprawl. So we have partnered with retail and manufacturing leaders like Toshiba, IntenseEye, Eversine, and Standard AI to deliver innovative edge solutions. These strategic partnerships bring together Google Cloud infrastructure with industry-specific expertise for transformative outcomes on-prem. These include quality control and safety, predictive maintenance, enhanced customer experience, and operational efficiency and insights. Together, we empower retailers and manufacturers with cutting-edge technologies for enhanced efficiency, safety, and customer experience. With Toshiba's Elira commerce platform and GDC, retailers can gain real-time AI-powered capabilities which can help with loss prevention, automated checkout, and give personalized shopping experience. Here you can see in this video, you will see a person at a self-checkout counter. We see that the system detects the non-scanned items, and then the point of sale is locked with a video clip of all the non-scanned items, so then you can go and recover all the non-scanned items. In the second example, we see that the system detects items that are left on cart and locks the point of sale, and so these are examples of how EverSeen optimizes retail operations by applying vision agents and advanced threat intelligence to mitigate loss, improve processes, and enhance the customer experience. Now let's talk about edge computing in really harsh and disconnected environments. For that, we have GDC air-gapped appliance, which is a human portable device, which is about 100 pounds, that has complete distributed cloud deployed on it for use outside of data centers, such as in the field or other remote sites. GDC air-gapped appliance, it consists of a mil-spec ruggedized chassis that holds two compute blades, one NVIDIA GPU accelerator blade, and a network switch. It supports Kubernetes, VMs, object storage, block storage, AI, and data services inside that box. It includes robust security features like encryption, data isolation. It includes firewalls and secure boot to protect your sensitive information, and it's designed to operate without any connectivity to Google Cloud or the public internet. The appliance has achieved impact level five accreditation, which is the highest level of security controls and protection required for unclassified but sensitive information. But it really is designed for, you know, to meet the needs of impact level six or even higher accreditations. And so we are working to get these certifications for the GDC appliance. It comes with built-in AI solutions like translation, speech, and optical character recognition. SAIC is delivering direct mission impact via GDC's AirGap appliance, providing secure orchestration, seamless integration, and access AI to the edge, transforming the needs of nation's most critical missions. Now imagine modernizing incident response with fleet management running on a ruggedized appliance, deployed directly to your front lines like wildfire or hurricane zones, right? So on this hardened device that you can see, a dashboard application could unify real-time data from diverse assets on site using secure APIs locally. You can see a dashboard here in this video where data from different systems like ambulance units and fire units and helicopter units and so on are being monitored, right? This art of possible could give first responders enhanced situational awareness and critical data sharing for mission planning for enabling faster and safer response and decisions right at the edge in challenging environments. Again, you can visit our hardware words where we've got a live demonstration of the solution. Thank you. Thank you.