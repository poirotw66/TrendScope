 Jason Wu Good afternoon, folks. Welcome to breakout session 020, Master Your Storage Infrastructure for AI Hypercomputer. My name is Jason Wu. I lead the product management team for Google Cloud Storage. Joining me today, we also have Sean Derreton, who is the Group Product Manager for Cloud Storage, and Bo Chang, who is the Senior Engineering Leader from Snap. So, this is today's agenda. First, we'll just give some context about the storage for AI and ML. Then we'll talk about some of the high-performance Google Cloud Storage offering and the product we're building and launching. And how you can leverage that for your AI and ML workload. After that, I will invite Bo to the stage to share their experience from Snap, how they're running AI and ML at massive scale on top of cloud storage and GCP. And then Sean is going to cover the parallel file system side of the new launches and the capabilities and how you can leverage that. And then we'll close it with some takeaways. So, Google Cloud has been recognized as one of the leaders in AI infrastructure space. According to a recent Forrester study, Google Cloud has been ranked 5 out of 5 for 17 of those 19 criterias. From roadmaps to vision to execution to innovation, from AI training workload to AI inferencing workload. So, Google Cloud provides the best infrastructure for your AI needs. We took a... At Google, we took a very differentiated approach. We took an approach where we can focus on the end-to-end integrated system, including hardware, software, and including the flexible consumption models, with the reliability and security you are expecting and you would need. So, we took that. We packaged them all together. We provide an end-to-end integrated AI system, which is optimized for your AI workload. So, we call this AI hypercomputer. This is really workload-optimized for these AI-specific type of patterns and needs. Within the AI infrastructure, storage is a critical part of the infrastructure. Oftentimes, storage, I.O. bottlenecks can have a big impact on your GPU utilization rate and all of those things. And also, storage is an integrated component from the data ingestion, data processing, to, you know, model pre-training, to training, to inferencing, and all the way to storing the data, storing the content, and the data protecting, data protection. So, storage has been an integrated part of the AI infrastructure. And for the AI workload, oftentimes, there are two types of storage systems which are commonly used. One is this object storage. Google Cloud Storage is a planet-scale object storage service, which offers virtually unlimited capacity, up to many exabytes of data you can store it, with a very compelling cost structure. It also offers a very impressive aggregated throughput. But normally, it has a latency in the range of tens of milliseconds. And the other common storage infrastructure is being used. There's parallel file systems, which offers capacity from terabytes to petabytes of data. It offers very good, low latency, and it also offers a very high throughput. So, to summarize, a lot of the storage requirements from this AI workload include following. First of all, we need to support the capacity from terabytes all the way up to many exabytes. And a lot of this AI workload, they also have a high demand for high throughput, both for read as well as for write. It requires millions of requests per second to be able to support that type of request. And ideally, with very low latency. And also, keep in mind that a lot of this AI workload, they're very spiky, so we have to be able to handle the big variation of the workload. So, before I dive deep into cloud storage and all of this high-performance cloud storage capabilities we're building, we have launched, and how you can leverage those, I just want to give a quick context in terms of the cloud storage product we have, which is, today, they're being used by many customers for many different workloads. We offer regional bucket, regional object storage, which is similar to a lot of these regional services you see from other providers. But in addition, we also offer dual region and multi-region. This basically provides two copies of data, get it stored in two different regions. So, you get that building redundancy out of this dual region and multi-region. They offer the highest availability in the industry. They also offer the best RPO and RTO with geo-redundancy. In addition, we offer four different storage classes for each of those location types I mentioned so far. There's a standard storage, there's nearline, codeline, and archive. These different storage classes are really optimized for the different access frequencies of your data so that you can get the, you know, the requirement you need, but also at the best possible costless structure for you. So, one of the biggest challenges you have seen customers running into with their AI workload is how can they get access to all of those GPU and the TPU capacity, right? And oftentimes, the GPU capacity can be available in a region which is totally different from where your data is stored. So, how to easily move the data from the region where your data is stored to the region where the accelerator, the TPUs are available, the GPUs are available, that's become a huge challenge for a lot of the customers. And, you know, along with that is how do you co-locate your storage data with your compute, with your accelerator so that you can get the best performance and lowest latency. That's another challenge for a lot of the customers. So, at Google Cloud Storage, we built, we GA the product last month which is called Anywhere Cache. This is a product which allows customers to easily co-locate their data, their storage, with their compute, like GPUs, CPUs, and TPUs in any of the zones where the storage bucket is located, right? So, there's many, many benefits of this, particularly when customers combine Anywhere Cache with our multi-region storage I mentioned earlier on. Essentially, customers can store their data once in a multi-region and get this geo-redundancy. And with Anywhere Cache, they also get access to all the GPUs and the TPUs in the whole continent where the multi-region bucket is located. So, basically, you can go to anywhere in the continent and figure out wherever there's extra capacity for accelerators or spot instances of compute, you can access them. So, that really solves the problem for a lot of these capacity constraints with these accelerators. On top of that, Anywhere Cache offers three additional advantages on top of that. The first thing is it's fast. With this co-location of compute and the storage data together, we can offer up to 2.5 terabytes per second of additional throughput on top of your GCS bucket. And we have seen that they also offer up to 70% of the latency improvement for accessing the data when it's cached. So, it's super fast. The second thing is that it's really simple. It's an option you can turn on at the bucket level and it's completely transparent to your application. It offers a strong consistency. There's no changes needed from your application logic point of view or anything like that. And then you just get that performance acceleration out of the box. The third thing I want to mention is that it's actually very efficient and could save you money. When we looked at some of the customers who are using Mater Region today, up to 70% of those customers would actually save cost by turning anywhere cash on top of their Mater Region bucket. So, it's a really wonderful product. It's been GA'd and I think we also announced it at the keynote this morning. So, go take a look at it, give it a try, and see how it can help you to accelerate your AI workload. And Surpick is a leading AI company which focused on this state-of-art large language model. They have been a big cloud storage customer. They have been using and leveraging anywhere cash on top of their AI workload. So, they have found that anywhere cash can help them improve the resilience of their cloud workload by co-locating their data with their TPUs they're using on GCP. They have been able to get terabytes of additional throughput from the GCS bucket by turning on anywhere cash on top of it. So, that's something they have been using and they have been very pleased with. Assembly AI is another AI company. This company specializes in enterprise grade speech AI. So, they have been leveraging a lot of these high performance cloud storage features and capabilities as well. With that, they have observed 10x improvements in terms of the throughput they're getting from GCS, from cloud storage, and up to 15x of training time improvement. So, all of those things are wonderful and great. I highly encourage you to go take a look and see how that can help you. But, at Google, at Google cloud storage, the team also looked at all of this and said, how can we take it even further? How can we build a storage system which can dramatically change the game? So, we want to build a new storage system which can have the latency like the block storage in the sub-millisecond range. We want the throughput and the interface of a parallel file system because a lot of the AI customers, they like to work with files and directories. We also want the ease of use and scalability of object storage system. Massive scale and all those. And, best of all, we want them all. We want everything that works out of the box. So, I want to introduce Rapid Storage, which is the latest high-performance storage product that we're launching. This is the storage product which offers the scalability and the ease of management just like a cloud storage system you use and you love. cloud storage It offers 11 lines of durability in a zone with three lines of availability. It's fully integrated with cloud storage fuse so that you get that out of the box. You can mount it as a file system just like what you use with cloud storage fuse. It offers up to 20 million QPS, 20 million requests per second out of the box in a zone and with six terabytes of throughput from a zone from a bucket. So, and the best part is that most of the cloud storage features just works out of the box. So, not only that, Rapid Storage is fast. That's one of the big goals we have. With Rapid Storage, we're able to deliver sub millisecond for both random read as well as for append operations. It's 5x faster than similar product out there in the market. You probably see some of those or played with some of those similar product out there. It's roughly about 10 to 20 times faster than the cloud storage, standard storage classes. So, to put this all together, I showed this picture early on. The way to think about Rapid Storage is that it's just another cloud storage classes. We already have a standard storage class, nearline, codeline, archive. Rapid Storage is a new storage class which is optimized for data-intensive workload. It's best suited for the data which are most frequently accessed. It's also a zone of storage product that means that your data is stored in a particular zone which can be co-located with your compute, with your GPUs and accelerators. So, not only from a storage class point of view, Rapid Storage in many other aspects is also just another cloud storage classes. It shares the same authentication, authorization, project management, all those experiences you get and you are used to with the cloud storage. So, all of those things just work with Rapid Storage. It supports cloud storage fields, and we also are going to support all the SDKs later on, which are coming soon. You can easily move data into Rapid Storage or move the data out of Rapid Storage with the Storage Transfer Service so that you can move your most hottest data to Rapid Storage, and once you are done with your processing and your need, you can move it to some of the lower tier storage with lower cost for long-term storage archive purposes. It has the same pay-per-use model so that you only pay what you actually use. There's no need to pre-provisioning any of the resources for you. You can just create a bucket, get to the endpoint, and then start using it. It's a completely managed service for you. And we charge the storage and throughput separately, so depending on whether you use a lot of storage or use a lot of throughput, you get to pay whatever you actually use. So how we build this? This is really a big project across many different teams and a big undertaking, a very ambitious project for the team. This is building on top of the Google Classes storage system, which is the foundational distributed file system where almost all the Google services run on. this system has been widely used and really been battle tested, which offers many of this scalability and performance we're looking for. We also built this new zonal serving stack, which can be co-located with compute, which is also on top of SSD, so that it can offer both low latency, as well as this high throughput needed for supporting this rapid storage. In addition, we build this read and append operation using the gRPC protocol. So this is a stateful protocol which allows us to achieve sub-millisecond read and append operations for the performance we just talked about. And last, we also provide some of the built-in directory support for the rapid storage, so that you can get atomic rename of the bucket, sorry, atomic rename of the directory, as well as atomic rename of the object, if needed. So the rapid storage bucket also comes with a much higher QPS limit to begin with. it's 8x higher than what you would get from a normal GCS standard storage class. So rapid storage today is in private preview. If you are interested, you should contact your account team, or you can express your interest through the Cloud Storage Management Console UI, and let us know. And also, if you are already using it, playing with it, let us know your feedback and how we can improve it. With that, I will transition to Bo. He will talk about his experience, Snap's experience running on top of our Cloud Storage. Thanks. Thanks, Jason. I'm going to do a little interactive first. So quick show of hands, who uses Snapchat? All right, now quickly pull out your phones with Snapchat and take a snap. quickly, tag it, best breakout next 25, and then snap it out. The reason I did that is within a few minutes, about 100 events on each of your phones is going to get sent across the cloud and stored durably in GCS. Within another 30 minutes or so, that data is going to get deduplicated and transformed, hydrated with additional data, and within about 35 to 40 minutes or so, that data lands in BigQuery and then also makes available for all downstream joint pipelines for our ML workloads. And as of Q4, our public earnings stated that we have 453 million DAUs, so transit at a scale, if a few hundred people here or 100 or so people here did that, what kind of scale are we talking about and why GCS is such an important part of enabling that kind of scale and growth? So if you have a little guess in your mind, any given week, we pump anywhere between 20 to 30 trilling events, all that data gets lands in GCS as a first step for durable storage. And as I mentioned, the one reason that we care about durable storage is that our metrics and our ML workloads have super accurate data. We don't want to lose any of it. And some quick stats, at four nines of user sessions, so out of P99.99 sessions, our loss rate is less than 0.08%. And the big reason for that is as soon as it hits the cloud, we flush everything to GCS as our first raw data storage. Related on a given week, when those events come in, we run very, very large pipelines that process X divisive data. A lot of workloads that you guys are more familiar with, from ETL workloads to hydrate transform data, but also at-how queries that kind of sits on top of GCS that we leverage Dataproc and Spark to kind of process those. But basically, every decision at Snap is underpinned by data, and GCS is a big function of that. One other thing I do want to call out is another heavy use of transformed data as it shuffles away through the pipeline is within every four hours, every single day, our AV platform cranks through 30 petabytes of data, all sit on top of GCS, and that computes thousands and thousands of metrics for about 10,000 active studies running at any given moment. So the experience that you just pull out of your phone, one person might have different experience than the other based on the experiments we're running. So where does that kind of all go? I mean, that kind of scale is massive, but what are we actually seeing as the business and technology going? We're really pushing the boundaries sometimes on some of these cloud vendors, but really, no surprise, given the theme, we're seeing 100% AI and ML use case growth. That doesn't necessarily translate to the ingestion side. The ingestion side, we're seeing something around 25% year-over-year growth, which is more in line normally, but 100% derivative kind of data growth. So you think about use cases like MLE wants larger training windows. They want to essentially dump massive tables that join together into a bucket, and then they want to essentially adopt streaming capabilities on topic. Can I take that into essentially a streaming workload, or can I do ergonomics around data selection, right? I want to take that large table, select a few columns down for the model or model family I care about, and essentially perform training on that. But cloud storage and GCS is a big part of enabling this growth, or else it wouldn't be possible. So instead of showing you those numbers exactly, how do we actually start to manage some of the data that we dumped into essentially a cloud storage? There are tables everywhere, but one of the things that we're really invested in is essentially this lake house, this term everyone should have heard of at this point. But effectively, that lake house allows us to essentially govern and aggregate all the data we have available in GCS, and we explore it naturally. So this is an example of the screenshot. If you have a keen eye, you can zoom in really quick, you see the sync type, we have GCS sync, we also have a way to pull data out of essentially GCS, run queries on top of it using a query engine like Trino, or in this case, there's also Spark involved here as well, but essentially allows us to read out data, write it out, which is really cool because with essentially things like anywhere cache, we are just, you know, when we evolved this platform initially, a lot of it was on multi-region buckets, but it's really exciting to see the GCS product team kind of thinking about essentially adopting features like anywhere cache to allow us to essentially save on costs, but also keeping the same performance benefits. Another one I'm really excited about, so tying this all back to the initial first slide, we have all the data coming in, super high TPS, actually at peak in that injection system, we're talking about 75 million TPS, so if you tie back to what Jason was saying about the, you know, for the astute of you in the crowd, with rapid storage, why the 20 million TPS is really important, because we are running at peak 75 million TPS, so like some of those high skill does matter, but if you look at this diagram initially, you can see where like the blend of where we want to go and where GCS wants to go with our product offerings, so this type of architecture is very common with a lot of companies, right? We want a durable storage, but like it's kind of an anti-pattern to write really fast IO to the existing like standard kind of buckets, right? So we typically have an SDK, those SDKs pump events across the pipe to essentially we have this massive fleet that collects these events, buffer in memory, and we quickly flush it to essentially GCS to make sure we don't lose it, but we don't really need to do this if you think about it. With something like rapid storage, we can essentially on the client just embed right directly GCS, achieve the same level of durability, but also the same level of high O throughput that we're hoping. So in this scenario, it simplifies our architecture, but it also allows us to essentially keep the same level of latency guarantees. So, and I'll pass it to Sean. There you go. Great. Thank you, Bo. It's really exciting to see that in practice and see how many more Snapchats we can get throughout this rest of the session for the best breakout at next, right? Hashtag. So, you know, one of the things that Jason talked about is the things that we've been innovating with cloud storage. So over the past couple of years, we've really been figuring out and working with you closely to understand how can we help you accelerate your AI workloads. And a lot of that comes down to certainly accelerators, but it also comes down to storage. And not everybody is born in the cloud. So not everybody's writing their AI workloads directly to object storage. And a lot of people are looking to mount buckets as if they were file systems. And that works great. We've been doing a lot of that for a lot of different people. However, there are many people that are on-premises and they're using NFS or they're using a parallel file system on-premises. And so one of the things that we've been investing in is giving you choice and options to fit your workloads. And so if you're coming from on-premises or if you're looking at a hybrid or a burst workload type, this is one of the things that parallel file systems can do well. You can work, you can have and manage those systems on-premises, but you can put 10% in the cloud when you need to, tear it down, shut it down, whatever, manage those costs and the timing. But also, not every application in AI workload is suited for either cloud storage or PFS. There are different workload profiles. Some work better. Some have fewer clients that need higher performing requirements. Some need lots of random IOs, small files versus 100 megabyte or gigabyte size files. Different characteristics will lead you down a different path. And one of the things that you want to keep in mind is that storage, while it may seem expensive, relative, it's a smaller portion of the cost of your AI workloads. It really is the accelerators and the compute. The closer you can get the data to your accelerators, the more you can saturate them and improve that good put, you're going to be improving your TCO overall. And the chart here that I have on the right is really just illustrative of running training jobs. Multiple epochs, certain amount of time for each one, and the shorter you can make each epoch, the more efficient your training is. The faster you can recover from a checkpoint error, the more you're going to be able to get that training job back up and running, maximize utilization of those accelerators. That's really going to improve the overall cost and optimization. And that's why we're excited to announce Google Cloud Managed Luster today. This is what we've been working on in partnership with DDN. This is based upon Exascaler, so it is Luster-based, but it's not just the open source Luster. This is in close partnership with DDN for a while now, but it is now a high-performing, fully managed parallel file system offering. So DDN, Exascaler, has been in the marketplace for years. If you're a student, you can deploy it yourself, spin up the VMs, deploy the OSS clients, et cetera, you have been able to do that. But most customers are looking for a fully managed service. Spend your time elsewhere that's going to provide more value for your company than provisioning resources, and that's what we've done with Managed Luster. This is Allowlist GA as of last week. We've kept it quiet, but it's Allowlist GA as of last week, as I just mentioned, but it is one of those things that, depending upon your workloads, you now have the option to start small from terabytes or scale up to a petabyte capacity. As you'd expect, that number is going to continue to increase throughout the year. But it also is delivering very high performance, up to a terabyte per second. And one of the advantages of parallel file systems is that you can scale the capacity and performance as you need. Similar to what Jason was speaking about, Cloud Storage Fuse and Rapid Storage, you have two different knobs and options to look at. But one of the advantages of parallel file systems is this consistent sub-millisecond latency. And this is one of the things that, whether it be a handful of clients or thousands, you can expect that type of performance that many of you may be familiar with and experience on premises, but now is in Google Cloud and available in many of the regions. So we have Managed Luster we just talked about. Parallel Store is also available. So we have two different options based upon your requirements. Generally, we're going to recommend you look at a persistent offering like Managed Storage. Sometimes you need a scratch offering, and so there are some differences between looking at a scratch PFS solution, like Parallel Store, or a persistent one like Managed Luster. And knowing those trade-offs of if it's a small training workload or if it's petabyte scale, you have different options to look at having a PFS satisfy those AI workloads. The other thing is that as we've done a lot of work with Cloud Storage Fuse to allow you to mount an object bucket as if it was a file system, there are cases where you need full POSIX, or you need full, in this case, full POSIX. And so there are some differences there. So again, coming back to workload profiles, and maybe it is file system related, you have that capability with Managed Luster. Whether you're using GCE and SLURM to provision compute resources and then mount the parallel file, mount Managed Luster, we also have the CSI drivers for GKE, so you can have the persistent volume claims there. But if you go back and think about that data pipeline that Jason talked about at the beginning, cloud storage is still a crucial part of your workload. From the first data preparation all the way to the archiving and extracting easily what data you need to train on is one of the things we do with our bulk API. So you can easily export and import exactly what data you need to from your cloud storage bucket. Now one of the things that we have with Managed Storage is the ability to provide that consistent latency that we talked about, the low latency, but also consistent performance to VMs. And whether you're looking at a handful of VMs and you need very high throughput per VM, we can saturate that 20 gigabytes per second link to like an A3 VM. As you go to A4, obviously you're going to be able to increase that performance. So that's one of the things that whether you're looking at handful or thousands of VMs for your jobs, parallel file systems can actually help to scale that performance as well as the capacity. The other capability that the parallel file systems have is the ability to do full duplex. So as you're doing your training jobs, you can also be writing out your checkpoints or you can be recovering from a checkpoint while you're doing training to a different job. So this is one of the things that from serving, you can actually serve from the same location that you're actually doing your training. And if you look at the performance of checkpoint writes and checkpoint reads as well as serving performance, this is oftentimes going to be higher performing than for your workload. So for cloud storage, it's relatively higher performing, but this is not the right solution for everybody. So kind of coming back to some of the things Jason talked about is a lot of times you can have and use cloud storage, but at the same time you may have requirements for a parallel file system. And so we have this ability to provide both options. The documentation is up there on the top right. This is one of the things that is, as mentioned, this is now allowless GA, so you can begin to check it out. There are a couple things to think about as you think about parallel file systems and checkpointing. I mentioned getting the data in and out of GCS. One of the advantages of managed Luster being a persistent offering is you can keep your checkpoints in that cluster longer than if it's a scratch offering. You can still export the checkpoints back out to cloud storage if you need to. And similarly for serving. If you're using it for training, you can export whatever serving models you need and then deliver those in the zones where you're currently looking at this. But keeping in mind, the parallel file systems like managed Luster are a zonal solution, similar to rapid storage. It's all about delivering higher performing storage solutions to keep those accelerators saturated and save as much money as you can from that. And Technology Innovation Institute has been using parallel store to do exactly that. But one of the things that I wanted to point out here is that I mentioned that not all AI workloads are the same. They actually use parallel store for 75% of their training needs, but they also use cloud storage fused for the other 25%. So depending upon needs and profiles, they pick and choose what is the best solution for what they need. And by using parallel store, they've been able to dramatically decrease their checkpoints from what they were able to do before, in addition to delivering 25% better performance and lowering their costs by 20% compared to the previous solution that they were using. So again, everybody has different workloads, different scale. Not everybody is looking at thousands of GPUs as is TII, but the scale and the capabilities are the same in terms of the technology to compare to. So we've talked about a number of things today in terms of what we've been doing over the last couple of years to innovate storage to really improve AI workloads. This is a slide illustrating a couple of those we talked about. Rapid storage, which is in private preview. Anywhere cache is generally available. And then manage luster is a new parallel file system offering that's also allow list GA. Generally available, that is. So one of the things I think about is this is not all the things that we have. In the time that we had, we didn't include things like hierarchical namespaces or other things like that on the cloud storage side. So keep in mind that this is one of the things that we will continue to improve upon. We have a lot of focus to make storage the not the bottleneck in the AI training pipeline. There's a bottleneck somewhere. It's always going to be somewhere. We try not to be it as much as we can. And that's one of the reasons why we've introduced Anywhere cache and rapid storage as well as managed luster. So in conclusion, there are a couple things. Number one, if you're an existing cloud storage customer, you can snap the first QR code, which will take you to the console. This is where we're going to introduce a lot of the things that we did not have time to talk about today in addition to some of the things we did. If you're a new customer and not using cloud storage today, the bottom QR code will take you to the web page talking about the AI storage offerings that we have, as well as if you're still here for the next few days, these are the sessions that are storage focused. I would encourage everybody to take a look at the second one, the breakout 025, the what's new in storage. That's actually right after this session in Ballroom F, and that's going to be an overview of our storage portfolio across more things than what Jason and I touched on, but it's a good overview to think about backup, to think about our file system, to think about other GCS, other cloud storage things that we didn't focus on today as they're not necessarily AI related, but perhaps analytics and lake house related. So I'll let everybody take a quick picture of this, and with that, we'd love your feedback afterwards, and with that, I will, I know Jason and Beau and I will be around for questions. I think it might be the easiest way. If there are questions, feel free to come up and be happy to have a conversation and answer those. Thank you.