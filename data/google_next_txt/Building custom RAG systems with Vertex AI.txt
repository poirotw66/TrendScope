 Hey everyone, I'm Greg Brosman. I'm a product manager in Google Cloud. Super excited to have you join us today. This is building custom rag systems with Vertex AI. I hope you've had a great conference so far. For those of you who went out last night, I hope you enjoyed the show. I was a big fan of the killers for a long time, so it was fun to see them in action. My corny rag and grounding joke is personally, I felt very grounded in the year 2003. It really took me back. I see what I did there. Thank you. Great. So we've got an all-star lineup today. Joining me from Google are Lavi and Lorenzo. Lavi is a developer advocate, and Lorenzo is a field solutions architect, and they're going to give some exciting live demos of how to package this all together. And then we've actually got an all-star lineup of guests joining us as well. Isabel is a director of enterprise AI products at AstraZeneca. Hari is a technology fellow at the Home Depot. And Michele is head of data science at the RCS Media Group. So first, let's look at how we're actually enhancing our search APIs to power custom rag. AI agents are here. I'm sure you've heard that multiple times over the past couple days. In the industry in general, the excitement is palpable. I think the reality of driving impact still with these agents is that they're only as good as the data you end up connecting them to. And rag is effectively how we actually do this. It's the proven approach to augment AI with external knowledge and ultimately depends on robust and precise retrieval methods. And this is where Vertex can help. When we design rag solutions, the recipe I think is well-known. Indexing, retrieval, and generation. And what this means is making enterprise data searchable, quickly and precisely retrieving relevant context for the query, and ultimately packaging this together and passing it the search results to the model for generation. We've got a spectrum of rag solutions that we offer to meet developers where they are in terms of their use case and requirements. Vertex AI Search is our out-of-the-box solution. It's a managed retriever that you can use to combine with Gemini via grounding or use as a tool in the ADK. And then for maximum control and flexibility, we offer our search platform APIs. And we'll talk about them in more detail. It's a suite of components that you can mix and match. And we think it's got the most complete set of components to build your own rag solution and build your own agent tools. And we think they're best in class as well. We'll talk about the layout parser, vertex AI embeddings, vector search, the ranking API, and the generation part of this as well. When you're piecing all of this together, I wanted to share how Google actually thinks about doing this, which is iteratively searching through an enterprise corpus and taking all that data and ultimately selecting a few precise results. Here at Google, it's actually a standard process to break this into a two-stage retrieval process. In the first stage, we're using a combination of dense and sparse embeddings and vector search for hybrid retrieval, a few hundreds, let's say, of candidate results. In the second stage, we're using ranking to score candidates and narrow them down to only the most relevant results to pass to the model. We've got exciting new features and launches to share for each of these today, so let's actually take a look at them. First up, our DocAI layout parser. This is our one-stop shop to prepare any document for your rag workflow. The reality is getting your documents into a format that's searchable is a foundational piece of this rag pipeline. And you can think of these raw documents like PDFs that include a lot of unstructured data. And layout parser helps transform these raw documents into structured, high-quality data. With multimodal processing now, you can move beyond text and get visual layout information like where elements are on the page and their relationships with the actual textual information. With our table-to-markdown feature now, you can extract the data from each cell in a table and actually understand its position in the table, so locating, for example, row and column, ultimately driving much higher accuracy. And finally, with our bring-your-own-schema offering, you can define a set of custom-specific fields and match the precise information and schema you need. So now that we have our data available, how do we actually prepare to hand it over to the model? And Vertex AI embedding's models are our power tool to do this, converting data into numerical vectors. The magic of these embedding models is that they actually capture the semantic meaning of the inputs, meaning that things that are closer together in meeting will be closer together in the vector space eventually than things that are further apart. We're really focused on state-of-the-art performance at this point, and you can see we've achieved leadership status in some of the key public benchmarks. With multimodal embeddings, we're also moving beyond text to images and video, and we're continuing to focus on driving additional efficiencies. As you can see with Matryoshka representation learning, this ensures that the most critical semantic information is captured in the first dimensions of the embedding vector, all the while still retaining very high levels of performance for retrieval tasks, even though we truncate the size of those down. Next up is vector search. And so we've got these embeddings now. How do we actually make them searchable and usable in our RAG solution? Vector databases store embeddings and enable extremely fast similarity searches. It's a very efficient way of comparing against millions or even billions of vectors when a query comes in and is ultimately converted to an embedding as well. There's a suite of enhancements we're excited to announce, starting with metadata storage. So now you can store additional metadata, like document sources, dates, or category tags, directly alongside the vectors. This allows for easy filtering and narrowing down the search space and ultimately removes the need for a separate lookup in a separate database, hopefully making your lives easier. Second up, for the users of BigQuery out there, we're excited to announce our BigQuery connector coming up, which makes it easier for customers that are storing data in BigQuery to leverage the power of vertex vector search still. So this means that customers can directly ingest their data from BigQuery and still get the power of vertex vector search. And finally, with our storage optimized here, we're making vector search more cost-effective for customers looking to reduce storage costs. And this will be particularly attractive and helpful for customers with lower traffic deployments as well. And finally, the last part of our pipeline here, looking at the ranking API. So we've got these initial set of candidates from vector search. How do we refine these further to yield, ultimately, the small set that will hopefully provide to the model to deliver the highest quality information? Here, we can leverage, actually, a purpose-built model to reevaluate and rescore each of the candidates that we generated. And we're looking at relevance and quality in relation to the original user query that came in. Here, we'll end up with a final, smaller set of context. And we're excited to announce the launch of our newest model, which is version 4, delivering leading performance across a number of the public benchmarks that you see up there. And for use cases where our latency is critical, we're driving up to 3x lower latency for deep ranking tasks. The final thing to highlight here is the ability to fine-tune the ranking API to your data and business context and support for large documents up to 32K tokens. Lastly, we can help you when you want to augment the model with context from your own retriever. And so here, we're really introducing this concept of an active retriever, which lets Gemini first analyze the query and itself intelligently decide how to query your retrieval methods, resulting in more accurate, helpful RAG. For example, with complex questions coming in, it'll first break down those complex queries into smaller, more manageable retrieval steps. Secondly, you can plug in various data sources that you already use, whether that's Vertex AI Search, you're going Custom Retriever via APIs, Elastic Search, or soon enough, MongoDB Atlas. Lastly, if you have multiple sources, one of the incredible things that Gemini does is grounding will actually blend them together automatically. This includes the power to do this with grounding with Google Search and now with grounding with Google Maps. And as the diagram shows, it's easy enough to implement. You just send the prompt to the grounding API in Gemini, and you'll then get the power of this agentic RAG orchestration, which eventually generates grounded answers for you. Great. So with that, let me hand it over. We've got a quick demo here showing how all of this comes together in a magic package. Thanks, Rick. All right. Let's do the demo. All the demos that I'm going to show, you can take a QR. This has a doc, and you have all the links that I'm going to show. Just take a photo real quick. Okay. Let's get started. So I'm just going to do a couple of quick demos. The first one, which is what Greg was saying, the grounding one, I'm going to show you a couple of very cool things that you can do. I'm going to ignore the setup and all, because when you get the notebook, you'll be able to figure that out. But let's start with how do we do, how do we sort of ground the data, right? So we'll start with Gemini 2.0 Flash, and the problem that we have is you're an expert in astronomy when it's the next solar eclipse in the U.S. Now, when you do this directly with the current API, and this is just the basic implementation where it sort of gives you a lot of different details, you may want a very updated information, right? Because you may not want, there are some facts, there are some recent events that has happened, and you may want certain things where it gets grounded with the Google search. So you see this example where we run this without grounding, and then the very next thing, and if you see from an API perspective, it's just one single thing that we add. So we define the Google search tool here, and then we pass it into the config, and then when you actually run this now, there are two things that you'll see. First, you'll see the next date, which is much better. So you can see the next date, which has come up for the solar eclipse. The other thing which is very important is it actually gives you the citation, which is more important because you may want to check whether that is still correct or not because internet has so much of junk already, right? So you may want to make sure that whatever you're trying to get is correct or not. So that is one very basic example, and you can, again, play with this, see how you can sort of break this, or maybe do something more cool with this. The next example is let's sort of try to build this with more multimodal sort of feature, right? So let's imagine you have a photo, and what you want to do is you want to ask that what is the current temperature at this location? So there are two things that we are doing here, right? One is we are seeing the capability of Gemini where it sort of understands what is there in the image, and then it uses Google search to figure out the current temperature of that. So there are two tasks that we are trying to do. Again, we're going to do the same thing. We're going to define the prompt. We're going to pass this image into the part.uri. You can just pass images, videos, and YouTube URL. And then the same thing that we did before, which is you essentially pass the Google search tool, and we get the same thing. So first thing, it figured out that this is Paris, France, and second thing, it actually gives you the temperature. The good thing that I like is that actually it gives you both in Fahrenheit and Celsius for people who do not like Fahrenheit. The other thing is it also sort of gives you the grounding chunk. So this is important because, like I said, the citations are very important. We don't stop there. One of the very critical things that a lot of customers often ask is that this is all cool, but we don't want our data to sort of go in this whole API. This is one of the very crucial things which a lot of financial customers ask us that, is there a way I can use this Google search where it doesn't take our data, whatever data that we are sending, whether it's image, video, or simple prompt? So we have that, which is what we call as grounding with enterprise web search, where we make sure that your data never goes back or it is not persisted beyond the call that it has to make. So this is the way to do that. Currently, we're sort of trying to embed this into the actual API, but this is a curl command, which you can see where I'm doing the same thing, which is who won the 2024 championship. It goes, none of this gets dragged, none of this gets persisted in the servers, and you get the answer along with the grounding chunks. This is also a good example to show that you can actually use this to integrate this with your internal systems because you may not want to use this directly with, let's say, Python API, and you have your tech stack in very different languages. So this is one great example of where you can actually take this, put this in behind Go, Java, whichever language your stack is. Now, let me show you one other example, which is how do we do Vertex AS search integration? So search is cool. Online internet is cool. Everything is cool. But what if I want to actually ground the data with the data that I have inside the enterprise? So before I go here, I'm going to quickly show you that inside the agent builder, what we have is you can actually create a bunch of different apps. You can create different data stores. What we typically call data stores are essentially stores where you can push your data. So you can put your data in GCS, and then that gets indexed internally and gets stored in the Vertex vector search. So you can see some of the sample data that I have uploaded, and the one that we're going to use is this, which is a very famous paper through which all of this got started. And I'm just indexing this. This takes a bit of time. Once you have that, I can just copy the store ID, come back here, and then I can just mention this. Now, this is important because now we'll do the grounding, or let's say asking and doing the rag, but this time we'll make sure that it only answers from the documents that we have. And that document sits in GCS. It's not internet data. It's not available outside. So the prompt is, what is scale.product? Attention. So again, I show you two things. One is I show you how you do this directly without any grounding, and you can see that Gemini actually gave you a huge amount of information around it because it's a common topic. You will find this answer right away. But the moment I actually passed this as a Vertex AI search tool, so again, from a code perspective, you can see just like we were passing the Google search, we are now passing the Vertex AI search tool. We have already configured the data. This time, it actually gives you answer from the paper. So this looks much simpler. It is because it's just trying to give you an answer from the document that you have. And the best part is, if you can see, I'm not sure, is that it actually tells you from which chunk it is picking that data. So it tells you from where it is picking from the document that we had in the back. And this is really impressive because you can actually put all the data. This means that you don't have to worry about chunking. You don't have to worry about embeddings. You don't have to worry about anything. You just put your data in the back. The data stores take care of it, and then you just configure it in the front end, and you're good to go. So that's the first demo. The next one, which I want to show, is the RAG engine, which Greg was showing you. Now, this is where, again, customers sometimes say that, hey, this is all cool, but we want certain kinds of customizations into it, right? We want to sort of control the chunking. We want to have a different retriever. We may want to have different models. We don't want Gemini. We want something local, llama or something. So I'm going to show you quick things that you can do with RAG engine. So the first thing is you can actually create your own corpus. The corpus can include GCS. Corpus can include outside data. Whatever data, wherever it is sitting, you can just create the corpus out of it. You can define your embedding model. So currently, I'm doing text embedding 005, but this can be replaced with the new Gemini embeddings that we have. Once you create that, this is a corpus that goes and sits in the vector store, vector search, and you can also change it to any vector DB that you would want. Inside the GCP, outside the GCP, whichever you feel comfortable with. Then I'm showing you an example where I'm actually offloading a data. So you create a service of corpus, and then you offload. This is an example of just the test MD that I'm doing. Not only that, but you can actually directly pass data from GCS or from Google Drive as well, if, let's say, you're trying to be, trying to do an adventure for some reason. Then there are two things. So once the data corpus is there, you can then define the retrieval services. This is, again, where you have more control. So one of the key things that you'll start noticing is that now I have some parameters, like top K and things like that. This is where I said that you now have control. So you can tune the model that RAG is trying to use, and it's not just sort of right out of the box for you. So I'm asking a question. It's sort of going, and it is telling me that, hey, the display is test.md. This is the text where it is being explained, and good for it. Now, we go a certain step above where we start adding a few more things. So, for example, you can see the vector distance threshold. Again, for those of you who have done RAG, you know what this is. For those of you who have not, you should know that there are controls that you can tune to make the output much, much better. The other thing is I'm going to also add the model. So this is where I said that you now have control of models. So you can actually add models as per your need, and you can just define this. This one example, which I couldn't run because you would have to deploy the LAMA model, is because you have integration with Model Gardens. That means any model that is available there, DeepSeat, LAMA, Cloud, all these models, you can actually use those models as well to run your RAG ecosystem. So RAG Engine really, really works very well for you because it's an abstraction that helps you customize all of these cool things. One quick thing that I couldn't sort of integrate in the demo is the ranking, which Greg showed towards the end. This is the ranking API that we have. We actually have two models. The latest one is 003. I've included the link in the doc, so you'll be able to see. And what it does, and if you see when you actually get the vector DB to work, it, again, sort of step beyond that is it tries to rank those filters that it figures out in the first step. So you can actually do this much, much easier in that way. One last demo, and then I'll hand it over to Isabel, is the document AI. So oftentimes what we do is there are documents in enterprise which has very complex structures. It has charts. It has diagrams. It has, you know, very weird things that you would want. So this demo sort of shows you how you can actually have a document, and I'm going to show you a quick one. So this is one of the documents that I'm trying to read. And you can see complex things like figures and things like tables and all of that. So with document AI, and I'm running this example using the BigQuery, so I put the data in BigQuery. This is where you will see that I'm actually adding the data into the BigQuery at the back. And it uses the BigQuery vector search integration, which Greg showed you. And once I have this, I can actually run document AI, parser over this. And what it does is, after many steps, is that you get the data extracted, and you can then store it as a BigQuery table. So this is what you see here. You have a bunch of different text for each page, and that's gold, because this is the data we would want to use. And this means that you can actually go beyond the free parsers of PDFs that we have, or Lama parsers, or any parsers that we have outside, because it's really, really optimized for that. Once you do that, you can simply now start asking questions. So, for example, I'm asking here, did the typical family net worth increase? If so, by how much? So you can see that how complex we are framing our queries, right? So it's not just, what is this? What is that? But we are essentially asking a comparative question, like, how much? So it does the filter. So this is the first step. This is where you add the ranking API to get this much more tuned. And then you can either get all the text, which is similar, or you can directly get the answer. So this is where you see that I actually made a query through BQML, and it actually generated the answer that between 2019 and 2022, the real net worth surged with this. So, again, these are the cool things that you can do, some of the things which are available right now. And, again, I've given all the links. If you have not taken a photo, please do. With that, I'll hand it over to Isabel. Thank you. Thank you. Hi, everyone. Good morning. I'm Isabel, and I'm presenting today one of our products in AstraZeneca. So I'm the director for Enterprise AI products. And one of the challenges that we have is that AstraZeneca, for everyone who doesn't know, is a global science-led biopharma organization focusing on developing medicines for millions of people worldwide. We have a broad range of different therapeutic areas and research projects going on. So as you can imagine, our company is producing a wealth of information on a daily basis, and we have a lot of situations where researchers sit in long panel discussions which are covering very bespoke, specific research topics, and they need to actually effectively query this information in a way that they can review it later on. So basically, now imagine you're being a researcher looking forward to answer a specific question if you have these video recordings that can be between one and five hours long of experts discussing various amounts of topics on a daily basis. how do you find the information in these very long videos? This is obviously very hard, and until recently, this has been a problem that we couldn't solve because standard parsers, translation methods, and so on didn't really do the job accurately enough. We need to, in the end, actually have a process that allows us to query the data on a very nuanced, detailed level for which it is relevant. So we don't just want to know the text or the context, we also want to know the tone, the nuances, and the implications for the company. So how do we actually understand what we believe is true right now, and how does this discussion actually impact how we see the world? For this, we built a multimodal rack application that actually allows us to combine not just the video recordings, but also the information from lots of different contexts and modalities. So we end up having PowerPoint presentations, research papers, and so on, in text format. We then have the videos, and in a nutshell, the data preparation process is really the one that actually was challenging for us until recently. So we are basically using Gemini now for all of it, so that's the very nice part. We take the videos and the information, we process everything simultaneously, in one model, so we align the chunks of the videos, we align the slides and the context to it. We have some internal tools, such as our Kazoo tool, which is a biomedical-named entity recognition tool that allows us to actually enrich the content a little bit more. And then once we have this, we can align this in Gemini and get a transcript, which is enriched and has all of the contextual information integrated into this. So this allows us to get a transcript, not just with the information, but also with timestamps, with speaker notes, with the sentiments around it, a much more richer context that also references the slides back and forth. So we actually have a high accuracy of the resolution. We also have a verification service that then actually checks that the translation that we did and the transcription that we did is really accurate. Now that we have this data, we actually just put it in a vector search database. And because the videos are very long and varying in length and topics and can span multiple areas, we also do multiple hierarchical summarization tasks. So we have the chunks, which are dependent on the context. So whether an expert in the area makes a comment about a drug or whether someone just heard about it is a very different context. So we have the chunks. We then have section-wise summaries of actually within a consecutive semantic meaning of a section, what did people discuss and we have a high-level analysis of the whole video or like the whole recording itself. We then have a cross-sectional session analysis where these sessions evolve over time and information get constantly outdated. Nothing in science is 100% certain. It's an ongoing research field. So information are changing constantly, which is why we actually need to then see how this video or how this recording has changed the perception that we have in the company. We then integrate this again with Gemini with our existing databases and knowledge graphs, which actually hold biomedical information, competitive intelligence information, and so on, so that we actually know what is different. Why do we do this? As a researcher, you don't want to hear the fifth time what this specific drug asset is doing. You actually just want to know what is new for you and how is this relevant for the company. So that's what we're doing. Until this, at this point, most of the components can be pretty much used out of the box. The reason why we need a bespoke solution is because the retrieval process is quite specific. So we actually need to make a re-ranker that not just integrates our business logic of what we want to have, specific biomedical entities, drug asset concept indications, but also understand the speaker authority, the recency, the timestamps, and so on, and how this fits into a bigger context. So making a business-based re-ranker actually really was the thing that unlocked the whole application and really brought the solution to the next level. So in terms of key learnings, we could build a prototype very, very quickly within two sprints, basically, which was very great for us. But what we really learned from Google was that we don't really need to spend so much time on the data processing. Gemini basically allowed us to use the data as it is without any big problems. We then focused really on the user experience, which is driving the actual value. Having just a simple summarization of a video doesn't really make any difference for us. What makes a difference is really giving the right level of information to the users. So domain-specific optimization really matters. Behind me, you can see, or you could see a small demo of how this actually looks in reality. It's basically a simple tool that just allows us to query the information and organize your data. This real-world application shows why scientific relevance is really what matters, and this is how we could build a solution very quickly that enabled us to unlock our potential in videos. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Good job. Good job. Yeah. Good morning, all of you. I'm sure most of you are familiar with Home Depot. This gives you a few quick metrics that really shows that one of the things we really need to do is solve problems at scale with $159-plus billion in revenue, over 2,000 stores in the U.S. and 2,300 stores across the board. We are the world's largest home improvement retailer, and we definitely have to solve problems at scale. In terms of experience, and this is one where I would just encourage all of you, I mean, you can all go try this. So one of the aspects for which customers always come into the Home Depot is for the expertise that our associates carry, and we want to translate that same expertise to the digital front end as well. So when you go into homedepot.com, obviously we carry a lot of products, and these products have a lot of details, and we want to make it easy for our customers to ask the right questions and get the right responses so they're guided in their journey to complete their home improvement projects that they're in the process of doing. So as you go to homedepot.com, and you can all try this, there are multiple features that we have as part of an umbrella that we refer to as the magic apron. One of the aspects is you can pretty much go into any of the products, and all the customer reviews are there, so there are complex pipelines that effectively summarize the customer reviews, so there's a quick synopsis of what the reviews have, so it's easy for you to make decisions on your purchasing. Another aspect is when you actually go into the product page on any of these products, and let's say you're looking at a refrigerator, and it can be, there's a lot of information and it can get overwhelming. So all that information is then available through this magic apron front end where you can go and ask questions to really understand whether that's product specifications, product availability related questions, what's available in what locations. So all this information is available in the web page, but it can get a little overwhelming, you know, in terms of actually searching and finding the relevant information. So what this allows you to do is a very quick natural language interface that allows you to just engage with the product and really understand all the details about it. In addition to that, there's a lot of information about these products that comes through in terms of, you know, installation manuals, product specification information, and so forth. And all these documents carry a vast amount of information that is a little difficult to expose or interact with. I mean, it's not every day that, you know, let's say to practically purchase a refrigerator, you may be interested in, you know, how high does the water hose actually come and connect behind this refrigerator. Now, that's not a question that you can easily find, you know, in any normal product information page. But magic apron now actually allows you, and hopefully all of you can try this as customers. We'd love to hear your feedback. And it actually allows you to go and engage and ask these questions that are hidden within a lot of the assets, whether that's, you know, PDF guides or other video guides, et cetera, that are being made. And now you can actually engage with it in natural language and get the responses that you're looking for, which hopefully makes your purchasing decisions a lot easier. And, you know, that's really what we're striving for. In terms of the actual process flow to implement this, what you'll see here is a lot of concepts that I think many of the speakers here were actually walking through. It's a multi-stage processing pipeline. It starts with a lot of curation on multiple different data sources that are available. One of the things we've always seen is it's really important that you curate the data sources because there can be a lot of it, and then making sure that all of it is accurate and really what you want is really important. But once you go through that, again, there's a multi-stage pipeline that's highly modular that does the appropriate document extraction. I think you heard about some of the techniques earlier. So once you go through the document extraction, it goes through the appropriate chunking that needs to be done. You can override and use different retrieval strategies. And the whole objective here is to maximize the quality with which you are indexing the information and then being able to retrieve all the relevant information so that your problems really get solved. And in the process of doing that, I think as was referenced earlier, the documents contain a lot of rich information, and not just documents, but I'll say the media contains a lot of rich information. So that could be images, that could be tables. So making sure that we're able to make sense of each part of this document and then running it through the Google's embedding model to actually generate the vector representations. And in our case, we do use both the dense as well as the sparse representations. And then we take the appropriate metadata-related information and store it in other Google databases with the vectors themselves in the vector search. That then gives us a foundation to address these complex questions that come through from the customers. As the question comes through, there's, again, a vast array of tools in the back end that these agents are using in order to actually respond to the customer. That could be a bunch of APIs that we already have, other machine learning models that we have. But in addition, it also leverages the information coming through from this corpus, a fairly large corpus of data that we've ingested. And as you actually go through and we're processing and extracting to answer the question, it's important that we also look at it through the lens of what the appropriate information is that needs to be surfaced, which requires that we run it through the appropriate re-rankers, that surface is the most relevant information so that customers get the best information that we have. And all of this is certainly work in progress. And the magic apron suite of solutions that you're seeing here in this product information page, we're really looking at embedding that sort of contextual intelligence in many other parts of the website, as well as in other aspects. And we're really excited about that. So we thank Google for all the work and partnership. Thank you. Good morning, everyone. Let's start with who we are. RCS Media Group is one of the world's leading multimedia publishing companies. We operate across all publishing sectors, newspapers, magazines, smart TVs, books, delivering trusted contents every day. We reach over 200 million users per month, 1.2 million subscribers to our digital contents and newspapers like Correa della Sera, La Gazzetta dello Sport for Italy, Il Mundo Expansion for Spain. But we are more than just a publisher. We also organize sports events, iconic sports events, such as Giro d'Italia, well-famous cyclist race in Italy and all over the world. We are also a market leader in advertising and distribution, connecting brands with audiences across platforms and borders. And we are not standing still. We are investing heavily in the future with a new office, RCS Innovation, a hub designed to boost AI dream and transformation across our entire ecosystem. This is because in RCS Innovation is not just a department, it's our direction. Our strategic vision is clear. Three main pillars. We want to improve readers' performance experience, boosting engagement with seamless multimodal access to contents, text, audios, videos, images, and more. We want to add smarter discovery with AI-driven intelligent search, so every user finds exactly what they need when they need it. Second, we focus on boosting editorial and advertising processes. We apply AI to streamline workflows, reduce complexity, accelerate decision-making. From content curation and indexing to campaign optimization, everything is faster, more efficient, more impactful. Third, we want to enable new forms of content monetization, both on the B2B and the B2C business levels, reaching new audiences, uncovering fresh revenue opportunities. That's our vision. Smarter experiences, smarter processes, smarter business. Let me show now the foundation search layer we have built that powers everything we are building in RCS. It's the backbone of our AI vision. Everything coming from agents world will be built on top of it. At the top, you can see the foundational search layer structured into the search preparation phase on the left and runtime phase on the right. At the bottom, we present the agent suite we are building, starting with the website search agent we will explore next. Focusing on the left-hand side of the search layer, we ingested PDFs archives spanning 150 years of printed newspapers plus digital articles with related images and videos. In the data preprocessing step, we enrich images and videos with Gemini, with the textual description of those contents, and we normalize and standardize all texts. In the contextual understanding phase, we implemented unthrobbing contextual chunking strategy. Each chunk is enriched with article-level context. At the end, we compute both dense and sparse embeddings, storing metadata and indexing everything within our vector DB, vertex AI vector search. On the right-hand side, at runtime, the user query is rewritten in Gemini to identify, to infer potential filters, filters, dates, journalist names, content typology the user wants to read about. And filters are applied before executing search. After search starts, results are then re-ranked by their relevancy and recency, and the content is ready to be consumed by the agent suite at the bottom. With this overview in mind, Lorenzo will now walk you through the details. Thank you, Michele. So let's dive deep into how we actually build it. So just a step back. Again, the data that we had was multimedia, so we had the text data in terms of online articles, but also newspapers, kind of articles, and then also we had the attachment like images and videos. So we had to process all of this to create this great foundation layer. So first of all, we had to prepare the data. So we started from the media data, and we converted that media data into text using Gemini. So for images, we asked Gemini to describe essentially places, people, and actions given the context of the article itself. We did a similar thing for videos, but for videos, we actually asked Gemini to generate, first of all, a short description of the video that you can see on the right-hand side, and then we also asked Gemini to describe a scene-by-scene description of what was happening in that video so that we could leverage that to really search for actions or things happening at a given moment in the videos. Once we had all this media data in the text format, we had then unified this data with the other text data, tried to identify commonality, like identifying similar columns, preparing and transforming fields, especially for metadata filtering that we can then apply in the search engine. Okay, so how we set up ourselves for success? As a good data scientist, we essentially started with a data-driven approach. So first of all, we started from evaluation. So we did that by focusing on creating a synthetic data set, starting from a sample of the articles. So we took these samples of articles, we used Gemini to synthetically create for each article a set of up to five queries that a user would insert when searching for that articles. Once we had this essentially query article pair, we could then set up our eval pipeline. So essentially, what we wanted to do was to, every time we would search for a query, we would want our relevant article to pop up in the top K elements that are retrieved from search. And particularly, we were interested in two metrics, iterate at five, iterate at K, that would essentially measure on average how many times their relevant article was present in the top K, and mean reciprocal rank at K that goes a step beyond that and essentially measure the average rank of that article in the top K results. Once we had this data-driven eval pipeline, then we could test several variations of our systems, including changing chunking strategy, parsing methodologies, embeddings model, hybrid versus semantic search alone, and then come up with incremental improvement up to getting the maximum return that we are going to see in a moment. So here, we started from naive chunking, so essentially what you're familiar on, you just take your data, you chunk it into pieces, and we only use semantic search. Then you can see how we improve the results by introducing contextual chunking, doing different variation of embedding strategies, and using also hybrid search. Here we used 70% importance on the semantic side and 30% importance on the sparse side, and this was the winning combination at the end, so we managed to increase heat rate of 5 by 4% and middle receiver rank of 5 by 6%. Why we focus on these metrics of 5 is because generally when then you take this data into an agent, you want the result, the top K 3, 5 results are the best possible because that's what goes into the context of the LLM or the agent for the final response. So naive chunking is essentially you take your data, you chunk it into fixed chunk. In this case, we tried the different sizes. We found 1,500 to work very well. And then we moved into contextual chunking that goes a step beyond. So take the same chunk in naive chunking and then use the LLM to describe the chunk in relationship to the overall article. The idea here is that we want to prevent context loss because you take your chunk otherwise and you detach it from the rest of the article and that chunk can lose a lot of context. So with the help of LLM, we can say, okay, this chunk inside this article played its role and this is what happened before, this is what happened next. And so by adding this overview and then embedding this context along with the chunk itself, we would improve a lot retrieval performance and decrease the chance of context loss. Then the next step we did for improvement was moving from semantic to hybrid search thanks to vertex AI vector search that allowed to combine the potential of both semantic search and traditional keyword search methodologies. So we use a BM25 embeddings here and we weighted essentially the two components together. Then we apply also re-ranking that essentially being a newspaper business, we wanted, so freshness was very important so we had to weight both this relevancy with the recency and we did that with the formula at the bottom. You see it's a simple weighted linear formula and the recency score essentially goes to one when the article is the most recent and then slowly decrease to zero as the article gets older. And then I'll add it to Michele. This is our first agent, the website search agent living on this search foundational layer. It provides a conversational answer based on the top key most relevant results from the search and the original user query complete with all citations, three follow-up questions to boost user engagement, reader's engagement, and all retrieved results both in terms of web articles, PDFs, images, videos, relevant to the user intent. We can now answer real-world queries by retrieving information, infographics, pinpointing time-specific content by a given journalist, and identifying the exact minute something happens in a video useful to respond to the user query. This agent has been developed in close collaboration with GoReply, Google Cloud Partner of the Year, whose expertise has been key in bringing this solution to life. Feel free to reach out to learn more about our work. Thank you for your attention. Thank you.