 I'm excited to talk to you today about how Google Cloud is transforming software development. We're innovating in three key areas. First, we're unlocking your ability to build the next generation of agentic applications. We're very excited to introduce our agent development kit to help you build your agents, our agent engine to help you run your agents, and agent space to help users interact with your agents. These allow you to create applications where agents and users can work together to achieve a common goal. We are helping you be the most productive software engineers with our Code Assist and Cloud Assist agents. These agents allow you to accelerate development and streamline your cloud operations across your entire software development lifecycle. All of this is powered by our Gemini models, allowing you to leverage Gemini's massive context window and multimodal support. In today's keynote, we're very excited to demonstrate how you can benefit from all of these innovations. We are going to show developers how they can build things, including agents, with Google Cloud and Gemini. And you can all try out our latest models, including Gemini 2.5 Pro in AI Studio. So let's see this in action. So we just shipped a brand new UI, which has been awesome. We have all the latest Gemini experimental models. We have Google Search as a tool, which works identically, and even new model capabilities like native image editing and generation. I have been wanting to remodel my kitchen for the longest time, and I bet Gemini can help. But, you know, I'm an engineer, not a general contractor. Gemini was able to cook up a really, really nice prompt, very detailed with all of the different features that I would want to use to reimagine my kitchen. Nice. Yeah. And so let's hit run again, and hopefully we'll see a result in just a few seconds. Amazing. So I love this. It's been able to take my layout. You can still see that door off to the left that we had off to the left in the original picture. It's given me a new stove, this beautiful pewter green backsplash, which is a new word that I learned. This is actually a really great example in practice. If you saw that first prompt, super deep, detailed, really verbose. For the second prompt, we can also just ask it to very explicitly change, hopefully, just one thing, which is going to be these lights up in the top. Yeah. I'm also curious to see what kinds it picks. It's one of the fun parts of playing with this model. And really, this is the power of Gemini all coming together, from understanding videos to native image generation to grounding real information with Google Search. These are the kinds of things that you can only build with Gemini. Absolutely. It all starts in the AI studio. What in the world is an agent? How are you defining that? An agent is a service that talks to an AI model to perform a goal-based operation using the tools and contacts that it has. In this next demo, we'll create an agent to help contractors by handling tasks like verifying building codes and looking up permits. Vertex AI is our end-to-end platform for building and managing AI applications and agents, as well as model training and deployment. Now that we are using Vertex AI to build an agent, how do we actually get started? Agent Development Kit. It's new, open source, model agnostic, and supports model context protocol. To build an agent in ADK with Gemini and Vertex AI, we need three things. We need an instruction, we need tools, and we need a model. In ADK, we support the model context protocol. MCP creates a standardized structure and format for all the information an LLM needs to process a data request. This allows our agent to use tools for retrieval augmented generation, also known as RAG. The first thing I need to do before we define an agent is to connect to Gemini in Vertex AI. This is very easy to do with a .end file. I can either configure my Google Cloud project, or I can easily access Vertex AI using an API key. We define our agent's goal using an instruction. I use Gemini in Vertex AI to help create this instruction. For this agent, the instruction focuses on taking a customer request and creating a PDF proposal. Next, let's explore tools. We're going to add this Analyze Building Codes tool. It allows our agent to perform RAG by accessing our own private data set for local building codes. The agent is going to decide on its own when it needs to call this function. So it needs to understand what the function does. To perform RAG, we need to retrieve information from outside the agent. I used the model context protocol server from Google's MCP toolbox for databases, which we contributed to open source. And I deployed it to this endpoint here. Since we're building an AI agent, we need an AI model from Vertex AI. I'm using Gemini 2.5. And the model is a key part of the agent because the model handles the execution of the tools we defined, and it produces the output we are looking for. A proposal for pages remodel. ADK comes with a dev UI for local development. Open localhost. Let's select our agent. The dev UI handles multimodal inputs like images for us. So I'm uploading pages ideas and her floor plan. We will integrate this UI with Gemini Code Assist and its supported IDEs later this year. Okay. Gave us an answer. Wow. Look at this. It created this whole professional PDF. Yeah. Agent Engine makes it easy for you to deploy and run agents built on any agent framework. It simplifies the deployment process so that you can focus on your agent's code and provides you enterprise-grade security controls, production-grade monitoring and logging, and even evaluation and quality frameworks. To make life easier for devs, we actually have something pretty awesome called Agent Space. You've been hearing about that this week. It's our hub for agents across a company. You can build these no-code agents directly in Agent Space. And you can register agents built with the agent development kit and make them available here. That's good. And some of the best part is that this supports third-party models and agents and provides a common surface for access control across all of your users. Now, we've already got the construction proposal agent that generates client proposals. Next, we'll add two more agents, one for permits and compliance, and another for ordering and delivering materials. Let's check out the code to see how we use ADK, that is Agent Development Kit, to build and orchestrate a multi-agent system. We start out by defining our root agent. So this can call one or more sub-agents based on the goal. First, we'll start with our instructions. This is very similar to how we do with a single agent. Here you see three sub-agents highlighted, proposal, permits, and ordering agent. All right. When my agent is ready, I can deploy it directly from ADK to Vertex AI agent engine. It is a fully managed agent runtime that supports many agent frameworks, including ADK. Now that our agent is deployed and running, let's test it out in a place where I want to make it available to other users, which is, you heard it, agent space. I'm going to go to renovation agent, which I just deployed. All right. So to start off, let me start uploading the proposal document that was generated in the previous demo, and I'll start interacting with the multi-agent system. I'll ask it to kick off permits process. Now, let's say I want to ask it something related to ordering. All right. Looks like my renovation agent has a bug in it. Let's go over to cloud logging. All right. That's a long wall of exception texts. I see investigate. I click that and create investigation. This is a new feature in Cloud Assist called Cloud Investigations, which helps diagnose problems with infrastructure and even issues in your code. Once the investigation is complete, I'll click view investigation, and the report is open. So in the hypothesis section, I also have the overview of the problem. I have recommended fixes along with the direct link to Gemini's suggested code edit, which will directly allow me to make the fix. Let's navigate over. Here we are. As you can see right here, it has the comparison and a side-by-side view of the current buggy version of the code, which has the wrong column referenced. And on the right-hand side, I have the fix to it that is Gemini's suggested edit, and the right column that I was supposed to reference was order underscore status. I have incorrectly referenced it as status. I like the fix. I'm going to accept code suggestion. I'm going to accept code solution. I'm going to accept code solution. I'm going to accept code solution. I'm going to accept code solution. Once it's deployed in about a minute, I could go and retest the agent and share it out to users without having to make any change in the multi-agent system itself. So building, sharing, and even debugging complex agent systems got so easy with Agent Development Kit, Vertex AI, and Agent Space, along with Cloud Assist investigations, right? You know, to make it easier to connect any agents together, we actually just launched that agent-to-agent protocol. Just like MCP standardizes how agents connect to and use tools, A to A makes it easier to discover and connect agents, especially when those agents are from different ecosystems or vendors. Next, we're going to show you how to take advantage of Gemini from your other favorite IDEs and tools. Let's hop over to Cursor to see how you can start making some specific services using Gemini as the model of choice for coding. There is a lot of code here, but this is just a budget controller Java file with an existing controller. Let's say we'd like to add some input validation so we know that everything looks good before we go ahead and process it. Using Cursor's inline option with Gemini as a selected model, we can ask to add input validation. All of this is done right within the existing file, recognizing any differences needed and even adding changes based on the type of entity that you need the validation for. Let's see what we can do in IntelliJ and the JetBrains family of IDEs. In IntelliJ, we're connected to Gemini via GitHub Copilot. We can go ahead and ask in the chat to add unit tests. Okay, looks like it's proposed a new file and maybe some modifications to a current file I have. And once this is completed, I can choose what I would like to accept and input into my code. But we're enabling devs to use Gemini wherever it suits you best, even in tools like Visual Studio Code, Tab9, Cognition, and Ader. Behold, Vertex AI's Model Garden. You can connect to some of the most popular models or even bring your own from registries like Hugging Face. Model Garden supports the latest and greatest models across creators, including Llama, Gemma 3, Anthropic, and Mistral. Here, we're in Visual Studio Code. And in this example, we've been using Claude, and it's great. But our apps run on Google Cloud, and the model's running somewhere else. With two small code changes, we can use the same Claude 3.7 model from Model Garden. We'll use AnthropX Library. And all we need to do is change how we initialize the model. Here we have the project ID and region instead of API key, and then change the model's actual name. And we can use other models via the Vertex AI SDK to do something similar. Your team can build great apps using Gemini as your IDE of choice, or you can use Vertex AI Model Garden to call your model of choice. So we've brought even more Gemini to our own developer surfaces. Android Studio now is supported with Gemini Code Assist. It's in preview. This adds Android-specific AI capabilities to Gemini Code Assist. And Gemini and Firebase provides complete AI assistance in the brand-new Firebase Studio. This is the Gemini Code Assist Kanban board. We're moving beyond editors to a new way to develop software with agents. The Kanban board will let you orchestrate agents to help you in all aspects of the software development lifecycle. It includes something we're calling a backpack that holds all your engineering context, style guides, security policies, formatting preferences, even your previous feedback. So Code Assist can be a more effective coding partner. All right, let's start here. We've got a technical design doc for a Java migration. We'll make a comment and assign the migration to Code Assist directly from our Google Doc. This new task will show up on the Kanban board so we can keep track of its progress. We can ask Code Assist, or send so to Code Assist, a message asking it to fix this continuous integration build failure directly from our team chat room. Or you could assign it a bug directly from your bug tracker. It can keep an eye on our repo to perform bug triage, do root cause analysis, and fix issues automatically. We can ask it to do code reviews on all incoming pull requests, leaving great actionable feedback. And, of course, you'll see all those assigned tasks, bugs, and each new issue as they arrive right in the Kanban board. While we're here, let's start a new project. We'll take a product requirement document for an earthquake monitoring web app and ask Code Assist to produce a prototype. And then we're going to preview that web app right from within the Kanban board. You can see how this now becomes our new development loop. I can tell Code Assist the changes I want and let it take another pass and repeat that until I'm happy with what I see. Or I can decide I want to just dive into the code right in my IDE. I think orchestrating agents is a big part of the future of software development. And we just showed a fresh development loop here for small, large enterprises alike where AI works with the developer to build the right things and then build it right. Even before Next, there has been a lot of buzz about VO2. That's right. And we love seeing what you all have been making so far with VO, from short films to magical animated scenes and so much more. All of that is available today on Vertex AI, which is amazing. You can build with this. But we wanted to give you one more peek behind the curtain at what's achievable when Google Cloud and Google DeepMind team up with partners to stretch the boundaries of what's possible in entertainment when you use this same technology. I really love that we showed so much that you can do today, not futures, right now. Let's get out there and start building. Thank you. Thank you.