 Karen Dayhut, CEO of Google Public Sector Well, hello, everybody. Good afternoon. It is fantastic to be here at Google Cloud Next. And I am Karen Dayhut, CEO of Google Public Sector. Thank you for joining us to close out day one of this amazing conference. I am very excited to share the stage today with Ali Farhadi, the CEO of the Allen Institute for AI. So, what is the Allen Institute for AI? It is a nonprofit research center focused on AI and focused on the mission of unlocking amazing research and discovery and to solve some of the world's most challenging problems using artificial intelligence. So, this mission, I will just tell you, deeply resonates with me. As a former Navy officer and as the daughter of a 42-year Navy veteran, this idea of purpose has been instilled in me from the very beginning of my childhood. This idea of service, service for a purpose. That drive simply is a big part of what brought me to Google. To lead Google Public Sector, where we connect the most innovative technologies to the most important missions. Today, we will be sharing with all of you how we are democratizing AI, accelerating open source AI innovation, and fueling a new era of innovation across science, healthcare, the public sector, and other regulated industries. I am particularly proud of this partnership with AI too. And the work we are doing together to, oh, advance cancer research and cancer care through AI discoveries in partnership with the Cancer AI Alliance. To support scientists and researchers to advance scientific research such as life cell science, neuroscience, through open models and tools and technologies designed for a researcher. And to help Google customers build medical agents, medical agents that can be used in remote locations where people don't have access to doctors. These are just a few of the examples that our partnership is forging. And we are basing those discoveries on a deep commitment and a shared mission to apply AI to help solve some of the most challenging things that humanity faces today. We are doing that together. And to be honest with you, we are at an inflection point right now. And I think we can all truly say we've entered a new era of innovation. Think about it. Every single industry and entire societies are witnessing sweeping change. And AI is the driving force. This is not about incremental improvement. This is about total transformation. This is the promise of AI. And it's here and it is now. And technology, of course, plays a critical role in all of this. At Google, we believe fundamentally that the public sector should have the same access to cutting edge AI and security innovations as the commercial sector does. And it should not be constrained by gov clouds that are not scalable. We took a completely different approach by certifying our entire commercial cloud for the public sector. We are pioneers of zero trust. And we offer a unique full stack approach to AI that enables us to bring the best of security and AI to every mission. From our Google data centers, chips, planet scale infrastructure to our world class research, to our models and tooling, to our products and platforms used by billions of people every single day. And underpinning all of this is our scalable, resilient global network for all levels of classification designed to help our customers achieve their mission. That is the bottom line. So organizations like AI2, and to be honest with you, all of our customers can benefit from Google innovation at every layer of this technology stack. AI2 in particular leverages our AI optimized infrastructure and our advanced AI and ML capabilities. And they've trained their models on Google Cloud. And their models are part of our Vertex AI model garden, which is Google's curated set of models that are best in class for their category. Partnering with Google Cloud AI2 is not just building open models. They are helping to advance the entire field of artificial intelligence. With ALMO, for example, AI2's family of fully open language models, they are sharing everything. The data, training code, model weights, and checkpoints. With MALMO, AI2's family of open, multimodal models, they've challenged what it means to be open source AI and sparked a really important conversation in this field about truly what open versus weight models really means. And with TULU, AI2's family of post-trained models, they are helping to lay the foundation for the next chapter of open post-training research. And today, they released the ability to look inside the model itself with ALMO trace, which you'll hear about from Ali shortly. I am so excited to dive deeper into these really incredible innovations and how they are helping to drive this new era of innovation. And the fastest way forward is together. And we believe, we strongly believe, that collaborations like ours reflect the promise and the potential in this new era of innovation. I think the best way to truly understand the impact of this important partnership and AI2's work is to hear directly from the source. So, to that end, I'm delighted to introduce Ali Farhadi, the CEO of the Allen Institute for AI. Please join me in welcoming him. So glad to have you here. Please have a seat. Oh, Ali, I'm so glad to have you here. It felt like we waited a long time to get here today. Super jazz to be here. Yeah, so good. Such an amazing conference it has been. It is. It's amazing. It's big. We were talking backstage about how amazingly big it is and just so well done. So I hope you're all experiencing that too. But let's get started. Tell the audience, tell us all about the important mission of AI2 and your vision. Absolutely. AI2 is a nonprofit AI research organization founded by Blake Paul Allen more than a decade ago. Our mission is to enable breakthrough AI, not for the sake of AI, to be able to help solve some of the biggest problems and challenges facing humanity today. We have a group of some of the world's best researchers and engineers in AI working really hard towards that mission, working on some of the hardest, most challenging, and in our opinion, most impactful problems there is. There are three main areas of focus, three main pillars of AI2 that we work on. The first one is, we call it open ecosystem. This is our effort in opening up every aspect of an AI system. As you alluded to and you talked about it, this includes the data, the algorithms, the training, the checkpoints, the evaluation, all together to enable a wider community of developers, practitioners, engineers to be able to benefit from these kind of technologies. Our second pillar of work we call the AI for science. This is us trying to use AI to help scientists with some of the biggest problems they face. This includes information overload. This includes discovery using AI. This includes having specific models for specific science domains. And our third pillar, we call the AI for the planet. This is our work in helping the planet with AI. And we've been hard at work in that domain for over a decade. So we now have solutions that are operationalized in more than 70 countries, monitoring land, monitoring sea, for animal movements, for anti-poaching, illegal fishing, wildfire prediction, climate modeling. These are such important missions, right, for humanity. And I think that's where the Google AI2 partnership has become so important. And you have made this really deliberate decision to be open. And I wonder if you could share a little bit with the audience about what it really means to be an open source model. And fundamentally how your approach is different from other open models that we hear about. Absolutely. So when open hits AI, open is an overloaded term. Yeah. So it has many different interpretations and many different subtle but very, very impactful differences. So when you think about it, there's a whole spectrum of openness. And one end of the spectrum, there are models that are behind API models. You can query these models, you get a response. In the middle of the spectrum, there are models that are trained behind closed doors, on closed data, but the weights of those models are shared. On the other end of the spectrum, this is where WISIT, it's truly openness happens. This is where every aspect of an AI system and AI model is actually truly open. This includes how we collect the data, how we clean the data, how we massage the data into the training algorithm, what are the details of the training algorithms, the training code itself, the training framework, and the evolution of the model during training. So these are the checkpoints in the middle of the way as we change something with the loss, with the parameters, the behavior of the model would change. And we're exposing those as well. Obviously, the model itself, the evaluation, the evaluation framework, the benchmarks. So we're basically exposing all of those. Why would that matter? When you think about it, sort of AI innovation happens when people could build on top of each other's work. If you want to go back in time when open source software became a thing and revolutionized the way we developed software, there was, to me, there was one key principle around that notion. And that key principle was, I could understand what you did to the extent that I could change it, I could use it as is, or I might be able to just grab half of it, use part of it, use all of it, change it in a way that I truly want to. And I customized it to my own problem. And I contributed back to the pool. So the next person who comes in, they could actually see my work, understand it, most importantly, and be able to change it the way they wanted to change it. So now, with this open ecosystem approach that we're pushing for, we're bringing that notion, that concept to AI. Empowering a whole community of developers, researchers, who could understand what we did, who could change it as flexibly as we could ourselves. We have exposure to the whole system, the whole spectrum, and they could actually play with it, change it their own way, understand it, and hopefully contribute it back to the ecosystem. And to me, that is the fastest, safest, and most effective way for AI innovation. It's amazing. You and I have talked about the fact that some people are inherently distrustful of AI because they don't understand it. And today at Google Cloud Next, you made an important announcement about Olmo Trace, which is this idea that not only is it fully open, but now you have this full data traceability. And how the model is making decisions and how the model is making decisions and a person can go in and actually understand through tracing that data how a model came to its conclusion. Tell us more about that and how you came to believe that this was so important for AI research going forward. Absolutely. So data plays a central role in modern AI stack. At this point, we all should believe in the fact that data defines the behavior of a model, data defines the expected capabilities of the model, and any issue in the data will surface itself one way or another into the generations of the model. But openness is a challenging, costly concept. These models are trained on trillions of tokens, billions of documents. And our thought was that openness was not as accessible as we wanted this to be. Yeah. With Olmo Trace, we're actually bringing accessibility to openness, enabling everybody to start looking into the inner workings of the relationships between the input and output of these models. This morning, as part of Google Next, we're just that we announced Olmo Trace. Olmo Trace is a technology that enables you to map a generation of a model into the input that was used to train that model. This link has been missing for a long time. And now with Olmo Trace, we're bringing that and enabling a wider community of technologists, developers, researchers, to have the ability to establish a link between the input and output of these models, enabling them to actually further their exploration with these models. Great. So let's actually see Olmo Trace in action. Yeah, please. So we're using Olmo 2302B. This is a model that we've trained on Google Cloud with partnership with Google Cloud. This is a model that is efficient in terms of it fully consumes one GPU box. So it's the most cost-effective solution that you could do. And obviously, like anything else we do, Olmo is fully open. This means that you actually have access to pre-training data. We have access to post-training data. We have access to mid-training data. And we just expose everybody to have access to these. Now we can actually start linking those together. So we're talking about open data. Let's ask Olmo a question about the relationships between open data innovation and what key innovations open data enabled. So if I could see, I would say that the question was how could an open data enables innovation. And you see a generation like any other state-of-the-art model, Olmo generates an answer. But a key difference, now as simple as click of a button, you can actually start asking Olmo, show me the trace. Show me the closest data points that you have seen that are closer to this generation. The issue is we now need to search trillions of tokens, billions of documents to be able to actually establish this map. And Olmo trace enables that technology. With the push of a button, you can actually establish that map. And this click of a button enables you to go through a list of all this training data that we use to train Olmo, establishing those relationships, and finding a set of documents that closely relates to the generation of the model. Now you see these colorful spans of text. These are long enough spans of text that we used to find the relationship between the input and the output. So I looked at this list, and I was actually puzzled, why do I see transportation here? And I was just curious, what's going on, why transportation is actually on this list? You can click on it. It will directly take you to the data, the document that was the closest to this match. If you want, you can actually sort of click on that document, look at the external link, and find the source that was used as part of that that came to this document. Now you're actually seeing the relationship between the generation all the way to the information on the web that was used as part of this model training. So this link has been missing for a while, and we're so excited that we're actually bringing this today for the first time mapping the input and output of these models together as part of our initiative to make openness accessible to many, many developers and researchers and practitioners. It's amazing, and I think it's just such an innovation in AI and in these models, and I don't know, I can't only imagine where we're going to take this. So let's make this real for this audience so we can now trace the data all the way back to its source. How do we make that real for a developer maybe sitting in the audience or an AI user in this audience? What might they use it for? What are some use cases? So let's break it down per persona. Yes. So developers oftentimes, what they do is they want to grab a model, they want to fine tune it to a further application. And oftentimes they realize that the model might actually drift in the middle of the fine tuning without knowing what causes a drift. This empowers the developers to actually fine tune and customize the models completely on their own, on your own data, because now this map is established, people can actually start tuning in an extreme way on their own data. But also, it's actually a great tool for debugging. How many times have you looked at a generation of a model and thought to ourselves, why did the model generate this response? Now with push of a click, you can actually establish it to the training data. And more often than not, we've been surprised with that fact, but more often than not, these kind of issues actually trace back to a data point somewhere in this massive training data. For researchers, this empowers them to start thinking about hallucination. How could I ground the response into a document? And taking it one level further, I would argue that this enables a whole class of new architectures in AI that would benefit from this linkage between the output and the input at the inference time. Most importantly for regulated industries, they have been long waiting for traceability, accountability, a notion that can I actually have human understandable rationale behind the generations. And normal trace empowers all of those for developers, for researchers, and for regulated industries. So let's talk about that for a minute. Because in a lot of regulated industries like healthcare, yes, they want to take advantage of these great models and really benefit from the openness and the data traceability. But they want to keep their data private. They want to keep theirs sort of in reserve and not shared broadly. How is this going to solve for that? Well, you're spot on. I think so when you start thinking about domains with crucial impact, and also regulated industries, healthcare, finance, all the public sector application domains, we need to marry these two concepts, openness and privacy. We think about openness with almost trace as a way to earn the trust of these regulated sectors, but also give them the accountability, the traceability that they have long been working on. But that's not enough, as I said. But that's not enough, as I said. I agree with you. We need to also empower the privacy. And for good reasons, many of these domains have fairly strict rules about the privacy of their data. We're thankful that they do. So how can we actually sort of marry these two technologies? So we've been working on a technology that enables us to actually marry these two concepts together, openness and privacy. And I'm actually pleased to show you, snipping into the inner working of these models and what they enable. So now we can actually start thinking about models that are trained in a way that they could benefit from all the benefits of openness, use public data, use shared knowledge, but are also purely tunable and trainable and customizable to extreme on the private data. Yes. So let's look at actually sort of some results in this space. These are very, very area results, but I'm actually very excited to the extent that I wanted to share it with you on stage. So when you start thinking about using AI for your application domains, there are three classes of approaches typically that you can take. Use a general model and call the API, get the response back. The first chart is showing you that response. General models are general models by definition. They are struggling to actually go into your specific domain in mind. Another common practice is let me grab a model and tune it, fine tune it, customize it to my application domain. Obviously, you get better at your domain. Sometimes when you do that, especially if you don't have open data, you drift in some other aspects. You tune your model to one specific solution and you lose capability in some other domains. And as part of Tulu recipe, we're actually been trying to establish those by showing people that if you have access to pre-training data, this enables you to actually have new recipes for fine-tuning your models that go well beyond standard practices of fine-tuning. But the middle bar shows you what you can actually get by customizing that model. It's an expert, specialized expert model. And the bar on the right is a new solution that I'm talking about. Benefiting from what the shared data, what the open data provides you, but also keeping the privacy of your private data. And if you can marry them together and the results show that you actually see significant gain. These are all models of shared structure, shared domain, shared data. And the benefit is just coming from the fact that you can actually train these models in modular way. If folks are interested about this, I would be happy to actually sort of go into the technical details of this offline. Yeah, I think one of the things that we should talk about is these three different approaches for regular, we talked about regulated industries, healthcare, financial services. You actually raised fashion earlier today as an industry that's super interested in this approach. So take it to that next level and talk about what is the real application in those industries and what are you seeing? Yeah, in my opinion, any industry that requires state of the art models, they have privacy concerns, and they want traceability, they would benefit from it. To us, the most obvious ones are healthcare, obviously. In healthcare domains, there are obvious privacy concerns. Patient data is one of the most private data in the industry. Hospitals and centers are actually sort of keeping that data extremely private. But they all want to benefit from the shared data. Finance follows the same. Financial data is actually one of the most. But fashion. Yeah. Fashion is also very, very similar because in that domain, in that industry, the owners of the designs are very protective of their design. Okay. And they don't want to share the designs that are not out there. So they're very, very protective to the extent that they actually use exclaves and all the other technologies that you see in finance and other sectors that are now actually sort of one of the common practices in fashion. That's awesome. My favorite example of this domain is actually CAIA, the Cancer Eye Alliance, the partnership that Google, AI2, major cancer centers in America, and a few other players are actually sort of all coming together to move the needle on some of the most important problems. But if you think about cancer, cancer centers have their own private data. For all the good reasons, they are not willing to share that data. No one could touch that data. Hopefully, not even the model trainers should touch that data. This is patient data. We should keep it sacred. We should keep it private. They all also know that cancer is a long-tail problem. There are so many types of cancers, and no one cancer center have enough data to train a model that's good enough for all these cases. They all benefit from a notion of sharing, but they also all benefit from not sharing. And these kind of private and public models or private and open models are examples that are extremely suitable and mappable to these kind of problems. Yes. Can we train a model that benefits from all the public data without sacrificing the accuracy of the privacy of the private data? I know. I'm so excited about the Cancer Center, Cancer AI Alliance. And, you know, they've been very clear with us, Dana-Farber and Johns Hopkins and Memorial Sloan Kettering, that the only way to advance cancer research is really to share the insights of all of their data, but not to share the actual data itself. Exactly. And so this architecture we're building together is super critical to advance that idea. Absolutely. Of open and private. So today we talked a lot about agentic. Hopefully you all have been able to go to the showroom floor and see a lot of the agents in action. Ali, let's, your imagination, let's let it run wild. What are you most excited about in AI and applications of AI now and going forward? And how does an open but private model really benefit those new applications? Yep. First of all, I'm jazzed to see these many, the whole gamut of agents as Google is unveiling as part of Next. Both the breadth and depth of these agents is quite phenomenal to see. To me, we are in the very early stages of agentic AI framework and approaches. I think the notion of agencies still is being developed as we are developing and building more and more autonomous agents. I think we have a long way to go to get there, but we now have agents that work in practice and enterprise data. We have seen so many demos of these agents. The domain that I'm most excited about when we think about agentic AI, and I'm biased in my own way, is when actually agentic AI meets science. We talked about information overload that the scientists are facing. We're lucky that we live in a moment that science progresses so fast. I think the rate of progress in some of our disciplines is absolutely unprecedented. We haven't seen things moving forward this fast. As an AI researcher myself, these days I am struggling to keep up with the pace of progress. And every single week I repeat the sentence, how did I miss this? So many, many scientists actually face this problem of information overload. But with this rate of progress also comes a broken signal to noise ratio. The amount of noise in the space is also growing much faster than the actual signal, the actual science. And scientists in various different domains struggle to find the actual piece when they're looking in their explorations. So an AI agent that could actually help scientists with their research. Do the literature sentences answer questions about the literature, being able to actually help them with hypothesis generation, with evaluation, is the kind of agentic work that I'm most excited about. At AI too, we've been working on that for a while now. And I'm excited that soon we're actually releasing the first version of this whole framework around what does it mean to do agentic AI work when we think about science and science discovery. Yeah, you know, we've talked about the cancer AI alliance. And one of the things I'm wondering is the future of agentic, is it human agent interaction or is it more about agent to agent interaction? I know I'm going off script here, but I'm curious about what you think. Absolutely. So the way I think about this is that in the short term, we're going to actually see a whole spectrum of solutions. I think there are problems that are well formulated enough that we see agent to agent interactions being developed. Yes. And we are seeing sporadic signal already that things are evolving in that direction. Today in the next door, there are demos that are actually showing agent to agent interaction. But in many of these harder problems, we're actually sort of dealing with also agent human interaction. And that gets complicated fairly quickly. Human biases, the way humans work, our inertia and actually I as a scientist work in this specific way. And now that I'm actually sort of this agent is telling me that no, that's the way to form the hypothesis, would actually cause a clash between the scientist and the agent. And naturally, these are the kind of things that I think we co-evolve with the technology. The minute that we actually people see the benefit of these kind of technologies, they co-evolve around it. And we've seen with coding, for example, we've seen software developers sort of guard against them early on. Now they're using it. Now people have learned when to use it, how to use it. And I think with these agents, we follow the same trajectory of some problems. It's now ready to roll. Some problems, human and agents evolve together. And some problems might be actually challenging enough that we might not see agenting AI workflows there maybe in the next couple of years. I mean, if you think about the technology curves, there generally is a new technology and advances very quickly and it plateaus. And then you get that second, third, second, third curves. And I'm curious, what are you most excited about on that, you know, upward trajectory? What are the things that you imagine for the future of humanity that agents, agentic AI or AI in general are going to really solve for? The list is long. Yeah. So many things to be excited about. If you look at sort of AI improvements, I do not believe that we have flat out yet. I think the rate of progress is still fairly fast. Adoption is happening in some domains. And we hope to see with these kind of earning the trust of few other sectors, we see more and more adoption in these kind of technologies. But my personal favorite space that I would hope we see more investments, more activity and more progress in, and we've been working on it, is AI for the planet. Being able to see AI helping our planet. I think AI is actually well-poised to start thinking about those. When we think about a planet, the planet data is highly multimodal. Like actually pushing the boundaries of multimodality the way we think about it as AI practitioners. We have all sorts of data. It's hard to align them. It's hard to actually map them. There's so much missing information within that data. But it's multimodal in nature. And I think the problem of training truly multimodal, native multimodal solutions for the planet that encompasses all of these challenging data with all the challenges that exist, would actually go a long way as we're thinking about how we protect the planet, how we monitor it, but also how we start thinking about predictive tasks. Disaster management, we're still being surprised by storms, by wildfires, being better at those. All of those things, we actually get better at them by having better multimodal models available and accessible to the people who actually care about this domain. Yeah, you know, here we are at Google Cloud Next. So in this room and probably across the 40,000 people or so that are here, you have strong believers in AI. But if you go out into the world, you have a spectrum of people from non-believers to people that question the capability and trust the models to people that really want to use them and apply them to true practitioners, to true believers. So this broad spectrum. How do we leave a conference like this and help people understand what the art of the possible is with AI? Take everything that we're learning about open and data traceability and AI in general. You know, what advice do you have for us to take forward and to help people become more, you know, interested and committed to this capability? Yeah. So I think AI is, if it's not the, it's one of the fast, most fast pacing technologies we've ever observed probably in our lifetimes. With that comes a few side effects. One of them is that we as sort of people outside the domain, we get surprised and we see these amazing demos. We get shocked. Yeah. Because of the amount of investments, because of the potential of the AI, there's also a fair amount of hype in the system. And some part of that hype is not serving most part of the AI ecosystem. Yeah. To me, one of the key things that at least at AI2 we're after is to bring the science back to AI. People trust science. If you actually earn the trust back and have a truly scientific approach to the problem, we can actually sort of subscribe more believers in this. I don't want to use the word believe because it's science. We can actually sort of look at it. You can understand it. You can start sort of thinking about what it does, what it doesn't. It would be great to be upfront about what it can do, but also what it cannot do. Uh-huh. What it has seen, what it hasn't seen. And what's the behavior of a model when you actually sort of hit it out of distribution? How could you shift the model to behave in a different way when they're facing out of distribution data? So to me, one key piece of the puzzle is being able to earn the trust back. And to us, that means we need to have more scientific approach. That means we actually have to open up as much as we can. So we can actually start thinking about AI in a more principled way. A key big missing piece in my opinion is evaluation. I've been public about it. We are in an evaluation crisis. With the rate of progress with AI, I think for this is probably for the first time at this in computer science technology history, that the rate at which we are actually developing new capabilities far surpassed our ability to evaluate. We're developing capabilities much faster than we can evaluate them. Therefore, we're in this mismatch where we're using old capabilities to evaluate a new technology that we truly don't know how to evaluate scientifically. So my biggest piece of advice is do not oversubscribe to those evaluation benchmarks. We developed a good portion of those evaluation benchmarks that people use in the industry. They're developed for different specific tasks. Now everybody's using it. It's a great thing to do to be able to sort of see progress. But for your application domain, it would be great to understand the application, understand the problem, and understand what does it mean to make progress. And now we're seeing it a lot that progress in these benchmarks might not necessarily translate to progress in your application domain, in your end tasks. So take those evaluations with a grain of salt. Do not oversubscribe to those tables. Again, we as practitioners, AI researchers, we do that on a daily basis. We should do it. But there's more to a capability of a model than those tables. And I highly, highly encourage everybody to think about what does it mean to evaluate the model beyond those standard tables. Such good advice. Thank you. So in your day job, you're the CEO of the Allen Institute for AI. You're also a professor at the University of Washington. You also have a young family. And I'm curious, like when you think about that spectrum of things that you do, and your students that you're teaching at the University of Washington, like what are you most excited about that they're engaging in, and that they're sort of interested in, and that they're researching. And how do you sort of keep them as sort of vibrant in their research as you are in yours? The question is the other way around. They are making me vibrant. Yes. So they are the true asset. They're the blood, the fresh blood. As a matter of fact, Olmotrace came actually from one of our amazing students. Really? We're sitting right over there. Wow. Jiachin. That's great. So the fact that these students are there are one of our dearest assets that we have. That's awesome. The ability that actually can get them involved in the ecosystem. And students these days in computer science, specifically in AI, are the cream of the crop, right? Because of the amount of investment in AI, when you look at the people getting into these programs to get to a PhD and look at their resumes, I'm just happy that I have my job. Because it's just hard to compete with this new generation. And they are really well-rounded. They are really capable. And I think enabling that group of future researchers and engineers with tools, data sets, models that could actually help us build these kind of solutions is a key. It's actually, I think, we're responsible for that. Making sure that they have the right set of tools. They actually understand the inner working of these systems. And what we're doing at AI2 and the University of Washington is completely aligned with that mission. Enabling not only the current generation, but also the next generation of innovators. And that is sort of a key to the success of AI and AI progress in this country. That gives me such a sense of hope. And I think we all need a little hope in this world. So that was wonderful. Thank you so much. Last question. You know, we have long believed, Google, this mission of, you know, really bringing technology to do important things for humanity. And we found this perfect partner in AI2 with your mission and sensibility around humanity and the use of AI to solve for these big micro challenges. So I see the value in that partnership. I'd love to hear in your words how you see this partnership working and evolving and what makes it special. Absolutely. So I do believe that AI will change our lives, will improve our lives, the ways that we've never seen before. Many, many, many aspects of our lives. And I do believe that getting AI to that state is a challenging, hard task. No single institution, no single company, regardless of their size and capabilities, is just capable of doing that. So you started your introduction with talking about working together. We truly believe in that notion that if you would like to move the needle in AI, seeing that future AI is actually improving our lives, the way we think about healthcare, finance, all other aspects of our lives, it's a partnership. It is a partnership that goes into true sense of collaboration, working together in pushing the boundaries of what's possible. Specifically, with AI, in general, most partnerships, we've seen so many that succeed, so many that fail. To me, the ones that succeeded beyond expectations are the ones that they started by a sense of shared alignment, a shared roadmap, a shared sense of mission. And I'm jazzed and delighted to actually that Google and AI found that shared sense of mission, working together on some of the hardest problems facing humanity. We're all invested in this. We exist because of that. And we're excited to see that Google is aligned with us in that domain. And because of that shared mission and the shared sense of target, I do believe in the power of this partnership. I'm going to repeat myself. We cannot do it on our own. I don't know about Google, but I would argue that no single institute can actually pull this off on their own. There's just so much involved about this, the domain expertise, the AI expertise, the infrastructure that needs us to do that, the power that you need to generate, how you want to cool that whole system, where do you want to get the data from, what are the next generation of the algorithms, what are the next generation of talent who's going to work on these problems. When you think about all different pieces of this ecosystem, to get this flywheel working together, it requires a whole town. And we're very, very excited about this partnership and hoping to actually bring in more and more partners to this coalition using AI to make the world a better place. Thank you so much, Ali. Thank you for being here. Thank you for this incredible partnership. I know how committed we are to it. And you're absolutely right. Together is better always. And I think it will take sort of an entire ecosystem to bring to life the power of AI. Thank you all for being here today. All of us at Google Public Sector, Google Cloud, and Google in general are so grateful for your commitment to this technology and what we can do together to really benefit humanity, to benefit the public sector. And I wish you a great Google Cloud Next. Enjoy the evening and we'll see you tomorrow. Thank you.