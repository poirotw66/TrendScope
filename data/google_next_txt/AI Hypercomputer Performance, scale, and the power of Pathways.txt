 Welcome, ladies and gentlemen, to AI hypercomputer performance, scale, and power of pathways. By show of hand, how many people here are very happy with their computer orchestration? Performance, reliability, no problem. The overwhelming majority of people in this packed room are very happy. Just kidding. 8.30, Friday morning, you are here. This means that there is something that we can do better in terms of performance and your computer orchestration, and we are going to learn it today. Let's get started. My name is Vabha Singh. I'm a product manager for AI hypercomputer at Google, and joining me today are my colleagues, Sharia Gupta, who is the lead for Pathways, and Kirat Pandya, who is the CEO of Osmos. And today we are going to talk about how to maximize performance with AI hypercomputer. What are some of the limitations of traditional orchestration and how you can solve these limitations using Pathways on Google Cloud? Then we'll hear from Sharia about how Pathways works under the hood. And from Kirat, how Osmos is maximizing flops for development using Pathways. Let's get started. AI is driving economic productivity across various domains. Code generation, content creation, research, writing, you name it. And this is driven by the underlying scaling trends. The first one, the pre-training scaling, which is to train larger model on bigger data set, produce superior results, more capabilities in your model. But these are just the base model. And then there is also a post-training scaling, where you take these base model, and then you fine-tune them using techniques like reinforcement learning or align them for your downstream task. For example, teach them to reason in order to then maximize performance on your downstream task. However, scaling is not just about the size of your infrastructure. It's about the speed with which you can build and deliver intelligence into your products. And this is why we created AI Hypercomputer, which is our end-to-end co-designed architecture that includes not just the performance-optimized hardware, but also the orchestration, the runtime, the framework, solution layers, and each of the components in these layers, as well as the consumption models, so that you can have access to these resources when you need it. Let's talk about performance. With AI Hypercomputer, we bring the best of cloud accelerators, TPUs and GPUs, to you, which is co-designed in our data center. So it's not just about those chips, but also the entire layout in the data center in terms of power and other details, as well as the storage and networking components, which are co-designed to maximize performance at this layer. And then we make it accessible to you using the most scalable orchestrators that there are. For example, GKE. You have tuned into, I am hoping, or if not, you can later tune into this session from Anthropic when they talk about how they're pushing the boundaries of scale using GKE for their AI acceleration. And all of this is available to you using XLA, as well as CUDA runtime, which means that you can take advantage of out-of-the-box performance from XLA, as well as build those bleeding-edge performance kernels if you need to, using either Palace or CUDA or Triton framework. And we are building tooling on top of this in order to make sure that your experimentation iterations are easy enough, as well as I'm pleased to share with you that we have also launched Cloud Diagnostic XProf, which is a package that allows you to create self-hosted instances of Profiler, and that allows you to share your profiling instances within your team for faster debug and have a better journey of performance optimization. And furthermore, I'm very excited to share with you that we have launched Torch Prime, our first reference implementation for language model in PyTorch for TPUs. This joins the already existing MaxRex, which is a reference model for JAX on TPUs and GPUs, where we continue to add support for latest open-source models like Lama and DeepSeq, as well as bleeding-edge training architectures like DPO or GRPO, you name it. And on top of this, when it comes to reproducing the benchmarks that we often look at, we have put together a set of recipes for GPUs as well as TPUs. These are called GPU recipes and TPU recipes, and using which you can reproduce our state-of-the-art performance benchmarks. Here I'm showing two examples. This is for dense model training using Trillium as well as A3 Ultra clusters. And all of this, we are pleased to share with you, all of this is available in one-stop shop on GitHub. We created a GitHub org called AI Hypercomputer, where all of these components and artifacts would be available as we ship more and more of these in future. Now, let's talk about orchestration. We'll begin by examining some of the details of training as well as serving orchestrations, and what are some of the limitations we are seeing in the things as they exist today or commonly used today. And then we'll talk about solving those limitations. Let's begin with inference. Here is an example of an LLM server where you supply it with a batch of prompts. Here is an example prompt and which are processed in parallel. All of these tokens are processed in parallel. And this phase is called pre-fill. And since these tokens are being processed in parallel, this is similar to what's happening in the training forward pass. And as a result, the flops per dollar or flops per byte read from HBM is reasonable, which means that you are reasonably utilizing your hardware accelerator. Then begins the autoregressive decoding where you sample one time step at a time for your batch of tokens. And for each one of them, it has to load the same amount of parameters from the HBM. And as a result, the operations per byte read from HBM is lower. And it is easy to see that while pre-fill phase is going to be compute bound in most cases, the decoding phase tends to be memory bound or memory bandwidth bound. And what goes wrong in a traditional orchestration is that we run them together. We have one instance of a model server that is running on some underlying hardware and trying to do both of them on the same machine. And this does not allow us the flexibility to balance these two phases in the way that can help us maximize the utilization of underlying accelerator. And this is why on the curve on the right-hand side, you see that in the region of interest where you want to deliver this low latency to your users, the throughput per dollar is suboptimal. So that's one problem in terms of inference. Now let's talk about training. For training, we typically build our training source code along with training environment in some image, and then we create replicas of this, and we run it on all of the available nodes. So here I'm showing these nodes here. In case of TPUs, this could be TPU slices. But we are running the identical copies of our program or the computation. And then begins this discovery phase where these nodes or slices discover each other. Then when we are ready to make progress, we start to step. There are periodic synchronization. And when we encounter a failure, the options are start from the last checkpoint. And we wait for either the spare node to be available, or if it is readily available, we switch on to that node and repeat that initialization process and so on. What is limiting in this picture? Number one, we are bound in this same competition on all host paradigm, and which you can clearly see is limiting because in modern training architecture, such as reinforcement learning, there is an easy case where you want to run training as well as inference on subset of your accelerator, and hopefully you want to do it in parallel, you want to innovate faster, and so on and so forth. And secondly, your training code has only one recourse for failure, which is to go back to the last checkpoint and start over. The training code itself lacks any visibility of what's going on in the cluster. There is no real failure response in your training code. And often it is plastered via, you know, some capabilities in the control plane and, you know, some logic which is plastered on top of that using telemetry from your monitoring and so on. And next, the interactivity is limited. We are not really able to experiment at anything beyond a single host if we want to step through the code, for instance. And therefore, debugging at a larger and larger scale is harder and harder. Anyone who has tried to find out where the failure happened and why my training hanged can testify to this. And all these limitations are happening when moving fast is more important than ever before. Going from ideas to prototype to production, that is the loop, that is your competitive edge. And therefore, we argue that scale, performance, and reliability are not the only axis that orchestration should care for. It should also care for interactivity and expressivity. And this is why we are pleased to announce Pathways on Google Cloud. Pathways is a distributed runtime that is powering today our large-scale training and inference infrastructure, including Gemini. And with Pathways on Google Cloud, we are bringing to you flexible orchestration. You will see how you can orchestrate different computations on subset of your computation clusters. You have integrated control in your training code. We have published a reference implementation for elastic training, but that's just one example. Now your training code is capable of handling failure because now your training code will have visibility into what goes on in the cluster, and if there is a failure, there is a certain amount of error aggregation that Pathways facilitate for your code. And finally, you have interactive supercomputing. And that is the ability to experiment using notebook or whatever is your favorite method on any arbitrary size cluster. Now we are going to see each one of these in details. First, let's start with how things change with respect to the traditional orchestration. For inference, we saw this interleaved scenario was difficult, and we are changing this with pathways, and we introduced disaggregated serving where we are splitting the pre-fill and decode functions. Remember, both of these functions are expressed in one Python program or one source code, if you may. And you can annotate or you can define these functions, not annotate. You define these functions to run on a subset of slices in your cluster, and another subset of your slices can execute decoding while one subset is doing pre-fill, and there is a, you know, KV cache transaction between them. This helps you balance this arithmetic intensity in a way that can help you maximize throughput per dollar in the regime that you care for, which is shown in this diagram. Okay. And for traditional training, how things change? We go from this fixed, you know, limited paradigm to a more flexible world where now, as was talked about for serving use case, you can define computations which are not necessarily identical that can run on different slices. Here, the different colors are showing potentially different computations which are running on those boxes that are representing different slices. And it can gang-schedule these computations. Notice we did not do any initialization step there. There is no out of order. There is no timeout. There is no distributed initialize that we are going to wait for our pathways. We'll gang-schedule these computations on the set of compute nodes. And then there is periodic synchronization. And when the failure happens, it has given you the power or the ability to respond to these failures. And with elastic training, for instance, you can choose to continue training with the fewer set of nodes. We call it scale down on failure and scale back up when your nodes are available and ready. This frees you up from limitations such as, you know, you sometimes are forced to plan that, okay, I'm going to use 75% of my cluster for running my training job. And I'm going to keep 25% for my experimentation and spares. and this is changing the game completely. You can start with 100% of your cluster and pathways can adapt if the failure happens if needed or a higher fraction. So, in summary, we are introducing flexible computing with pathways. Your training code has the control now. It is much more than, the options are much more than just go back to the last checkpoint and try to rerun. And we are bringing interactivity at scale. And we are going to zoom into this a little bit. And we are going to also see an example of this. First, let's see what are the limitations beyond that single host business that we talked about. Yes, you are limited to a box. Typically, a researcher or an engineer who is attempting to do some interactive development is given a box with precious accelerator resources. And I have not met anyone who works 24 or 23 hours. I have met many people who work 10 hours and six days and so on. So, there is an implicit underutilization of resources, a waste in the system just because of this limitation. And secondly, the ideas or innovations are also inhibited because you are only experimenting on a single node. And we are changing this with pathways. So, now, your dev box can be a CPU machine, which is relatively inexpensive. You set your training environment there. Great. And then, you can define a back-end for your training code, which is pathways. And in this case, we are setting pathways proxy as the back-end. And then, whatever computation you are doing can now leverage any arbitrary scale of compute. it can leverage any arbitrary scale of compute. And this gives you innovation beyond bound. Interactivity at any scale. And you are not paying for accelerators attached to that box assigned to a one team member or one for each. So, this is just completely reinventing how interactive development can work. Now, how will this all come together? So, here is a quick two-step process. So, first, we deploy pathways. We have created a pathways job API, which allows, with a very few lines of code, you can express what the pathways cluster should look like. And what is a pathways cluster? It is a set of components that you are deploying. First, there is a pathways proxy and resource manager. These are the components that run on that CPU node pool over there. And then, we have set off what we call the pathway worker containers, which are then scheduled on each of those TPU VMs, which are part of the TPU node pool. And when you are ready to experiment, you simply need to point your JAX backend to pathways proxy. And pathways proxy is going to do this execution on this cluster on your behalf. So, you're not limited. And this is where you can also see how your program can have this single point control in contrast to this multi-controller world where each program, the copies of the program were running. They only knew their own world and they controlled what was going on. The only recourse that you had was resumed from the last checkpoint and so on and so forth. Now, let's see this in action. So, here I have a dev box where I have deployed pathways. At the beginning of my program, I defined two environment variables. I defined JAX platform, which tells JAX that instead of the local runtime, the regular multi-controller world, you're going to use proxy. And JAX backend target, this tells where proxy is located and which port it needs to talk to. And that's it. And with this, I can run this here. So, this execution completed. In this cluster, I have two trillion slices. A slice is that interconnected set of accelerators, which has ICI connectivity between them. And when I say two slices, here I have two four-by-four, i.e. 16 accelerators in each of the slices. So, if they talk to one another, they talk on ICI. If they talk to their neighbors, they can talk over the data center network. So, here what I'm showing is that this notebook running on the CPU machine, after setting this, can discover all of the 32 devices. So, now when I look at slice 0 and slice 1 devices, it reports 16 and 16. So, I have both of these slices available to me. Now, I can choose to run experiment only on one of the slices if I want, because it may not be that I always run experiment on entire computer available to me. So, I can do that. Here, what I'm doing is that I am creating a physical mesh. For those of you who are familiar with Jack's distributed parallel programming, then these are familiar concepts. You create a physical mesh, then with the help of that, you create a logical mesh where you have named axis, and you can use this axis to express how you're going to shard your tensors, and so on and so forth. So, here I'm just sharding a tensor, and I'm visualizing it. So, within a few seconds, I have the visual. I just put a tensor on all the 16 devices here, and it is sharded, and I have this logical view of this tensor as well. I can run a collective operation here, and then, in this case, the out spec is none, that it is replicated on one of the axes and sharded on the other. So, these kind of computations are great, and I can do a feed forward layer if I want, which is also great. And now, switching gears, I can also run experiment on all the slices. So, here I'm creating a hybrid device mesh that consists of both ICI as well as DCN axis, and here I am representing DCN axis as D. This is taking all the 16 devices in one ICI, putting them on one axis, so whatever collective you run is going to leverage the entire slice, and this is just a logical axis. In reality, it is two dimensional, and it has the DCN axis as well, where you can have the collectives or whatever operation you want run on these axes. And there is experimentation beyond bound that is available in your fingertips. And all I did to start this was use this pathways job manifest, and we simply specify what type of DPU we are going to be using, what is the topology, I described four by four, and what is the number of slices, I mentioned two earlier, and this is what I also use, for instance, once this is done, then you will see the list of pods here which are running, so this is the head pod that runs on that CPU node pool that I showed you earlier, and these are the pods which are corresponding to pathways workers, which are running on each of the VMs inside your slices. And the combination of this pathways head as well as pathways user, the name of the job that you have defined is what I used in order to construct this back-end target using cloud DNS. So with that, let's get back to where we left off. So with pathways, we have interactivity beyond scale. We have just given a superpower to our training code to respond to failure beyond that one recourse of starting from last checkpoint. Now there is more that is there happening in the reliability. Let's zoom into it. So here is a typical interruption event. Every time there is a failure, there is some unsaved progress since the last checkpoint, which is last. Then there is some time that you wait for the slice or the node to get reprovision. And then once it is ready to start, then you reschedule your training job in whatever, if you're using Kubernetes as an orchestrator, then this job starts, it traces and compiled the program, hopefully you're using the cache, and then you restore or restart from the last checkpoint, and then you are off to the races. Each of the buckets there was corresponding to the time lost or what we call the bad put. And we defined a metric, we introduced it last year, which is called good put, that was a measure of the useful work done by a training job. For example, if you use an accelerator for 100 hours, and if it made forward progress only for 90, your good put was 90%. So, with traditional approaches, there are a set of techniques like asynchronous checkpointing. This year, we also launched in-memory checkpointing within cluster replication, which allows you to resume quickly, and this also allows you to checkpoint more frequently, and that reduces the time loss since the last checkpoint component. So, if we put these components in context, then with traditional approaches, we have all these buckets of bad put that we save. With pathways, we have all of them available, but it has some additional components. For example, because of elasticity with pathways, we no longer have to relaunch the program. We only trace and compile once, once we are scaling down, and that's it. In the subsequent iterations, we no longer have to do that. Here is an example of how it proceeds. On the left hand side, you have a traditional orchestration. On the right hand side, you have the same cluster working with pathways. When the failure happens, the cluster on the left stops or waits for the spare to be available, and thereby, you are going to incur the cost or the bad put in the process, while the cluster on the right can already start to make progress with pathways, when the spare node is available, then both the clusters can operate at the same scale. And with elastic training using pathways in our experiments, we notice the constant time to resume the job, irrespective of the scale of the cluster. And that constant time means that larger the scale of the cluster, since you have linearly growing number of interruptions, the savings you have using pathways is also growing linearly, and in our experiments with 49,152 trillion chips, we noticed an overall improvement of 15% in your good put, and that also leads to quadratic savings in terms of your overall cost. So with this, I'm going to pass it on to my colleague, Sharia, who is going to tell us how pathways works under the hood. Please welcome Sharia. Hello, everyone. My name is Sharia. Web have already gave a good introduction about pathways and its benefits. Now let's dive deeper into its architecture and understand how it gets those benefits. Pathways unlocks a new programming model for Jax developers. It is a drop-in replacement for the default Jax runtime, which is also called the multi-controller runtime. Jax pathways allows a single Jax client to orchestrate computations, fine-grained computations, across thousands of accelerator devices. Now let's dive deeper into the architecture to understand each of these. things. I'll start with a really simple piece of Jax code. Here we have this Jax code, which is simply adding one to every element of an array. So Jax developer begins writing this Jax code, uses the default Jax backend, which opens lib TPU and accesses the local devices connected to that TPU VM and produces the results. Now the same piece of Jax code can run with pathways without any changes by simply swapping out the backend from the default TPU backend to a new backend, which we have developed for pathways called Proxy. This runtime provides the same interface to upper layers like Jax and produces the same output for model authors, proving to be a drop-in replacement. Now let's try to understand more about how this pathways runtime works. So we'll start with the same piece of Jax code at the top. The user authors the Jax code, runs it through the Jax compiler, which lowers that Jax code into an HLO. Jax delegates the compilation and execution of that HLO to a distributed runtime, which is implemented using IFRT. For pathways, we are using the Proxy backend, which serves as the entry point into the pathways runtime. The first step is to compile the user HLO. We use the XLA compiler for that and store the compiled artifacts into compilation service, into a compilation cache. The next step is to execute that compiled artifact. For that, we use the resource manager to schedule the operation. operation. The resource manager allocates TPUs to that incoming Jax client, GANG schedules operations on all the assigned TPUs, and the TPUs pull the compiled artifacts from the cache. So as we can see here, the resource manager is the brain of the Pathways runtime. It has three main responsibilities. So first, it does TPU assignment to any incoming Jax client. Second, it does GANG scheduling of operations across all the assigned TPU workers. And finally, it performs health checks on TPU workers. So all of these properties enable a Pathways cluster to not just be able to detect failures, but also propagate them all the way up the stack to Jax and give this opportunity to Jax authors to write failure-resilient code. So we'll see how we can write elasticity primitives using standard Jax APIs. We are going to show one possible way how we use these primitives to build fault-tolerant code, which we call elastic training. But this is just one of the ways we are going to show the Jax author is free to choose however they want to adapt to elasticity. So in the demo, we'll see that we start training on three V5 V32 slices. We'll simulate a hardware failure on a randomly selected Pathways worker. We'll see training detect that error and reconfigure itself to two healthy slices. We'll eventually see GKE is able to reschedule our disrupted slice. And finally, we'll see training reabsorb that new slice and resume training on three slices. So here we are going to use the same Pathways job API, which Webhub demonstrated, to start a Pathways cluster. We are going to see all our pods are running. We have the same Pathways head pod and the Pathways worker pods. We are now going to navigate to Cloud Logging Console to see the logs from our application as well as the Pathways controller. So we see Pathways controller is reporting all three slices are ready, and our main jackshop also reports the same thing. So we are going to see that the training is starting to step on three slices, which are the three healthy slices at this time. We are going to let training continue for a few more steps before we interrupt it. Now we are going to back go back to our terminal and use kubectl drain on a randomly selected Pathways worker pod, which will evict all the pods from that node. So here we chose the worker 1.0 to be disrupted. So we see the same worker is going into terminating state. Now we go back to our logs and we'll see that error getting caught. So we see the Pathways resource manager has detected a worker has went down, and it's going to propagate that error all the way up the stack to jacks. So we see here the Pathways resource manager is reporting only two slices are healthy now, and our jacks job has reported one of the slice indexes are bad, and only two good slices remain. So all of this is happening without any restarts in our Python job. And now we are going to see training has resharded the state to two good slices, and training is again starting to step while one of the slices are not available. So training would continue for a few more steps on two slices while GKE reschedules a disrupted slice. So now we see the worker one which was disrupted is back to running state. So we'll see shortly that the resource manager would detect that new slice. So here we see that it has detected the third slice is healthy again. And the same thing would be reported on the main job as well. So we see here the main job has reported a new slice is available, and it's back to three good slices. So it's going to reshard the state from the previous two good slices to three good slices. And we see that has happened now, and the training is again starting to step on three slices, which was the original slice count. So before going into how to make your training loop elastic, let me talk about a new primitive which we are launching today called Elastic Manager as part of our open source GitHub repository Pathways Utils. So this primitive is something which you can use in any JAX training loop before starting to step. You simply need to initialize an Elastic Manager and configure it to how frequently you want to snapshot the state of your training, how frequently you want to check for new slices to be available, how many reshard events can your training accommodate over its lifetime, and so on. So let's see how to make a typical training loop work with elasticity. So this is typically how your training loop looks like. You just step through each step. A new thing we are introducing here is to snapshot the state of your training every so often. So for that, we have an API maybe snapshot in the Elastic Manager which would snapshot the state based on the frequency you have already defined. Next thing is to check for new slices to be available. So we are also going to use another API from Elastic Manager to maybe reshard up. So this would check if training is already running in a degraded state and if a new slice is available, reshard everything. Otherwise, it's a no-up. And finally, we wrap our training loop in a try-except block. This will allow a training loop to catch any error and pass it to the maybe reshard down API which will check if the failure was due to a hardware event and if that is the case, reshard down to only the available healthy slices. Otherwise, it would just rethrow the error back. So this shows how Pathways can enable a JAX developer to co-locate their fault-tolerant code alongside their model code. Failure resilience does not need to be an afterthought anymore and you don't need to handle it through any kind of external scripts. So finally, what are we shipping today with Pathways on Google Cloud? We are shipping some key building blocks in the form of Docker images for different Pathways components which can together create a Pathways cluster. we are shipping developer primitives like a new Pathways job API to simplify deployment of Pathways in Kubernetes clusters. We are going to ship a new co-located Python API in JAX to run Python code on TPUs and a new proxy backend in JAX for seamless migration of JAX workloads to Pathways. And finally, we are shipping elasticity primitives in our Pathways Utils GitHub repository besides other helper utilities. And finally, we have reference implementations for elastic training and for disaggregated serving in MaxTix and Jetstream respectively. So with all of this, we can't wait for you to try out Pathways and innovate with it. Now, I would like to welcome Kirat on stage who would talk about how Osmos is innovating with Pathways. Thank you, Sharia. All right. Good morning, everyone. Thanks again for 8.30 on the last day of the conference. The true believers. So let me tell you a little bit about what we do kind of set the stage for what problem we're trying to solve. Data ingestion, like taking a step to the left from AI for a second, if you think about large organizations, big data platforms, terabytes and petabytes of data, bringing that data in, running that data platform is an extremely painful process and that has historically been some mix of manual and programmatic. And so what Osmos is doing is we bring self-driving to your data ingestion. We automate your data platform and make it fully autonomous. So what we're seeing is a transition from data platforms kind of driven by humans with increasing AI assistance to flipping it upside down and saying the machine is autonomous, it goes, solves the problem, the human is the supervisor that's brought in as necessary to kind of keep things in check. And so what does that look like? We've built the world's first fully autonomous data wrangler and it's basically what it says on the tin, if you will. The idea is that you can point us to your source data, upload it, point us in your lake house, wherever that is, and we already know about your destination, what you want it to look like and you walk away and you come back to your data fully set up as you wanted it, ready to go. No kind of human toil, manual bits of Python, any of that required. And how does that actually work in practice? So behind the scenes, it's a team of agents that do everything from data analysis, sampling, to code generation, sandbox execution, end-to-end kind of doing the same thing a human would as they try to kind of work with data and reshape it and transform it. And the interesting technical part of this is that this entire multi-agent, multi-step architecture operates on a large Monte Carlo tree search sort of self-tuning tree that's guided by a process and outcome reward models. So in order to solve this problem, one of the key things you have to solve is your models have to dynamically figure out how much compute to throw at the problem depending on the complexity of the data you're dealing with. That is how you scale these systems to be, hey, I have simpler data, you throw less compute, I have complex data, you need to be able to spend more tokens to solve that problem. And that can mean choose a bigger model dynamically, or that can also mean run your inference for longer sequences and do more kind of quote-unquote thinking or reasoning, if you will. And so to train this, what we've done, so we've been working with MaxText for a while now, and so the first round of what we built is this entire tree, if you're familiar, will let you do easy rejection sampling, and so you can use that to do techniques like direct DPO, direct preference optimization, to do good path, bad path, learn, do better, right? Now, I'm simplifying a pretty complex end-to-end cycle, but that's the general idea, right? And you can do, effectively, it's a mild form of offline reinforcement learning. Where we're going is that we want to do more complex online reinforcement learning. We think there is a huge opportunity to go faster. But today, what that means is developers have to build the code, package it up for a multi-node script, and then deploy it and check the logs and iterate. It's an extremely painful process to get working and to experiment with. Every time you make a change, you're waiting for minutes for things to spool up and down, and it's painful. And where we're going with Pathways is a notebook interface. So, your reinforcement learning job for your 70B, 100B, 200B parameter models needs many, many accelerators because you're training and inference all at the same time. But now you can do it with an interactive experience with a notebook. And so, why does that matter? Right? Let's kind of stop, start from the bottom up. So, at the lowest layers, if you think about the reinforcement learning loop for language models, you end up with this, the naive implementation is basically, you've got two copies of the model and you are generating your inference outputs, writing them to disk, and then collecting a bunch of them, and then coming back around, loading the model from disk, doing it the other, you know, doing the backward pass based on your reward calculations, and so on and so forth. So, you're hitting storage every time. It's slow, it's painful, and it's resource intensive at scale. What you really want is you want to be able to make progress on both sides in parallel. You want your forward passes to be going on on one slice at the side of its own scale, right? So, if you're doing GRPO, for example, you'll maybe do 64 forward pass rollouts, calculate the reward, and then do a backward pass on that. And so, you can have two different slices with pathways and scale inference in the train side, the policy side, separately. And across all of that, you can move the weights around between the slices directly over DCN between the two slices. And so, you're not hitting storage every time, which means you can queue the operations on both sides and get parallel progress happening. This is basically unlocking the ability to do arbitrary-sized online reinforcement learning, multi-agent, multi-step. And so, the reason this matters is I hope everyone in this room has read the essay by Rich Sutton, The Bitter Lesson, if you haven't, highly recommended. Effectively, it has been clear over the last 30 years that general methods that prioritize compute over algorithmic tricks tend to win when it comes to machine learning. And so, we at Osmos have taken that to heart and we see that in two different ways. Number one, we want to simplify iteration for our developers so that they can iterate quickly, try more experiments, try larger experiments, try more experiments in parallel while, you know, keeping things economically viable, let's just say, right? And that reduced speed, the reduced pain and reduced interruption increases dev cycles, increases output, right, to the earlier point that was made around innovation cycles. And then, that brings you to the question of what is the metric we track? And the metric we're tracking is flops per developer. So, if you take a step back, traditionally, like, when I say traditionally, like, probably in the last few months, even, if you look at some of the state-of-the-art organizations, everyone's running slurm clusters and there's got a team managing the cluster and you're deploying jobs into it. The way we're able to operate with Pathways and MaxText is the research team is able to go straight to the cluster. And that lets us maximize the amount of flops we are deploying per developer. And effectively, it's a realization of the bitter lesson from an engineering perspective. So, thank you for coming. Thank you. Thank you.