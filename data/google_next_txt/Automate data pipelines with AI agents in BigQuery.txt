 Hello, hello, hello, everybody. Thank you for coming to today's session on how to automate data pipelines with AI agents in BigQuery. I'd like to introduce myself and my team here. My name is Michael Kilberry. I run product for AI ML and agents for data and analytics within Google Cloud. Hi. Good morning, everyone. My name is Terrence Yim. I am the technical lead in data integrations in BigQuery. Hi. I'm Matt Iams. I'm with Deloitte. And I'm a senior manager there and a data modernization and ML lead. Thanks. And you'll be hearing more from my team a little bit later today. And so what I wanted to do is walk through the agenda about what is the problem with today's data pipelines, and ultimately, how are we going to look to solve those with agentic behaviors? And then I'll pass it to Terrence. We'll walk through how we're actually looking to build that and do some really compelling demos to show you exactly what we have in mind. And then we'll also pass it to Matt to walk through how Deloitte will be using these AI agents across their portfolio to help streamline things like migrations, data management, and other operations they do on behalf of their customers. And then we'll end it with some Q&A. So to dive right into it, this is probably the most belabored slide in all of the week. You've heard this term a billion times already this year. But what is an agent? I'll say very succinctly, in our context, is looking to pursue goals on your behalf, complete tasks, and do it ideally in a proactive manner. Right now, many agents that you see are often very code-assistive or semi-autonomous. But in the long tail of time, we really see agents really doing true autonomous things, things on behalf of the user. And the things we'll be showing you today, and the vision that we have for data engineering agents, will be showcasing what ways we'll be playing in both assistive, semi-autonomous, and eventually autonomous manners, and how you'll see these agents progress. And there's no shortage of agents, as I mentioned. And you've probably already seen from our keynote, from Brad Calder and Yasmin Ahmad, my colleague, where they were showcasing a family of new data agents. And so data engineering agents is this one pillar across all these different data-centric agents. So they're doing really data-centric work on your behalf. And across the gamut of data types of workloads, you would have to do, whether it's data engineering, which we'll be covering today, or other sessions that were here this week, such as a data science agent, which is embedded inside of the Colab notebook, to help streamline how to do advanced analytical capabilities using data science, even if you don't know how to do it yourself. Another compelling vision that we'll be showcasing in other sessions here is a long-tail vision that we have around data governance agent, which really continues the work that we'll be showing you from a data engineering agent to help manage things like metadata management, catalog refresh, governance applications, et cetera, to remove a lot of that toil to managing your catalog and governance at scale. And lastly, one thing that we've actually launched for a little bit longer out relative to the rest of the agents is the conversational analytics agent, which just went in preview this week, which is really showcasing how to do data Q&A, being able to democratize insights to business users so you can have natural language conversations to pull those insights, not having to work through analysis team, but even do interesting things like forecast, multimodal search. So really compelling things that we're doing across the data suite with agents. So now I want to dive into what exactly are data engineering agents. Obviously, there's a lot of different data agents that introduce, but particularly for data engineering agents, what we're really focusing on is building that pipeline. You probably have heard of the medallion architecture, moving from bronze to silver to gold, or raw to stage to standardized business product ready data. Well, data engineering agents, their really sole goal is to automate and remove the toil from that complexity, looking to understand how pipelines are written within your organization and be able to learn that holistically and be able to apply that to new pipelines on your behalf. Or whether it's looking at how you manage thousands of pipelines, which can be a very cumbersome task to manage at scale. How do you do troubleshooting? How do you do proactive tuning? How do you make sure that the pipelines are running most effectively as possible? Data engineering agents are really looking to remove a lot of that toil that exists for the data engineering teams and humans at scale, and being able to do these things on your behalf. So whether it's building a pipeline using natural language, being able to use agentic reasoning over a pipeline failure job, and being able to assert why that failed and prescribe a fix to do that. Or even do things like learning about more, how one data pipeline is built, and then being able to elicit instructions for that to build thousands of other pipelines like that, to ultimately help scale this type of work. And if you think about the flywheel effect of data, moving from raw data to insights, there's never a time in history where businesses will run out of data to glean insights from. So we see this fortuitous flywheel effect of, as you get more data to process, the engineering agents can help move through that process of getting those raw data to be staged for insights to be delivered through other agentic means to the business users. And so I've already kind of teased this a little bit, but what is the actual problem of data engineering at whole? And you think about how long it takes for typical onboarding of new data sets, or maybe a request that comes in from a business user to that data engineering team. They typically have to process the ticket, understand what ultimately is being requested, understand the source system, write some logic code to do the extract, load, transform into a source system, or excuse me, into a destination system. And there's a lot of toil that exists within that. And there's often a lot of very manual and intensive time processes within there. Also, many of these systems often type into disparate connectivities, where you have external data that you need to pull in, or maybe you have pipelines across multiple areas, maybe within the side of BigQuery, maybe inside of other players, like DBT and et cetera. There's a large array of things that you need to manage within pipelines, and we're really looking at trying to streamline and remove the complexity from that. And so data engineering bottleneck, I kind of already alluded to this a little bit earlier, but you think about troubleshooting and how long it might take to troubleshoot a pipeline. You might see an error, but maybe you can't discover is that error from a downstream system, or is it something logical that's failed within the pipeline itself? And there's a lot of processes that will take for a data engineering team to be able to discover those issues and ultimately sort those. There's also often slow or a slow iteration on that, too. Think about where a data engineering team received a ticket from a bid analyst. They might have to ask clarifying questions to the bid analyst. They have to triage that ticket. It might take maybe a few days or a week to be able to complete that request, obviously slowing down time to get insights to the business. But ultimately, we're looking at providing a solution to this issue. And so I'd like to introduce really data engineering agents as a preview that's, excuse me, as an experimental release that's coming later this year. And it's really solely looking to solve a lot of these issues that I've just outlined. These agents are looking to act in a collaborative way with data engineering teams as they exist today and being able to scale their efforts to be able to do multiple pipeline management simultaneously using things like system instructions, natural language, or even work over a CLI API. We are looking to be able to have you define your data goals and be able to have the agent do that work on your behalf at scale. A little bit more about the data engineering agents and kind of giving you a little bit of a teaser about what Terrence will be showing in a little bit. So we talk about automating toilsome tasks. You think about what it takes to build a new pipeline. Again, you have a source system. You need to do some kind of business logic application. You might need to prepare that data, make sure it's the right syntax, make sure it's the right unit measures, et cetera. And there's a lot of processes within there. We have baked in some other capabilities already we launched prior. You might have seen we GA'd our AI data preparation tool. That's actually baked within the data engineering agent experience. We're able to invoke that tool to be able to build a node within your data pipeline to do data preparation. But we're not just stopping with that kind of toilsome task. We're looking at other aspects within the full gamut of data pipelines, whether it's feature extraction, classification, aggregation joins, other things beyond data preparation. We're also including that within the space of data engineering agents. And we're looking to make this really easy and powerful to use, whether it's natural language that you can invoke directly within the pipeline builder or being able to access that through different interfaces like CLI API, if you're looking to build this more programmatically within your organization. Also, it's looking to be context aware. As I already kind of mentioned earlier, one key capability with data engineering agents is we're looking to learn from your prior pipelines. So Terrence will be showing a demo later where we're able to have the agent look at a current pipeline and first system instructions, essentially the steps that was taken to prepare and organize that data, and then be able to use that system instructions and creation of thousands of other pipelines simultaneously. So again, being context aware of what your business is and being able to apply those business rules in a fast and efficient manner. And as I already mentioned, it's integrated, embedded, not just within BigQuery Studio, but also extended through CLI and API over time. And we're looking also at other potential areas too to partner with other data pipelines down the road. So what is the way we're looking to solve this problem, which I actually kind of already mentioned. But if you think about it here, looking at this large tail items that are solved with data engineering agents. So we look about low-code UI. Think about many data engineering agent, excuse me, many data engineers today, no SQL, maybe no SQLX. And they speak in a pipeline language, maybe in the DBT. But ultimately, we're also looking at expanding the means of how maybe there could potentially be self-service down the road for easy data preparation or easy pipeline building by other personas, not just maybe data engineers, but potentially opening up the door for general data workers. We're looking in the long tail too, you might have seen something like agent space, where you see agent space aggregating all of the agent experiences together for workers. We're also looking at potentials to integrate this there, where there might be needs for a data worker to do some level of data preparation and ask an agent to do that cleansing on their behalf. So we're looking at things like low-code and UI integrations as well to be able to handle that. But also, like the larger tail thing is, many data engineers struggle with doing creation of pipelines at scale, so bulk operations, being able to create hundreds of thousands of pipelines simultaneously. Whether it's you have onboarded a new vendor that maybe has 20 different sites that you need to pull in data streams from, and they all follow the same type of schema and business rules, but you don't want to manually build those out, and you just simply have a systems instructions for one of those pipelines, you can instruct the agent to create those pipelines with those settings of instructions across all those different sources, as I mentioned, again, really helping streamline how to do bulk operations at scale. And not just for creation, but also think about modify. Think about you have a downstream system that had a schema change or a data type change, and you need to change all of a sudden 1,000 pipelines. Like we quite cumbersome in many other systems, but the agent is able to intelligently operate over pipelines and understand and reason where they need to do those updates. And so I also kind of mentioned this a little bit earlier, but I kind of want to just double-click on this. The data engineering agents are really simple but powerful, and they're really going to be opened up over time to other personas. Initially, we're again focusing on the data engineer persona to help streamline and promote how fast they can build pipelines. But again, we do believe that there will be some potential here for some self-service aspects, whether it be for analysts or for data practitioners over time. Whether it's to be able to simplify data transformation and enrichment and onboard new data sets, or shorten ultimately the time it takes to get data into a business-critical system. But from there, I'd like to pass it over to my colleague Terence to actually walk through some of the features and a demo capability. Thank you. Thank you very much, Michael. So how are we going to build it, right? So first of all, when we think about typically what a data engineer needs to do, right, there are a couple of things. You know, you start with something, you want to drop a pipeline. You know, based on what you know of, maybe based on the requirements, you want to create a new pipeline, right? The pipeline can be simple or can be arbitrarily compressed. But oftentimes, you also have a need to modify existing pipeline, right? Because usually, oh, I have a bunch of pipelines already running in portions. Some of them might be, oh, maybe they need to be adjusted for the new data that comes in or some of the business route changed, right? So there's a need to change it. And oftentimes, when things are running in portions, right, they might fail. And you want to know why. And not just knowing why, but also be able to fix it, right? Like, for example, there's an upstream schema drift, and then now it doesn't match with the downstream tables, or it's starting to affect the data quality, right? So we want to figure that out and see, how should I adjust the pipeline to adapt to that? And also be able to figure out what are all the, you know, related pipelines that might be affected by it as well. And, you know, as an engineering, right, we like to automate stuff. And typically, a very simple way to automate stuff is you write a script and look through something, right? So we call it bulk, right? Oftentimes, we want to do some bulk operations. So the agent that we're building also will be exposed through a CLI and like a command line, as well as API, for easy scripting or integrations to your existing ecosystem as well. And last but not least, the pipeline builder that we are exposing is also through the Google Cloud console interface, so that you can have a visualized way of seeing, like step by step, when you're building your pipeline, when you're interacting with the agent. Okay, so as you can imagine, right, depending on the persona, you can use the agents and use the pipeline builder in various ways that can fit your needs, right? For data analysts, most of the time, you have some data, maybe sitting on GCS, the Google Cloud Storage, that, you know, you want to put the data in. And oftentimes, you need to clean the data, you need to transform schemers, you need to, you know, maybe remove some of the, you know, bad data or default some values before you can, you know, continue your job. So with this, with the agent help, it becomes a very simple task for the data analysts. For data engineers, most of the time, you know, you're kind of creating a multi-step pipeline, right, that you want to take the data from kind of the bronze to silver and then maybe silver to gold as well. So it's like a multiple step pipelines. And oftentimes, you also have a need to kind of build and manage and operate a lot of them as well, right? So that's where we can help as well. And also with the ability of, you know, BigQuery have access to, you know, many different type of data sources and data types. And our whole pipeline ecosystem is built on top of BigQuery. So you naturally enjoy all those capability from BigQuery as well. All right. So let's look at the live demo. All right. Yeah. Okay. Here we go. So first of all, we are at BigQuery Studio, right, on the screen in the Google Cloud Console. So let's say I want to start with a very simple thing. I want to create a new pipeline. Click on the pipeline. Just choose a service account so that, like, later on, when anything runs, it's run with this account's permission. I'll just give it a name. Let's put a demo today. Okay? So bear in mind, it's a real-life demo. So since we're in Vegas, so let's see. Let's hope everything works. Sorry, I have a little bit cheat sheet here so that I don't need to type everything. So let's say I want to ask the agent to help me to load the data from a GCS packet with some CSV file there. And I want to write the data into, you know, a BigQuery table. It seems like a simple task, right? So let's see what the agent can do for me. So the agent actually has accumulated some knowledge about your organizations, about from your existing pipelines, knowing that some of the common patterns and formats and ways how usually you do to clean your data. So here you can see the agent is, like, kind of thinking, and inspecting and working through. And then voila. You have a pipeline being created for you. You know, you have a source reading from the CSV file. And then followed by a step, we call data preparations. If we expand a little bit on what actually the agent is doing, you can see it's kind of inspecting your pipeline, inspecting your data sources, and then, like, creating some of the suggestive way of cleansing your data. Of course, looking at it, it's not fun. So if we click on this node, you can see, well, a very quick summary about what this node is trying to do. It's basically trimming some white spaces from sort of the columns, changing the category, the lowercase, et cetera, et cetera. And this is how the code look like. It's actually generated by the agent, right? It's in pipe syntax as a BigQuery SQL syntax. And furthermore, actually, you can see a preview of the data, how does it look like, and through the data preparation interface, so that you can actually see the data, how does it look like after every single step being applied. All right. So now, let's say I'm kind of happy with this. I can go back. And, however, I want to make one more change. I want to say, let's see, I want to trim some character. I want to trim the product descriptions to be less than 150 characters, right? So now the agent will, based on whatever the existing context and whatever the existing pipeline so far, right, and then add on to it, right? So this is probably pretty fast because my instruction is very direct about what it has to do. So, of course, naturally, you'll see one more step out of here, at the last one, to trim it less than 150 characters. Okay, last but not least, let's say, I actually want to join the table with another table to pull in some product review. So I want to know the rating of my product, right, based on another table that we collect separately. So now the agent get these instructions, but then it doesn't really know exactly how you want to join the data, right? As you're familiar with Zykl, there are various type of joins. So in this case, I want to keep all the product details and only fill in the columns product rating that has review rating on it. So say, here's a left join. All right, so now it's trying to validate how to do the join, what join key to use, whether it's valid or not. If it is, okay, it helps you to create a join in the pipeline. So now if we look at the data preview in the data preparations again, you can see an extra join step being added, and this is kind of a more graphical view of how the Zykl statement logically look like. It's basically like first reading from the product source, do some clean thing, and then join with the product reviews, and then finally write to a product details clean table. So, all nice right here. So let's say at the end, I'm happy with that. So remember, all these are proposed changes or creations by the agents. So after you review it, let's say you're happy with that, you can apply. So basically accept all the changes that are made by the agents, and now you have a pipeline that you can share, you can schedule it to run, or you can just run it immediately. All right. So now it's all good that we have a UI that somehow you can talk to the agent, do some stuff for you. But as I mentioned earlier, a lot of the time, we have to do things more than just one pipeline at a time. There's also time that we actually don't use a graphical user interface, but rather we write the code directly or we manage the code directly. So how do we do that? So let's switch to a command line interface. All right. So I have some cheat code here as well because I don't want to type all of this. So let's say I have an existing pipeline that's sitting in this repository. It's big enough, right? Okay. It's sitting in this repository. So I want to use the command line tools to say, oh, agent, summarize this pipeline for me, and write out the instructions to a local file. So now what the agent is doing is it will basically fetch the pipeline that I specified there in that repository, analyze it, and based on that to create a set of instructions that is kind of summarizing what the pipeline is trying to do, as in here. Okay. So now that's not it, right? So now let's say I have this instruction file. I want to create a new pipeline. So what I'm going to do is I create a kind of an empty local directory here. All right. So it's a good repo. And then I ask the agent to help me to create a new pipeline based on that instructions. However, with some modifications. All right. So in the modifications, I say, oh, the product review is actually coming from a GCS bucket instead of from an existing BigQuery table. And also the categories columns would be uppercase instead of lowercase. As we saw earlier, it's lowercase, right? And then for a product name, I want to default the value to unknown if the, you know, column is null. So now the agent will text the instructions together with this kind of context prompt for this particular pipeline to create a new pipeline. So what happens behind the scene is all this will get, of course, sent to the agent in a, from the CLI. And the agent will kind of create a pipeline and kind of send back all the details back here. So now a new pipeline is being created. So if I do a div, so these are the changes that are made. And of course, it's all happening on the, on your local disk. And so you can use your, you know, own favorite gig tool and the gig provider. You can create like a full request on it. You can reveal, you can change. But pretty much like you have all the files and changes here. And if you look at the changes, like a couple of things that's being done by the agent, for example, the category is using uppercase instead of lowercase. And in fact, it noticed that there's a where course that happens afterwards, right? A couple of steps later, that used to filter the category in lowercase. But since you ask the agent to change the category column to uppercase, it knows where to find the related columns and change the, and change the condition accordingly, right? Because otherwise you'll be like, you'll be filtering out wrong, right? If you don't, if it doesn't do this step for you. All right. So it's all good. Now, now you have a local pipeline. So let's do something a little bit more. So let's say I have a file. That has, for this one, I only have like two different gs path. One is from a vendor to some name that I make up. The other path is some files that, you know, it's like a drop zone, right? That allows the vendor to drop the files there. And I have a vendor for it. So I created a very like small script here. Let me just paste it out. That simply just do a while loop. It's a simple standard shell script, right? That do a while loop reading in the file to read in the gs file path. And then for each path, it's basically kind of temperatized a little bit to construct this command to say, I want to create new pipeline with different name based on the same instructions, but with the prompt override like this. Because I need to override in a way that I want to say, oh, the source is actually coming from different gs path, right? Based on each iteration of the loop. And then I want it to create like different pipelines for me. So now if I hit enter. So what it does is, it's still doing the same pretty much as before, but now it's like happening in a loop. And also the end result is being written back to BigQuery data form and also like creating a pipeline underneath for you. So now if I open the first one. So data form is our kind of a CI CD response free solution on BigQuery. So you can also see like the file being created here. And you can see it's creating. Okay, this is from a different table, right? You see it's a pretty six. Turn table name vendor two from the vendor two path, right? And then it has the pipeline in sequence format. They're written here. You can also look at like a more graphical representation of that. That you can see the, how does the pipeline look like? All right, so here it is, right? And same if I click on the free, just quickly show you that I'm not lying to you guys. That this one should be saying vendor free here. Okay, so you can see with the CI tools, with a very like little scripting actually can help you to create a variety of pipeline that's similar, but yet they're going to process like different data from different data sources. All right, so let's get back to the slide. All right, so one last thing that I want to also point out, this one we don't have a live demo yet, is the troubleshooting part, right? As mentioned earlier, when you start to schedule and have the pipeline to run, right? You would like to see the history of each of the runs, whether it's a seed or fail. You might also want to turn on the data qualities metrics so that you have the metrics to monitor to see, oh, if all the pipelines are still healthy or should I need to make some changes to that, right? Whenever you detect some errors and that you need to kind of analyze what the problem is, you can actually go to the page and then say, oh, there's some error. I want to see what happened. I want to talk to the agent to that. An agent will be able to kind of pull out all the related runs of the pipelines and inspect the errors and suggest to you how to change it, right? And then you can review the changes. If you're happy with that, you can apply the changes and then deploy the pipeline again. All right, so that's the wrap for the demo. I hope you guys enjoy it. And so we are going to be running an experimental trial very soon. And this is the QR code that you can scan and that will bring you to a sign-up form. If you're interested, please do so. And then for what I'm going to do next, I'm going to hand it to Matt. Matt S.: Thank you, Terrence. I'll give you guys a few seconds to scan. Matt S.: Three, two, one. Matt S.: All right. We're going to talk about a situation where technologies like these are already being employed and this will make your job of doing the same much easier. Matt S.: Specifically, so Michael had mentioned that many enterprises will have tens if not hundreds of thousands of data pipelines, similar to the BigQuery pipelines that Terrence had demoed. Matt S.: Now, a portion of those are going to be regulatory reports. Matt S.: And these span industries, though they are more important to some industries than others. Matt S.: You can think of healthcare, for instance. Matt S.: They will need to report on their uncompensated care, maybe Medicaid, Medicare recipients. Matt S.: And especially if they're nonprofit, much of their funding could be dependent on this. Matt S.: And you're going to see something similar for utilities, especially REAs. Matt S.: Specifically, we're going to dive into banking. Matt S.: And regulatory reporting in banking is extremely important because both state, federal governments, as well as local governments, are very interested in both the capitalization and the liquidity of these financial institutions. Matt S.: And I should say that this goes similar for insurance as well. Matt S.: And the number of reports, the size of the reports, and how often you have to file those reports is very dependent on your assets under management. Matt S.: So you will move up the chain based on those assets under management from up to CAT 1, which is kind of the biggest of the big. Matt S.: And they have the most stringent financial regulations. Matt S.: And what is very nice about this is we, it was demoed how you can create a report or a pipeline. Matt S.: And you can also change a pipeline. Matt S.: Once you get up to the very large financial institutions, many of the reporting requirements can be daily. Matt S.: And they're extremely complex, which means they take a while to run. Matt S.: They are dependent on many critical data elements, which are then dependent on others upstream of it, upstream of them. Matt S.: So these are large chained pipelines. Matt S.: And you can imagine that if it needs to be done daily and your pipeline takes hours to run, if it comes out and it is not good, Matt S.: You may only have a few hours to make changes and then start running it again so that you can finish it in time to make that daily report. Matt S.: So you can see how any ability to edit a pipeline quickly can be, can basically make your life easier, but also save companies millions of dollars from fines. Matt S.: So we gave finance as an example. Matt S.: And now we're going to talk about how a financial institution would use an automated pipeline capability. Matt S.: So you can see highlighted there, we are using this tool to basically map data, map your data model that you have to the, Matt S.: The report and we're going to be, we're going to basically be creating both the pipeline as well as many companies will have a quality dashboard. Matt S.: So they actually monitor everything that comes out of there and says, hey, did you pass this test? Did it fail? Matt S.: And they're running those daily, sometimes hourly. Matt S.: But if we look at this in the entire chain of events on what you can agentify. Matt S.: The example above, and it's difficult to see, it looks like a tax form. Matt S.: So that is a regulatory report from the federal government. Matt S.: And it will basically come in, it looks like a tax form. Matt S.: You have to fill that in with critical data elements from your pipeline. Matt S.: And they will usually give you a section, basically another document, we call them instructions, Matt S.: That basically show you how to do this. Matt S.: Now, those instructions, they're basically written in plain English, but sometimes going from those instructions to actually getting the logic to fill in that form can be complex, Matt S.: Especially if you have to make last minute changes. Matt S.: So we're able to basically take that, the form and the instructions, ingest it into an agent, and it will find the critical data elements that it sees are necessary per the instructions. Matt S.: So now you have your CDEs for the report. Matt S.: The next piece is we then map it to what CDEs in your data model are needed to fill in these report required CDEs. Matt S.: Once you do that, you're ready to build your pipeline. Matt S.: And it's basically as easy as Terence had demoed. Matt S.: The next piece is, of course, we have in there building your data quality rules and launching those such that you can see when they're run on a given schedule, Matt S.: Whenever your BigQuery pipelines runs, that it populates a quality dashboard similar to the one you see on the bottom left. Matt S.: Oh. Matt S.: So. Matt S.: So, in summary, we basically went through and we said, hey, we picked an industry, and this cuts across all industries. Matt S.: It was just finance and insurance have some very critical reports that they need to do. Matt S.: But we cut through and we basically said, hey, how do I go from getting a reporting requirement to building the data pipeline to building the quality rules on top of it? Matt S.: And with technologies like this, it's making it such that we can actually build those at the speed in which we need to report to federal government and other organizations. Matt S.: I really appreciate everyone's time. Matt S.: Have a great rest of the next. Matt S.: Thanks. Matt S.: How?