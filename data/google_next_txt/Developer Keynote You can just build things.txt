 SCAFFING Thank you. Developers, please welcome to the stage Brad Calder. All right. Hi, everyone, and welcome to the Developer Keynote. All right. What we've been waiting for. I'm excited to talk to you today about how Google Cloud is transforming software development. We're innovating in three key areas. First, we're unlocking your ability to build the next generation of agentic applications. We're empowering you to build agents that are AI-powered services. These agents can plan, reason, learn, collaborate, and act to achieve goals on behalf of users or in concert with other agents. Now, these agents can be incorporated to be a key part of your application and seamlessly integrated with the rest of your cloud stack. And at Next, we're empowering you to build and deploy these agents with our new framework and products. So we're very excited to introduce our agent development kit to help you build your agents, our agent engine to help you run your agents, and agent space to help users interact with your agents. Now, together, these allow you to create applications where agents and users can work together to achieve a common goal. Second, we are helping you be the most productive software engineers with our Code Assist and Cloud Assist agents. These agents allow you to accelerate development and streamline your cloud operations across your entire software development lifecycle. Then, finally, all of this is powered by our Gemini models, allowing you to leverage Gemini's massive context window and multimodal support. This allows seamless integration of video, images, and voice with your data, creating deeply immersive, real-time insights and experiences. And in today's keynote, we're very excited to demonstrate how you can benefit from all of these innovations. And to do that, I have two very special guests coming to the stage. Please welcome Richard Schroeder and Stephanie Wong. All right. Thank you, Brad. It is great to be here again. Shout out to all of you here in Las Vegas, everyone on the live stream watching, and the hundreds of people backstage who have done heroic work making this event happen. So thank you, all of them. So what kind of Google things do we have in store today, Stephanie? Yeah, well, we have heard from so many devs that they want to see the best ways Google Cloud can help them build great software. So that's what we're going to showcase today. From getting started to scaling and ultimately a sneak peek at the future with some of our newest creations. Awesome. This whole thing is a bunch of demos, so we're going to get started on this. So set us up, Steph. What are we about to see first? Well, we are going to show developers how they can build things, including agents, with Google Cloud and Gemini. It starts with the models. In fact, Gemini still offers developers one of the longest context windows and bi-directional multimodal capabilities. And you can all try out our latest models, including Gemini 2.5 Pro in AI Studio. So let's see this in action. All right. To kick things off, I would like to welcome to the demo desk Paige Bailey and Logan Kilpatrick. Hey, everyone. In the next few minutes, we're going to showcase how you can build AI-enabled applications with Gemini. So where does building with AI start today? Well, for Paige and I, it often starts with a simple question. Can the model actually help solve this problem that we have in our head? Yep. And that's exactly where Google AI Studio comes in. It's a place where you can rapidly prototype anything. But, Logan, do you want to tell us what's new before we dive in, like that UI refresh from Master Day? Yeah. I think the question is always what's not new in AI Studio. So we just shipped a brand new UI, which has been awesome. We have all the latest Gemini experimental models. We have Google Search as a tool, which works identically. And even new model capabilities like native image editing and generation, which we'll see in a second. But, Paige, I think you had an idea for an AI app that you potentially wanted to try and see if we could build. Absolutely. Today, we're going to be multitasking. I have been wanting to remodel my kitchen for the longest time. And I bet Gemini can help. But, you know, I'm an engineer, not a general contractor. And I don't even know where to start. You know, there's all sorts of things from regulations and fixtures, electrician work. You know, I really feel like AI could help here. Yeah, I think I can. So let's use Gemini to break this problem down. We'll have Gemini sort of generate a super detailed starter prompt for us. We'll have it put together a renovation plan. And last, we'll see if we can use Gemini to actually visualize what that remodel might look like. Awesome. Three steps. Let's head over to Google AI Studio and get started. Awesome. So you can see here a very, very detailed prompt that Gemini has created only from the short description that I've given the model. You can see it's including everything from my current kitchen dimensions to things like budget, project scope, countertops, cabinetry, all sorts of details that I would have never even considered that can help us get started with this renovation project. You can also see models here off to the right. And when you hover over them, you can get additional insights into things like latency, what the model might be really great at, and also really nice details like costs. Awesome. Paige, so let's take this prompt that Gemini actually generated for us. We'll copy it. We'll jump over into another tab, which has a bunch of pictures of your kitchen, a picture of the floor plan, a bunch of other unstructured data, and we'll send that prompt off to Gemini and have it sort of kickstart this renovation for us. Awesome. So I'm running that now. A bunch of things are happening behind the scenes. This is a thinking model, which means that we get 65,000 tokens of output, which are great for long-form analyses and detailed docs. We've got our super long prompt that's giving a lot of really nice insights. And then also this mysterious thinking box. So, Logan, break down what's happening behind the scenes as Gemini is doing all this work. Yeah. So we're using a reasoning model, which means that before the model actually gives us this final renovation plan, it's trying to think about all the different things that we might need to take into account. And as you mentioned before, Paige, you and I have never done a kitchen renovation before. So this is a great example of why reasoning is so powerful. And it'll hopefully come up with a bunch of stuff that you or I never would have been able to. Awesome. So in this thinking section, we're seeing an information gathering strategy. It's pulling in things like Seattle costs and local regulations. It's giving me pros and cons about the specific materials, assumptions about the layout just based on this floor plan that we sketched out. And this is still just the thinking box. We haven't even gotten to the main model output yet. This is amazing. So if we keep scrolling, we can start seeing the major output. But you know, Logan, oh, wow, it's even done a little sketch in the kitchen. This is amazing. That's awesome. Yeah. So, you know, budget is everything. So how can I make sure that this renovation project would actually fit in my budget? Yeah. Great question. So we're using grounding with Google search, which means for all the information which the model might not have, you know, recently trained on, like things like up-to-date material costs, even like local regulations and codes, which can always be changing. The model can pull that information in and actually make this not only like a theoretical renovation plan, a super practical one that's grounded in reality. Amazing. And it looks like Gemini is still cooking. You know, I would really love to be able to visualize some of these things that it's describing. And you mentioned that Gemini might be able to help with that, too, right? Yeah. I don't think an AI renovation use case would be complete if we couldn't actually see what that renovation is going to look like. So let's jump over to your third tab. And we have a picture of the kitchen. And I think this was another super detailed prompt that Gemini actually generated for you. Yes. Gemini was able to cook up a really, really nice prompt, very detailed with all of the different features that I would want to use to reimagine my kitchen. Nice. Yeah. And so let's hit run again. And hopefully we'll see a result in just a few seconds. Yeah. And we're using Gemini 2.0 Flash. So hopefully this actually literally only takes a few seconds. Amazing. So I love this. It's been able to take my layout. You can still see that door off to the left that we had off to the left in the original picture. It's given me a new stove, this beautiful pewter green backsplash, which is a new word that I learned. And also it looks super nice and airy, but I don't see a lot of lighting. I'm going to ask for Gemini to add tube globe lights. And that's something that it should be able to do as well, right? Yeah, this is actually a really great example in practice. If you saw that first prompt, super deep, detailed, really verbose. For the second prompt, we can also just ask it to very explicitly change hopefully just one thing, which is going to be these lights up in the top. Yeah. I'm also curious to see what kinds it picks. It's one of the fun parts of playing with this model. I love that. That's awesome. Yeah, look at that. The beautiful globes. Amazing. And this is excellent. I really am excited now to kind of bundle this up, send it over to a general contractor and get started. Yeah, I love this. And really this is the power of Gemini all coming together from understanding videos to native image generation to grounding real information with Google search. These are the kinds of things that you can only build with Gemini. Absolutely. It all starts in AI studio. From here, we can take this prototype. We can go ahead and grab an API key and we can scale this out into a full application using the power of Google Cloud and Vertex AI. So thank you so much for being my renovation buddy, Logan. Yeah, this was a lot of fun, Paige. Yeah, next time we're doing your kitchen. I hope so. Thank you. Nice. That was great. I feel like that original kitchen has seen some things. I don't know. It might have. Makes me want to still DIY my own project though, maybe. Yeah. Amazing. Yeah, I might legitimately start here with my next home improvement project. That was a pretty rad use case and not a way I would have thought of using AI myself. Okay, so what you're saying is you're not just going to keep watching old reruns of this old house? I'd have to watch it the first time to see a rerun. Well, Paige, can you tell me more about what you use Gemini's long context window for? Absolutely. So long context windows are a game changer when you really know how to use them. In this example, we saw some things like photos and images and a few different sketches. But with long context, you're able to pull in full videos or even full code bases to use in your projects. And I use it a lot, especially when kind of pulling things in to be able to generate really, really detailed code generation in my favorite IDs. That's great. Yeah, I mean, you showed how easy it is to get started, which I love. But at the same time, the more you learn about the features, the better the whole experience gets as the team keeps building some great things here. I'm really excited about some of the things you unlock with native image generation. Tell me more there. You can change floor plans. You can change different spaces. You know, I loved it for my condo renovation. But it really gets magical when you can imagine all of the different kinds of projects that you can pull in. I especially loved that we were able to pull in search as a tool and search grounding. And when you're able to add all of these agents into your projects, they're great for automating these multi-step processes. Yeah, it's amazing. I do love that multimodal reasoning. That long context is such a big deal. But, Steph, I am more of a software engineer than a construction engineer. And I'm kind of psyched about the coding in Gemini 2.5 Pro. Amazing at coding. Yeah, I mean, I think we're all kind of happy that you're not a construction engineer. I don't know if you'd be standing here right now. The world is a safer place. Well, we're seeing so many developers swarm to this model because of the quality of the coding responses. So, Logan, I got to ask, how did this happen? Yeah, it's a great question. I know we didn't show it in this demo. All four of us, though, have been spending a bunch of our time playing around with the new coding capabilities. I think it's honestly a story of all these unique Gemini functionalities coming together. It's long context. It's natively multimodal. It's, you know, bringing in native search. I think you can do a bunch of stuff for code use cases that weren't possible. Plus, innovation in pre-training, post-training, reasoning. So much stuff. Well, thank you so much, Logan and Paige. Thank you. Great. All right. Here we go. It's a good start. I love that we're building for builders in this whole show. I could not have planned this any better. Ten out of ten. No notes. But what's next? We actually use Gemini here to help us with how we might restyle our place. But what could an agent do here? How do we do something that would help a business scale this type of thing? Well, agents can automate processes, unlock entirely new use cases, and improve the way we do the things that we already do every day. Yeah. The million-dollar question is what in the world is an agent? How are you defining that? Yeah, we should probably start there. Well, an agent is a service that talks to an AI model to perform a goal-based operation using the tools and contacts that it has. And agents can help us accomplish many goals. Some agents will automate tasks that we need to scale, and some agents will help us solve complex problems, either autonomously or by asking for human help when needed. I see. So building an agent is a lot like building a service, but it has some of those unique traits you mentioned that push an agent beyond just a classic cron job or a web service, no? Yep. That's exactly right. Got it. In this next demo, we'll create an agent to help contractors by handling tasks like verifying building codes and looking up permits. As input, it'll use Paige's ideas and any supporting documents like floor plans. The agent's goal is to generate a PDF proposal we can share with Paige to hopefully win her business. Man, that sounds like a giant time saver. What is actually the best way to build an agent? Well, you start with Vertex AI. Of course you do. Vertex AI is our end-to-end platform for building and managing AI applications and agents, as well as model training and deployment. All right. Since all I apparently have is questions for you, now that we are using Vertex AI to build an agent, how do we actually get started? Well, a great question, Serge. That's all I got. You're lucky I'm here. That's all I got. Agent Development Kit. It's new, open source, model agnostic, and supports model context protocol. Great. We're making it easier for developers to build agents. Let's see how. Please welcome to the demo stage, Dr. Fran Hinkelman. Thank you, Richard and Stephanie. I'm here to show you all the goodness of the Agent Development Kit, ADK, that we just released. To build an agent in ADK with Gemini and Vertex AI, we need three things. We need an instruction, we need tools, and we need a model. The instruction defines the agent's goal. The tools enable the agent to perform actions beyond a plain LLM, the additional function and API calls. And the model handles all LLM tasks and is responsible for calling the tools. In ADK, we support the model context protocol. My favorite protocol. MCP creates a standardized structure and format for all the information an LLM needs to process a data request. This allows our agent to use tools for retrieval augmented generation, also known as RAC. ADK is a Python SDK. Let's write some code. Here I am in Cloud Shell Editor using ADK. The first thing I need to do before we define an agent is to connect to Gemini in Vertex AI. This is very easy to do with a .end file. I can either configure my Google Cloud project or I can easily access Vertex AI using an API key. Okay. With that established, let's look at our agent code. Remember, I said an agent needs instruction, tools, and a model. We define our agent's goal using an instruction. It's the foundation of any agent. We describe in natural language what the agent will do. I use Gemini in Vertex AI to help create this instruction. It's quite detailed because we need to cover a lot of edge cases. For this agent, the instruction focuses on taking a customer request and creating a PDF proposal. And we set the instruction right here in our agent constructor. Next, let's explore tools. We're going to add this analyze building codes tool. It allows our agent to perform RAC by accessing our own private data set for local building codes. The analyze building codes function defines the tool's arguments and return value. And the agent is going to decide on its own when it needs to call this function. So it needs to understand what the function does. And we use this doc string here using natural language to describe what the tool is doing. And this is how the agent knows what the function does and when to call it. The function takes a building feature like plumbing or windows. It searches our RAC database for local codes. And then it returns a summary. Clear doc strings are vital. They guide the AI model in selecting the right tool. To perform RAC, we need to retrieve information from outside the agent. For this, I used a model context protocol server from Google's MCP toolbox for databases, which we contributed to open source. And I deployed it to this endpoint here. The analyze building codes function uses RAC to ground the agent's responses in data from our database. With this, we've defined a tool for our agent to accomplish its goal. Okay. Since we're building an AI agent, we need an AI model from Vertex AI. ADK is model agnostic. So I could use models like Lama or Claude, but I'm using Gemini 2.5. And the model is a key part of the agent because the model handles the execution of the tools we defined and it produces the output we are looking for. A proposal for pages remodel. In essence, building an agent in ADK boils down to just three things. Instructions, tools, and a model. Okay. Time to test the agent. Let's open the terminal. ADK comes with a dev UI for local development. You call it by running ADK web. Okay. Open local host. Let's select our agent. The dev UI handles multimodal inputs like images for us. So I'm uploading Paige's ideas and her floor plan. Please create a proposal. We will integrate this UI with Gemini Code Assist and its supported IDEs later this year. Okay. Gave us an answer. Wow. Look at this. It created this whole professionalpedia. What a massive time saver. Agent DevKit is public today. I want you all to build something with agents on Gemini that makes your life easier. Thanks so much. Great job, Fran. Awesome stuff. Yeah, that was terrific. I love seeing that. Yeah, I mean, it's really wild to see how easily Fran could take some of the fun ideas from Paige and Logan's demo and make an enterprise class agent from them. I mean, it's not very hard to see how this could be useful for lots of other tasks and businesses. Anything that requires just a lot of reasoning, no? Oh, there's so many things we can build with agents and they really unlock a whole new level of automation. I thought the agent development kit looked pretty cool. Right? You all like that? That's pretty cool. Yeah, there we go. A round of applause for agent development kit. Was there anything unique, though, that you had to think about differently when building with it? I know you hit on those three things that make up an agent, but was there anything otherwise you had to think about? I mean, at the heart, it's just code. Prompt engineering is really important, so be really explicit with your doc strings and your instructions. That's good advice for everybody. All right. Thank you very much, Fran. Terrific job. All right, Richard. Well, let's shift a little bit to talk about how developers are building even more with our products and frameworks. Yeah? Yeah. Yeah. Let's talk about one of my personal favorites or so I'm told, Vertex AI Agent Engine, which was recently made generally available. It was. And Agent Engine makes it easy for you to deploy and run agents built on any agent framework. It simplifies the deployment process so that you can focus on your agent's code and provides you enterprise-grade security controls, production-grade monitoring and logging, and even evaluation and quality frameworks. And, Richard, what else do we have for agent discovery and sharing? You want more? It's all agents. Yeah, always. To make life easier for devs, we actually have something pretty awesome called Agent Space. You've been hearing about that this week. It's our hub for agents across a company. As you saw in the demo in the main keynote yesterday, you can build these no-code agents directly in Agent Space. And you can register agents built with the agent development kit and make them available here. That's good. We can also use this to share agents that are built by developers and can be surfaced to all the employees within their own company or to all companies that use agent space from the cloud marketplace. And some of the best part is that this supports third-party models and agents and provides a common surface for access control across all of your users. So, you might have all heard about teams of agents working together. These combination of agents are able to take on complex tasks with both high agency and high automation, as we're about to see next. So, let's see some cool new things we have for building multi-agent systems in Vertex AI, managing agent systems at scale, and some of our newest tech to help with debugging and fixing our agent services. Let's make some noise and welcome to the stage, Dr. Aburami Sukumaran. Thanks, Tiffany. Thanks, Richard. Hi, everybody. Hi, everybody. Complex processes require more than one agent. So, we're going to create a system of multiple specialized agents and go through the process of deploying and debugging a multi-agent system. Now, we've already got the construction proposal agent that generates client proposals. Next, we'll add two more agents, one for permits and compliance, and another for ordering and delivering materials, both built with Vertex AI using agent development kit with Gemini. We'll need these three agents to work seamlessly together as a single agent that can handle the end-to-end process. Let's check out the code to see how we use ADK, that is agent development kit, to build and orchestrate a multi-agent system. We start out by defining our root agent. So, this can call one or more sub-agents based on the goal. First, we'll start with our instructions. This is very similar to how we do with a single agent. But since this is a multi-agent system, we'll give additional instructions that define how these additional sub-agents will be used and routed to all in natural language. And define any additional instructions or dependencies that are required between these sub-agents right here. Next, we'll need to declare these agents as sub-agents. This is similar to how we declare tools. Here you see three sub-agents highlighted. Proposal, permits, and ordering agent. All right. When my agent is ready, I can deploy it directly from ADK to Vertex AI agent engine. It is a fully managed agent runtime that supports many agent frameworks, including ADK. Here we have the code snippet that allows me to deploy my multi-agent system to run on agent engine. Once I execute this, in a few minutes, our agent will be deployed and available to call and share across apps and users. The call to create returns an endpoint, which looks something like this. Perfect. Perfect. Now that our agent is deployed and running, let's test it out in a place where I want to make it available to other users, which is... You heard it. Agent Space. Here I am at the Agent Space console, where we can see all of the agents available to me in the left pane right here. I'm going to go to renovation agent, which I just deployed. So, to start off, let me start uploading the proposal document that was generated in the previous demo, and I'll start interacting with the multi-agent system. I'll ask it to kick off permits process. The renovation agent will route our request to the right sub-agent based on the instructions we have specified. Great. It's already created a checklist of permits. Now, let's say I want to ask it something related to ordering. I'll ask it to check order status for some of the materials that I've previously ordered. This should call... Oh, snap. All right. Looks like my renovation agent has a bug in it. Well, since I've deployed my multi-agent system to Agent Engine, and since it uses tools from this Google Cloud project, I'm going to go to Cloud Logging to see if I have any information that helps me debug. Let's go over to Cloud Logging. Logs Explorer. There it is. Yeah, that is why I have a bunch of red. All right. That's a long wall of exception texts. I see Investigate. I click that and create investigation. This is a new feature in Cloud Assist called Cloud Investigations, which helps diagnose problems with infrastructure and even issues in your code. Once the investigation is complete, I'll click view investigation, and the report is open. I have seven relevant observations in the report and a hypothesis section. Let's see what the observations are. All right. It has identified the Postgres SQL column not found, and that's status. And it's also identified the column name that is status. And also, it takes me to the source code to debug it. And it has a direct link to make a Gemini-suggested code edit and make the fix. I can navigate over. So, in the hypothesis section, I also have the overview of the problem. I have recommended fixes along with a direct link to Gemini-suggested code edit, which will directly allow me to make the fix. Let's navigate over. Here we are. As you can see right here, it has the comparison and a side-by-side view of the current buggy version of the code, which has the wrong column referenced. And on the right-hand side, I have the fix to it that is Gemini-suggested edit. And the right column that I was supposed to reference was order underscore status. I have incorrectly referenced it as status. I like the fix. I'm going to accept code suggestion and save and redeploy. All right. In about a couple of minutes, it will be deployed. How cool is it that I saved so much of my debugging time with cloud investigations? Now that I've fixed our error, once it's deployed in about a minute, I could go and retest the agent and share it out to users without having to make any change in the multi-agent system itself. So, building, sharing, and even debugging complex agent systems got so easy with agent development kit, vertex AI, and agent space, along with cloud assist investigations. Right? So excited to see what you'll all build. Thank you. Wow. That was amazing. Tech influencers are just like us. Their code doesn't work either. That's amazing. I mean, it's easy to see how the scale and number of agents can really grow, right? Bringing together agents seems like it could be kind of complex, but you actually made it look pretty easy. It did. So, what did you learn using agent development kit that can help all devs build multi-agent systems? Right. I personally have three key takeaways from building this multi-agent system with vertex AI. First, it's important to have a proper understanding and reasoning for the specialization of each of the sub-agents. Second, do you know, like, what is the root agent? How are you going to connect and route between these and make sense of all the responses and gather the results? And finally, I learned I could do multiple types of agent routing, handle context, state, and do a lot more with agent development kit, especially when I have to do complex flow control. Well, you absolutely had a great example, Avi, of local agents working together. Yeah, it was great. And, you know, to make it easier to connect any agents together, we actually just launched that agent-to-agent protocol. Just like MCP standardizes how agents connect to and use tools, A to A makes it easier to discover and connect agents, especially when those agents are from different ecosystems or vendors. And it's all open source. We're building this with over 50 other companies, so you can build agents on any of those platforms, and those agents can communicate and collaborate to help you achieve the goal. That's really cool. I showed how we can have a bunch of agents built on Vertex AI work together to accomplish a goal. But in the future, I know we'll want to use agents that are built with different frameworks and different vendors, and I'm really thrilled and excited that Google is actually working with so many others to create a standard for exactly that. Yeah. Now, I know we're obsessed with agents, but you did sneak something in there about cloud investigations, and why do the developers in this room actually care about this? What does this really help us do? It's hard enough to build systems that orchestrate complex agents and services. Developers shouldn't have to sit around debugging and all these multiple dependencies, getting to the logs, going through the code. And all of this can take a lot of time and resources that developers typically don't have. So that's where cloud investigations can save the day. It helps identify and fix code across the services of a multi-agent system so we can quickly get an app back up and running. All right. Well, everyone, give it up for Abby. Thank you. Amazing. That was so great. All right. So, so far, we've shown you lots of ways to just build things with Google and AI, but we also do love the ecosystem, and devs love using what's new and exciting, and frankly, what's the most useful for a specific job. It's the nature of how we build modern software. Gemini is actually so powerful in Google Cloud and our tools, but next, we're going to show you how to take advantage of Gemini from your other favorite IDEs and tools. All right. Wait. Hold on. Hold on. Richard. Yeah. Let's not forget the other way we're enabling developer choice. What else did I forget? Vertex AI Model Garden. Hello. Going to the shed. That's terrible. It's enabled over 200 models to date to be deployable for developers worldwide. You can choose what models work best for each task. Got it. With that, let us see how building with Google AI gives you unprecedented choice in both IDEs and the models. Please welcome Debbie Cabrera to the stage. Thanks, Richard. How's everybody doing? Can I get a woo? Woo! Woo! Woo! Wow. Okay. That was great. You know what else is great? Google is empowering developers to use the tools and models that you like best to build amazing apps. First, we'll showcase how you can use Gemini in your IDE of choice, and then how you can bring your model of choice to Google Cloud for your apps. Let's start with our IDE experiences and pretend that we're building a new service that'll be used for our agents to help with budgets. We'll use three popular IDEs that are enabled to use Gemini. First up, have you heard of vibe coding? Yeah. People are doing a lot of it and more with Windsor, a new and popular IDE with a streamlined experience that focuses on agentic coding. Here, we can chat with Gemini 2.5 Pro to start creating code powered by AI. You can ask something like, build me a Java microservice app with sample services for budgeting. The model will then iterate with you a lot over multiple steps. It might prompt you for commands to run in the terminal and even create some files on your behalf. It's really simple to go from zero to template and continue building from there in Windsor. Huh. Looks like I'll go to my backup machine because Gemini got a little tired over here. I want to show you what it built. To get there, I'm going to take the escalator. And I'll take the elevator up. Okay. Looks like it had a little bit more energy on this side. There you go. You can see how great it is. Okay. Now, let's hop over to cursor to see how you can start making some specific services using Google. This is using Gemini as the model of choice for coding. Cursor excels in code-based analysis and reasoning. So, keeping with our budgets example, there's a lot of code here, but this is just a budget controller Java file with an existing controller. Let's say we'd like to add some input validation. So, we know that everything looks good before we go ahead and process it. Using cursor's inline option with Gemini as the selected model, we can ask to add input validation. And hit submit. And there we go. You can see the highlighted text that was automatically generated by Gemini. Looks like it's added a few lines in case there's a bad request. And all of this is done right within the existing file, recognizing any differences needed and even adding changes based on the type of entity that you need the validation for. Now, since we're already working in Java, let's see what we can do in IntelliJ and the JetBrains family of IDEs. In IntelliJ, we're connected to Gemini via GitHub Copilot. We're using the same code base. I have my code on the left and on the right, I've given Copilot some context for different files for my budget controllers. We can go ahead and ask in the chat to add unit tests. And I speak Spanish, so I'm actually going to ask in Spanish. Agrega pruebas unitarias. Let's see. Oh, give it a second. Oh, muy bien, Gemini! Okay, looks like it's proposed a new file and maybe some modifications to a current file I have. And once this is completed, I can choose what I would like to accept and input into my code. And the most exciting part is that all of these IDEs now support the Gemini 2.5 family of models out of the box. And these are just three tools. Windserve, Cursor, and IntelliJ and Copilot. But we're enabling devs to use Gemini wherever it suits you best. Even in tools like Visual Studio Code, Tab9, Cognition, and Ader. Okay, now let's talk about running models. Now, my favorite model is Gemini, of course. Yours too, right? Right? Yeah. But there are so many great models out there. And some are particularly well-suited for specific tasks or use cases. So, behold! Vertex AI's Model Garden. You can connect to some of the most popular models or even bring your own from registries like Hugging Face. Model Garden supports the latest and greatest models across creators, including Llama, Gemma 3, Anthropic, and Mistral. So, let's check out the Llama 3.3 model from Meta. It's offered as a service via Model Garden. Here, we can test the model to see what the response looks like by asking something like, what capabilities can you offer for designing renovation budgets? And there we go. There are the results. We can also use the results we get and compare with other models that we may be considering. And we can do this without having to spin up our own infrastructure or pre-allocate expensive GPUs. Or, if you decide to deploy this, you can deploy to your own endpoint or cluster. So, let's implement a model from Model Garden in our application code. Here, we're in Visual Studio Code. And in this example, we've been using Claude, and it's great. But our apps run on Google Cloud, and the model's running somewhere else. We want to run our model co-located with our app to minimize latency and get the benefit of a simpler security surface. So, with two small code changes, we can use the same Claude 3.7 model from Model Garden. We'll use AnthropX Library. And all we need to do is change how we initialize the model. Here, we have the project ID and region instead of API key. And then change the model's actual name. And that's it. Then we're done. And we're set up to use the Model Garden version instead. And we can use other models via the Vertex AI SDK to do something similar. We're striving to meet developers where you are. Your team can build great apps using Gemini as your IDE of choice. Or you can use Vertex AI Model Garden to call your model of choice. No matter what you use, we're excited to see what you come up with. Gracias. That was pretty great. Those demos were muy caliente. I don't speak Spanish. I don't know. I think we can talk about it. That's something. Give it up for Debbie. That was great. Yeah. Wild stuff. Two things are true of developers. We are very particular about our tools, but we're also excited to try new things. I'm excited by the fact that we've taken such an open approach here. And you can use Gemini from so many different tools so developers can keep working in the tools that they love. Yeah. So I don't know about you, Richard, but personally, I've been playing around with Cursor, and it's great. But I love that we're making Gemsors. And I'm excited. I'm excited. We're excited. And we're excited about you. And we're excited about that. I'm excited about that. So we're excited about that. And we're excited about that. And we're excited about that. And we're excited about that. And we're excited about that. And we're excited about that. And I'm excited to share our latest Gemini model 2.5 Pro is now available in Gemini Code Assist for individuals. You can use it right now. Not right now. Wait till after. But we've brought even more Gemini to our own developer surfaces. Android Studio now is supported with Gemini Code Assist. It's in preview. This adds Android-specific AI capabilities to Gemini Code Assist. And Gemini and Firebase provides complete AI assistance in the brand new Firebase Studio. So we know AI is transforming the way that we build software. But the impact of AI extends far beyond the lines of code. That's right. It is changing how we experience the world around us, like playing or even watching your favorite sport. For me, that is Major League Baseball. Go Padres! Okay, wait, hold on. So you're saying that AI can finally teach you how to throw a knuckleball? I'm hopeless. Zero percent chance. Nothing can help me. But it can revolutionize the way that fans analyze player performance. And the very best part, anyone who is passionate enough to change the game with AI can do that with Google Cloud. Well, that's why we partner with MLB, who's using Google Cloud to process 25 million data points per game to compute previously inconceivable stats. Using Gemini and Vertex AI, MLB can compare live plays with its massive StatCast dataset in real time and find fresh insights. We wanted to see what the community could build with this data and our AI. So together, we created the Google Cloud MLB Hackathon. Yeah, we had tons of submissions, but we were thrilled to introduce the grand prize winner here on stage, who will show you how he used Gemini and Vertex AI to actually analyze baseball pitcher performance. So everyone give a really warm welcome to our winner, Jake DiBatista. Hey, everyone. I've always been impressed by how good MLB's motion capture and analytics are. It seems like every detail of every play can now be measured using high-speed cameras. But not everyone has access to that. See, back in college, I was a shot putter, and I used to spend hours watching videos of myself throwing and searching for errors in my technique. There had to be a better way to analyze a throw without the need for high-speed cameras to capture biomechanical data. So I built a fully customizable prompt generator for analyzing a pitch. It only took me one week to create this using Gemini API and Google Cloud. Let me show you how I did it. Since every pitcher has different preferences and skill levels, Gemini would need to be able to adapt its analysis depending on the pitch. Let's start with an experienced pitcher with lots of historical data. I'll analyze a video of one of the greatest left-handed baseball pitchers of all time, Clayton Kershaw. I'll analyze his pitching mechanics to take a look at the last pitch he threw in the seventh inning of a near-perfect game. This will show how far off his ideal performance he was at that point in the game. Cool. So we start by pre-processing the video using OpenCV, a computer vision library, and then storing it in Google Cloud. From there, selections are made such as pitch type and game state to pull in MLB data, including Kershaw's profile, into our prompt. Each selection here helps to generate a unique set of system instructions and prompt details tailored to the pitch being analyzed. Finally, this is all sent to the Gemini API. The model indicates Kershaw was throwing his signature curveball with nearly no deviation from his ideal. But this isn't just for the pros. This is a tool for all of us. That's why I added an amateur mode, which uses parameters for less experienced players. Our co-host Richard loves baseball and threw a pitch a few weeks ago. Let's see how he did. I'm scared. Are you sure you want to show this to thousands of people right now? No, not at all. All right. Winding up. Just not that bad. Well, you're no Kershaw. I've already generated the results in another tab. Let's go over there and see how he did. Here we go. Oh, okay. I have some major leagues for you, Richard. I don't know. I'm better at this job. I think so. Yeah, maybe don't quit your day job. Hey. So you have some potential. Look at what it says. He has to tighten up his arm a little bit and add some leg drive to maximize his power. Here, Gemini uses a different prompting framework for less skilled pitchers. After a little prompt engineering on my end, it's able to quickly adapt from a professional model like Kershaw to an amateur like Richard in a single click, altering the grading rubric accordingly. And what's cool about Gemini is with simple prompts, it can process multiple frames simultaneously, allowing me to analyze the entire motion, not just individual snapshots, giving a much richer understanding of how each part of the pitch contributes to the final outcome. This essentially worked out of the box, meaning I didn't need to implement a custom model or build overly complex data sets. As an athlete and front-end developer, I was shocked that I could build a computer vision application using Gemini by myself in just one week. This was built with pitching prospects in mind, so coaches and athletes at any level could help players without the need for high resolution cameras. Now, it's possible for anyone to have a personalized coach right in their pocket. Thank you. Awesome. All right. Give it up for Jake. Congrats again on your hackathon win. We had a ton of submissions. That stood out. If I knew he was going to talk like that, he wouldn't have won. Stephanie, what is the takeaway other than I probably need to work a little bit on my pitching performance? I'm glad you got that out of it. Could you have imagined, though, using generative AI for this type of analysis a few years ago? Nope. I love that this is valuable anywhere, that visually analyzing a process makes sense. I mean, you think about quality control in manufacturing, process optimization on some large production line, even troubleshooting parts inside a generator. That's amazing capabilities there. Yeah. The list is pretty endless. Yeah. And this is something that you just couldn't do well at scale before AI. Yeah. I'm still thinking about sports and AI use cases. It's a problem. But even within Google, we have that AI basketball coach right here at Next. Have you tried that? Well, I tried it yesterday. Here we go. Where it actually coached me on elbow position, height, and knee bed. So basically everything. You're roasting my pitching when that's what we're looking at. What? Okay, fine. I deserve that. I deserve that. The recent work we did with our CEO, Jeremy Bloom and his team at the Winter X Games to build an AI commentator and score analyzer. That was awesome. Yeah, it was. So that work added another dimension to what Jake talked about, which is essentially that Gemini can see and help make sense of information that isn't immediately apparent to the human eye. In just eight weeks, a small team of cloud developers built an AI commentator that could add an entirely new element to the fan experience for men's super pipe. AI scoring and analysis for the first time ever during a live action sports event. That's so cool. And correct me if I'm wrong. I think Gemini accurately predicted the top three winners based on just analyzing their practice runs. Yeah, exactly right. And X Games is super interested in this idea of AI as an unbiasing mechanism, pairing AI insights with human expertise to help remove bias from competitions. These things can watch longer than we can. We might miss something for a second, but the AI doesn't, which is great. Yeah, it's true. I also love this idea that Jake touched on of AI as a coach. Like how can you use vast amounts of nuanced data to improve performance? I feel like we're going to see a lot of momentum in this space next year. No doubt. It's great that AI is making things faster, less expensive, but I'm actually really excited about the ability to do entirely new things that weren't possible before. Where is this all going? What should we all be preparing for? So far, we've seen what's next. What's after next? Well, let's take a look at two areas where our industry is changing, even for a lot of years. We're changing even further. We're giving you interfaces that are more powerful, responsible agents that are more helpful, and a cloud platform that helps you orchestrate it all. Nowhere else you get all that. Yep, AI is showing in more places, not just your IDE and notebook, right? Think of agents as teammates that partner with us in all of the different places that we work. We'll show you how an agent powered by Google AI partners with you to do deep data analysis using BigQuery, Vertex AI, and CoLab. This will make getting insights and answers from your enterprise data dramatically easier and will make you a hero to your business stakeholders. Love that. So please give it up for Jelf Nelson. Here we go, Jeff. Thank you, Stephanie. Who else here loves data? All right, all right. Me too. I've worked with data for over a decade and know firsthand just how hard it is to turn that raw data into something useful. Today, I'll show how our new data science agent helps us turn that raw data into a data app. This app will help sales managers access personalized forecasts at our construction company. I'll start in a BigQuery notebook that's powered by CoLab to build our forecast. To do so, I'm going to check out my product sales table with a little bit of SQL code that I pasted in from the clipboard. We can see the table here. And if you'll notice, there's a DF beside the input. DF stands for data frame. And that's BigQuery loading its results into a Python data frame. Python allows us to use libraries like big frames to execute my code over tables of any size. So I'll use it to paste in a little bit more Python to drop a few columns we don't need for our analysis. And I'll write one more cell to aggregate total sales by fields like order date and customer state. And this just helps give us some metrics for the rest of our forecast. This tabular view here is great. But seeing the data is even better. So when I switch over to the chart view, I can start to spot interesting patterns or potential issues before going deeper. Next, I'll use this data to forecast sales with the Gemini Data Science Agent that's built into this notebook. All I need to do is click Ask Agents. And I'll input a prompt to generate a sales forecast from our input table. From here on out, all code is generated and executed by the Gemini Data Science Agent. This is a collaborative process, so the Data Science Agent acts kind of like my peer. We can go back and forth in simple natural language in this chat box. And you'll also notice that the agent uses Spark for feature engineering. This is only possible because of our new serverless Spark engine in BigQuery. So switching between SQL, Spark, and Python is now easy and allows developers like you and me to use the right tool for the job. That's something I think we could all get used to. So here the Spark code reads from our input table, applies some light transformations, and writes to a transform table. Next, to build the forecast, our agent uses a new Google Foundation model called TimesFM. It's meant for forecasting and it's accessible directly in BigQuery. Unlike traditional models, this one's pre-trained on massive time series data sets. So I can get forecasts simply by inputting my data. Let's take a quick look at the output. I'll exit out of the Data Science Agent. And we can see for every product ID and date combination, we now have a forecasted value as well as lower and upper bounds for a 95% confidence interval. But our chart view helps us see this even better. And this is the kind of thing I want to share with my sales managers. But for that, I'm going to need a data app so that all of my assets are packaged up and easily shareable. But here's a secret. I don't really like building apps. So watch this. Right within this notebook, I can click Create Data App. Select the cells I'm interested in publishing. In this case, it'll just be the visualization. I can click Next and I'll leave the default inputs here. When I click Create, BigQuery packages everything up and gives me a link that's external. That means this forecast is now a data app that's accessible to everyone. So sales managers can now directly access this app. They can input their own parameters and get personalized forecasts without needing to know any data science at all. Now that's powerful. So... I'm psyched about this too. So we easily transformed raw data into a fully deployed forecasting data app, made possible by this data science agent. But wait, there's more. We're also launching specialized agents for data engineers, data analysts, and business users in preview. And the data science agent you just saw, integrated in BigQuery Notebooks, is coming soon. You can also get started in CoLab today. So scan the QR code on the screen and start building. I can't wait to see what you create. Thank you. That was amazing. Did you hear him say he does not like building apps? Congratulate Jeff on his first and last time on the main stage keynote. Congratulations. What's going on there? But that was a ton to see in a short amount of time. That was super duper impressive. Yeah, I mean, I know the data science agent is really taking off based on the reaction here. It's true. And I'm excited to see our next set of agents for data launching this year. Yeah, I like this concept of agents as teammates. It's not just about using a single call to a model and that's it. It's more than that. As devs, we're more empowered with agents to do ongoing and complex analyses with all of our data. Yeah. Well, Richard, what else do we have for everyone today? I think we've got one more big demo. We want to give you a sneak peek into what we really see as the future of software development. We've got some amazing features coming later this year that will make building software so much faster and easier by using a software engineering agent. What else? To further boost productivity. To show us how all this works, get excited for my friend Scott Densmore. Hey, everyone. I want to walk you through an early view of how you'll build software using agents designed specifically to help developers. This functionality is coming to you later this year. Gemini Code Assist provides an extra pair of coding hands to help you create applications and remove repetitive and mundane tasks so you can focus on the fun stuff. This is the Gemini Code Assist Kanban board. We're moving beyond editors to a new way to develop software with agents. The Kanban board will let you orchestrate agents to help you in all aspects of the software development lifecycle. It includes something we're calling a backpack that holds all your engineering context, style guides, security policies, formatting preferences, even your previous feedback. So Code Assist can be a more effective coding partner. All right. Let's start here. We've got a technical design doc for a Java migration, a task I think we'd all like to hand off. So we'll make a comment and assign the migration to Code Assist directly from our Google Doc. When it has questions, those will get added to the comment thread where we can respond directly. This new task will show up on the Kanban board so we can keep track of its progress. Google Docs isn't the only place you might want to start a new engineering task. We can ask Code Assist or send so to Code Assist a message asking it to fix this continuous integration build failure directly from our team chat room. Or you could assign it a bug directly from your bug tracker. In fact, why even assign bugs manually when we can just ask Code Assist to do it? It can keep an eye on our repo to perform bug triage, do root cause analysis, and fix issues automatically. We can ask it to do code reviews on all incoming pull requests, leaving great actionable feedback. Much better than I get from Richard. Nobody puts baby in a backpack. And of course, you'll see all those assigned tasks, bugs, and each new issue as they arrive right in the Kanban board. While we're here, let's start a new project. We'll take a product requirement document for an earthquake monitoring web app and ask Code Assist to produce a prototype. Okay, that looks good. We're going to fast forward through this a bit. And then we're going to preview that web app right from within the Kanban board. So, you can see how this now becomes our new development loop. I can tell Code Assist the changes I want and let it take another pass and repeat that until I'm happy with what I see. Or I can decide I want to just dive into the code right in my IDE. So, you know, it's been a few weeks since I started this project. I think Richard has been doing some check-ins and he likes to do a lot of talking, but not a lot of coding or playing baseball. So, let's see what he's done. Oh, Richard. Of course, he left me some to-dos. So, I'm going to hand this off to Code Assist and then I'm going to look into his code access. Oh, don't do me like that. Oh, you just wait. I think that was at Build Failure was you earlier, wasn't it? Probably. Probably. All right. So, designed to tackle all kinds of developer tasks, Code Assist and Kanban board will extend your reach and allow you to do more as a developer and turn even more of your ideas into reality. I invite all of you to check out the AppDev Spotlight right after the keynote where Ryan J. Saab will show you some of these great new features in much more detail and tell you how to sign up for the preview. Thank you. There you go. Give it up for my former friend Scott. That's nice. Do I need to get in between you two? Gosh. So rough. Well, that really felt like the future of software development, right? Amazing. I love the idea that it will let me spend the vast majority of my development time focused on solving interesting and important problems and less time on the boring stuff like writing tests and crud code. You're doing a lot of crud code right now? It won't be. Yeah. I mean, building a prototype or MVP like we're showing here is definitely a bit different from adding new functionality for an enterprise with a big, massive, complex code base. But what this shows is that AI can write good code for everyone and interact with all of us on whatever surfaces we work with. I'm excited about that. I think orchestrating agents is a big part of the future of software development. And we just showed a fresh development loop here for small, large enterprises alike where AI works with the developer to build the right things and then build it right. Yeah. So I don't know about you, but I cannot wait to build with my own fleet of agents. You cannot wait to build a fleet of agents. It's a fleet. All right. And I know we have one last thing to share. What is up next, Richard? Agent fleets? No. Good guess. No. Even before next, there has been a lot of buzz about VO2. We saw some stuff here as well. It's a really cool model. There is so much excitement out there for what's possible. Yeah, that's right. And we love seeing what you all have been making so far with VO. From short films to magical animated scenes and so much more, I personally really love the things that aren't possible outside of animation or special effects. I mean, so much of what I've seen is just so fun and whimsical. That would be our band name. It should be. Yeah. All of that is available today on Vertex AI. Which is amazing. You can build with this. But we wanted to give you one more peek behind the curtain at what's achievable when Google Cloud and Google DeepMind team up with partners to stretch the boundaries of what's possible in entertainment when you use this same technology. The Sphere just announced that it will be presenting a fully immersive experience of the 1939 film The Wizard of Oz. Let's take a look. What we're doing with The Wizard of Oz at The Sphere is make you feel like you're there. The process of getting this immersive experience off the ground has 10,000 moving parts. How could we adapt that 4x3 image? And how could we spread that on the media plane and take you to another world? We had to figure out super resolution, outpainting, and performance. Google was the only company that was actually capable of doing this. We actually had to add in and restore more detail than existed in the original film. The second problem is we needed to complete the characters that were cut off outside the field of view of the original cinema production. The third problem is with a much wider field of view, we could see characters that should be there but were never captured on camera. And so we needed to generate those performances completely. We needed to use AI to train on all of the material in the original movie to effectively reconstruct a memory. Then we could transport the audience back into that memory of the original movie and reproduce that experience in the Sphere. The key for us in all that we're doing with AI is to maintain the integrity of the original filmmakers' intent. It's part of our cultural history. You can't just take it and do anything with it. That challenge led us to a series of guiding principles that became one of the most complex productions I think I've ever seen or heard of. So I know which shot I need to upscale. So I come to this program that I've written. So this program launches a pipeline on Google Cloud infrastructure. And what it does is it takes low resolution input and produces high resolution output. Now I've run this exact shot through our SR model and this is what we get. So to get to this stage, we had to push the boundaries of the technology that was available. VIO is one of our well-defined video models. VIO is one of the biggest super resolution breakthroughs. Yeah, how does VIO work? Okay, say you zoom in on a video and you take a patch of it and you train a model that learns, here's that image in high resolution. And then it learns how to take a small, low quality, low res version of that and give you a beautiful high res video. A new technology and a new tool enables a new story to be told in a new way. We went from an era of models and miniatures and special effects. And now we're entering a new era where teams of artists are working with AI to follow in the footsteps of filmmakers for the last hundred years. We're in a new era where we can't see what we're doing. Wow. It's really cool, right? Amazing. Every time I get chills. Yeah. And I really think it's only the beginning. And I can't wait to see what other entertainment projects are possible with this technology and what our customers will do with VO2. I really love that we showed so much that you can do today, not futures, right now. Use AI Studio to experiment with the latest Gemini models, including those that have that advanced reasoning we talked about. Develop agents that change how you work with the agent development kit and run agents at scale with Vertex AI. And combine multiple agents working together into a single application or service to do work for you. Use Gemini in your favorite IDE or bring your favorite model from Vertex AI Model Garden. And in the very near future, you will use agents to help you dive into your data without needing to be an expert in Spark or other big data frameworks. Software development agents will handle the toilsome dev work for you so we can focus on the fun, complex engineering problems. Look, if you work with Google Cloud, in some ways you're already working in the future. Well, that was all amazing, but I personally loved Jake's baseball demo. Me too. Gemini makes computer vision so easy and accessible, which made me realize that I can use Gemini to improve in so many things. For one, basketball. Baseball. Learning guitar. Even my cooking skills. See our latest demos, try the products talked about on stage, and get your questions answered by technical experts in the Google Cloud Showcase in the heart of the main keynote Expo Hall. Amazing. And that is the show. I am really excited to see what you all build next with Gemini and Google Cloud. Let's get out there and start building. Thank you. Thank you. Thank you.