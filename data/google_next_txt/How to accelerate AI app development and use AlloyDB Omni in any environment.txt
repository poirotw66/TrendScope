 Hello and welcome to, I didn't want to say the last day, day two or day three, day three of Google Cloud Next. I hope you enjoyed the killers last night and you must have heard so many things about Google innovation, what you and other customers and partners are delivering and building with Google Cloud. And as you already might be thinking about going home, you might be thinking about how can I take this innovation and the things that we do in AI and performance, how can I take them and make them useful in my own applications? And not all of your applications might be running in Google Cloud Platform today, so I want to talk to you about how you can use some of these things and make them useful for you in other clouds or on-premises or for development on your own laptop where we also ship innovation with LRDB Omni. My name is Bjorn Rost. I'm a product manager in cloud databases for LRDB and I'm joined today by John from Ivan, who's going to talk about our partner solution and then Emyr, who's going to round us up with talking about how other partners have accelerated and integrated with LRDB Omni to make it faster for you to deploy. Databases is really important in the context of our data cloud. Data cloud to us means that you can use the whole lifecycle from generating data in your transactional applications through storing it, making it secure and useful in different formats, and then getting insights through intelligence, through AI, and through business intelligence tools like Looker for Insights. Our data cloud is unified. You can store all of your data in one place. It is open. It is open. We love using open source technologies or open standards wherever possible so that you do not need to be locked in and you can bring existing applications as they are. And our cloud is intelligent, infused with AI to make it easier to run and for you to get maximum benefit and really supercharge the value of your data for your business. We understand that one database doesn't always fit all so cloud databases actually consists of a number of solutions that are managed services or solutions that help you run databases. Some of them are open source technologies, some of them are legacy databases, and some of these databases are core Google technology like Cloud Spanner or Bigtable which we invented here at Google. LRDB sits kind of in the middle between the cloud native world and invented at Google and open source because it's also built on Postgres. It's built on Postgres with 100% compatibility. So I want to talk to you about what is LRDB Omni and then we go into how do you build generative AI vector search, semantic search applications with LRDB Omni that you can run wherever you want on your laptop and somebody else's cloud or in Google Cloud. And I wanted out myself by talking about all the things that we launched this week and this year for LRDB Omni before handing it over to John and Amy for the rest. So the story of LRDB really starts with Postgres. Postgres is the most popular database in the world today. Stack Overflow runs a survey and they found that over half of the professional developers choose Postgres over any other database. Datadog does run a study where they look at the presence of stateful sets and stateful containers both in Docker and Kubernetes and they found that no other relational database is run as often in containers as Postgres. And then Timescale asked a bunch of Postgres users about their use of Postgres and they found that most Postgres users, or 46% of them, found that they expect to use even more Postgres or a lot more Postgres this year. And then 25% of Postgres users have already used Vector Search with PG Vector in their applications. Enterprises look at Postgres and they do see some challenges. They wonder, is it scalable enough to run my most demanding workloads? Is it scalable enough to run migrations from legacy databases? How do I manage not just a single database but tens, hundreds, or thousands of instances? Can I use this as a unified data platform to also build Vector database or Vector search use cases or real-time analytics? And then lastly, by putting data, my most valuable asset in my company into this database, who do I call for support? What do I do when something goes wrong? How do I know that critical security patches are applied? And so with this background and these user needs and ask, we created AlloyDB. As the combination of Postgres, which is an amazing database already, to which we can add Google innovation in the space of AI, performance, and many other areas. Now we understand that AlloyDB is obviously critical to run into Google Cloud Platform, but we also know that you run applications not only in Google Cloud, but you run them on premises. You run them in multiple clouds. For example, ISVs might want to deploy their SaaS application in the cloud that their customers choose. We know that workloads run in sovereign clouds, like disconnected public sector or highly regulated industries that are not allowed to connect to the internet. And we also know that there are use cases at the edge where latency matters or where the ability to run without internet connectivity matters for retailers, manufacturing, and other industries. And so for all of these use cases that are not in Google Cloud, we built AlloyDB Omni. And AlloyDB Omni is a downloadable database software that you can run and download and install anywhere you like. On-premises, laptops, data centers, servers, VMs, Google distributed cloud, hybrid cloud, multi-cloud, anywhere. It is faster than open source Postgres, allows you to scale further and get more transactions out of the same amount of resources. It has a columnar engine that speeds up analytical style queries by 100x. It's easier to operate with autopilots and automation. And it's a truly unified database that is actually three different databases in one. And by that I mean, if you want to build modern applications, you probably have use cases for relational storage and storing transactions. And Postgres and AlloyDB at the core are relational database optimized for these types of use cases. But you also might have use cases where you want to get real-time insights over large amounts of data, scan and aggregate data. And you could consider moving them into a dedicated data warehouse for these purposes or a data mart. But AlloyDB's analytical accelerator actually means that you don't have to move your data. You can get insights on your actual transactional data without spinning up a second system. And then if you've paid any attention at Google Cloud Next, it's all about vectors, vector search, embeddings, and AI. And again, you might look at dedicated vector databases. But I don't think you should because AlloyDB is a perfectly good vector database that is able to store, create, and query vectors at scale. Let's look at how that looks in a little bit more detail. And I brought a little use case here. So the use case is I'm a travel website and I have a list of descriptions of resorts. And they go like the symbol beach is something or these others are something. Somebody that looks for this, an end user, would query for something like I'm looking for a family-friendly beach resort. And if you look for like an equality type search, that's really hard to find because family doesn't show up in any of these text descriptions. So just filtering for certain words doesn't work. And so machines are not really good at matching this and giving you a good recommendation. So what machines are really good at is numbers. And so if we could just turn these descriptions into embeddings or vectors, then it becomes much easier to compare them. And so that's what a vector embedding is. It's a machine learning model that takes a text description or any other unstructured data and turns it into a number of vectors of hundreds of dimensions that you can then compare the distance from one to the other. So you create that for all your data. You create it for the user prompt of give me my family-friendly resorts. And the vector search will return the results where the description closely matches the one that you asked for. And so how does it look like in the actual application? Well, step zero is what I just showed you. You create vector embeddings of your internal data using an embeddings model. And then when a user asks a question, like in this case, what's your pet policy? It would then go and turn that into a vector embedding as well. Look for all the documents that are closest to the question of, hey, what's your pet policy? LRDB can find this query results very efficiently. Turn those documents back to a large language model. And it can actually interpret this and say, you're lucky. Your family-friendly beach resort is also dog-friendly. You're allowed to bring your dogs. So the components to build this consist of this. You need to create embeddings. And in LRDB, we wanted to make it as easy and secure for you to do it as possible. So we actually created a custom function called embedding. And when you run select embedding, you pass the embeddings model and you pass the content, which is either a column or a text string. It'll create the embedding for you and return that as an actual vector data type. So that makes it super, super easy to run these things. And you notice that in this case, I'm using text embedding gecko, which is a pre-existing model in Vertex AI model garden. You can also bring your own models. So in addition to just using pre-existing LLMs and models in Vertex AI, you can train your own models and either deploy them in Vertex AI or in the platform of your choice. You're not just bound to Vertex AI. You can use also chat GPT. You can use hugging face or other models, local models that you host offline that don't require the internet, which is really, really important for LRDB Omni because if you're running disconnected at the edge, you also want your machine learning model to run at the edge. So that is a really, really important feature in LRDB Omni. So once you've created embeddings, you want to store them. And we just extended the existing PG vector extension in Postgres. This extension allows you to store a custom data type called a vector and then create indexes on top of it. Indexes are really important because as you can imagine, if you compare the proximity of one vector to hundreds or millions or billions of others in multiple hundred dimensions, that's a lot of computation you have to run on. So there's indexes to optimize it. Open source Postgres gives you a choice of two indexes, HNSW and IVF flat. And in LRDB Omni, we added a third index type called scan, which is short for scalable approximate nearest neighbors. Scan is actually based on the technology that we've been using at Google in Google search and YouTube for way over a decade. So a lot of thinking has gone into this. And if we compare scan indexes with HNSW indexes, you will find that they provide faster queries, faster index creation, allow you to have a higher write throughput, all by having to store less data in memory. And so scan really is your optimal index type for vector search in Postgres or in LRDB for millions to billions of vectors. Another thing that we found that we optimized is that when you run vector search, you're actually often doing a combination of traditional filters, like in this case, sorting or filtering by resorts where the price is less than $500 per night and a vector proximity search. In these cases, it's really important that the optimizer chooses the best plan to do this because depending on the cardinality, it might be faster to filter first and then do the vector search after for just for sorting or to get filtering through the vector search and then do the actual filter. And so we enhanced the optimizer to be aware of these things and to adaptively choose the best execution plan based on the combination of vectors and SQL. And so that's not all. I kind of just talked about the stuff on the right column here, the vector search features. We announced a number of other features, both for vector search, model access, natural language access. One of my favorite features is the ability to just express a filter as a natural language term. I can say, give me something if AI and then just write out a query. One of my favorite features. If you haven't had a chance to join Tabby in a session just before this, find these sessions on YouTube and just watch them for, like, more details on what we've done in LODB AI. All right. I want to talk about other things that we've done with LODB Omni this week and this year. First of all, LODB Omni, as I said, is an installable software package that we package as a container. You can run and install it as a single command using Docker. And we follow the same parameters and the same command that the official Postgres image uses for Docker. So if you have that already running, it's easy enough to swap it out for LODB Omni. What is new is that we support Postgres 16 as a major version as of this year. And we support Linux and Mac on both Intel and ARM. I do want to remind that we have a free developer edition that is free to use for evaluation and for non-commercial purposes. And if you require a license for commercial purposes, for production or for the need for support, you can upgrade any time just by purchasing a subscription to our cloud console. LODB started about performance. And so last year we presented more than two times the performance improvement over open source Postgres for transactional workloads. Excuse me. This year we optimized the way we handle storage and IOs and we introduced atomic IO in peer review. Atomic IO is really relevant for large datasets and optimizing IOs, reducing CPU overhead from running them. And we found that especially on large datasets, you get up to a four times performance improvements over standard Postgres. Like in this case, we ran a TPC style benchmark over an 88 terabyte, over an 11 terabyte database on 88 vCPUs. Another feature we've added for performance for large databases is the ability to configure this cache. So with LODB Omni, you configure your own backup storage and your storage could be, could be fast, but it could also be slow network storage. Maybe it's an old storage array that you've been using for a number of years. And so databases perform at their very best if they're able to compute things in memory and get things out of DRAM, but not your whole database will fit in DRAM and then things will have to be fetched from disk. Our disk caching feature means that you can configure a local SSD or an NVMe drive that is super fast, can cache warm data for you, so you don't have to store it, fetch it from the backend disk format. And you can use the ultra fast cache both for shared buffers, which helps with OLTP performance, or to extend the size limitations of the columnar engine beyond what fits in memory. And this is available in general, generally available as of this week. Actually, no, as of earlier this year, we launched this in January, I believe. We created reliability architectures that follow three different design patterns for standard enhanced and premium tiers that use things like backups, high availability, standby instances, automatic failover, cross-regional replication to reduce the recovery point objective and recovery time objective in these different tiers. And we have these pre-made recipes, if you will, that will show you how to do it. And we have these beautiful diagrams to go along with them. All of these architectures are available both in standalone, meaning on VMs or laptops, computers, and they're also available in Kubernetes. Companies love to use Kubernetes for infrastructure as code and to optimize and automate database lifecycle operations. I also want to mention one of the other quickest ways to get started with LODB Omni is the pre-installed marketplace images, which we have available on GCP, on Amazon, and as of this morning, we're also available on Azure. If you use our LODB Omni operator to deploy on Kubernetes, this is actually all it takes to deploy a redundant database that does failovers and many other things. You just copy this manifest, you fill out things like the version number, the number of standbys you want to use, the resources, the CPU count, submit this to our operator and it will orchestrate all the steps necessary to implement this database cluster for you. Another new feature is an observability. We've always exported our metrics at the system, container, and database level through open standards in Prometheus. What is new is that we now also have a pre-built dashboard on Grafana that helps you to get started really, really quickly. We added active directory integration in preview orchestrated through the Kubernetes operator that you can use to authenticate your end users using active directory. And we have a number of new features that I don't want to read them all. I just want to highlight load balancing as one of them where we actually took PG Bouncer as a multiplexing load balancer that really helps you if you have thousands of connections to your back-end databases. And we made that part of our operator as well. It orchestrates it for you and treats it kind of as like a first-party puzzle software. We are excited to keep making amazing improvements in Kubernetes and in VM automation and in LODB Omni core. But I also want to give up the stage now for John to talk about how you can use LODB Omni and the innovations that we added in LODB Omni in not just Google Cloud, but in other clouds or multi-cloud as well. Thank you very much and welcome John. John Kennedy. Thank you. John Kennedy. Thank you very much, Bjorn. And hello, everyone. I appreciate we are competing with lunch. So I really, it's great to have you all here. So my name is John Kennedy. I'm head of databases at IVAN. We've been working together with Bjorn and the Google team for about a year now. And this week we were proud to announce the launch of IVAN for LODB Omni. This takes that downloadable database format and we host it on IVAN's platform to give you a managed service on all those clouds. So the patterns of high availability and replication, the PG Bouncer pooling, all that capabilities we manage for you and you get it straight out of the box. So what I'm going to do today is I will run you through who IVAN are, because many of you may not have heard of us, what IVAN for AlloyDB Omni is and how you can get access to it. And then I'm going to run you through a demo of exactly that semantic building an app that uses the capabilities of Omni to build a semantic search. And we'll run through that in detail. I haven't been praying. The demo gods and I are not friends. So I have prerecorded it all and you will see the videos. Hopefully those will all come out OK. So first up, who are IVAN? We've been partnering with Google since about 2016. We are a data platform for hosting and managing open source databases and data products. That's why Google and IVAN get on so well. We both have a core commitment to open source technology. And where we differ is we deploy into any cloud. So AWS, Azure, Google Cloud and Oracle Cloud. So wherever you run your databases, we can run them for you. Our key focus has always been and continues to be simplifying work for developers and those who are building tech stacks and solutions for their companies. What you'll see in my demos is the console. But very much like Bjorn pointed out, it's all API first with us. You can take Terraform. We've got Kubernetes operators. Everything I'll show you on the demo is available for you to codify and use programmatically. So if you think this is, we're not just a shiny UI. Although we do have quite a shiny UI. So let's go. Ivan for Allodb Omni. It was a relatively easy partnership to strike up because we've been hosting Postgres for nine years and doing a really good job of it. It was great when the engineering team started syncing together the benchmarking and all the testing went really well and it was really easy to get going. But it delivers all those bits, the features and capabilities that Bjorn highlighted, we manage for you. So you don't have to concentrate on any of the ops. We're high availability, backups, zero downtime, minor upgrades, patching, security, backup to another region, point in time recovery. All the capabilities that enterprises and companies need when they're managing a database, we will do that for you. Okay. So first demo. Let's see how this goes. It takes a while to load. There we go. So what you're looking at here is the Ivan console. As I said, this is all available via API. You do not need to come into the console. So we are going to, for comparative process, we're going to start an instance of AlloDB Omni up in AWS. So in the flow, you check your availability, the level of high availability you want, two nodes, three nodes, one node, how many regions, whichever region you want to put it in. Our pricing is very clear, concise. You know how much you're going to pay. You can add additional disk storage that we will scale into as and when you need it. You can add labels, names. Yep, I forgot the label and had to go back and do it. And that's it, getting started. So we'll come back to this AWS instance later on in the demonstration. But there it is. It's being started up. The nodes are being created. And we'll come back to it later on. We'll go through all the features and capabilities on the left in a few minutes. But that one in particular, the generative AI, will be key. Because in that configuration in the database, we will put the connectivity out to Vertex AI, which we will be using so that the database can directly create the embeddings when data is loaded, which is a real shortcut for your data AI pipelines. Okay, so let's talk a little bit about the demo. We will leverage that exact capability. We will put the connection configuration into the database. It will be stored in the database such that when we load the data into the database, a trigger will take an action to go and create the embedding, bring it back, and store it in the database. Then we will run a semantic search against a new data item that we have added to the database to show that it works. It does work real time. But when I did the demo, I had an awful lot of tabs open. So there was a little bit of lag. But I didn't shorten it. So we will have to go through that. And as Bjorn mentioned, one of the real accelerators here is Google's proprietary scan index. Having that inside PG alongside PG vector really improves the vector search capabilities. We've seen really good responses. And I think one thing to mention that Bjorn slightly touched on, we have customers who come. We have multiple databases. And when they come to us to do a bake-off, a technical bake-off around vector search, they often pick PG, now that we're picking LRED Omni, OpenSearch, and then they pick a dedicated vector database. And the difference in performance for their use cases is often minimal. Often people will choose the vector database or the vector capabilities in the database their engineers already know. Because the lift is smaller, they can get started more quickly. And that's why we see the high value in having these capabilities within and based on a Postgres database. Okay, next demo. So we're going to run through the specific database that I set up for this demo. So it's got a name on it. You can see we can configure PG Bouncer pools down there just below the connection details. This one is up and running. Let's have a look at the generative AI. So here you can see I've already put in the configuration to connect that to Verdict AI. You can do that all via API again. I actually did it via API. I didn't use the console. Some of the capabilities we have available, you can see the backups here that are available. We don't have a secondary setup, but you can have a secondary backup in a different region for disaster recovery. You can fork and restore from these backups if you're doing development work. You want to check if feature works. And you can configure the retention and the number you take. So all of these capabilities we manage for you, but they are fully configurable. You get access to the logs here, query statistics to make sure that everything's running okay, see what's actually going on in the database. And within an instance you can obviously have multiple databases and that capability can be evaluated there. One of the key other value points is, Bjorn and others might know, cloud vendors have so many products it's difficult to know how to connect them together, what works with what. Ivan, we make it very simple both for internal services on Ivan's platform, also those you have running elsewhere. So we make sure you can get the most out of the databases you have running on our platform. Okay. Nearly finished? Yes, perfect. So I am, I haven't been coding for a long time, but when I was asked to do this demo, I had to do a paper-scissors stone off and I won. So I had to create the demo from scratch using one of our blogs. What I did was leverage cursor and an MCP server and I actually gave it the blog and we built everything out in about two and a half hours, which is a bad football match in the UK. What I'm doing here is I'm interrogating the database via cursor to see that the triggers are in place. So we're going to, rather than me trying to explain and show you the inner workings, the LLM within cursor will do that for us. So it takes a little bit while, but there's the question, just asking what the trigger does and it will give us that answer out. I know Vibe coding has a very bad reputation with engineers, but as someone who's learning tech on a daily basis, but isn't coding on a daily basis, I find it extremely useful. And here you can see we've got the trigger created. Every time we add a row into the database, the database will go and collect, get an embedding created, put it back in, and store it for us. So that's the fundamental magic of the database of Allergy v. Omni and why it's useful for this. So let's take a look. Is there data in the database? That's always useful. Again, being quite lazy, this will just do it for us and tell us what's data in. It's a fairly simple setup. It's just products, their description, and what was the other field? Bear with me a second. Again, I didn't shorten this. This is real time. Product table. Product schema and then the features table. So this is the data we'll be using for the rest of the demo. Very good. Thank you very much. I should have shortened it a bit. No more. And again, I'm quite polite to my LLMs. I don't know if that's going to save me when the end of days comes because I've been nice to them, but maybe it's just being British. Okay. And you'll see at the end I've commented and it actually built the app for me as well. So what happened for this demo is it created an app locally on my laptop that I was then able to carry out the actions that we're trying to show. So here is, I'm not very good at UI. It did it all for me, so it's not particularly pretty, but we have a very single pane app. You can search in the database for a product. Now, someone asked me why I chose chainsaw. There's nothing. I was watching a gardening program. They were cutting back. He was using a chainsaw. I decided that might be a useful product to buy. So as you can see, we've got the details, but there on the search results, you've got the similarity, which is the nearest neighbor index being leveraged to find out how similar one thing is to another. The higher the rating, the more similar it is and the higher it will appear in the search, the comparison of those numbers. So we'll add a new product. I decided to add one called chain with a lock. Close enough that it would test that similarity to some extent once you've got it added, but not different enough so that it would prove that we were actually doing something useful. So add the description, add the price. You can also add an image URL. That was just part of this app, but you can extend it as far as you like. And we add the item to the database. It will take a few seconds, but we'll get there. Again, this is because I had four tabs open, not because the product was working slowly. Once it's added, we'll go straight back into the app and we'll search for it and we will see the semantic search running against the new content that's been added without me doing anything else. I haven't gone into the database. This is real time, as you've seen from the delays. I haven't gone and done something anywhere else. We're just searching for the chain with the lock. The debug levels are on and there it is at the top. Highest similarity and then the details below. So that's a very simple but effective way to show you how you can leverage the capabilities within AlloyDB Omni as a managed service to get shortened data pipelines when you're interacting with LLM models. And as Bjorn said as well, we're using Vertex here. You could use anything you want. It doesn't matter at all. The database can connect out to anything wherever it is, local, in other cloud vendors. So you're not restricted in any way, shape or form. That was particularly easy to do, but there are more and more advanced use cases coming for interacting with AI. That was a one-stage interaction with the model. You can go as complex as you want. The capabilities of the engine are pretty phenomenal of what we've seen. One thing I didn't run through was due to the capabilities of the platform and the development work our team did, you can configure the resources you allocate to the columnar database really easily in the UI and via the API. So you can, if you need more analytics than, say, transactional workloads, you can tweak that to make AlloyDB exactly what you need. Okay, so I think I've covered everything. Let's go back to the very first AWS instance we created. So your CTO now says we're no longer using AWS. Everyone, we're moving to Google Cloud. That's where all the good stuff happens. And this is the process to migrate that database from AWS to Google Cloud in Vyvn. It's literally a four-click process. We'll migrate that in the background, move all the data, and then flip the DNS over. For databases, depending on size, it can take a couple of minutes. But when you need that kind of cloud flexibility, this is exactly what Ivan delivers on. And that was all I had to, and I'd like to hand over now to Amir. Thank you very much for listening, everybody. Thank you. Thank you. Thank you. So as Bjorn was also saying, we are committed to open ecosystems and standards. This means that we want our customers to have the best experience when deploying their tools of choice, while they're leveraging AlloyDB for modernizing their applications. So for this, we built Google Cloud Ready program. Two years ago around this time, we announced this program for AlloyDB. Google Cloud Ready designation formally recognizes partner products that work well with AlloyDB. AlloyDB. These solutions have met a certain functional and interoperability requirements and are ready for your deployment in any environment, be it on our cloud service, on Ivan's platform, or wherever you run your AlloyDB Omni database. Knowing that these solutions work well with AlloyDB, you can confidently choose one of these and reduce the time and costs associated with product evolution and focus more on data to value. Over the two years, we have added 55 partner solutions to our AlloyDB program. Recently, we expanded our program to include partners that address the specific needs of customers that deploy their AlloyDB Omni in multi or hybrid cloud environments. I would like to welcome five of our newest partners focusing on hybrid and multi cloud environments with AlloyDB Omni. These are PlyOps, Silk, Hitachi Vantara, Pure Storage, and CommVault. I'll tell you more about four of these partners in the coming minutes. First off, Silk. Silk provides a software-defined cloud storage that supercharges your AlloyDB deployments in any cloud environment that you would like to deploy it. With Silk, users can achieve much higher performance compared to direct deployments. And higher performance can mean multiple things. It can mean more responsive applications, a larger scale, or lower cost through using fewer resources. But in many cases, actually, all of these combined. Silk provides additional storage features such as instantaneous zero footprint clones that can simplify data lifecycle management. Now, we have Portworx by Pure Storage that helps you manage your AlloyDB environments at scale across multi and hybrid cloud environments. Portworx provides a Kubernetes operator that seamlessly integrates with AlloyDB Omni. Through Portworx, you can build enterprise-grade resiliency features such as high-availability storage, deploy your AlloyDB Omni databases agnostic of which cloud you choose, and manage their daily operations in a consistent way. When you prefer hybrid cloud deployments for your AlloyDB Omni databases, look no further than Hitachi Bantara. Hitachi seamlessly integrates with AlloyDB Omni, allowing you to deploy your databases in a pre-configured way. So you can get the optimal environment's best performance. Hitachi Bantara provides a high-performance infrastructure with enhanced security and efficiency and strong data availability guarantees, which simplify your hybrid deployments. And finally, we have Commvault, which brings a powerful suite of data protection capabilities for wherever you deploy your AlloyDB Omni databases. Commvault provides unparalleled cyber resiliency through features including air-gap storage, automated comprehensive backups, and managing all your database protection assets across a unified view makes multi-cloud management and compliance easy and simple, especially as you're modernizing your applications. Additional features like deduplication optimizes your resources and costs. To recap, AlloyDB is PostgreSQL the way only Google can deliver. Multi-modal enterprise database for transactional workloads with an outstanding generative AI and real-time analytical capabilities. Ivan delivers a managed AlloyDB service, allowing you to deploy and scale your applications on a consistent way in multiple clouds. And AlloyDB is ready for your enterprise. We have these recommended architectures that Bjorn showed, the partner solutions that I showed. You can use these to jumpstart your secure and reliable AlloyDB journey. We talked about some of the free trials. You can try AlloyDB and AlloyDB Omni free today. You can download AlloyDB Omni Developer Edition, which is free to use. You can have 30-day free trial with Ivan or on AlloyDB's Google Cloud service. If you would like to read more on our features that we discussed today, you can go to our documentation. This QR code takes you to AlloyDB AI docs. And finally, if you're ready to migrate your workloads, check out our database migration wizard. That simplifies the migration journey for you. We have allotted some time for your questions. We'll pause the presentation now. You can come and find us next to the stage. Thank you. .