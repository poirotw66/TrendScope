 . Hello, everyone. Hi. Thank you for coming to our talk. Welcome to Connect the Dots with Google AI from Models to Machines. You're probably sitting in the audience because you've been mystified, perhaps, by the Google Cloud AI stack. Well, in today's talk, we'll demystify the Google Cloud AI stack in three ways. One, when to choose which Google Cloud AI product. Two, Derek from Etsy will walk through how Etsy chose to leverage different Google Cloud AI products for different use cases. And three, how to leverage NVIDIA in your Google Cloud stack to accelerate your AI workloads as quickly as possible. I'm joined here today by Derek from Etsy. He's an ML engineer. And he'll be walking us through how Etsy does AI on Google Cloud stack. If you haven't checked out the Etsy booth yet, I highly recommend it. It's a great booth with a lot of curated, interesting items to purchase. And we're also joined by my colleague, Alan Kristoff, who is a product manager on GKE. And I'm Kate Teen. I'm a customer growth engine lead for Google Cloud AI. Let's connect the dots on the different layers of the Google Cloud AI stack. As you heard in Thomas Kurian's keynote yesterday, we're adding innovation at every level and also co-optimizing the layers. So starting from our AI hypercomputer layer, we have state of the art GPUs and TPUs. So you can train and scale your use cases. And then on top of that, we have our first party models such as Gemini 2.5 Pro, which if you haven't checked out, I highly recommend. It is an industry leading reasoning model as well as over 200 models from our partners and from the open community. On top of that, we have our Vertex AI platform, which allows you to deploy, manage, build your predictive AI models, as well as your generative AI models and your agents. On top of the stack, we've got our AI agents. And now with our newly launched agent development kit, we're able to not just use Google built agents, but also customer built agents. And of course, Google agent space, which combines the best of search and AI agents to improve productivity for every organization. What we're hearing from data science and machine learning practitioners is that their needs are evolving and expanding. We're hearing that they can't have their data in silos because you need to be able to bring value from all of your data together. So they're looking for unified end to end data and AI platform. We're hearing from our customers that they want to empower who can use AI. They're looking for no code, low code and custom code options so that everyone in the organization is empowered to use AI in their day to day work. And we're also hearing from practitioners that they want to avoid lock in while future proofing their platform so that they're able to build on top of innovation and accelerate even faster. And finally, we're hearing that you need to have the right data at the right time. You can't have AI without data. And how do you ensure that everyone has access to data that they should have access, no more than what they should have access to, but everyone has the freshest data available to them? We've got a great agenda today. We're going to walk through the different Google Cloud AI products and how to pick which one. Then we'll be joined by Derek, who will walk through how Etsy made that decision making process to accelerate their AI use cases. And finally, Alan will walk through how to leverage NVIDIA within Google Cloud to accelerate your use cases. With that, I'll turn it over to Alan. Thank you, Kate. Thank you, Kate. Hello, everybody. Good to see you. Thanks for coming. I'm going to talk a little bit, as Kate alluded to earlier, a little bit about the decision tree when it comes to making or choosing your AI infrastructure product or going directly to Vertex AI. So how we refer to that is Google managed and self-managed. Google managed are your managed services. Self-managed is your do-it-yourself, right? While these and this decision tree took a lot of time, thank you to my product leadership, but this decision tree gives separate swim lanes, but we're also going to talk about how a customer can use these products together. So moving from left to right, we start with Google managed, Vertex, go all the way to GKEGCE. So starting with Vertex. Five things describe Vertex, right? Prioritize managed services, customers who don't want fine-grained control of their infrastructure. I would also say training and serving custom predictive machine learning models. So that's your embedding order, your ranking order, your customization. Then I will also say Vertex model garden, right? If you're looking to integrate your AI quickly, you're not trying to start from scratch, Vertex model garden is your product. And then I'll probably also throw in early adoption exploration. So I'm an early adopter, first time using AI ML. You'll probably start with Gemini. But Vertex more or less giving you that opinionated platform will be your first course of action. And then this is what Etsy is going to get into a little bit later and what I want to touch on. It's MLOps challenges. So think if you're a bigger organization, you have a large AI ML workflow, and then you also have where you're responsible for a lot of smaller AI workloads. This is where Vertex makes sense. And it actually spills its way into Cloud Run. So today I'll speak about Vertex, Cloud Run, GKE, GCE. But Cloud Run as well has that small model online inferencing that we're talking about when it comes to inferences. And also, when you have your AI workloads and you need access or expiring CUDS, you can use your Cloud Run. Now, opinionated platforms, smaller workloads. Vertex AI is actually pushing that curve and going to larger workloads. Then as you move further to the right, you're starting to get into more specific larger Gen AI unicorns, starting with Kubernetes. Right? Kubernetes, these are customers or partner engineers who have a preference to working with Kubernetes. I can work with K8s. It makes sense. I have the head count to manage it. Then, with GKE, you're also looking at multi-cloud portability. So you can work with different clouds in a larger organization who's acquired multiple organizations over the year. The multi-cloud portability is one of the unique factors for GKE. And then, really, GKE's bread and butter, and what I focus on, is distributed training and inference. Okay? This, we're really talking about the models larger or the architecture of models larger than 70 billion parameters, where time and value are your primary concerns. So 70 billion parameters is really, when you start to touch, that's probably where GKE starts to play and you're moving away from the Google-managed platforms and you're more or less going to do it yourself. So GCE, high-performance computing, working with tools such as Slurm, if anybody worked with GCE in the past. And then, when you talk GKE, GCE, building your own, having that control of your infrastructure, you're also talking about configuration, custom pipelines, access to open source tools, kernel-like configurations. These are the things that describe your higher level infrastructure or higher level fine-grained control of your infrastructure. And this is what we reserve for GKE, GCE. So if you're a seller within Google or if you're a seller or partner working to provide services to customers, we really put this here just to show what our products can do and how we really have a product for every stage of the AI workload. And I'll speak about this a little bit later in how NVIDIA provides GPUs for every stage of the AI workload. But this is really talking about how Google really tries to solve the problems your customers or you, the customer, facing. Okay. So this more or less narrows down the four products we're going to focus on today and what my colleague Derek here from Etsy will be speaking about. So moving left to right, Vertex AI, I just touched on that. What really separates these products outside of what they do is really supported GPUs. When you're moving to serverless, the GPU support isn't as vast as GKE and GCE. However, they're moving, right? Recently, and as of yesterday, I announced it next. Their GPU, their 20% discount cloud run on GPUs is now GA. They're also expanding their GPU portfolio. But the primary GPU offerings and TPU offerings, that's primarily for inference. We'll touch on that a little bit as well, are reserved for self-manage and do-it-yourself. And then most of our products is, all of our products less serverless as they're getting caught up to speed, has the regional availability that you'll have for cloud. So this just gives a little bit more words to what I was saying earlier in the chart. But this is how you double click into our products. Cool. So, separate swim lanes, but in some instances our products do intersect. And when I say intersect, you should be using these products together, not against each other. So I alluded to larger training workloads earlier, or distributed training or inference workloads at 70 billion parameters or more, especially with time to values of utmost importance. However, there are instances where our products intersect. So starting at the top, if you're large-scale ML workloads, training for 1P recommendation models, it's usually self-managed, GKE, GCE. However, if you're training and fine-tuning, right, you're using managed. For Vertex, you can use managed ray for training LLMs. You can have custom rays with VLLMs on TPUs for GKE, for serving. You can have other instances, and I don't want to steal Derek's thunder, where the products are used together, right? So you can think spinning up a small model, testing that for training with Google managed, and then really having the larger AI workflow with a self-managed product such as GKE. And then finally, line of business teams using GEMINIs, talking about those early adopters, ML Ops challenges. You're primarily focused on Google managed. Cool. I'm going to hand it over to Kate, and she'll talk a little bit about our infra customer stories, attributes. Great. Thanks, Alan. Thank you, Alan. Brally speaking, we're seeing about three categories that customers are evaluating when they're determining when to put their AI workload on which Google Cloud AI product. So, broadly speaking, the first category is customer attributes. And what we mean by that is the customers, the organization's domain expertise, business model, industry, and also the skill sets within the organization itself. Meaning, is the organization looking for more out-of-the-box functionality? Is the organization looking for more no-code, low-code options as opposed to custom code options? For category number two, what we mean by workload profiles is what does the workload look like in terms of is it more ad hoc? Is it more a long-running workload? Is it something where there's constant experimentation ongoing? Or is it something that is more batch-oriented? That's also going to dictate where this workload should ideally live. And, of course, with any decision that you make, whether it's technical or not technical, there's always tradeoffs. And we'll go through what the tradeoffs look like for the different Google Cloud AI products as well. So the sweet spot, I would say, for Google-managed AI use cases, so many run on our Vertex AI platform, the sweet spot for this tends to be if there is more of an MLOps focus. If your team is looking for more out-of-the-box functionality so that they can focus on the high-creativity work as opposed to managing infrastructure, right-sizing clusters to workloads and so forth. If they're looking for more productivity improvements, then using a managed platform such as our Vertex AI product is going to make a lot more sense. But, of course, there's always a tradeoff of managed offerings generally have a bit more of a cost premium to it. But if you look at the TCO and you base it on the skill sets within your team, if there's a dedicated DevOps team, dedicated MLOps team, for instance, it could make sense in many cases. Moving on to self-managed AI. So the sweet spot for self-managed AI use cases tend to follow these patterns where, for the organization, there's a dedicated Kubernetes practice, for instance. There's a dedicated DevOps team, dedicated MLOps team. And the workloads tend to be more long-running, tend to be very involved. And there is a strong interest, a strong preference for cost controls, for flexibility, for fine-tuning, making sure that the infrastructure is as optimized as possible for that workload. And, of course, the tradeoffs inherent with that are having a dedicated team and sometimes meaning that some people may be a little bit delayed on running their workloads because they need to have it moved into more of a central location as opposed to being able to do more ad hoc work with, say, a managed platform. As you can see in this chart, Vertex AI and ML and GKE, they're converging. And they will continue to converge as our platforms, as we add more and more value to our platforms. Meanwhile, we're going to meet customers where they're at, whether they're looking for more out-of-the-box functionality with Vertex AI, they're looking for more flexibility and control with ML on GKE. And, of course, we have customers such as Etsy who choose to use both, who use Vertex AI as well as GKE as well as Cloud Run. And let's hear from Derek on the reasons behind that. All right. Thank you, Kate. So now I'm going to talk about some production use cases at Etsy using both Vertex AI and GKE and our decision process in doing so. So Etsy is an American e-commerce platform that specializes in handmade, vintage, and unique items. Etsy leverages multiple Google AI products to create personalized experiences for our millions of buyers around the world with state-of-the-art search ads and recommendations. Our ML platform team builds on those technologies, integrating them into our technical infrastructure so our ML practitioners can rapidly prototype, train, and serve ML models at scale. We currently use three main Google products that help form our ML platform. First, Etsy uses Vertex AI for efficient and easy model training. Also, we use Gemini in Vertex AI for our large-scale Gen AI production use cases, which I'll talk about in a second. Also, we use GKE for our in-house model serving platform to serve hundreds of models at scale in a variety of serving frameworks. Third, we use Cloud Run for simple and agile service deployment. In terms of why we use those Google AI systems, we find that Vertex AI increases our productivity in ML due to its useful abstractions and its ability to scale. For instance, we went from prototype to production for a mission-critical large-scale Gen AI use case in just three months. Also, our ML scientists found a 50% reduction in time to go from an idea to an experiment. On the flip side, we find GKE gives us improved cost efficiency and control. By adding automated tuning of infrastructure, we can reduce costs significantly. So let me go more into the depth about these use cases. So here is an example of Etsy using Gen AI and Vertex AI. We use Gemini Flash to generate image captions for Etsy listings to improve search engine optimization. Previously, only a small fraction of images had seller-provided alt text. We use Gemini for its seamless deployment, scaling, and capacity on TPUs. This allowed us to focus on core business values and implementing Etsy-specific workflows, instead of dealing with infrastructure details around capacity planning and scaling. Our process from development to production was as follows. First, we ran batch inference offline on Gemini through BigQuery for rapid prototyping and engineering of our prompt, making sure all of our edge cases were dealt with. After a successful experiment, we then ran batch inference using Gemini at scale with Dataflow for all the hundreds of millions of images in our inventory over a period of a week. Then we wrote our results into Bigtable to be used by both our web and mobile products and also to be used as a feature for our ML models. This was a critical project for our business. It was made successful by Vertex AI. It improved our business metrics such as visits and attributed conversions significantly. So I just want to give a shout-out to Dr. Tom, Mehar, and the rest of the Google team for making this possible. So I talked about LLM inference. Now let's focus on serving of traditional ML models. Etsy uses GKE to manage infrastructure so that we can move up the stack and focus on managing our ML applications such as model serving. To deploy models for inference, our internal customers deploy stateless ML microservices in our GKE cluster to serve requests from Etsy's website and mobile app. We manage these deployments through an in-house control plane, which creates and orchestrates Kubernetes resources needed for model serving. The flexibility of GKE allows us to deploy advanced and custom methods for monitoring and observability, automation, and load balancing. For instance, the control plane supports cost monitoring per model, distributed tracing, inference payload logging, and traffic mirroring. So we talked about inference. Now let's focus on training. Etsy's training and prototyping platform relies on Google Cloud services like Vertex AI and Dataflow, where customers can freely experiment with the ML framework of their choice. These services let ML customers easily leverage complex ML infrastructure such as GPUs through comfortable interfaces like Jupyter Notebooks. Massive ETL jobs. Massive ETL jobs can be run through Dataflow, while complex training jobs can be of any form and submitted to Vertex AI for optimization. The customers can experiment with any ML framework using ad hoc mode mix or managed training code. So now I'll hand things off to Alan, who will talk about NVIDIA technologies and Vertex AI. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. So I touched a little bit on NVIDIA and what NVIDIA does in terms of providing services for every level, I'd say, of the AI journey. And what that translates to is offering a comprehensive suite of performance optimized microservices, fully loaded sensors. But what that is is really data prep, training, evaluating, data retrieval, and serving. For GKE, they provide the chips in many instances for those training, for distributed training and serving workloads. Those chips are captured on the bottom here. For anything compute, we have a strong partnership with NVIDIA. We trust NVIDIA's tools. And when I talked about time to value being paramount for managing an infrastructure, it's also price performance. And as anybody in this room knows, if you've ever used GPUs or TPUs or running hundreds or thousands of them at the same time, these can get very expensive. So time to value is important. So having the right-performing GPU chip or proprietary Google TPU chip is paramount. And trusting or having that NVIDIA-GKE partnership is necessary for us to deliver the value to our customers. G2, A3s, A3 Mega, A3 Ultra VMs, A4, A4Xs. This was updated as of Monday. So if I'm missing one of them, apologies in advance. But these are all the offerings that are available, that NVIDIA has made available through our GKE platform. All right. So a little bit of a busy slide, but I'm going to start from bottom or go bottom up and talk about NVIDIA's inference engines and how they're using hardware and SKUs to deliver on parallel performance. But it's really what they're doing in the inferencing that's helping us, Google, really expand the offerings that we have on a GKE perspective or self-manage in general. So this shows what it goes from optimized to domain-specific, calling standard industry APIs, and how they're using pre-built containers and the Helm chart. Finally, this is just an overall view of all the products they support. And I hope this brings it back to what I started with in this original deck, which was our separate swim lanes for the products and then going forward and how they're used together. But essentially, we do rely on NVIDIA. And NVIDIA is essentially a customer similar to Etsy in providing us these services for our customers. And that's with GCE, Kubernetes, Vertex AI, and Cloud Run. We interact with NVIDIA in some instance along the AI journey. So when we speak from a Google perspective, we're also saying we're a trusted partner of NVIDIA, and we can provide these services to you confidently. And this is just, I think, a nice visual that the NVIDIA team put together to show how we're doing this. All right. Folks, please feel free to scan this QR code. I believe we'll share this after. Our premium tier is getting an upgrade to include Gemini Code Assist standard. And I'm going to have to read this verbatim because this was added from our Google Developer program. The Gen.AI Developer Credit and Google One AI Premium. All Cloud Next attendees, you get a 25% off of premium with the code Next25. So please take that there. And finally, some other sessions that I believe interact with what we went over today. This is more or less on the Vertex side when we're talking about Gemini. These sessions have yet to occur. So feel free to join them. One of the sessions that I'll be going to is building custom rack systems with Vertex I. I think this is really instrumental in understanding Vertex. And if you understand Vertex, you'll start to, I think, easily understand infrastructure if you haven't started with infrastructure yet. And then here are some other instances for GPUs, TPUs. This is more the infrastructure part where we're talking about the hypercomputer instance. Serving models and TPUs and GKE. And one of our bigger customers, Anthropic, how they're pushing the computing limits of AI at scale, especially when it comes to training. So these are other breakout sessions that I think will make this story, bring the story together, outside of what we just spoke about today. And, of course, your feedback is greatly appreciated. But myself, Derek, and Kate, sincerely appreciate you guys taking the time today. And hopefully we've eliminated some confusion when it comes to the Google-managed, self-managed infrastructure stack. But we also explain how folks are using them together. So smaller spinning up models with Vertex, getting a managed opinionated platform, especially if you have MLOps challenges. And then really when it comes to infrastructure, you want that control. You want to use GPUs. You want to use TPUs. You want to have that distributed serving and training opportunities so you can really provide that time-to-value opportunity for your end customer or your end user. And that's it for today. Thank you. Thank you.