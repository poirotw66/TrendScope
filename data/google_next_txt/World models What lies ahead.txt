 Hello. Hello. I'm Tim. I'm going to talk today about world models and what lies ahead for them. I'm a research scientist at Google where I work on training large-scale generative models that simulate the world. A key piece of technology to the idea of simulating the world is a generative model called world model. I'm going to start by talking about why world models are so exciting, what's going to motivate the reason for this technology. We can think about how we interact with AI right now is largely through language conversation. Here I have this conversation with Gemini where I'm saying, hey Gemini, what do you think the world will be like in 100 years? And we talk back and forth in text. And now large language models, they can do lots of amazing things. You can chat with them in conversation. They can do coding. We can use them as agents, which is a really important thing right now. But for some types of intelligence, what we really want is something that understands and interacts with and reasons about the physical world. And what world models will enable is for us to say, okay, rather than just asking it, like, what might the world be like? Could I step inside that? Could I interact with AI like how we interact with our world? And so that's exactly world models will enable us to interact with AI the same way that we interact with the world that we live in. And this is actually a two-way street because part of it's how we interact with AI, but the other part of it's how AI can interact with us. And with world models, they will enable AI agents to be able to better reason about the physical world and interact with us in the way that we're accustomed to interacting with each other. So this is the motivation. Next, I'm going to talk a bit about what exactly world models are and how they work. So world models take in some current state of the world. That's the input. It takes in the current, like, if we were in this world, what's the state right now? And then you give it some action. This is like, if you were a human, you're going to take some action. You're going to do something. If you're an agent, here I give it the action. Go pick up an apple from the stand. And then the task of world modeling is given this past state in the action, predict the future state. What comes next? And here it would output. This is a motivational example. So this isn't actually the output of our world model. But this demonstrates the type of capabilities that we're aiming towards when we're making world models. That condition on the past and some action to take, you predict the future. So now that we've kind of explained why we're so excited about world models and what exactly they are, I'm going to talk through three projects from Google, DeepMind, that push the frontier of world modeling in three different respects, and then talk a little about what I'm really excited for, what lies ahead after that. So the first one is Genie 2. Genie 2 is a large-scale foundational world model. It was made by some close colleagues of mine, this really amazing team, the Genie team. They came out with the first version of Genie around a year ago. And then in December last year, they had a research announcement of Genie 2. And it was a big leap forward in world modeling. The way that Genie works is it's a world model condition on keyboard mouse actions. So we talked before about the different types of actions you can have. Here, you interact with it by pressing keys and moving the mouse around. And I'm going to talk about a few capabilities that Genie has, action controls, long horizon memory, and object interactions. So first on action controls, I showed in the beginning maybe you'd say something like pick an apple, but here it's conditioned on the keyboard mouse actions. You can see in the corner it has WASD, so if you're navigating through the space, and then also on the mouse or using the arrows to change the perspective of the camera. And these are all generated scenes. The first frame is generated using Imagine. So it uses the text-to-image model to generate this environment. And then it uses Genie to navigate through and explore these generated environments condition on actions. So this is a world model. It has some other really cool capabilities. One is consistent memory. And this is something we kind of take for granted the fact that we have consistent memory, but it's a big part of our intelligence about the physical world. I know that if I turn away that this stage still exists and that there's the screen behind me, even though I'm not currently looking at it. And similarly with Genie 2, you can see that the camera tilts, and at some point out of view are the pyramids. But we know that they should still exist. And it is the case with Genie when it tilts back that those pyramids are still there. And that's a really important part of the physical intelligence actually that world models have, that it can have persistent and long-term memory of the world that it's modeling. And the third capability of Genie that I really want to highlight is the ability to interact with objects. So it's not just being passive in a world, navigating around it, and seeing things. But it's that you can actually interact with objects in the environment and influence and cause change to those environments. So here, pressing the space bar, it jumps up and it bursts this balloon. So the balloon actually changes because of the action that was being taken. So that's some capabilities of the actual Genie model. But I want to talk about a really cool use case that they highlight in the Genie 2 work, which is the ability to deploy agents inside the Genie model. And so here, we have this input frame. This is a person. You have a red door on the left and a blue door on the right. And using an agent, the agent, this is the Sima agent. There's a cool blog post that talks about this. This was a research work from DeepMind also. But rather than providing as a user the input to the keyboard and mouse actions, you can have an agent provide the actions to take. And so here, the agent is given the instruction, open the red door. And based on that instruction, the agent decides the actions to take in order to achieve this, all in a simulated environment. And then you can do the opposite. You can say, open the blue door, and the agent will then figure out which actions to take in this simulated environment. Okay, and why is this so interesting? Because we want to make really amazing AI agents. And there's a question of how do we train them and how do we evaluate them? But through models like Genie 2, we have the promise that we could actually simulate environments and use those simulated environments to train at a really large scale agents and evaluate those agents. And we have no limit to the amount of diverse and new environments that we could generate in order to train these agents. Next up, I'm going to talk about Veo 2. Veo 2 is Google's frontier video generation model. Again, this was made by a really awesome team. The Veo team released Veo 2 last December. And it was a big leap forward in video generation capabilities. This is a video that was generated by Veo 2 that really just blows my mind at how accurate this video is and how realistic it looks. And I'm sure you guys all saw or heard of in the sphere this really cool Wizard of Oz project that's using Veo in order to create it in the sphere setup. But I'm going to talk in particular about some of the capabilities that Veo has and how those relate to world modeling. And so three of them are enhanced realism, its motion capabilities, and greater camera controls. So realism, when we think about world modeling, it's really important that we can realistically model the world, not just that we model it, you know, approximately. So Veo 2, because it has really, really good accuracy and fidelity and quality, is part of the path there towards building world models. And here is just one example where you can see it has really realistic details and quality. Another is on motion. So it's not just about the quality of individual frames or images looking really high and sharp, but that we actually get the changes in the dynamics and complex things that are happening, like this swarm of bees, or that the level of the coffee actually rises when more of the coffee is poured into it. Which again, like, of course that should happen, but in order to get that type of intelligence in a model can actually be really challenging. But it's really important for generative models to get it correct. And another is on the camera. So it's not just the content that we're describing, but actually the view into this generated world that we're describing when we talk about the path that the camera takes. So in the first one I put these prompts up here, it says that the tracking shot and that the camera plunges underwater with a dog. And it does indeed make the camera follow this path as it was prompted to. Or on the right, it says that it's a low angle shot and then it generates this video from that perspective. And people do really like Veo2. The Veo team did a bunch of experiments where they compared it to other video generation models. And users prefer Veo2 when you look at its overall preference and also when you look at how closely it follows prompts. So this is really exciting. The model's really great. And it can help lead us toward world models. Because similarly to how for language models when we're predicting the next word, if you're predicting video, you need to learn about the world. Just like how when you're predicting the next word you need to learn about all this intelligence that exists. You need to learn about conversation and about people and about coding and about math. And all this stuff that relates to kind of linguistic and symbolic intelligence that language models cover. But with video generation models, when you learn to generate video, the model has to learn about the underlying world that the video is capturing. But there's a really important part, which is that they need to have accurate physics. For world modeling, having accurate physics is so essential. Because the whole idea of them is that we're going to interact with these models the same way that we interact with our world. So we need them to resemble reality. And that is why it's so great that Veo really pushed the frontier of accurate physics. And here's an example of a video that a colleague of mine generated that I think really demonstrates the physical realism of Veo in a great way. Because even just the physics of cutting through the tomato and the friction that it has and as it falls off the knife is truly amazing. And we certainly have more work to do to make these even more physically realistic. But this is a really great step in that direction. And now the last model that I want to highlight that really relates to the idea of world modeling is Gemini. Gemini 2.0 and its native image generation capabilities. So Gemini 2.0 Flash, which is the faster version of the model, you can use it to create images directly. And it actually uses the language model to create those images. And then you can converse back and forth with it to edit or to modify images or to generate them. And so this is done in AI Studio. You can go try it out in AI Studio. And here, for example, it says, can you add some flowers? And it does. And then you can change things about it, too. Like, okay, that's nice, but I prefer if they were tulips. And let's see how it does with that prompt. There it goes. So it changes the flowers into tulips. So this is really amazing for a lot of creative applications and use cases that you can do. generate things for educational use cases, too. You can make content. But it also relates to world modeling because Gemini's native image generation model can model these complex interactions with the world. So I'm going to talk through some examples of that. Here we give Gemini 2.0 an input of an image. It has a door. And we say, how would you open the door in three steps? And then it generates three images showing the trajectory to how you would do that. And so this is really cool because it's actually showing intelligence about the physical world. Yes, there are many ways in which we can use this creatively, but the model understands the task of opening the door, how to break that down into getting closer, that you isolate on the handle, which is what you need to open it. And then you finally open the door. And you can do some other really impressive things. Here, there's an aerial view here of the Eiffel Tower. And we give it the text also, can you generate this view from the ground? And then it's able to generate this image from that. So the model, because in Gemini, in a language model, it has so much intelligence just about the world and about all kinds of general information. When we couple that with image generation capabilities, it's able to do some really complex interactions and generations that require reasoning about and understanding the world and the environment. And in this case, like different perspectives of the same object. There's another one, pan to your left 20 degrees. There should be a switch somewhere. And so then it generates this image. It's a bit hard to see, but there is a switch that it put on the left-hand side. And then we can pass that back in. But so now we put a red circle around it and say, update the view as a result of this action. And then it actually turns the light on. Which is, it's pretty awesome. It is understanding the types of interactions that we have within the world. And it's modeling those. And this is really crucial to understanding how the model could interact with us. How it could reason and do intelligence that relates to the physical world. And this is a big part of what world modeling is, that we actually want to be able to take these actions and see how they would impact the future state of the environment. And the last one to show here. The red circle represents the user's interaction. Update the scene accordingly. And then it turns on the stove. We say, okay, pan right 30 degrees. And reveal a kitchen sink next to it. And then, thanks, can you please zoom out? We know what else is there. And it fills in the rest of the environment. That there's a kitchen countertop and some cabinets. So, this, you can try out this model. I mean, both Veo2 and Gemini Flash with image generation are available. And they're really fun models to play with. But I think something that's really unique and interesting with this Gemini image generation is the intelligence that it has about the physical world. The ability to really take actions and navigate and see how different changes influence the images that it generates. All right. And now, just quickly, I want to leave you with two thoughts about what lies ahead. What I'm really excited about. And the first is this idea of physical intelligence. We talk about AGI and trying to make artificial general intelligence. And when we think about making artificial general intelligence, we're generally thinking of making these systems that have all the same cognitive abilities that you have. And a big part of our intelligence relates to the physical world. Some of it, of course, relates to being on a laptop, texting each other, making documents, coding. And that's a big part of human intelligence, too. But another part of human intelligence is this. It's this world that we live in, that we move around in, that we talk with each other, that we build physical things, that we understand physical space. And physical intelligence is a general term for that type of intelligence that we're trying to build. And something I'm really excited for is how world models fit into this. I think world models are going to be really important for this. And the combination of language models and world models, I think, is going to lead to AI that can better reason and act in the world. And when we talk about act in the world, I actually want to highlight for a quick second some of the work from Gemini Robotics. Because that's a big part of acting in the world, too, is actually getting AI to be in our physical spaces doing useful things for us. And the Gemini Robotics team had a really exciting release just a few weeks ago where they demonstrated some abilities where Gemini is actually used to help power robots. They have a really nice kind of blog post and some videos that explain this in much more depth. If you're interested, I really recommend checking that out. But that's cool, because just how Gemini can do a bunch of things with images and videos and text, Gemini Robotics adds this ability to take actions for robots with generative models. And I'm really excited for the future when we have better world models that this will, together with robotics, empower us to have really useful robots that can do things for us in our environments. And this last idea I want to leave you with is the idea of world simulation. That as we see these models getting better and better and more and more realistic and more and more capable, that over time actually approach models that are so realistic and so good that they simulate reality. And world simulation is the project that I'm working on and I'm really excited about. So that is all I have. But we have some time for Q&A if anyone has questions. Also, it would be great to provide feedback. So please fill out the survey on that. And that is all. Thank you very much.