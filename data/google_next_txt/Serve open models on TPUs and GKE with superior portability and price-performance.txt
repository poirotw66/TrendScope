 . Cool. Good afternoon, everyone, and thank you for making it up here. And it's just few more sessions to get from now to the concert. Are you all excited for the concert? Yeah. Who are going? Show of hands. Everyone, okay, fine. But you know what? Before you go there, make sure you catch up this so that you know how to save cost so next time you can keep coming back here and going to concerts on the company dime. That's what we are doing. In the meantime, let's make sure this time together is worth it. Yes, we are worth it. You'll see how. Let's get going. Today in this journey, we have John Lee, who is an awesome engineer from Google Cloud, and he's right there. Hi, John. And then we have Mustafa from HubX. And Mustafa is going to show us how they have optimized performance and cost for the image generation application called Momo. Do you guys see here the buttoned up? Okay, first tell me, how are our images? Do you guys like it? Like our images here? Awesome? Okay, this is generated by the image generation application from HubX, their Momo app. And I was really surprised to see this image of mine. After giving all the bad angles, the blurred images, it just gave me this awesome image. And I was like, man, my AI doppelganger is much better looking than me. What do I do? Okay, now it's time to go hit the gym and get the chiseled face. You know, that's next. Wait for it. Next year, you'll have the real my picture there looking like that. Awesome. We have very big packed agenda for you guys today. And let's stay attentive. Let's get started. Okay, a lot of challenges with AI serving. Who do you, who do here do AI serving, AI model serving? One, two, three, one. Good, good, good, good. Awesome. That's amazing. So let's try to imagine a Formula One race. And a Formula One race, you need a powerful engine. That's your performance. But wait a minute. It has to run on the lowest fuel possible cost. And then the Formula One car should be able to take parts and remove from this and that. And should be able to run on all kinds of tracks. That is ice track, dirt track, paved track. Wait a minute. That does not happen in real world. But that's the high stakes game that we are in for AI inferencing. So it's the triad of latency, cost, and portability. And today, as you know, in the real-time application, real-time inferencing, it's really very important as the latency and the performance and the throughput. But you can achieve all of this for sure if you get some beefy TPU, GPU accelerator. But that's going to drive your cost way up. You will not be able to sustain it. And definitely there's spot and other things, but you cannot build architecture. People have tried to build inferencing on spot here for this. One. Wow, great. Is it working well? Yeah. It's great, right? It's good, but it's not going to help you because spot is actually, as it says, it's spotty. We cannot guarantee you the availability of it. And then the portability. There's multiple libraries that you can use and the frameworks to optimize your model so it can be very performant. And I think Mustafa is going to explain more of that here. But that meaning to say you have, especially with TPU, you have framework and libraries. TPU is known with JAX and Jetstream. It's known to provide great performance per dollar. But that also adds learning curve for you because it comes with its own libraries, right? So that triad of the thing is what we are speaking. At Google Cloud, we're uniquely positioned to solve this problem. And you'll see how in the next few slides. This is, wow, this is a powerful slide mixed with so much of data. We have been really working hard last year to do some of it. And this is also not all of it. I've tried to collate a few of it for you guys to see how to gain performance while optimizing on cost for your AI inferencing. There's a lot to take in. Let me take you through each one of this. We have slides later on talking about each one as well. At core of superior serving lies optimized performance. We have meticulously engineered our infrastructure to minimize latency and maximize throughput, starting with Trillium. Trillium is our next-gen TPU that we released in December that actually provides 3.5x inference throughput compared to our previous generation models. That's your infralayer. Coming above is the storage. We have also optimized storage for you on GKE. We have secondary boot disk. We have GCS fuse. We have hyper disk. That helps you for the uptime as well as it also helps you with the faster data load time. We'll see how and what in the next one. At the keynote, I think I said, right? At the keynote, TK's keynote, we also saw that GKE released the inference gateway that's one level higher, which now accesses superconductor and knows how to route your traffic to provide the higher efficiency and higher performance for your applications. And all of them are packed and wrapped with GKE optimized container compute that provides the greater startup time and scale. And that's actually 7x of the improvement in performance. And not only that, we also have multiple ways for you to save cost. One is that, have any one of you heard about DWS? Yes, one, two? Great. Do you use DWS today with CPUs or what do you guys use it with? Do you know? Okay. DWS is a dynamic scheduler and we are releasing DWS Flex and Calendar, which is basically pay when you use model. And this one actually is known to provide about 50% lower cost for TPUs. And then we have container compute class that's on GKE, which allows you to keep switching whichever way, whatever way, whatever compute, low compute, high performance, as based upon the availability and will help you save more and provide higher obtainability along with spot. However, TPUs also provides an optimized performance per dollar with its integrated vertical libraries and framework. That is well known today and we are very happy to announce in at Next, which also we announced in the keynote, is that TPU now supports VLLM. You don't have to build your own libraries or TPUs own libraries to do inferencing with TPU. You at last get that fungibility. You can take the same VLM and then move it in TPU and move it on GPU. But wait a minute, we just not only support you with VLLM, we also make sure it provides you the best performance per dollar on TPU. We are going to see how. And definitely, as you know, PyTorch is supported on TPUs like JAX and we'll continue investing in that. Let's get to our Trillium. Who knows Trillium here? How many of you use Trillium here? Okay, right one. Trillium is our latest generation TPU. Trillium provides 4.7x performance per chip, twice the HPM, twice the interchip interconnect. And what does this really mean for you? That means it provides four times the performance per training and 3.5 for your inferencing. And Trillium also with this multi-slice approach is going to provide you the scale that's necessary. About 100k chips Trillium cluster is what we have run up to and there will be more announcements for more higher numbers. And as we said, Trillium delivers the highly competitive price performance advantage. It's built for cost efficiency. As you can see here, this is our image diffusion on the benchmarking that we did on Trillium. Trillium delivers the best inference performance for image diffusion on any TPU available today. And as you can see here, it is about 22 cents to generate 1,000 images on Trillium. We just don't stop right there. We are overachievers. So, with GestStream, Trillium can be optimized even more. And it provides up to three times the performance for your dense LLM models, LAMA, and Mistral. Trillium is optimized for performance and cost for your LAMAs, your Mistral, for your DeepSeqs, all of those models. GestStream is our Google-owned inference engine that is high-performance engine. And it has various different mechanisms to help you with more performance, such as continuous batching, sliding window attention, prefix bucketization, and many, many more things. And coming on, moving on from Trillium to our storage performance, the secondary boot disk that you can attach to each node, where you can preload your data or container on GKE significantly improves the speed and efficiency of your port startup. We have seen about 29 times. And that is, yes, it's not 29%. It's 29 times. And we have seen many of our customers talk about this and really be appreciative of this. You'll see also how Vex is talking about this as well. And then GCSFuse and HyperDisk provides you the faster data load time. And GCSFuse is available with CSI drivers and persistent volume claim. And HyperDisk is another one that gives you the faster load time. And here comes our GKE optimized gateway. So today the round-robin, the way of doing the load balancing, doesn't work for your VLLM inferencing. That's the reason we built the inference gateway where you can use custom metrics. Custom metrics from your VLLM server metrics, like KV cache utilization, queue depth, to route your traffic. Basically, a conductor, an intelligent conductor, who understands your inference, and according to the metrics that you have defined, it can route the traffic to make sure it's more efficient. It's proven to provide 60% more efficiency. Also, the inference gateway has affinity towards LoRa. So that way the requests go to the same LoRa, reducing the LoRa swapping at the back-end, which is known to increase the efficiency by 30%. And this is where we are trying to meet the customer where they are, right? There are two options. TPU now supports both VLLM and Jetstream. And who use VLLM here? Anyone? Awesome. One hand. Two hand. Three. Great. Four. Awesome. VLLM has quickly become one of the most widely adopted open-source LLM inference engine. I think with some numbers, it's about 10x growth last year alone in usage. If you have not used it, please try it out. And I think this inference is a great thing, right? You don't need to go and build some model, train it if you want to, definitely. But you can use today, every day. There's such amazing models that come out. I'm, like, so surprised. It's like in a candy world. Every time everybody releases an open-source model, I'm like, wow, that's amazing. You don't have to do much. You can now use these models and attach to your business use cases. And the VLLM makes it more easier. And with both TPU and GPU, you can take the low-cost, high-performance advantage on both of them. And when would you use VLLM? I would use VLLM when it's an open-source community. You want familiar tooling and that portability between GPU and TPU. And Jetstream is when it's Google's own high-performance, cost-efficient. It's also open-sourced LLM inference engine that's built for TPU on Jaxx. So LLM is fantastic for getting started. And then if you want that portability between GPU and TPU. And we have seen people using Jetstream where they have their own AI team. And that dedicated AI team is pushing the boundaries. Pushing the boundaries of AI every day. And they're taking advantage of full Jaxx ecosystem and the mighty power of the TPU. Awesome. What did we do? The OpenXLA is the open-source project from Google. And we built PyTorch XLA on top of it for TPU. And that is the back-end that we use for VLLM. And this helps you now easily switch between GPU and TPU. Let's dig deeper into what. What does that support mean? What do you mean by supporting TPU on VLLM? We just not support you. We just do much more than that. So the first easy thing is easy portability. On GKE, run VLLM inferencing. On TPU or GPU, whatever you want with just a little bit of config change in your deployment YAML. And John here will show very shortly how that's possible and how easy that is. And definitely, as I said, we never stop by just supporting you something. We'll also provide you the best price performance that we can provide. And then the obtainability. Has anyone tried to get TPUs here? Have anyone worked with TPU? Except here, a few of them in the first row. Try it. Try it and you'll get to know how it's not that easy to obtain TPU. And even to that matter, GPU, right? And the level, the version of, it also depends on what kind of version you want. We have several great announcements that are going to help you with the obtainability as well. We'll see each one of them. And now I'm going to call upon our awesome engineer, John here, to show you how easy it is. And I'll be right back after that with our second demo after the first one. Come on, John. Take it over. Thank you. Thank you, Kavitha. Running VLM on TPU is easy, like Kavitha said. And here I'm going to show you how to do, how to run a VLM inference application for both TPU and GPU on the same Kubernetes cluster on GKE. What I have in front of me are two Kubernetes deployments. Both of them are for VLM. If you inspect closely, they are very much the same. I'll point out a few differences. What I have on my left-hand side is for TPU. And there is a pod selector, sorry, a node selector looking specifically for TPU V5E nodes. And for the GPU, it's looking for NVIDIA all four GPUs. And for the container image, both of the container images come from the VLM Docker image repository. However, there is a special image built just for TPU. When it comes to the command and arguments for running VLM, since we're running the same exact model, they are identical. Lastly, for the Kubernetes resource limits, they're also set differently for obvious reasons between TPU and GPU. Now, let's look at our cluster configuration. So prior to this, I have created a Kubernetes cluster in the US East 1 region. And I have two node pools created, one for TPU and one for GPU. And they correspond to the node types that I showed earlier for the Kubernetes deployment. And previously, I have also deployed the two deployments onto the Kubernetes cluster. And now you can see both of the workloads. And there are two pods for each of them. Just for demonstration purposes, I'm going to also create a Kubernetes service as a load balancer to route traffic between the four pods. And this is only for demonstration purposes only. For production, I would recommend to use inference gateway, which Kavitha just covered earlier. With this load balancer, it's going to load balance traffic between the four pods using a pod selector targeting a common name across TPU and GPU. I'll show here real quick. This is a name we're selecting. And earlier, we have both of them. Now, with the Kubernetes service deployed, we should be able to hit this service from the Internet. Let's get the external IP of this service. Now, I should be able to just send requests with a very simple prompt hitting the four BLM backends. Now you see the response is coming back. And in this case, we're just running it in an infinite loop and just going to keep going. I'll just keep this running here. When I also created this cluster, there's a feature I turned on. This is called automatic application monitoring. And with this enabled, it gives you this super awesome BLM Prometheus dashboard. And in this dashboard, you will be able to see your throughput as well as your request latency on the request level as well as the token level. This concludes my demo. Back to you, Kavitha. Was it cool? Easy? What did you give me three words that you took away from the demo? Shout it out. Just shout it out. Easy? Okay. Optimal? Can you please be louder? Sorry. Portability. Yes. Awesome. Somebody watch my slides. Like it. Anything else? One more? This is your quiz. How much attention to you are on my slides? One more word. Experiment between GPU and GPU. It's just not experimentation. It's production ready. It's for you guys to go and do this right away. Awesome. Now let me come across my promise. I told you about the performance per dollar for Trillium on VLLM. We really, really, really, really worked hard. And just within two months, we provided about 3x optimization on inferencing on VLLM on TPU. As you can see here for the Lama 2.8b, the throughput jumped about three times, and for 70 billion parameters, and for 70 billion parameters, about 75%. And I'm really proud to say Trillium provides exceptional performance per dollar on pre-fill heavy workloads on VLLM. I'll repeat this again. Trillium provides exceptional performance per dollar on pre-fill heavy workloads on VLLM. What does that mean? Your summarization, your recommendation use cases, your metadata extraction, your code use cases, code completion, AI agent, AI supervisors, all of them will get the exceptional performances on Trillium using VLLM. And this is big, guys. Now let's move on to see how can you further reduce the cost. How can you further reduce the cost of using Trillium, using us? And there are two ways to do this. One is the dynamic workload scheduler I talked about. So there are two parts to it. How many of you use this? Only two of you, right? Dynamic workload scheduler. So it's basically dynamic workload scheduler is nothing but pay when you want to use it. So there's select start, and then there is calendar. And we support this on GPUs and TPUs. GPUs, I think, released as of this week as I'm talking. TPUs will come shortly. Please look out for my blog on the Google Cloud blogs. So what does that mean? So calendar is, you can say, I want to reserve certain capacity, let's say Trillium, from this time, and you can go all the way up to 28 days. And in that 28 days, or whatever time duration that you have given, that capacity is for you. It's not like spot, where, you know, you will just go reserve it, and, you know, you just want to use it, but it can be there, it might not be there. But this, for that duration that you have requested, it is going to be there for you. And then definitely the other one is the FlexStart. What does FlexStart do for you? FlexStart is similar to Candor, but it is just the duration. You do not say when to start. You're just going to request, I want this for up to seven days, and then you're going to get the same experience. You'll get uninterrupted capability, capacity until those time. And when should you use what? I think this is one of the most important things. As you can see, you want DWS Calendar and Flex for your training. When your training is coming and you know you need beefy chips, you know certain infrastructure that you need, you can reserve it for that, and you'll just run for a certain time, right? You don't need to run for a long time. You're not building foundational model. In that case, you can just request for DWS Calendar or DWS FlexStart and go with this. Model fine-tuning, model experimentation, and even for your batch inferencing. In case of inferencing, for batch inferencing, this provides the lowest cost. As I said before, tell me how many, let me see how attentive you guys were. How much cost saving do you get with DWS Flex? How much percentage? Now, he said 30. Up to 50%. You get up to 50% cost advantage on DWS Flex for TPU. Please don't repeat these numbers for other accelerator because I cannot promise. It's for TPU. And calendar mode can give you up to 30%. And this will be released shortly. You can now, the TPU obtainability has been an issue with great demand. And hence, it's not easy for us to give it to everyone to try. And I think these are the ones that's going to enable that. And then you can try using TPUs across. And then, I think I've saved the last for the best. So now, this is called container compute class. Has anyone used container compute class with GKE? Container compute class helps you improve your obtainability. As you can see here in the example, what we do is, I say, whatever John showcased you, the VLLM serving, is that load balancer, send all the requests to TPU node pool. That's where my VLLM is serving. But wait a minute. What happens if my TPU node pool, they say HPA is scheduled and there's a scale and there's not enough trillion? Then you say, okay, fall back to GPU node pool for spots. Even that is not available. Fall back to GPU on demand or anything. This is just an example here. You can fall back and GKE takes care of this automatically for you. You don't have to go find TPUs and GPUs, create multiple clusters here, here, and then you do the routing or you figure this out. No, all of this is runtime. And that improves the obtainability. This can be used on both ways, either for, especially for cost. Also, you can start with saying, start with spot, then go to my on-demand or to my reservation or DWSplex and can do in the future. So this helps the overall obtainability and also portability. For the very first time, you can use the same VLLM inferencing, both running on GPU and TPU, as we have shown here. And we take care of switching the traffic and managing and creating that infrastructure for you when we see one is bottlenecked, where when it's trying to scale, we don't have enough of that accelerator. And now I'll bring back again, John, to show you this and how you can do this one. Welcome back, John. As Kavitha have said, obtaining TPU, sometimes it is hard. Even GPU, it is hard. That's why we introduced custom compute class. So here, I'm showing a simple example where you can have a priority order in terms of your production preferences for different VMs or different capacity classes. In this specific example, I have TPU V6E Trinium as my highest priority, and I wanted to consume my reserved capacity first. If for whatever reason we couldn't get that one, it can now be forwarded back to L4. In this case, L4 on-demand instances. And lastly, we can have also Spot just in case we couldn't get anything else. And for this to work, you would also configure a horizontal pod autoscaler. And in this case, we're going to use the VLM queue depth metric to have that determine the number of replicas we want to have for the deployment. Lastly, in our Kubernetes deployment configuration, we can leverage a two-container configuration, one for TPU, and one for GPU. And in this case, there will only be one of these active at runtime. Depending on the hardware, at runtime, it will actually check what type of hardware it is. One of these paths will succeed at the other container or just simply sleep. Also, last thing to mention is in here, I have a node selector looking for the custom compute class that was shown earlier. So with this simple three YAMLs, that will let you get more capacity expanding your access pools to be able to, you know, leverage across TPU and GPU for your VLM inference workload. Now, I would like to introduce Mustafa to tell us how HubX optimized TPU for image generation. Thanks, John. I think we should... Yeah. Hello, everyone. Good afternoon. My name is Mustafa Özüysal and I am a researcher at HubX. Today, I will walk you through our journey, specifically how we deployed and scaled large-scale generative AR workloads using GKE and the powerful Trillium TPUs. HubX is a venture builder developing mobile applications across various verticals. These include AR chatbots, image and video generation. And most of these apps use state-of-the-art generative AI technology, which brings us to the core of today's discussion. Let's have a look at two example applications that highlight our inference needs that exceed generating more than half a million images per day. The first of these apps, Momo, lets you generate AI photos and also videos and create personalized professional headshots for specific users. So this includes not only inference but also a small-scale training, a fine-tuning to create smaller LoRa models per user. So let's do this and submit a job on our GKE cluster using our application. So let me switch to the demo. And then we can launch the application. So you can see we are not writing prompts here. We have some fixed styles. So we can go and generate some images. Some styles involve generating videos for the specific users again. And we have some styles that include multiple people. So let's go and create some headshots for LinkedIn. So let's get started and upload some photos from our photo library. So these are some of my apps, some of my images. Some of them are good, some of them not so much. The app warns us if we have duplicates and so on. And once we do this, we can continue. We can specify some tokens to customize. And then finally, I am not able to... So let me autofill this. And then I will take it out and then fill this part myself because I think the onscreen is not working. Okay. Okay, now the app is actually uploading the apps. And as we are... As I am walking you through the process and the architecture on GKE, this process will finish and then we can go through the results together. Okay, so let's go back to the slide, I think. Yeah, the second example application, Tattoo AI, lets users to create customized tattoo designs and even try them virtually on. Here, this is a very latent sense of operation because we want to show the users the results immediately and we are generating multiple potential outcomes per request. So this means that we need to keep the latency really long to keep it more interactive. The scale and the complexity of these apps means that optimization this workload is quite complex. So when we are optimizing generative AI workloads, this comes down to balancing three key factors. First, the newer, larger models actually have higher output quality but this comes at the cost of increased latency. for user-facing apps like ours, keeping this down is critical for user experience. When demand spikes, the cold start time to take a node to an inference-ready state is critical for horizontal scaling and also for making use of the spot machines. Finally, deploying and running generative AI workloads is usually quite expensive. We are always looking for a sweet spot that balances quality versus the cost of deployment. So to solve and tackle these problems, we rely on the Google Cloud specialized infrastructure. We are looking for a solution that works seamlessly together to tackle all these challenges. For reducing latency, we rely on trillion TPUs. Coupled with the XLA compiler, this means that even for the larger newer models, we can keep latency down. To mitigate cold start delays, we use GKE to efficiently handle the node pools and integrate with specific storage solutions like HyperDisk for the large base models and FileStore for the smaller dynamic loading of smaller LoRa models. On the cost front, GKE helps us with the scaling of the workload and it efficiently integrates with the storage solutions. We make use of spot instances where possible and finally, of course, we optimize models heavily using distillation and caching algorithms. So let's walk through Momo Apps architecture. We build our infrastructures on Google Cloud relying on GKE as the core orchestrator. First, the user request hits our Cloud Run service. This is passed together with the training images from the user into a pop-up for asynchronous processing. tracing. This triggers a training job on our training node cluster on GKE, which actually, as we are doing now, creates a small LoRa model on top of the base model that actually can create images of the user. Now we need to pass it to the inference node cluster very quickly. So we use here FileStore as some sort of cache to make it available to the inference node cluster. The inference node cluster is making use of Trillium nodes to quickly generate many images which are then passed back to the user. So for Momo, managing the model IAO is critical. Here we have two problems. We have tens of gigabytes of base models that needs to be loaded. We tried different solutions. Secondary boot disks provides amazing performance and this was the best option. However, for our use case, our team later evaluated HyperDisk ML and we found that it offers the most cost-efficient alternative with good performance without creating delays on our pipeline. For the smaller LoRa models, we are using mounted FileStore to pass the small model from the training to the inference node, but we are also using standard GCS buckets for archiving these models for later inference jobs without the want requiring training. So let's look at the impact of introducing Trillium to our latency-sensitive Tattoo AI application. Here, by taking advantage of the storage solutions and the Trillium chips, we have lowered latency, cold start time, and cost significantly. And these addresses actually all these three key challenges that I have mentioned at the beginning. These are not incremental updates, but these let us create a responsive generative AI inference service. To better quantify the impact of the Trillium introduction, we benchmarked the Flux family of models on the Trillium TPU versus A100 GPUs that we used to use. And for different models, we can obtain three to seven times improvement, which highlight the power of the Trillium chips to generate much more images at a much lower latency. Of course, this wasn't just a matter of switching hardware. It's always when you go from GPUs to TPUs, and especially a new TPU, you have to also make some changes. Basically, one of the optimizations we did was to tune attention block sizes to take advantage of the larger Trillium hardware caches. Rotary position embeddings is usually a bottleneck in these models, also for GPUs, so we had to play with the Python PyTorch code a little bit. At the end, XLA compiler was able to create much more efficient code that has less memory traffic. Finally, because we want to use four chips, and we don't want to have large cold start times, we used multiprocessing with Torch XLA queues to actually preload four different base models into four processes and communicate with Torch XLA queues to achieve the simultaneous generation of four images. So, this has been actually possible thanks to the power of the Trillium chips and also the flexibility of the Torch XLA software stack, which is open source software. So, now we can go back to the demo and then have a look at the results. Hopefully, you're ready. As you can see, we have generated a lot of images even within the duration of such a talk. I actually do have a suite that I wore to a wedding 10 years ago, but I actually don't know, honestly, where it is at home. So, it's kind of nice to not have to look for that. And I think this is the end of my talk. So, again, this hasn't been very easy for us to handle all the complexity but within a short time span, we were able to manage GKE and the Trillium tips to generate a responsive AI service. So, with this, we go back to Kavita for some final remarks. Thank you. Thanks a lot. Didn't he look handsome in those pictures? Yeah? Awesome. You should try the Momo app. That's great. We want your feedback. Please use your Cloud Next mobile app to provide us feedback on the session. And then, here are the next sessions. I recommend a few of this for you. The first one is actually happening after 15 minutes from here. We're just going to talk about inference at scale, which talks about JetStream as well as VLLM. And there's VLLM also later. With VLLM, I think the CEO is coming and talking along with our team on the Google Cloud. And then, we also have tomorrow another session that talks about how GKE can reduce 80% lower cost and 30% lower latency. And with that, this is another application that we just created to generate your own image using V5e, our previous generation TPU, and go for it and look for our next generation TPU called Ironwood, which we announced, and wait for it. I think mostly coming this year. Thank you so much and thank you for coming and listening and really enjoyed presenting this to you. and I'm coming for you. Hey, hey, Toyota. A