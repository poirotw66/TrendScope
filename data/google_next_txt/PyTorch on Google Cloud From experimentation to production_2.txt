 Deepak Patil, Product Manager, Google Cloud Hello and welcome. I'm Deepak Patil, a Product Manager at Google Cloud. And today, we'll quickly recap the section on PyTorch on Google Cloud. We'll talk about how you can run workloads for PyTorch and run them from experimentation to production. We are going to talk about two key capabilities here, performance, and resiliency. PyTorch is a very popular framework among ML developers. You have the latest models and innovations available, a robust ecosystem with tools and libraries, and providing the best performance through compiler technologies, custom kernels, and hardware optimizations. PyTorch XLA is a library for running PyTorch workloads on XLA devices, such as Cloud TPUs. It's designed for scale. It's optimized for performance through the XLA compiler. And it provides advanced observability and resilience. The goals for PyTorch XLA are to provide robust community support through well-tested code and reference implementations, to support our customers through the best performance, and to provide interoperability between GPUs and TPUs for all production tasks. And finally, to support a very good ecosystem through tools and libraries, integrations with Hugging Face, PyTorch Lightning, and other choices. We also introduced Torch Prime, which provides a reference implementation for PyTorch XLA's distributed training features in a simple code base. It provides the best performance for large context-length models and custom kernels using the latest distributed sharding techniques. Finally, it provides ease of use through a single trainer code base for different types of models. Next, let's look at resiliency. We are going to focus again on LLM training with PyTorch on GPUs. A typical training workload has a certain number of GPUs and a certain duration, usually of the order of weeks. Your goal is to run the training workload as smoothly as possible. So you start on day one, run a certain number of steps every day, finish your computations, and complete your workload. The reality, however, looks like this jagged curve due to a lot of factors. You see a lot of interruptions happening with the white columns because of failures, whether they are unforeseen failures or planned maintenance. You see the lost productivity in the red columns where you have to go back to your previous checkpoint on a failure. The end result is that the training job takes a lot longer to complete and there's a significant delay in time to market and a significant increase in cost. What can we do about this? A typical training job workflow looks like this. The bulk of the training job, shown in the middle in the blue bar, includes training state migrations, monitoring, and live remediation. Our job is to make sure that you spend the minimum time on these activities so you can actually spend time doing what you want to do, which is calculating the training steps. Therefore, we are introducing a series of techniques here, such as multi-tier checkpointing, which provides optimized checkpointing with async saves and fast restore, better visibility and notification for failures, degradation, and stragglers, and elastic training with the policy-based options for in-place GPU restart, node hot-swap, and elastic scale-down and scale-up. Finally, they're all packaged together for easy deployment with DLSL containers, NVIDIA NEMO, and GKE integration, and good-put recipes. This picture shows how you can take all these techniques and they come together to improve good-put. The user can configure a customizable Python script for the remediation policies. The supervisor listens in the background to signals from diagnostic service for failures, and then executes, based on the policy, the remediation workflow. Depending on the error, if it's a correctable error, like a software failure, you can do a quick in-job GPU restart, restore from local checkpoint, and resume the workload. Second, if it's an uncorrectable hardware failure and you have spare capacity, you can do a quick node hot-swap, restore from peer checkpoint, and resume the workload. And finally, if you have no spare resources, you don't have to stall anymore. You can automatically scale down and continue the workload. In the future, when you have replacement nodes available, you can scale back up automatically to the full capacity and continue the workload. This is how you can maximize good-put of productivity. For delivering these capabilities, we are also integrating with NVIDIA's solutions, such as NVRX Resiliency Library and NVIDIA NEMO framework. In an internal case study we did recently with over 1,000 GPUs for a typical training workload, we saw good-put improve from 80% plus to 90% plus. That's a significant improvement. Even a 1% good-put improvement can translate to savings of a million dollars or more. Think about it. That's the power of these techniques. This table shows all the capabilities and their specific impact on the various metrics and the good-put improvement. To summarize, you can maximize good-put and productivity for a training workload using PyTorch on NVIDIA GPUs on Google Cloud through a combination of elastic training techniques and multi-DR optimized checkpointing. And it's made easy to consume through simple packaging in a single container. And we have ready-made good-put recipes for you to try out today. Thank you.