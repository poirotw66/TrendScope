 Let me welcome you all to this session where we will be talking about scaling data science with Python on top of BigQuery. I'm Sandeep Karmarkar, a product manager with BigQuery. With me, I have my engineering colleague Josh and Wu, and we also have a guest speaker, Ferri, flying all the way from Germany, representing Deutsche Telekom. Thank you, Ferri. Today's session, we'll be mostly focusing on an open source library that we launched last year at the same venue called BigQuery DataFrames, or BigFrames to be short. We'll describe how BigFrames enables Python data science on a large scale data. Then we'll look into how BigFrames enables large scale data science, but then also how it is going further and adding new features. Then Ferri will come here and describe their journey of modernizing machine learning platform on top of BigQuery and BigFrames. We have a live demo. Josh and we'll do that. So stay tuned. If you have enough time, we'll take questions. Otherwise, we can always stay back. All right. So let's get started. Why Python? Python is one of the most popular language when it comes to data science and data analytics. It has been like that for many years. But if you look at the graph in the last two years, the popularity has grown even further. And this aligns with the advent of Gen AI and multimodal analytics. Python is even more popular than it was before. But here is an interesting thing about Python and data science. As long as you're working in a small enough data and your compute requirements are small, everything works very, very smoothly. But as soon as you relax one of those two dimensions, maybe you bring in more data because you want to train larger models on larger data, or you have thousands of features to process. That's where we see most of our users transition from a single notebook experience to one of the distributed frameworks. Now these distributed computing frameworks bring in their own challenges. Here are some of the tangible challenges. When you have to use these frameworks, you have to invest in new infrastructure. So there's a cost of setting it up and managing this new infrastructure. You also have to rewrite your Python program to fit into these new frameworks. So you have to rerun those libraries. And sometimes some of these frameworks are compute parallel, some of these are parallel, some of these are both, and you need to be aware of that. Which brings me to my third point. You really have to squeeze performance out of these distributed frameworks and infrastructures. So you have to learn all those few labels. So if you step back for a minute, what really happens when you make this switch is you spend less time on your business logic and more time dealing with the infrastructure. And that was the problem that we wanted to solve when we launched this open source library from BigQuery called BigQuery DataFrames. BigQuery DataFrames enables Python data science on large scale data. So how does it work? BigQuery DataFrames, in a nutshell, is a transpiler. So it offers you as a user the APIs that you are used to. Pandas APIs, Pandas APIs, Psykit APIs. But then behind the scenes, you are translating them to BigQuery SQL, BigQuery ML SQL. We are building the SQL tree on the client side and then doing a lazy push down and optimal execution when you actually need the data. Now here's the kicker. The data never leaves BigQuery. So a data frame in our library is a permanent or a temporary table on BigQuery. So what's happening here is within the same notebook where you are processing tens of gigabytes of data, now you can start processing tens of terabytes of data very easily. So the story here is with a single switch of a library, you can go from gigabytes of data to terabytes of data without any learning curve. And we are bringing the distributed computing power of BigQuery to your notebook without asking you to learn a new infrastructure. And what's more, you can also take your own custom Python function and push it down using simple decorator so that we can execute them on the server side as well. How do we do this? We do this with three sub packages. There's bigframes.pandas which emulates pandas library. Then there's bigframes.ml which will emulate a scikit API. And then it will push it down, a lot of those using BigQuery's native serverless machine learning. And lastly, we also have extensions into the LLM world where we give you access to Gemini. We give you access to third party LLM models through simple Python APIs that work alongside your data frames. Bigframes was launched a year back. And we have seen phenomenal success. We have seen that the data processed by bigframes have grown 30 times year over year. There are thousands of customers around the world processing hundreds of petabytes of data every month. A couple of them listed here. Deutsche Telekom. Fairy will come and describe their story on how they adopted bigframes. But we also have customers like Trivago who have moved away from Pi Spark, one of those distributed infrastructures and adopted bigframes because at the end of the day they wanted to focus more on the business logic and less on the infrastructure. Now, let's look at how we are taking our next steps here. As we look at what's new with BigQuery data frames, we really look at it in two dimensions. One, how do we make our core Python data science offerings stronger? How do we bring more value to our users? And second, we also have our eye on future. Data science is evolving. And how do we prepare our users for the future of data science? So let's look at it one by one. Starting with the first one. How do we make our core stronger? So we're excited to announce that we are now launching our next major version of our Python library, Bigframes 2.0. Bigframes 2.0 is packed with many features. But I want to pick on three. First is a performance feature. Our data frames, just like Panda's data frames, by default are strictly ordered. But maintaining the strict ordering on a data warehouse comes at a cost and comes at a performance. And many times, maybe most of the times, when you're doing workloads like feature engineering, when you're processing large-scale data, you don't really care for the exact order. And then you don't want to pay for that cost as well. And that's why we are putting a spin on Panda's ordering and introducing a new GA feature called partial ordering. Partial ordering gives you significant performance improvement, and it also reduces cost significantly. And Darche Telecom will vouch for that. Second one, we are also extending our data type support to support some of the advanced data types within BigQuery. Things like arrays, structures, JSONs, which are very common with BigQuery users. We want to support them natively within our data frame, and those are generally available now. And we'll keep extending. There are more advanced types coming very soon. And let me talk about the third one here, which is we are now integrating with the newly launched feature in BigQuery called BigQuery Managed Python Functions, or Python UDF to be short. So now you can use BigFrames to auto-create this Python UDF with a simple at UDF decorator. Python UDF simplified the end-to-end user journey with a complete managed experience and improved performance. So it's definitely something for users to start using now. So these were three of those features. There's much more that's in this release. So look out for our release nodes. I do want to switch our attention to the Python UDF for a better. Python UDF, while not the core functionality of this library, these are some of the essential and really powerful extensions to BigQuery for data science, data analytics, and advanced data users. Now you can use BigQuery SQL to create a Python function. Bring in your own code, bring in your own dependencies, and BigQuery will manage everything from you. BigQuery will manage the images. BigQuery will manage the execution. We will also auto-scale the execution. If you have more rows, we will spin up more compute behind the scenes. And you can also call external APIs. So this is available in public preview today. Go ahead and try it, and let us know how you find it. The next one is an exciting partnership with DBT. DBT is now also on our GCP marketplace as an offering. But we have partnered with DBT to also enable Python models in DBT's BigQuery adapter. So for all those DBT users out there, you can now mix a SQL model along with Python models. So Python models could be written in BigQuery data frames. These models, the Python models support referencing other models, so you can build DAGs. You can bring your own dependencies. And you can bring in your own, some of the templates as well. So what we do behind the scene, the DBT adapter behind the scene, is it will use BigQuery's notebook, auto-create a BigQuery notebook for a runtime only. So that's where we push down the Python code, and the Python code will, at the end of the day, will transpile into BigQuery SQL. It will be pushed down to BigQuery at the end of the day. So now you can get started with a SQL and Python workflow within, or just Python workflow if you may, within your DBT pipeline. This will be available in DBT cloud, as well as the latest release of DBT code. We are excited to see what you build with this. Now let's switch to the other segment. As we launched our library a year back, the world around us has evolved significantly. The advancements of GenAI means that it is really easy to now get insight, not from structured data, but also from unstructured data. So as we look forward in future, we do not believe that the data science of tomorrow will be restricted to the rows and columns of structured data only. We truly believe that the mixed modalities will be part of data science in the future. And we want to enable our users to use those mixed modalities with an equal ease. And that's why we are excited to announce that we are extending our FindAs data frame abstraction to accommodate multi-model data. We are introducing a new data type called blob data type, with a lot of operations on top of it, which you can mix with the other structured data type to have structured and unstructured data alongside in a data frame like abstraction. Just as with structured data, the data will never leave BigQuery. And to do that, we are utilizing one of the recently launched BigQuery feature called object references, to reference to your unstructured data. And to process this unstructured data, we are utilizing some of the LLM Gemini capabilities that are available within BigQuery, but also providing you some of the first-party Python functions, so that you can do things like chunking your PDFs easily, you can transcribe your audios easily, you can embed your text easily. All of that is possible alongside your structured data, so you don't have to go away from the abstraction that you are used to. Multi-model data frames will be available in public preview very soon. So you have to sign up using that QR code if you are interested in trying it out. Moving on. The next one is an exciting evolution BigQuery is doing. We truly believe that we want to simplify how do you extract semantic insights from your unstructured data. It has not been easy so far. Imagine I have a table with product reviews, and I want to find out what are the top five complaints, or top five issues raised in those product reviews. Or I want to know what are the other products that are recommended in those product reviews. What it means today is you have to use multiple systems to act, or multiple steps, for example, generating embeddings and whatnot, and then multiple systems to store this, and then multiple people are involved to actually get to those insights. With AI Query Engine, we are putting in a lot of LLM techniques and the LLM models right within the BigQuery's query planner and query execution. So now you as a user who have been writing SQL in BigQuery, you can also get the insights from unstructured data with an equal ease just by writing a SQL. AI Query Engine, again, will be available in experimental mode very soon. Use that QR code at the top to sign up for AI Query Engine. And within the Python universe, we already have started giving you some AI operators, as we call them, so that you get a sense of how this will actually work. Python AI operators in our AI space is available for experimental right now, so you can get started. And then, for the advanced developers, who want to build things like similarity search for their RAG applications, for their own Gen AI applications. BigFrames is also integrating with technologies like vector search. So now you can use a data frame to generate embeddings, store those embeddings, do your searches, while behind the scene, we utilize a lot of this BigQuery advanced features, which in a nutshell can be an implementation detail for you as a user as you keep working on this Pythonic interface. Our integration with vector search is generally available now. And lastly, for those set of advanced users who want to synchronize their data between an offline store like BigQuery to an online store like Bigtable, we also have a new abstraction called streaming data frames. So imagine you have your features ready in a BigQuery table. You can load that in a streaming data frame, and simply call an interface 2 underscore Bigtable, and we'll start streaming this data to Bigtable. Again, behind the scene, we are utilizing now G8 feature BigQuery continuous queries. But then again, that's an implementation detail. Our goal here is for you as a Python user to keep working in the abstractions that you are used to, and leave the details and optimal execution to us. Lastly, we are taking one extra step to cut the learning curve. BigQuery's notebook now have an ability to generate and recommend code for BigQuery data frames as well. This is part of Gemini and BigQuery offering where you don't even have to specify tables and columns because it understands your context, which project you are in. All you have to do is tell this prompt that you need to generate a Python code and generate it with BigQuery data frames or big frames to be short, and your code will appear in this BigQuery Studio Notebooks. This feature is available already in public preview, so you can get started with it today. So there we are. As we look forward in future, we see exciting opportunities where data science will evolve to multi-modality, and we want to be part of that evolution to help our users get to that place without adding an extra learning tax. Our goal here is really to keep making our core offerings stronger and keep bringing value, but we also want to prepare our users for the data science of tomorrow. With that, I'm going to hand it over to Feri to walk through their story with big frames. Thank you. Yeah, thank you Sandeep for the warm welcome and also for the opportunity to share our experiences with big frames. My name is Ferens Hechtler. I'm a DevOps engineer at Deutsche Telekom. Deutsche Telekom is the most valuable German brand ever. You might know our US brand T-Mobile. Deutsche Telekom has 250 million customers and serves in more than 50 countries with a revenue of 115 billion euros last year. With nearly 200,000 employees, Deutsche Telekom is committed to sustainability and social responsibility with diversity, equity, and inclusion programs. So here you can see our Google Cloud based machine learning platform. It's named One Data Ecosystem. It was presented on Google Next last year. I will not repeat the session but give you a very short overview. Instead, the goal of One Data Ecosystem is to harmonize our data ingestion and processing. Instead of having 15 sources of truth, there should be only one source of truth and should be accessible via BigQuery. Our data scientists can create experiments and work interactively with the data in the analytics layer. But as soon as it goes to production, all manual interactions have to be replaced with CI, CD pipelines or machine learning pipelines so that the results are reproducible and explainable. Today, I will show two examples how we successfully migrated existing PI Spark code into our One Data Ecosystem with the help of Big Friends. The first example is about feature engineering. Feature engineering is the second step in the general ML workflow. The first step is the raw data collection. You can think of tabular data with columns and rows. The columns are named features like article ID, price, date of birth. There is one special column named label. Machine learning models are trained to predict the value of the label based on the value of the features. So it is possible to train a machine learning model on the raw data, but you can get better results if you are doing feature engineering. Feature engineering means transform the data and add additional meaning to it. For example, if there is a value missing in the features, what does it mean? Does it mean it is zero? Does it mean it is a mean value? Or is it even better to ignore the complete row for the training? Outliers can be removed. Data can be transformed. For example, a string containing a date of birth can be converted into a numeric value of age. The machine learning model is trained on the transformed data, and so it is important to see the model and the transformation as one unit. Both have to perform well together to give good results. Here you can see our initial situation. For partially automating the feature engineering, we created a framework in PySpark, the transformer generator framework. This is used in multiple projects. At the top, you can see the prediction pipeline. From the left, there comes the data. Then there is a chain of transformer. This chain is exactly the feature engineering we talked about. And at the end of the prediction pipeline, there is one estimator which is doing the prediction. The transformer generator contains a library of higher order transformations, so-called meta transformations, which are configured using a config file. This config file defines which transformations should be applied to my data. And then, based on the training data, the transformer generator is creating the concrete instances for the transformers. For example, the categorical encoder takes a look at the number of categories. If it is a small number, it will make a one-hot encoding. If it is a larger number, it will do a target mean encoding. So, now to our design decisions. As we want to migrate to the one data ecosystem, all our input sources are replaced with BigQuery tables. And also, we wanted to have the prediction pipeline implemented using native BigQuery services. So that we can leverage the use of the given services like model registry, model training, model monitoring. A simple lift and shift approach of the PySpark sources would break this architecture. So, this was not in scope. For the transformer generator sources, a lot of knowledge was invested to generate the transformer library. We wanted to keep this and reuse as much as possible. And that was the reason why we decided to try out big frames. Because on the client side, it supports the Python pandas API. But on the server side, it creates real BigQuery native services or SQL. And for the config files, we decided that it should not be changed at all. So, the other projects which are using the transformer generator should use the same config file as before. So, let's see how we could implement our design decisions. The pandas compatibility was greatly appreciated by our data scientists. So, that was a good choice. But when we started in June 24, we were early adapters of big frames and had to struggle with teasing troubles. And so, personally, I was very skeptical in the beginning. If big frames would fit our needs with regards to performance and functionality. But as an open source project, big frames evolved very quickly. Every week, a new release was published. Adding new features and improving the performance. And what was really a game changer for us was the partial ordering mode. What Sandeep just told in version 2 will be the default. Using this mode, BigQuery, big frames performance is on par with the native BigQuery services. So, this worked out well. We also benefited from big frames being an open source project. As we needed custom transformers. And big frames did not support custom transformers. It only supported 8 predefined machine learning transformers. And so, we decided to contribute from our side what was missing. We created a pull request in GitHub. And after a three week very constructive discussion, it was approved and released with big frames version 1.20. Using our contribution, we were able to implement our transformer generator architecture. So, at the top here you see this is our contribution. At the end, it was only one class. We started with much more. But after the discussion, we found out what was really needed in general. And based upon this space class, there's a class hierarchy for single column transformers. This has to be implemented in Python with BigQuery SQL knowledge. I said we don't want our data scientists to work with native SQL. But in this case, for this low level transformers, it is necessary. But after this, the transformer library which uses the low level transformers, this is completely implemented using Python, Pandas, or in this case, big frames. And for the projects which are using our library, there is no change. The config file is unchanged. And the deployment is completely automated using pipelines. So, no manual interactions is necessary. So, for the first example, I can say big frames was a good choice. And we could meet our design decisions. But as a framework, it's a little bit different than other projects. And so, we wanted to know how can we migrate, let's say, a normal project which only contains PI Spark code, maybe in a notebook. And we asked Google for support to give us some guidance and best practices how we can do this. And the requirements were the same as I said before. So, we wanted to stay with the technology, with BigQuery, native services. And the data scientists should be able to work in a well-known environment. So, the result was a two-step process. In the first step, there is an intermediate source generated in Pandas. So, the PI Spark sources are split into logical pieces. And each piece is migrated with the help of Gemini into Pandas sources. In 95% of the cases, this worked without any post-processing. In 5% of the cases, it was necessary to modify the code afterward. But the effort was much less than completely migrating the sources manually. And the advantage of the intermediate Pandas sources is that they are executable. So, we were able to run the migrated sources against the original sources and compare the output. If the output is the same, we can say the migration is fine and we start the next step. In the next step, the migration from Pandas to BigFrames is not a hard task because of the great compatibility of BigFrames with Pandas. Mostly, it's just changing the imports. We had some findings mostly related to data types and user-defined functions. And again, as you heard, BigFrames 2 adds user-defined functions in a preview. And there are additional data types which are supported. So, we are also progressing. Using this process, converting a notebook with 1,000 lines of PySpark code can be migrated in less than a week. So, in summary, I can say using BigFrames for the migrations of PySpark code was very successful. And also, our data scientists use BigFrames as a first choice for creating new projects. Thank you for your interest. And now, I hand over to the demos to Jasun. Thank you. Hello, everyone. I'm Jasun from the BigFrames team. Today, I'm so excited to showcase how you can build a giant-powered data application on top of BigFrames data pipelines. Let's assume we are building a customer console for a fake company called SymbolTelecom. So, in SymbolTelecom, we have a lot of data. So, for example, we have terabytes of structured data like customer information or service information. At the same time, we also have millions of unstructured data files. For example, we have the customer audio file from the call center like this one. Thank you for calling SymbolTelecom. How can I help you today? Hi. I'm calling about my latest bill. There are some charges on it that I don't understand and don't think I should be paying. And we also have the PDF invoices files just like this one. So, the biggest challenge we are facing is how to manage and process both of the structured and unstructured data together. Thanks to the new capabilities introduced by BigFrames multi-model data frame, today, it's so easy to manage and process both of the structured and unstructured data together at scale. For example, here we are building two data pipelines. One to extract the content from the PDF invoices. And the other to generate the dynamic FAQs from the audio files from call center. Both of these pipelines are orchestrated by DBT thanks to the new integration between BigFrames and DBT. And last, we save the results to BigQuery table for future consumption. Alright. Let's take a closer look at the invoices page. As you can see here, we have a list of PDF files. By clicking one of those, we can get the fetched bill details. It's now very easy to consume, so we can send the bill details to Gemini to generate the Q&As on the fly. As you can see, it's really easy to find the total amount due on this bill. At the same time, we are also providing this bill analyzer, where users can ask a question against their usage data in BigQuery. Let's say, what is my average data usage in the last three months? By clicking on the generate code, we send this question to Gemini to generate the BigFrames code. In reality, we won't show the code to customers, but just for the demo purpose, let's take a look at the sample code. If you are familiar with Pandas, this code should look very familiar to you. Because BigFrames.pandas just inherits the Pandas API and the interface. Here, we first read in two BigQuery tables into data frames, and then apply the filters on top of those data frames before the merge. After we apply more and more filters, we can calculate the mean as the aggregation function. It's also worth mentioning that BigFrames use the lazy execution to save the cost. And we also push down all the calculations to BigQuery for scalable execution. Here you can see the return results. You may wonder how this pipeline is built. Here's an overview of the data pipeline. We first read in the PDF invoices into a multi-model data frame, and then apply the built-in function to extract the text from it. We save the results to a BigQuery table for future consumption. The whole data pipeline is managed by dbt. Here's the code snippet. At the top level, we define a dbt model as the stage. Inside the model, you can see how simple it is to create a multi-model data frame out of a GCS bucket. It's also worth mentioning that we are now reading in the raw bytes. But instead, we're just reading some metadata, like GCS addresses. And then, we apply the PDF extract, which is the Python UDF, on top of this blob column. And save these ready strings together with the unstructured data. So now, you have both of the structured and unstructured data in the same data frame. Let's take a look at an even more interesting use case. So in SymbolTelecom, instead of using the predefined hard-coded FAQs, which are pretty much from imagination, we are generating the FAQs dynamically directly from the customer calls to capture the real-time customer needs. For example, here we have five different categories. And in each category, we have sample Q&As. All of these are generated from the call center audio files. Here is how it is made. We have a more complicated data pipeline here with three different stages. First, we generate transcription from the audio files. And then, we create embeddings on top of those transcriptions and apply K-Means clustering to cluster those embeddings into five different clusters. In each cluster, we sample some random transcriptions and send those samples to Gemini to generate the top-level categories with sample Q&As. And we also remove PII data from it. The whole data pipeline is orchestrated by DVT. Here is the code sample. Similarly, we still define the top-level DVT model. Each model corresponds to one stage in the data pipeline. Inside this first model, we create a multi-model data frame out of the GCIS bucket. And then, we apply the transcribe audio function on top of the blob column and save the transcription together with the audio files. And next, we create two models. One for embedding, one for clustering. We first generate numerical embeddings on top of the transcriptions, and then apply the K-Means clustering on top of those embeddings to cluster all the embeddings with transcriptions into five different groups. At last, from each cluster, we randomly pick up five different transcriptions and send those samples to Gemini to generate the top-level category names and the sample Q&As. We also ask Gemini to remove PII from the sample Q&As. All right. That's all about the demo. Just to do a quick recap, we have demonstrated how simple it is to create a Gen AI-powered data application on top of the big frame, multi-model data frame data pipelines. We've also highlighted a few new features, including one, the multi-model data frame, which is powered by the BigQuery object reference column and the data. We also show you how to use the predefined unstructured data processing functions, which is powered by the Python UDS. And third, we also show you how to use LLM and ML functionalities, which come from BQML. Last but not least, the whole data pipelines are orchestrated by DBT. We highly recommend you to give it a try, and any feedback will be appreciated. Let's shape the future of data science and data engineering together. Thank you. Thank you. .