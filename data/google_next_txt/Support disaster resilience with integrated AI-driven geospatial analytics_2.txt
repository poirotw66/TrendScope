 Hello. Welcome, everyone. We're at the home stretch of Cloud Next. Thank you for joining us today. My name is Shiba. I'm a product manager for a geospatial tool at Google called Earth Engine. And today we're going to be talking about how to support natural disaster resilience with Google's advanced geospatial capabilities. Across Earth Engine, BigQuery, and more. If any of these tools seem unfamiliar to you, don't worry. We're going to unpack all of this over the next 45 minutes. You'll also hear from my colleagues, Jeremy, who is a cloud geographer for public sector, and Jeff, who is a technical director at the office of the CTO. So, as I mentioned, today's session is going to address a sustainability issue that we think is really top of mind for our cloud users, and that is climate resilience. With natural disasters becoming more and more frequent than ever before, we are really focused on addressing every stage of the natural disaster lifecycle. So, in this session, we'll start with a 101 on geospatial and the geospatial technologies that we're building to enable resilience. And we'll also talk about a new launch that I've been personally involved in called Earth Engine and BigQuery, which brings together our two geospatial tools all in one place. Then I'll hand it over to Jeff, who will talk about weather next, our weather prediction AI models. And then Jeremy will take us through an end-to-end example with wildfires, taking you through the entire full lifecycle of wildfire disaster assessment, risk assessment, response, and recovery. And then we're going to wrap with a discussion on some of the resources that you can use to learn more, and we'll direct you to the demo stations where you can actually try out some of these tools yourself. All right. I'm going to start with a question. Who here has been caught in a natural disaster in the last five or six years? Just a show of hands. Yeah, I would venture to say that the number of people who raised their hand today is a lot more than a few years ago. And we actually posed this question to our Google sustainability community, and we received an influx of photos and stories of Googlers who had been experiencing natural disasters recently. We had Googlers in Brazil who had experienced the recent landslides and flooding in Sao Paulo. And we had Googlers in California who also experienced the devastating wildfires that happened earlier this year in L.A. I know for myself and Jeremy and Jeff, being from New York City, we had many discussions about the Canadian wildfires that happened in June 2023 that left the entire city of New York covered in smog. For me personally, reflecting on this, it really made me think how these natural disasters are not only happening more frequently, but they're actually happening in communities that we least expected. And we also know that the impact of this shift is evident not just in our own sustainability communities, but it's also happening sort of worldwide. And Google Trends shows that the world is searching for sustainability more now than ever before. And we actually estimate that about 36% of U.S. searchers on Google have searched for a sustainability-related topic over the course of the last year. And if you zoom in on natural disasters specifically, wildfires, we actually saw the highest spike in wildfire searches in the United States in June of 2023 with the Canadian wildfires, and then the second highest spike earlier this year with the L.A. wildfires. So trends like these really demonstrate that climate change is having an impact on the world, and people really want to understand it. But the impact of these natural disasters is not just about understanding them. We actually know that there are a lot of losses and damages that result from these natural disasters, and that's impacting not just individuals, but businesses and governments as well. So in 2024, the U.S. experienced 27 natural disasters that costed over a billion dollars each, resulting in collectively $180 billion in losses and damages. So developing solutions for these natural disasters and making them more manageable is really top of mind for us. And we believe that geospatial technology is the way you can improve the process of not just anticipating, but also responding to these natural disasters. So let's talk about some of the foundational Google technology that will help us with some of these resilience efforts. So just looking at the room, surveying the room, I see some familiar faces, some Earth Engine power users, geospatial power users. I also see some unfamiliar faces, which is actually very exciting for me. That means that folks in the room are coming to learn more about geospatial. So I do want to just quickly address what is geospatial and why it's important. So geospatial is really just a broad term that is used to describe any sort of data that observes what is happening on or near the surface of the Earth. So geospatial has two different types of data. One is vector and the other is raster. Now vector data is really anything that you can fit into a row or column. So tabular data, think of counties, zip codes, property boundaries, agriculture, maybe field boundaries. Raster data is all about pixels, aggregate pixels. So for our purposes, that's going to be satellite imagery. Now, you might also be asking why geospatial? Why does Google care about working on sustainability issues through geospatial? Well, the answer to that is that there really is a spatial awakening happening with regards to geospatial. So in 1972, the first satellite that was launched into space that observed the surface of the Earth, Landsat 1, was the first. And since then, we've had hundreds of other satellites that are serving sort of the same purpose but different use cases. So we have satellites that are looking at methane emissions, land cover, and wildfire risk, actually. In fact, recently, Google Research has gotten involved in a new constellation of satellites called Firesat. So we now know that these satellites are becoming more accurate. They're easier to analyze. And there's so much geospatial data. And there's really an opportunity for us to use cloud computing and cloud storage to turn this data into insights, particularly around resilience. And so through our suite of products, we've knit together a seamless experience to take you from the raw data, the raw pixels, to data-driven decision-making. And Earth Engine is our cloud-based platform providing users two capabilities. The first is a vast data catalog of satellite imagery and raster data. And the second is the ability to aggregate a massive amount of raster data and process that data if you have the remote sensing expertise. But when it comes to processing the data, we've also made it easier than ever before by bringing geospatial analysis to BigQuery, our fully managed warehouse for SQL analysts. And this is actually where we're really focusing our efforts. And I'm going to talk more about this in a few slides. But what you need to know now is that BigQuery has a lot of interoperability with other cloud tools, in particular Earth Engine and Vertex AI, our unified AI platform. Now, the output of leveraging all of these tools together is that you can develop insights and ultimately visualize them in a number of compelling and familiar interfaces, including GMP and Google Earth. Now, I mentioned two important geospatial tools at Google. One is Earth Engine and the other is BigQuery. And I want to talk a little bit more about both of those. So let's start with BigQuery geospatial. So BigQuery has historically been focused on vector data from a geospatial perspective. It acts as a managed warehouse for storing and querying structured data. And you may have seen demonstrations in the past, maybe at Cloud Next, on BigQuery geospatial. It's all about querying geospatial data through geometries. So it allows you to answer questions like, what is the viability of a parcel of land in a specific county? And the functions that you use to leverage this are really about vector data stored in tables that are either in the public BigQuery data catalog or maybe they're tables that you brought in yourself to ultimately be able to drive insights. Earth Engine, on the other hand, is designed to handle and process massive amounts of raster data, so the satellite imagery. And the architecture of Earth Engine is really optimized for pixel-based computation. So users who are accessing Earth Engine are usually going into the data catalog, and they're starting by preparing the data. So maybe you take a number of raster layers from a number of days or a number of months, and you first remove the clouds from that imagery, and then you merge the imagery together into a composite. Now, maybe then you want to actually understand what has happened in the past historically. So you take some of the insights of what's happened in the past, the imagery of the past, and you look at it over time, and you build a time series, right? Maybe you're looking at how surface water has changed over time, like this example, or how the glaciers have receded. And then an even more advanced use case is then to predict the future based on the history of what you're seeing from the satellite imagery. So maybe you want to predict what crop yield looks like or what wildfire risk might look like, and you'll use a combination of different types of satellite imagery or combine it with other data like soil moisture. Okay. All right. Now, Earth Engine isn't all about the raw data. There are derived data sets as well in the Earth Engine data catalog. And those data sets are really interesting because they combine a number of other data sets or other computation that's been done on those data sets to come up with analysis-ready data. So you're already getting to see elevation data, emissions data, flood risk data, and wildfire risk data all in one place. So as you can see, there are two really powerful tools, one that's built for processing a large amount of vector data, which is BigQuery, and the other that is built to process a large amount of raster data. And we also know that Earth Engine requires remote sensing expertise to do this type of raster analytics, which we also know that the larger analyst community might not have. But a variety of businesses and individuals can definitely benefit from leveraging raster analytics. And so here at Google, we want to bridge together the geospatial capabilities of these two products together and make it easier to derive insights across both of them. And so this week, we're very excited to announce that we have launched Earth Engine and BigQuery to public preview. This is an extension to BigQuery's current geospatial capabilities, which will broaden access to raster analytics among our widest community of analytical users. So with this launch, we're bringing derived data sets and the ability to do raster analytics from Earth Engine to BigQuery, allowing you to do geospatial insights for both in one place. So specifically, we're launching two new features. One is a new geography function in BigQuery called SD region stats. And what this does is it allows you to derive statistics. So I think emissions, wildfire risk, flood risk for an area of interest based on where the raster data intersects the vector data in a BigQuery table. So for example, what is the mean water runoff, which is the raster data, for a county in Florida, the geography data on a given day? Secondly, we also launched 20 new derived data sets into Analytics Hub, making them available to BigQuery users. These were only ever previously available in Earth Engine. And we'll be adding more data sets over time. The data sets that are currently available in BigQuery and Analytics Hub will cover our most common use cases, including climate risk, wildfire risk, flood risk. In addition, you can also bring in your own data sets or your own Earth Engine assets for processing in BigQuery. Now, Jeremy is going to take you through an end-to-end example for wildfire risk. But before he does that, I do want to take you through how easy it actually is to do ST region stats, which is called reduced regions or zonal statistics in Earth Engine. So first, you're going to identify a BigQuery data set with vector data. So that's actually the geography data type shown here. Then you're going to identify a raster data set, which is one that you pull from Analytics Hub or one that you bring in yourself. And that's the raster ID string here. Then you'll determine the scope of the analysis, band name, date filter. And then you're going to run the ST region stats function, which will then process where the pixels intersect in a chosen geography. And the output of all of that is a number of statistics. So mean, min, max, standard deviation, sum, count. You'll receive all of them in the output, which you can then visualize, whether that's in GeoViz or Carto or any other visualization tool that you choose. Okay. So as you can see, Earth Engine and BigQuery is easy to use, much easier than Earth Engine. And it covers a variety of really important sustainability use cases, including climate risk. So actually, over the last few months, we've been testing this feature with a number of our top partners in Private Preview. And we've received really great feedback. Carto and UbiLabs were two of our partners who leveraged this tool. And they've told us that it's been impactful, scalable, easy to incorporate into their decision making. So we're really excited to be launching to public preview to all of you. Now, before I hand it over to Jeremy to take us through the example, I'm going to have Jeff come up and talk a little bit about weather next. Thanks, Sheba. Once again, my name is Jeff Sternberg, and I'm going to be talking about weather next. So weather, as we all know, is a key element of disaster planning and resilience. And in the last few years, researchers around the world have begun to use AI to actually forecast the weather. And this has yielded some very promising results. And here at Google, teams in Google DeepMind and Google Research have developed weather next, which is a new family of AI-based models that can produce state-of-the-art weather forecasts. And the good news is that these models are now available in Google Cloud. So let's dig in a little bit to these weather next AI models and kind of understand what they are. Both are global, high-resolution weather forecasting models with 0.25 degrees spatial resolution. So that's about 30 kilometers. Both were trained from decades of global weather observations. And both create medium-range, 10- to 15-day weather forecasts. Graph is a deterministic model, so it produces a single weather forecast, and it predicts weather at six-hour time steps for 10 days. Gen is a probabilistic model producing an ensemble of 50 forecasts, and it predicts weather at 12-hour time steps for 15 days. Simple, right? So next I'll work through how these models were developed and how we can measure their performance. So WeatherNext Graph was trained on 40 years of historical data, historical weather data, and it uses a graph neural network architecture to model more than a million grid points over the surface of the Earth. At each grid point, the model predicts five surface variables. So that's like temperature, wind speed and direction, and mean sea level pressure. It also predicts six atmospheric variables at 13 different altitude levels, right? So you get this kind of global understanding of the weather. To make forecasts, Graph requires just two sets of data. It requires the state of the weather six hours ago and the current state of the weather. And then the model then predicts the weather in six-hour increments into the future and regressively rolls forward, auto-regressively rolls forward the predictions up to 10 days. And it turns out that this process is really quite accurate. On the bottom, we can see an analysis of Graph, which is the red line, versus a model called HREZ from the European Center for Medium-Range Weather Forecasting, or ECMWF, called HREZ. And that's basically the leading deterministic model. And lower is better on this graph, so you can see there's less error, and therefore, you know, higher accuracy across the time range. WeatherNextGen was also trained on 40 years of historical weather data, but it uses a diffusion model. And this is actually the same modeling approach that models like Imagine and VO use to generate images and videos. But in this case, instead of creating, you know, cat videos and so forth, we're actually determining the probability distribution of future weather. So the way Gen works is in model training, we add random noise to a global grid of atmospheric data, kind of represented at the top left there. And then we train a denoiser neural network to remove this noise using a loss function. And so this is, as I mentioned, a probabilistic model. It can produce an ensemble of 50 different independent forecasts using this technique. This fact is actually very helpful for disaster planning because we can look for tail risks using this distribution. So, like, what's the 1% chance of extreme wind or extreme precipitation or high temperatures or low temperatures? So Gen shows state-of-the-art accuracy. And so the rest of the slide here is basically the scorecard. Blue is better where it's outperforming the comparable model. And you can see that, you know, through different levels of the atmosphere and variables, it's outperforming the baseline model in many of those cases. So that's great. But what can you kind of do in the real world with this type of information? So it turns out that more accurate weather forecasts can mean earlier warnings about where disasters like hurricanes will have impact. In July of 2024, both the American and European forecasts were predicting a track for Hurricane Beryl that had it kind of going into Mexico, according to these two lines. Around the same time, a hurricane track prediction model that used Weather Next Graph actually predicted that Beryl would turn upwards and hit the coast of Texas. And that was much closer to the actual path of Hurricane Beryl. And this was, you know, an earlier warning than we would have had otherwise. So as you can see, you know, the AI-based forecast really predicted the storm track in a more accurate and unfortunate path. So how can you actually incorporate these models into your environment? We now have Weather Next Graph and Gen available as experimental data sets in BigQuery, in Earth Engine, and cloud storage because we like options. We are delivering real-time four times per day forecast through these experimental data sets. And there's also historical data going back to 2020. And once again, you know, it's the same models, Graph and Gen, that I just went through. So you can hit that QR code to sign up for access to these experimental data sets. There is a review process. Here's what Weather Next looks like in Earth Engine. So this animation shows precipitation and wind velocity from the graph model. You can clearly see a hurricane forming in the Atlantic. And if you want to check out more visualizations, please do stop by the booth and the expo to see more examples. And before I hand it over to Jeremy, one more thing. Weather Next is now available in Vertex AI Model Garden in a preview stage. So this is new at this conference, at Next. And the idea here is we want to be able to give you more control over the inference process. So that includes using different model types, configuring the number of forecast steps for the predictions, and even the number of ensemble members so that you can really control that probabilistic distribution. Check it out also in the expo. So with that, I'll hand it over to Jeremy to walk us through technology in action. Okay. Let's see how it all works. So as Sheba and Jeff kind of laid out, we have capabilities here at every kind of step of the disaster lifecycle, from what we need to assess risk and understand where assets are. We have abilities. We didn't really talk about it just here, but I'll touch on it, how to deliver this data in near real time to people in the field and in the way of disasters through our consumer products and through Google Cloud that organizations can leverage themselves. And then we have this, we can lean on these same AI-driven tools to help rebuild with resilience and see where, respond to that disaster and inform what we need to do next to fix things. So we'll walk through for a simple example for Wildfire. I'm not an expert in Wildfire, so be kind to my analysis here. But this is how we can kind of bring it all together with the tools and the data sets that we just saw here. So first we're going to start with imagery, right? We need the right imagery. This is a visualization in Earth Engine using, actually, it's a visualization in a Colab notebook. I think I said in my LinkedIn post that I wasn't going to throw any code up, so that is a little bit of Python right there. Hopefully it doesn't scare anybody. And Wildfire Risk to Community is a U.S. Forest Service project which endeavors to use the best available science to not only identify risk but provide resources for communities to manage and mitigate those risks, right? So there's a really great website at wildfirerisk.org, super approachable for non-experts and experts alike. And they distribute those products as data sets and interfaces through that UI there. But we've exposed this data set in Earth Engine as an image that covers the United States. I believe it's at 30-meter resolution. And it has several bands that describe different components of risk. So it's a very useful thing to index something that we care about and want to try to understand how it's affecting a community. And then, so imagery is great for indexing risk in a uniform way across broad geographies, right? So we're using wildfire as an example here, but many other risks, as Sheba demonstrated in her section for other types of risks. Property is often indexed as tabular data, right? So those geographies, vector drawings, so points, lines, polygons that describe assets on the ground. Could be buildings, parcels, roads, what have you. And BigQuery is really good at storing data in that format, in that modality. And it's also really great at doing spatial intersections across many, many of those objects, right? And that's something that's been traditionally kind of difficult to do in desktop GIS systems that most folks are using. And so this is a key hurdle that we wanted to overcome with Earth Engine, both to, you know, store that tabular data using BigQuery, which it's gotten really good at, but do that fusion between the two of them. You also have access to enhanced commercial data sets through Analytics Hub that can add, enrich that data even more in the non-spatial dimensions, right? So, for example, Dun & Bradstreet has a data set called the Enhanced Building File, which has over 100 fields of information on over 51 million building structures and commercial properties across the United States. has information like roof types and external wall construction, the type of construction, the type of building that it's, maybe it's a mobile home park or apartment complex. So all these things could be relevant to an analysis, right? So, like, these are the kinds of attributes that you want to understand, like, how are these different structures and these attributes of structures affecting that risk and the ultimate outcome? And we want to bring it all together into one place. Okay. So we're going to hop over into the actual demo to string that together. And this is where I get real nervous. Look at that. Got to relaunch Chrome within five days. Yeah. Yeah. Got it. We won't do that. We'll put that one off for a little while. Okay. So here we are in BigQuery in my demo environment here. So this should be more or less identical to what you all have. We're going to go into Analytics Hub, hit Search Listings, and I'll filter it to the climate environment. Datasets, this captures all the ones that we've added here. So these are basically pointers to datasets that are in Earth Engine. So we're highlighting these datasets for the initial launch, but you can actually access any image in Earth Engine right off the bat. You just need to know the image asset ID. We're just indexing these few that we think are important for the first rollout. And these, once we add, so we'll go find the wildfire risk to community dataset here. And this little subscribe button comes up. And so what that does is adds a new dataset to our project. I'm also going to add weathernext. Graph Forecast. So that adds the actual Graph Forecast dataset, or a subscription to it, pointer to it in my project. This is also where you could get access to the building file. They have to actually go and provision that for you. It doesn't quite work like the normal subscription. So they were kind enough to provide a copy of it for California for this demo. Thanks, Dun & Bradstreet. There's about, I think, 12.5 million structures that are in that file. And it comes into Earth. It loads into BigQuery nice and seamlessly. Okay, so we'll go back to BigQuery Studio. And I should be able to see both of those datasets there. There's our weathernext data, our wildfire data. You can see the fire. And so what this is is a table that follows the spatial-temporal asset catalog specification that basically indexes that image. Other datasets will have many more images. This one just has a single one. So you might have a dataset that has time components to it where you would want to filter on that to get the ID that you're going to run your region stats over. We can see the bands that are in it and information about those bands. I'm going to use this conditional risk to potential structures band. That is a description of the consequences of a fire at a given location, like how bad it would likely be. And then also the risk to potential structures, which is just the relative risk, like how likely is there to be a fire. So any kind of fire at that location and how awful would it be if that actually happened. And that kind of, in this wildfire risk to community, that relationship is something that they compare at the community level. We're going to do that at the building level here in this demo. Okay, now I'll go to the query. And I did say no code, but I did not say no SQL. So I'm going to step through some SQL here. I'm using the new pipe syntax that we've been rolling out in the last few months. This is a different way of expressing SQL instead of a normal select statement that kind of, it's much more condensed, but it also kind of flows a little nicer with how brains work, at least how my brain works, where you can kind of start from what you're interested in and kind of add iteratively to it and then select the thing that you want from it. So we'll start by stepping through some prep. How much time do I have? I don't know. 13 now. We're good. Okay. We'll start with an AOI. I always start with an AOI, right? So area of interest. We're going to use the, and actually I'm going to hit run and see if I can describe this while it's running. Uh-oh. I broke it. Oh, that's why. If you select code and then hit run, it runs just the selected code. And you don't want to do that. So we're going to run the whole thing. Okay. So while it's running, it shouldn't take that long. It might usually run in like 30 seconds or so. We select the zip code for, this is the Pacific Palisades. So that'll pull a geometry for that out of our public datasets catalog. The buildings. So this is from the Overture Maps dataset that Cardo maintains in our public data catalog. They're kind enough to keep that up to date once a month. And so this is going to pull all the buildings in that dataset, building footprints for that whole zip code. We'll pull some building attributes from that Dun & Bradstreet file. There's hundreds of them, but the ones that I thought might be interesting are the property type, building roof material, exterior wall material, building construction material. And then the raster ID for that wildfire risk dataset. So you just have to select that, this Href from, or this URI that points to that, that dataset in Earth Engine catalog. We could just hard code that in if we wanted to too as a string. Okay, so it's finished already. I'm going to pull in some weather next forecasts to make Jeff happy. And so this is relatively coarse spatial data, so there isn't like a whole lot of variability over a zip code, but you might be interested in seeing what the maximum wind velocity or the average wind speed over a period of time. You have to do some math to do that because you get the two different components for wind, right? So we have to go back to our junior high trigonometry days and compute that and then aggregate that over the course of a day to give us an average and a maximum, do the same thing for temperature, and I switch it to Fahrenheit because I don't think in Kelvin or Celsius because I'm an American. And then this is primarily risk, so that's really all we kind of would need, but I also had pulled some data from Cal Fire about the perimeter of the Palisades fire, so we can see it was this building in or outside of the perimeter that was observed and some damage inspection data for structures that they compiled in the weeks after the fire, which was pretty complete. Like, there's a... They give you a few different levels of damage that those buildings experience. Okay. So that all ran while I was talking, and I put it in a table so we can see the output. We can go through here, table explorer, and see what the different values look like. Building material. Let me move off our damage. And this kind of compiles, and it gives you kind of an overview of, like, what the distributions look like. We can go and map it in BigQuery GeoViz, so this is a simple, just, like, quick visualization tool that we provide alongside BigQuery. You'd probably want to use something more sophisticated like Carto or plum it into a GIS, or there's another product called... There's another Descartes that you can plug into BigQuery to be able to make rich visualizations, but this allows us to kind of color by that wildfire risk value, which we can see play out in the hills of the Palisades here and click around and see those values all compiled together. So I think the idea of being able to fuse all this data together into a single table and make that kind of derivative that gives you a unique view into that particular risk and location is the new hotness here in Google Geospatial. Personally, this was something that I struggled with for a long time doing in my previous life studying biodiversity, where we have to do a lot of big spatial joints, right? And BigQuery was always the place that you wanted to do it, but having that pixel data was always kind of just a little bit over the wall, and you had to do a whole lot of nasty plumbing to make it available to your BigQuery analysis. Now you can just do it with a boom, that one query there. That query that we just did ran over. Thank you, Chad. That query was 12 million records intersected with, you know, terabytes of data, and it ran in 30 seconds, so pretty great. Okay, so let's hop back over to the deck and talk about... I should mention that once you have that, it's really fun to plug it into the AI stuff. I wasn't brave enough to press that button in a live demo, but it can make charts and do some really... Especially if you use the Sheets Analyze button, it picks out a lot of trends in the data that are not very obvious. So how can we deliver stuff to folks who this is happening to out in the field? Like, Google uses the same technologies that we just kind of illustrated here to build out and deliver wildfire boundary tracking in the consumer product, right? So this is the team at Google Research that's using AI to detect wildfire to map those boundaries, make this information available through the consumer app, which is also incorporated into the routing and place searches that folks are using. So we're trying to really help people on the ground and augment the systems that are in place. And in the case of California, they're actually very data-rich and highly capable, but many places in the world don't have that. So anything we can do is additive here and give you the ability to do the same kind of thing with the Maps platform. So your organizations, if you have some of these capabilities, you want to be able to present it in that same familiar and intuitive way. We have made it easier to do that in Google Earth as well. Earth is the... Earth is not Earth Engine, right? Earth is that spinning globe that we got to see back in 2005, I think it was. Now you can use Earth Engine to compile data near real time. So in this case, we were ingesting data from Airbus, who was kind enough to make this public and allow us to make this public and push daily imagery up into a public Earth drive folder so that folks could see the fire in real time and be able to know what areas were affected. We know the people were trying to drive into areas that had been devastated and they want to reduce that amount of activity as much as possible. So that was one way that we were able to push data out. And folks could do this the same way themselves if they wanted to. So using Earth Engine as the processing engine to mosaic and then deliver it through this interface that people are used to seeing. And then in the rebuild phase and the response phase, assessing that damage, Sky is a Google research project that leverages machine learning and satellite imagery to generate rapid and accurate building damage assessments. after natural disasters like this. And it's really aiming to be able to provide some sort of assessment of what's happened in that 72-hour time frame when it's really hard to get folks in on the ground. In the case of L.A., there's so many people there and this is America. We have lots of communication. It was a little less important to get that quick turnaround. But in the developing world, places that don't have great communication, it's been very transformative to be able to deploy this. This is an open source project that folks can run on their own, but we developed it with World Food Program to be able to do those assessments in rapid time. It works the same way that we do other machine learning deployments on top of Earth observation data in Google Cloud. So we get imagery before and after that's high resolution enough to be able to see those buildings. We go and label 500 to 1,000 of those and build a bespoke model just for that area, for that disaster, to be able to detect where that damage has happened. And the output is hotspots and per-building damage. So we can actually get a look at the building level where that's happening. We also have this open buildings two-and-a-half dimension temporal data set where we have... This is also from Google Research, and it's available in the global south and can be made available in North America where you can... It was inferring from the Sentinel-2 imagery, so this is 10-meter data at the 50-centimeter resolution. It's actually... Well, 50-centimeter pixels, but it's about a 4-meter effective resolution for building likelihood and height above ground. So you can use that to detect where buildings were and where buildings are no longer so you'll be able to see where the effects of a disaster are happening. And I'll show you in a second how we were using this in L.A. to help them deal with places where they didn't have complete records of how tall buildings were before the fires. Actually, can we hop over to that right now if we have time? Are we good? Yeah, here we go. So this is an Earth Engine app. Let me pull this out. We can see the Airbus pre-fire imagery. If I turn that off, the base layer shows the post-fire damage. The sky data, when we ran those detections, you can see here for that whole fire, did a pretty good job of picking up buildings that were in and weren't included. And then that building data, that building layer, so that's down at that 50-centimeter resolution. This is basically a super-resolved data set from the high-resolution imagery so we can see it through time. Allows us to pick out the height of those buildings that were there before. That one's still there. Here's one that wasn't, so shorter home. Actually, turn that down. Yeah. So this was something that we brought to them as an option for finding the height of things that were maybe not recorded in the permitting. Buildings that had been there for decades, right, and they hadn't necessarily been maintaining those records for what had been rebuilt. And when the fires happened, they wanted to build like for like, so we needed some way to say what was the height of what was there before. So this would be an option for doing that kind of thing, assessing damage that has happened by rerunning this model post-disaster, but also being able to see what was there before the disaster. Can we hop back to the deck? Thanks. And then we could run that against that same thing to do that at scale, right, using ST region stats to compile the maximum height within a polygon over the whole city if we wanted to. So this is ways that we can really fuse together the imagery and the polygon data in a way that we just haven't been done for. We're really excited to see what everybody does with it because I think there's a lot of potential here, and this is going to feed a whole lot of things downstream when we are able to bring this additional context together into a place that can actually be actionable and not have to involve a coder or a software engineer to plumb together some crazy pipeline to be able to do this. We can just hit this one little function, and you can go off and do great things. Okay. That's it. Please take a shot of these QR codes to get more information about whether next in Earth Engine and BigQuery, and please continue learning about this stuff out on the expo floor. We have demos in the sustainability section, demo screen one, and screen seven for the weather forecasting data. So super excited to see where you go with this, and thank you. невig humour, thank you.ık