 . Hey, everybody. Looking forward to share a little bit about the session for Ray on GKE, building a next generation AI ML platform. Here's just a couple of highlights over the next couple of slides, just to tell you what we talked about. Here at GCP and with GKE, we're really excited about the partnerships and the community for Kubernetes. Kubernetes turned 10 last year, GKE turned 10 this year, and we've continued to lean into that ecosystem in CNCF. One of those areas that we really leaned into and seen a lot of excitement around has been for Ray. We've been working with Ray for the last couple of years, initially with Kubray itself, but now we're actually excited about working even further with you on that. Some of the things our customers love about this comes down to some of the things that Kubernetes brings to the table, as well as Ray. On the Kubernetes side of things, it's best in class for orchestration, production handling, reliable at scale, and we're continuing to invest in those areas to keep moving things forward and continue to make it reliable at scale, with new features capabilities to support AI ML workloads. We've seen a large push and move towards customers running AI ML workloads on GKE, and we've even seen even greater push towards interest for Ray on GKE as well. We've now seen overall about 4 billion new containers per week on GKE. It's part of the mainstream, it's what customers standardize their platforms on, and are now taking it to bring on AI ML workloads too. As I mentioned, we've been partnering with Ray, and what our customers tell us that they love about Ray, and what they're excited about is the fact that this Python native API and ecosystem, so it's perfect for their data scientists, performance and scale, accelerator optimized, and they get hyper-efficient, great scheduling and execution, and it's an open source standard now for AI ML practitioners. We think that Ray and Kubernetes are going to be better together. In fact, we think that's going to be the default, de facto OS for AI ML workloads. What we've been doing so far has already been leaning into making it possible, and now we're actually going to be doing more. We want to make it a little bit easier for you to use label selectors for coordinating scheduling, leaning into that awesome scalability with Kubernetes to size the environment to what you want dynamically, so in-place pod resizing to improve utilization, resource utilization for tasks and actors so you can get those heterogeneous groups in there, as well as native TPU support for scheduling so you get that awesome price performance. Now on GKE, when you deploy Ray, you get all the benefits of what we've been doing so far to make it AI optimized already. And there's a number of things that we've talked about at Next so far, especially on AI hyper-computer and cluster director. We're bringing all that to bear for enabling those use cases you have for running Ray, and also AI ML workloads on top of GKE. From a Ray perspective, what we've heard from customers that's resonated with them, is that they're looking for that awesome performance and scalable and reliability. Already we know that with the things that we're doing underneath the covers, we can make it easier for you to train quickly at 10x larger cluster scale. These large workloads are pushing us to do more there. We're seeing 65,000 nodes. We're seeing 50,000 chips for TPUs, and we're working hard to make that all possible, while still continuing to lower the tail latency by 60% and 40% higher throughput than what you're going to get otherwise with Kubernetes. Startup matters. You want to make it efficient in terms of cost. Make advantage from the moment you deploy those nodes to getting those workloads deployed on top of it, so you can start getting the most of it. Making faster startups possible, actually reducing through secondary boot disks and image streaming, up to 29 times faster from image pulling. Economic obtainability through DWS. You can get the GPUs and TPUs you want when you need them, and best in class from an accelerator perspective as well. And you'll notice on the right-hand side, a lot of those capabilities that we've been talking about, we've been plumbing that all the way through from the infrastructure layer, now into GKE to make better throughput and performance from networking with RDMA over converged Ethernet, topology-aware scheduling, and then plumbing all that back through from cluster director and supporting Ray, and even Ray Turbo as well for your deployments. Now, what's really exciting here that we talked about in our session is that Ray Turbo is coming to GKE. We're coming together with AnyScale to push things forward even further, both in open source first to make Kubernetes the dominant platform, the best platform for running Ray workloads on top of it, by making it super effective, leveraging all the cool things that Kubernetes has to bear, and now an offering for Ray Turbo on GKE as well. So you get that awesome performance of Ray Turbo, that hyper-efficient performance and scaling, and efficiency of consumption with support for GKE, all deployed leveraging Kubray. You're ultimately going to get a 6x cheaper inference, 50% higher QPS, and 4.5 times faster for loading large data sets. We're really excited about this announcement, looking forward to hearing from all of you and any interest you may have, and thanks for your time. And thanks for your time.