 We're here to talk to you about master serverless Gen.AI with Gemini and Cloud Run. So my name's Preston. I'm here with my colleague Lisa. We'll have Oliver come up as a customer story later in the presentation. But we're here to really talk to you all about how these two things, Gemini, Gemini Code Assist, and Cloud Run essentially can combine to help you build generative applications all the way through the entire software development lifecycle. So we can all recognize these very common stages of development, everything from design, coding, testing, building, deploying, operating, and optimize. We're going to take you through that journey. For each of these three areas, we're going to talk about how these tools can be combined. So to start off, designing and coding, and of course, we've got to include testing as part of this. We're going to be looking at what it takes to build a generative AI application on Google Cloud. Now, there are many ways to do this, and there's variations on applications. So we're going to start with really a high-level archetype point of view. So what are the functional primary components of a generative AI application? Of course, you have the client. This might be a mobile app, an API client, a browser. You need some sort of front end. Where is this thing communicating? You ultimately, in an AI-powered application, have a core model inference part of it that's happening. And now, beyond that, you can get more and more sophisticated. So the first thing we'll talk about is RAG retrieval augmented generation. This incorporates a subsystem of your application. You can go farther to the right here with things like multi-agent orchestration, et cetera, but we're going to stop here. And what you see is for each of these functional components, you really have a choice of how you choose to implement that on Google Cloud. And we give you a lot of options between open models, hosted models, hosting the inference yourself, et cetera. So we're going to show you a couple of these patterns. So when it comes to these RAG systems, this is really a distinct part of a generative AI application. And so, Lisa, let's dive, let's take a quick sidebar into what it takes to make a successful RAG subsystem. Yeah, absolutely. So the key step for RAG is about data ingestion, right? This is where you prepare your data and then convert them into embeddings and load them into the vector database for LLM to use. So large language models have context windows, limitations, right? So this is where you want to divide your data into multiple chunks and add metadata such as timeline components. And then this is really for the fast retrieval. Similar how you want to divide a book into multiple chapters, add chapter titles, right? And then you also want to choose the right embedding models for, based on the type of data you have. And then the embedding models, there are different embedding models. You do not want to use an image embedding model for your text-based data. So there are also very domain-specific embedding models, such as models pre-trained for legal documents or medical terms, right? So we want to choose right embedding models so they get accurate, your data gets accurate representation in the vector database for your application to use. And data ingestion is not a one-time task. So you need to have a strategy for incremental data updates. And this is where you also want to set up automation pipeline for data ingestion. It's kind of recommended because you can set up the automatic event-based triggers, or you can use some sort of a scheduler for the periodic data refresh and updates. So in this session, we're going to showcase two different Gen.AI design patterns, and each with a corresponding demo application. First, we're going to build application together. And then CloudRine is going to serve as a front end, you know, managing the user request and response. And then we're going to, at the back end, we're going to use Gemini, hosted in Vertex, and we'll use the AlloyDB for our vector database. And CloudRine is also going to use it as a serving and orchestration service between managing the data flows between the LLMs and the vector database. So Preston, would you like to show us what our demo app looks like? Sure, yeah. So we're going to look at a common scenario that has been around a long time, which is like movie recommendations, right? So movie recommendations or recommendation engines are a thing that have been around a long time. Historically, these were always built on what I'd say is sort of first-generation ML, right? This was pattern matching, looking for signals within what people might have viewed in the past, etc. But now with generative AI, we can add a lot more sort of conversational, rich context about why a certain model, why a certain movie might be the recommended one. So we're going to build this application, sort of we're going to jump through in the demo, sort of we won't show it all end-to-end. But I think the first demo we'll show here, so let's go ahead and move to the demo screen, is we're just going to start in Vertex, right? In the Vertex studio, we're going to say, hey, I want to create a system prompt here that says you are a movie recommendation expert who will help someone pick between two movies based on a scenario, right? And then Gen AI can be used to explain more why we're picking a certain movie. Now, the great thing is here I can kind of test this out and I can say, I'm going to use the same pair of movies throughout this, like, let's choose between Rambo and Toy Story for a kid's birthday party. Now, if you know those movies, hopefully Gemini will suggest the right one. And so, you know, it's saying definitely Toy Story. Okay, so we're just going to run through this as a fun example. Now, this is a prompt. I've kind of created my bare bones app in the studio. I can actually now in Vertex studio just deploy this to Cloud Run as an application right from Vertex studio. I'll show you what that looks like. I get a Gradio built UI. This is now running and hosted in Cloud Run. And I could be done in minutes with a very simple chatbot, custom chatbot. Now, in our case, though, we really want to build this as a more custom application. I actually want to take the two possible movie choices as specific fields. I really want to now go and not just use this quick prototyping, but build an application. And so, Lisa, what are some general practices if I'm going to embark on this practice of building an application? We'll go back to the slides for this. Can I go back? Go back to slides? Yeah. All right. So, the same software engineering principle, of course, applied to the Gen AI applications, right? So, first, large language model evolves really rapidly. So, you want to check model versions like how you would check the code changes, right? This will ensure that you get the reliability of your applications and have a rollback strategy handy in place in case something does not work, right? So, for each new model, you also want to have a good set of test suites, right, obviously, in place. And this also ensures you have the consistent and reliable application behavior, especially when a new model version comes into your image. And then the application consistency is super important. And then you also want to separate model configurations with the main application logic to ensure the code modularity. You do not want to rebuild applications, you know, if you are updating any of the model configs. And on the prompt design side, obviously, the common practice is use the system prompt in addition to the dynamic user query user prompt, right? The system prompt basically defines the persona of your application and controls how the app behaves based on the nature of the inference application. And in order to control the LLM behaviors, we have a number of parameters you can configure. And since like output number of output tokens, and then you also have configuration parameters such as temperature, top K, top P, those really helps you to, you know, give you kind of a fine-tuned opportunity to balance between the creativity or diversity and the accuracy of the output responses. Another thing I want to mention is about application level context. So this is extremely useful if you have a large document and then you have, you know, lengthy conversation. You want to cache some of those contexts locally in your app. And this helps you to reduce the LLM API costs, right? That helps you save some costs. And a more advanced strategy would include, like, using things like semantic caching for caching based on the meaning of the whole context rather than just the words. Now, actually, Preston, let's take a look at how we can use Gemini Code Assist, actually, to help us to write part of our code for the demo application. Great. So I think one of the things I just want to clarify is here we're using Gemini not only in our application, but we're going to use Gemini Code Assist as an AI assistant to help us build the Gen AI application. So let me catch us up. So before we were just doing this in Vertex Studio, I'm going to jump ahead and I've written some code here, which is some Python code that is ultimately responsible for calling the Gemini model. So you can see things like the system prompt here is now in this function. And I have things like some of the parameters of temperature, max output, tokens, et cetera. So this is where I've configured the ability to call the Gemini model. However, I want this to be part of a web application. And so I've already got some of this. I have a very simple application route that just serves my home page. Now I need essentially my app's API layer that will call this function to call the model. All right, so I'm going to use Gemini to generate that function for me. So here I can pull up Gemini and say I'm going to generate some code. And I'm going to, instead of typing out this long thing, use a little bit of clipboard help. So I'm going to write a recommendations route that calls my recommendations function and returns a JSON payload with the recommendation. So let's go ahead and generate this. And when I generate this code, Gemini's going to first show me a diff. Gemini code assist will say, okay, here you can see things like I am calling my vertex movie recommendations function with the two parameters it needs. And it's returning my response as JSON. So I can just go ahead and accept this. Great, it's written this route of my web app for me. Of course, we want to, you know, always use best practices and highlight this and again choose Gemini to help generate my unit tests. So here you'll see it not only will generate the unit tests, but it sort of explains to me that it's going to understand the code. It'll not only then generate the tests, but explain to me what they do. So once this is done, I have my test file. I can now go ahead and simply put this into a new file and save it as test.python. Great, I've got some tests. The other thing is since I want to deploy this to Cloud Run, I probably want the option of building a container. So in Gemini here, I can go ahead and generate a Docker file for this application, which installs the requirements and serves via G Unicorn. So let me go ahead and generate my Docker file. And once that's done, great, I will save this again as another file in my repo. Great. So this application is now sort of ready to go. I think we want to know some more things about how to get it deployed. And I'll say, how do we get this in front of our users? Yeah, absolutely. We'll go back to the slides, sorry. Yeah. So this is actually why we switch. Yeah. So this is where we're going to build and deploy. This is where the build and deploy comes into the picture, right? So you need to build a container image, of course, based on the source code we just produced and store it in a artifact registry and then deploy it to the production ready environment. So next slide, please. Yeah. So security is really paramount in most organizations. And a secure software supply chain is a key step in the overall enterprise security measures. And there are a few things that we can do to improve the security, especially during this phase, right? So number one is that we want to use a multi-stage build, right? This is where you can separate your build environment with your runtime and then to create a minimum container image to reduce the attack surface as well as that would help with the code startup time. And then one of my favorite feature on Cloud Run is actually to use the Google Managed Base image for automatic security updates. And then that also gives you zero application downtime. So it's like automatic security with zero downtime, right? What's not enough about it? And it's super useful to adopt. Another thing is the vulnerability scanning. That one automatically scans your container image for no vulnerabilities and providing kind of detailed reports so you can get really proactive to address any of the security issues. Last but not least, we have binary authorization. This really gives you an extra layer of security by only allowing the trusted images to be deployed in environment. And that gives you kind of protecting the integrity of the entire production environment, right? And in case of emergency, you can actually break glass as well with this feature. So, Preston, would you like to show us how Cloud Run makes it easy, you know, for us to do the build and deploy? Yeah, absolutely. So, you know, earlier I showed you I generated a Docker file. So often you would need to then build a Docker image, push the image, then deploy the image. We can actually do that in one gcloud run deploy command because we're able to push the source and then build the Docker image all in from one command. So this is going to take a while. I'm not going to necessarily run that. We'll kind of jump to one of the images that's already been built and uploaded to show the security scanning. So this is an artifact registry where I have my movie recommendation images. I can go through here and see that they've already been scanned for a certain number of vulnerabilities, and I can click into that and see specifically which vulnerabilities might have been found, how severe they are. Of course, one of the things I can do is I probably need to at least fix my high to medium ones, and I can probably either view the fix and I can probably use Gemini Code Assist to help me remediate some of these when it's involved in code changes. Maybe there's requirements I need to update, et cetera. And finally, I can come into the service where this has been deployed, and in our security settings, I can go through and enable binary authorization. So right now, we don't have any binary authorization enabled currently, but we can set policies that might say things like, I only want to trust images built by my system, et cetera. So I'm not going to go into policy syntax, but this is essentially a deploy time enforcement of security policy that uses signed attestations around what kind of images can be deployed. All right. So let's go back to slides. So the next final chapter here is sort of the operate and optimize. Now, this is sort of a continuum because operations might be something I tend to think of things that you do in the sort of minutes to hours, right? This is monitoring application, responding to incidents. Optimizations often are very similar, but they might be things that span, you know, days to weeks. How can we improve the performance or availability in the architecture of our system? So we're going to run through a few of these. Yeah. So first, of course, is about observability, right? So robust observability is really kind of essential in operating your Gen AI services. And then first of all, you want to establish something called service level objectives. And then this, you want to define a very clear and measurable targets for service availability, latency, and then error rates. This kind of helps ensure you get a consistent, high-quality user experience and also allows for the proactive issue detection. And then if you want to get a more detailed performance analysis, we have something called a cloud trace. And then this basically allows you to trace the flow of requests into your applications, especially for LLM API calls, right? You want to identify, you know, where is the latency bottlenecks and how you want to optimize your performance. In addition, Cloud Run gives you out-of-the-box monitoring dashboards, and it also integrates with a lot of third-party tools such as Datadog, Dynachase, right? But for the Gen AI applications, you may want to enhance above with some of the customized metrics tailored to the specific inference application. For example, you may want to check a large language model, the token usage, or you want to check the frequency of unanswered queries or some domain-specific request classifications. All of this matrix really help us providing a deeper understanding of the service behavior and also enable you to make kind of data-driven improvements. And the resilience and high availability are also important, especially for those business-critical enterprise applications, right? For incremental, you will have incremental data updates all the time, right? So you want to take advantage of Cloud Run's building feature of the revisions that really support the traffic splitting and with very easy rollback strategy. And then fast autoscaling is also one of the key differentiations on Cloud Run. Literally, on Cloud Run, you can scale up to tens of thousands of containers within seconds. And then it's designed to handle the most demanding cyber Monday, Black Friday type of traffic. But you want to actually do some fine-tuning beforehand, right? So you want to tune the container CPU size and concurrency settings to ensure the smooth, fast auto-scaling. One more thing that I want to mention is that Cloud Run is a regional service by default. But some of the applications may have the high availability for multi-regional support, right? So this is another area where with Cloud Run, you'll be able to deploy one service to multiple regions, use one single command. And then we also support, there's a new feature that we just announced. We also will support the automatic failover and failback when the service in one region becomes unhealthy. There's automatic failover to a new region. And when the service restores back, the traffic gets direct over to the original region. So it's a great feature for the high availability type of application. Now Preston's going to show us how some of this feature works. Great. So I've deployed the service. And again, these are out-of-the-box capabilities as part of Cloud Run. So if you haven't seen these before, it's always sort of delightful for me to be able to demo this kind of stuff. So I've deployed the service. I've got all of my standard kind of common metrics built in and available right here around latencies and memory usage, et cetera. I've also got integrated logging. So I can see all my HTTP and request logs interleaved with specific custom application logs, which is great. I can take these and then further build them into an SLO. So if I want to have SLOs and SLA sort of built on these metrics or including my custom metrics. Now some of the autoscaling stuff I think I'll show just quickly here while we're in the revision space. Here I can show maximum number revisions, things like these configurations. These are things that don't exist at the container image level. These are runtime parameters, part of the runtime environment. Now beyond just the standard logging and metrics, here we have the Cloud Trace application. So what we're seeing essentially is a heat map of latencies. These bars that are higher up in this chart are slower requests. And so particularly when it comes to things like LLM responses, we might have a number of our... You can see that this cluster of slower responses are ones that are hitting my recommendations route, right? This is because these requests are hitting an LLM call. They're obviously going to be much slower than requests that are just serving HTML. So if I'm trying to really optimize the overall performance of my application, I can use tracing. Now for availability, we've gone ahead and deployed two instances of this application, each to two different cloud regions. And that allows me to put these behind a cloud load balancer. And the cloud load balancer now will make this a multi-region application. Again, it's important to understand that even a regional service in Cloud Run is already highly available because it's multiple cloud zones. So now we have two tiers of availability. And by putting this behind a global cloud load balancer, I can kind of easily put it behind a custom domain and be able to do my... Now I can run my application and say, here I've got my custom form. I'm going to do my two choices again, Toy Story. And my scenario is going to remain, you know, a kid's birthday party. And now this will hit the load balancer, pick the nearest or most available region, route it to Gemini, and I have my app running as a global multi-region Gen.AI application. I think we'll switch back to slides. So when you talk about optimization, there's things that are tightly coupled. I think performance and cost tend to be two of these things. So in terms of performance, I think we mentioned things like concurrency. There's a lot of the parameters in Cloud Run service that you want to match to the behavior of the application code. So some code might have a lot of asynchronous calls to storage systems. You might have slow databases, right? These don't use so much CPU for every request because they're waiting for a database response call. So you can actually increase the number of requests any one container can handle because it's not using a lot of CPU per request duration. Other services might be using more CPU. And so you can do things like balance the concurrency and CPU and memory usage to sort of really optimize the performance of each of those. Now, obviously, there are times you want to trade and maximize performance at some cost because it's just worth it. And so you can do things like always running CPUs so you no longer have cold starts. If you want to cap some of your costs, you can do things like set maximum instances, right? So if you have an application that you simply never want to scale, say you're worried that maybe you're going to get DDoSed and you simply want Cloud Run to set a limit, you can do that. Or you might use max instances because you're relying on some legacy database that, while Cloud Run can scale up to thousands of containers, your poor database that it's calling that's still running something older can't really handle that kind of volume. So you may want to cap that at the runtime layer. So let's move to our next application. We're going to go back and refresh that this is our archetype. We can choose multiple blocks from this sort of grid of implementation options. And we're going to look at our next demo application, which is image generation. Now, in this case, what we're doing is we're going to be switching the model inference from Vertex. And we're going to actually host an image generating model in Cloud Run itself. So we're going to use Cloud Run for pretty much, you can see it's a straight flush there. And this is going to take advantage of a new feature in Cloud Run, the ability to attach GPUs. All right. So Cloud Run GPU is now generally available earlier this week. A media L4 GPU is supported. And we will have additional GPU types support come along the way, right? With the Cloud Run GPU, this is you get on-demand GPU availability and scale to zero for cost savings. And my favorite demo is actually scaling from one to 100 GPUs in less than five minutes. If you haven't seen the demo in the previous sessions, you check it out in the Google booth tomorrow in the modern infrastructure area. We have a really cool GPU autoscaling demo there. Next slide, please. So there are some commonly used tools when it comes to self-hosting the large language models on Cloud Run GPUs, right? So OLAMA is basically a management tool that simplifies the local management and running the LLMs and providing a user-friendly interface for the model deployment. Of course, Hugging Face, most people have heard about it. It's a vast repo for the pre-trained models, offering a wide range of options for your Gen.AI applications. And on the inferencing serving framework side, we have, for efficient servings, you want to consider using the VLLM or TorchServe. VLLM is really kind of a general-purpose inferencing engine. And with special memory optimizations, especially for giving you higher throughput and lower latencies. And then for the models that are trained in the PyTorch, you want to use the TorchServe, right? And then to glue all these different components together, you can use Lanchain, Lanchain, Lanchain, or Google's own GenKit. Like, for example, Lanchain can act as an orchestration framework and enabling the data flow between the LLMs and external components, such as third-party APIs, tools, and memories that facilitate the creation of the complex applications. So this is only a small section of the tools, as the Gen.AI application ecosystem is really vast. And then its landscape is actually constantly evolving over the, as well we've seen in the past two years. Now, actually, let's move on to our second demo, Preston. Great. So in this case, I'm not really writing the application. I actually am just using a sample application that I've downloaded from one of our examples, and I've just deployed it to Cloud Run. I'm going to just start this off. So this is already hosted and running in Cloud Run. Let's just do a puppy with wings. Generate that image. While that's generating, I just want to show you what this service looks like in Cloud Run. Because here's where you can see that, if you know where to look, this is where this container is hosting a secondary container with the GPU attached. So here you see the NVIDIA L4 GPU. Further information about it, right? This is something that is allowing me to actually accelerate the stable diffusion model directly in Cloud Run. And there we go. We have a puppy with wings. We have a puppy with wings. Now, I'm going to go back to my Cloud Shell editor here, and I've just got to quickly add in this folder. My workspace. So I'm going to bring in this sample code. And I'm just going to show you one more example of using Gemini Code Assist. So we use Gemini Code Assist a lot to generate our initial app. But in this case, I've downloaded a sample application that's using PyTorch, one of the tools Lisa mentioned. And, you know, this is kind of unfamiliar to me. So I'm going to go through here and find that part of the code. And sort of going to go in here and ask Gemini to help me learn more about it. So there's a section in here which is actually calling the stable diffusion pipeline. And I just want to ask Gemini, can you explain this to me? Because it's using a few things I'm not too familiar about PyTorch, right? So by going through here, there's a Torch D type. And I can get Gemini to basically walk me through this code and explain that this explicitly sets the data type for the model's tensors to TorchFloat16. I can even in here follow this up with, hey, can you just explain to me more about the tensors and why PyTorch? I can sort of stay in my editor, right? And understand if I want to try modifying this sample code, I can have Gemini help me understand how it works so that I can start tinkering and maybe learn a bit more and maybe take this app somewhere else. With that, I want to invite Oliver up. He's going to tell us a little bit about what he's doing at Cafe Financial. So come aboard, Oliver. We'll go back to slides. Thank you. Okay. Thank you, Pristan and Lisa. And my name is Oliver. And we are Cafe Financial Holdings with a customer base, 15 million, and a workforce of 56,000. And our operations spread across more than 900 locations. And we have six major sectors. And since launching our cloud migration initiative in 2020, we have been working closely with Google Cloud to migrate a wide range of applications to the cloud. And our goal is to migrate 100 applications to cloud by 2025. So today, I'm so excited to share our collaborative journey with Google Cloud and how this partnership is driving innovation and transformation for us. So let's talk about our project. This project is generate credit risk report from code cases. And this is our data processing. So our solution is to build a data pipeline in your team with Google Composer to orchestrate automated ingestion of record cases documents. And as we know, these documents often contain complex legal jargon and multilingual information. So we proceeded via Gemmine to extract something like that. And Gemmine's advanced initial language processing capabilities enable us to extract critical data points with exception accuracy. And after extracting, we store it into our database. So this data empower us to generate highly detailed credit risk assessment by cross-refreshing loan applications with relevant code cases. By identifying correlations between applicants' profiles and past court cases, we can provide our employers more transparent and comprehensive understanding of the potential credit risks. So to ensure optimal control and flexibility over our model deployment, we've selected Cloud Run. It allows us to seamlessly employ and match our own custom models. So I want to take some technical side from that. We learned a lot of things from this case, things like deployment LAM model and innovative technologies. So for about the LAM model, we use the Vertex AI to quickly build our customer GMA model and then deploy it on Cloud Run. And it is so fast and easy to deploy it and with the Gen AI innovative technology and LAM deployment. And of course, we have to take on LAM deployment optimization. Things like cost optimization and inference performance and resource scaling. And about the technology, innovative technology, we get two breakthroughs. One, to keep things efficient, we use the low-run-fire tuning to fit our requirement. And the second, in this case, we set a lot of some evaluation metrics like accuracy or fidelity to choose the best model. So I think this is a great beginning of our application, Gen AI applications. And we will continuously figure out it more and move on it. Thank you. And that is my sharing. And that may be a pass to Lisa. Thank you. Thanks, Oliver. Thank you, Oliver, for a very insightful use case. We'll stay on slides, yeah. Thanks. Yeah. All right. So I know I'm standing between you and this great evening party that's coming up in the stadium. So let me quickly review a few key considerations for self-hosting models on Cloud Run GPUs. And first, you want to select a robust base image, right? This is where you want to use images from Google Deep Learning containers or you use those images from the media container registries because they are optimized for the machine learning workloads. And then that will ensure you get compatibility and performance out of those images. And then in order to attach a GPU on Cloud Run, you need a minimum of four vCPUs and 16 gigabytes of memory. Another really key important thing to remember is about the startup probes. You want to configure them on the Cloud Run because it's crucial for determining when your container is ready to serve the traffic. You want to allow the sufficient time for the model to load and initialize. The container itself starts up on Cloud Run really fast. It literally takes only five seconds. But to download the inference framework as well as the model downloading takes longer time. So with this startup probes, it literally helps verify the model is fully ready before serving the request and then preventing any premature traffic that could lead to any errors. One last thing I want to mention is that about scaling. If the container doesn't have significant CPU usage and Cloud Run will be able to scale out for the request concurrency, right? So it's kind of important to set an optimal maximum concurrent request per instance for the efficient GPU autoscaling. All right. So how models are loaded to GPUs may affect your application's performance, especially the cold startup times. So here's a breakdown of some of the key options, how you can download your GPU, your models to the Cloud Run GPUs. First option I'm putting there is downloading via the internet, right? So this has the easiest setup, and you can literally, I have this demo in the booth area. You can literally set up and host a larger language models on Cloud Run GPU with three easy steps. However, with this particular internet download, you can see the drawback is kind of could be unsecure, right? Because you're going to use the internet, and you could also have very unpredictable container startup time because of the, due to the network conditions or any of the model size you choose. And so building the models into a container image or downloading via the GCS Google Cloud Storage bucket are recommended for your production workloads. So in the second option is the container image option, right? It's kind of secure because you do not need to attach your service to the internet, no internet access, and it's kind of, you can customize it with your own dependencies, your own data. However, the approach could result potentially a large image size. So, and any model updates may require you to rebuild a whole container image, right? Which can be time consuming sometimes. Google Cloud Storage bucket offers you a kind of fast and reliable way for transfer data to the Cloud Run. And you can load large language models to via GCS APIs or Google Cloud SDK. And then it does, but it does require you to do some code modifications to integrate a GCS API within your own container image. So the, my favorite option is actually doing the Google Cloud Storage bucket via the volume mount. So that's a really kind of great option because it does require minimum change to your container image. And then it's kind of, you get a kind of very similar integration with the rest of the service. So, all right. So that's all I have, Preston. So what do you want the audience here to take away from the session? Yeah, thanks, Lisa. Yeah, I think today we've shown you that we really have a core dynamic duo between Gemini and Cloud Run, right? Not only Gemini to power the AI features of your application, but Gemini to help in the coding assistance of building those applications. And that really, on GCP, you have a broad range of choice in this software development lifecycle around from prototyping in Vertex Studio, quickly deploying those prototypes to Cloud Run directly from Vertex. You can choose to, you know, mix and match our hosted models, open models that you host yourself in Cloud Run. And that really, this is a toolkit to help you along many stages of this journey of development. I think what I wanted to mention is that we're also announcing at this conference something I've also been working on, which is this idea of having a developer program, right? So how can Google offer your development teams benefits and tools that make not only things like Gemini Code Assist, but have environments and ability to more easily support a developer workforce in that stage that is often sort of pre-production, right? Where are you prototyping? Where are you experimenting? Where are you learning GCP? This is really the goal of the Google Developer Program. Mutual Messenger