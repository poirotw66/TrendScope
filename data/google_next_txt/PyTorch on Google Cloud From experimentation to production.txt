 CHRIS CHANNELAXI-CAN CHRIS CHANNELAXI- Hey, everyone. Thanks for joining us today. My name is Chris Chan, and I'm a product manager at Google Cloud. We're excited to share our progress today in serving the PyTorch community, and we'll cover a bunch of ways in which we're helping our customers achieve performance and scale on GCP. I'm joined today by my colleague Deepak Patil, also a product manager here, and Jihoon Lee, director of product at NVIDIA. Here's our plan for the session. I'll kick things off with an overview of PyTorch on Google Cloud. Then I will dive deep into performance. Finally, Deepak and Jihoon will cover our efforts in resiliency, a critical aspect of large-scale training. Let's get started with PyTorch on GCP. PyTorch has seen phenomenal growth, and for good reason. We see three key pillars driving this. First, models. Innovation is happening incredibly fast. More and more state-of-the-art models solving complex AI problems are being implemented first or primarily in PyTorch. Second, the ecosystem. It's not just about the core library. There's a vast and rapidly expanding ecosystem of tools and libraries. Think Hugging Face, PyTorch Lightning, VLLM, and many others, built around PyTorch, making development faster and more efficient. And third, performance. PyTorch users demand top performance, and the framework delivers through compiler tech, custom kernels, and hardware optimizations, letting you get the most out of your infrastructure. Google is deeply invested in the PyTorch ecosystem. We're a founding member of the PyTorch Foundation and are working to ensure that users of PyTorch solutions, such as VLLM and Ray, have a great experience on GCP. Crucially, many of our Google Cloud customers rely on PyTorch every day to power their critical AI ML workloads and achieve their business goals. And because of that, we provide support across Google Cloud, ensuring PyTorch works for training and inference on our diverse hardware, including both TPUs and GPUs. Now, how do we bridge PyTorch to diverse hardware like TPUs and ensure high performance, especially at scale? A key part of the answer is OpenXLA. As you move from experimentation to production, efficiency and scale matter, and this is where ML compilation and OpenXLA can help. OpenXLA is an open source ML compiler that is supported by a growing list of AI accelerators and provides optionality, allowing you, the customer, to target different hardware backends. And importantly, it provides common elements. Frameworks like JAX and PyTorch can leverage the same underlying compiler stack, benefiting from shared optimizations and development efforts. OpenXLA is the open source version of Google's production-grade ML compiler, which runs three of the world's best foundation models on both TPUs and GPUs. Every capability in it is rigorously tested, helping our customers achieve scales of tens of thousands of GPUs and TPUs in over a decade of production deployments. Let's quickly break down the core components of OpenXLA, which work together powerfully, but can also be used modularly. First, you have stable HLO. Stable HLO acts as a crucial intermediate language, facilitating the lowering of PyTorch into a hardware agnostic representation that can easily target different compiler backends. Next, you have Shardy. Shardy is an MLIR-based tool focused on partitioning and parallelizing large models across many accelerators, enabling scaled-out distributed training. Next, you have XLA. XLA is the core compiler engine, and it performs powerful whole-graph computations and generates highly efficient code specifically for GPUs, CPUs, and TPUs. Pallas, developed by Google, lets you write custom, high-performance kernels for GPUs and TPUs using a Python-like language for fine-grained optimization. Triton, from OpenAI, is another popular Python-like language for writing efficient GPU kernels, offering an alternative to CUDA. PGRT provides the clean runtime API that connects frameworks via stable HLO to the hardware-specific compilation and execution managed by XLA. While these components can be used independently, they deliver the most power when composed together. XLA aims to be the go-to compilation stack for anyone running ML workloads at scale. This brings us to PyTorch XLA. This is the library that specifically enables PyTorch to run efficiently on XLA devices, most notably cloud TPUs. It's designed for scale, bringing large model training capabilities familiar to TPU users into the PyTorch world. It aims to optimize for performance by acting as a bridge. It translates PyTorch operations into stable HLO, which then feeds into the powerful XLA compiler, the same one used by JAX for optimizations like operator fusion and distribution. And it's enabling advanced observability and resilience features for the PyTorch ecosystem. So how does this work in practice? Well, with PyTorch, you have torch.compile, which allows different compiler backends to plug in. PyTorch XLA acts as one such backend. When you use the PyTorch XLA backend, Dynamo captures your PyTorch code. PyTorch XLA then lowers the PyTorch operations into stable HLOs. Think of stable HLO as that common language ensuring portability. From stable HLO, the XLA compiler takes over, applying powerful optimizations like fusion and handling distributed training strategies like SPMD. Finally, the compiled code runs on the target device, such as TPU or GPU, via the PJRT runtime interface. This modular architecture means you can leverage the power of XLA with minimal changes to your PyTorch code. Our goals with the PyTorch XLA project are clear. First, support the PyTorch community. Provide well-tested code, examples, best practices, and engage openly. Second, serve cloud customers. Maximize performance for all kinds of production tasks. Training, inference, RLHF, and ensure interoperability, giving you accelerator optionality. Third, foster the ecosystem. Ensure key third-party libraries like Hugging Face and VLLM provide abstractions and solutions to program TPUs. Looking ahead, we're focused on three key directions. The first is modularity. PyTorch XLA aims to function as a modular backend within Torch.compile, allowing users to flexibly combine different execution strategies. One goal we have is to mix compiled XLA graphs, standard PyTorch Eager execution, and custom palace kernels within a single workload. In addition, we're pushing to a world where advanced XLA-specific capabilities like GSP-MD are distinct opt-in modules. The second area is performance. This is paramount. We aspire for PyTorch XLA to be the most performant backend for all PyTorch models and users. The third is ease of use. Continuously improving the onboarding experience, documentation, guides, tutorials, reference implementations, and providing better tools for observability and debugging. We're committed to a regular release cadence aligned with PyTorch core releases. This ensures that we're always building on top of the latest and greatest from the PyTorch community. I won't get into all the features here, but you see investments in kernel optimizations, new model architectures, and inference. This roadmap shows our continuous investment in bringing cutting-edge features and performance to PyTorch on XLA devices. To help users adopt best practices, we've introduced Torch Prime. It's a reference implementation showcasing PyTorch XLA's distributed features in a clean, minimal code base. It's designed for performance, demonstrating optimal configurations for large models using various sharding techniques and custom kernels. And it focuses on ease of understanding using a single trainer code base for diverse models like dense LLMs, sparse LLMs, and diffusion models. Check it out if you want to see best practices in action. So now, let's take a deeper look at some of the key performance features and the metrics we help our cloud customers hit as a result of PyTorch XLA. So I mentioned Palace earlier. This is a key technology for unlocking performance. It's a special library that allows developers to write custom kernels for XLA devices. This lets us create highly optimized implementations for operations like flash attention, page detention, or specialized kernels for advanced models. Crucially, it uses the shared code and compiler technology that also powers JAX custom kernels, ensuring efficiency on TPUs. And of course, PyTorch XLA users don't have to be kernel experts to benefit from them. They can take off-the-shelf Palace kernels to achieve higher performance for their training and serving workloads. Another important capability in a recent release is host offloading. This is designed for training very large models that might exceed device memory. It allows offloading parts of the model, optimizer states, or activations to the host's CPU's memory. The goal is performance. We can use the host memory as an additional tier or even overlap host computation with device computation. Users get control via simple APIs to specify which tensors should be offloaded. Let's see how host offloading works in more detail. Imagine a tensor with a long lifespan during training, consuming valuable device memory. With host offload, we move that tensor to CPU memory, freeing up TPU memory. The key is that copies between host and device are overlapped with computation, minimizing the performance impact. This is particularly useful during backpropagation and training where many activations from the forward pass need to be saved. Offloading these can significantly reduce peak memory pressure on the accelerator. So how does training look like in code? We want to make using PyTorch XLA as seamless as possible. Torch.compile provides a fantastic way to apply compiler optimizations with minimal code changes. Look how simple this is. You get your XLA device using XM.XLA device. You move your model to that device just like any other PyTorch device. You define your optimizer. In the key line, you wrap your training step function with Torch.compile specifying backend equals open XLA. That's it. Now your training loop is using XLA for TPU execution without needing to rewrite your core PyTorch logic. Now let's briefly touch on the kind of performance you can expect. Here's a preview on Trillium, our latest TPU. For LAMA 3.8b, a very popular LLM for cloud customers, we're seeing over 7,000 tokens per second per chip. Even for the massive LAMA 3.405b model, we achieve over 113 tokens per second per chip at an 8K sequence length. This showcases excellent scaling and compute efficiency. Moving to sparse MOE models, for Mixtrel 8x7b, we're achieving 3,770 tokens per second per chip during training. And it's not just LLMs. For text-to-image generation using the Flux 1.2b model, we can generate a 1K by 1K image in just 5.9 seconds on a single Trillium chip. So summing up these preview results, competitive performance across a diverse array of architectures. Dense LLMs, sparse MOEs, diffusion, and we're working on more. And excitingly, we're bringing these benefits to inference serving, too. This week, we announced GA for TPU support within VLLM. Through PyTorch XLA, VLLM users can take advantage of features like page detention and continuous batching on TPUs. It's great for developers because there are no code changes, just a few lines of configuration changes, if you're already using VLLM on GPU. Building on a community effort also means you get access to the latest models, like Gemma 3 and LAMA 4, for TPU. We've also been investing steadily in performance for TPU as a back-end. In the past two months alone, we've tripled performance for our customers and have a steady stream of POC requests. You can learn more at the VLLM breakout tomorrow afternoon at 5.15 p.m. Thanks for your time. And with that, I'm going to hand it over to my partner, Deepak, to share more about our resiliency efforts for the PyTorch community. Thank you, Chris. I'm Deepak. And today, we're going to focus in this section on resiliency. Specifically, we're going to focus on LLM training workloads with PyTorch on NVIDIA GPUs. So, if you are doing an LLM training workload, typically your goal is as follows. Given a specific infra footprint, let's say some number of GPUs, and a time window, which usually runs in order of weeks, you want to train the model as efficiently as possible. What does efficiently mean in this context? You want to maximize the good put or productivity of this workload across the entire training job duration. That's the key goal. So, what are the key challenges we face today in achieving this goal? Well, firstly, there are frequent interruptions. There could be hardware issues. There could be software issues. Or there could be planned maintenance. In each case, the workload is interrupted. You lost productivity. Second challenge here is that traditional checkpointing methods are slow and in line. When you take a checkpoint, you have to interrupt your training job to transfer the state. Also, when you have a failure and when you're restoring the previous checkpoint, you lose all the steps you have done since the previous checkpoint. And finally, because these are large, complicated distributed systems, identifying and localizing the failures is pretty challenging. So, what's the net effect? Your ideal curve for a training job should look like this. You start your training job. You execute some number of steps every day. And you finish it and end up your duration. But the reality looks like this jagged curve. What you see here is the white columns which show the interruptions due to either failures or planned maintenance. The red columns show the lost work every time you have to go and reset back to your previous checkpoint. What's the net impact for this? For a typical large-scale training workload which spans thousands of accelerators and with a duration of several weeks, a 1% improvement in ML good put translates to cost savings of more than a million dollars. Just think about it. 1% good put equates to a million dollars of cost savings. This is why it's crucial to improve your good put. Well, how do we do that? Here's a very simple workflow of a training workload. You have the pre-flight deployment stage. You have the training job which is shown here in the blue bar, which includes things like state migration, monitoring, and remediation. What you want to do is for all these things, you want to spend the minimum time. So in the training job, you're actually spending as much time working on the training step compute. And finally, you have the post-flight. What we are introducing today is a set of cool technologies to help maximize good put. Firstly, we are introducing optimized checkpointing with multi-day checkpointing, which can support async saves and fast restores. We will also support improved visibility and notification for failures, degradations, and stragglers. And finally, we have a policy-based elastic training, which supports various remediation workflows, such as in-place GPU restart, node hot swap, and scale down scale up of workloads. This is all packaged into a single container for easy deployment. We integrate with NVIDIA, NVIDIA, NIMO, and GKE. And we have ready-made good put recipes for you to deploy. So let's take a look at a couple of these features in more detail. Firstly, let's look at optimized checkpointing. First of all, with async checkpointing, you reduce the amount of time your training job is interrupted when you do a checkpoint save. First of all, you transfer the state to the host, and then you move it to the storage in the background. Secondly, you have multiple tiers of storage, so you can do fast checkpoint restore. You're always restoring from the nearest storage. And you access durable storage only for catastrophic failures. Finally, you do a lot more frequent checkpoints. So instead of order of hours, you can do now checkpointing of the order of minutes. So that way, when you have to go restore to the previous checkpoint, you lose a lot fewer amount of steps. What does three-tier multi-checkpointing do? As it says, there are three levels of checkpoint storage. The first one is in the node in your local RAM or SSD. The second copy of the checkpoint is in the cluster on a PR node. And the third copy is in durable storage like GCS. The most important aspect here is this replication happens seamlessly and automatically in the background. Let's take a look at a short video now to see checkpointing in action. Our recipients utilize several checkpointing optimizations to improve throughput. The first optimization is to minimize checkpointing overhead. Async checkpointing and email blocks training while checkpoint preprocessing and offloading to those occur. Training then resumes as the subprocess is created for writing to file. Our recipes add onto the existing logic by defining a dedicated checkpointing subprocess, an offloading state immediately so that training can resume as soon as possible. Let's see this in action. When the first checkpoint is saved, the dedicated checkpointing process is created. Then, whenever checkpointing occurs, the subprocess is queued and is saved. Training resumes as async saving happens in the background. After a few seconds, the checkpoint is fully saved for 5. Here is a plot of the training tokens per second for a LAMA 405b training workload. The drop in training tokens per second is significantly smaller when applying the saving optimizations. Our recipes also support A-Cluster checkpointing, allowing for faster checkpoint saving and loading. Here is how A-Cluster checkpointing works. Take a training workload with a hot spare. Now, one of the nodes goes down and needs to be replaced. The hot spare is swapped into training. However, an in-place job reset is not guaranteed, meaning that training processes can have the wrong checkpoints saved locally. In this scenario, the checkpoint is replicated so that every rank has the needed files. After replication, training resumes. Let's see if it's in action. Here is a workload that just had to hot swap and restart due to a node going down. We see that the workload is able to identify necessary peers to replicate data. For example, rank 15 has to send its local checkpoint to rank 511 and has to receive a local checkpoint from rank 183. After replication is complete, we see that training resumes from the loaded checkpoint step. Let's now take a look at elastic training. Elastic training can help improve good port in three ways. First of all, it allows you to resume the training job as quickly as possible, even if you see a failure. And you can define a policy to customize the remediation workflow. First of all, you can do an in-place GPU reset for a software failure or a node hot swap or a scale down of resources and continuous job. Secondly, training can automatically scale up when the replacement node is available. So you're always maximizing your GPUs being utilized. And finally, your hot standby or spare pull can be reduced or eliminated. This picture shows how everything comes together. On the top left, you see the user who can customize a Python script to define your specific policies. The supervisor process runs in the background, listening to signals from diagnostic service, which identifies failures, degradations, and stragglers. After that, the supervisor takes action based on the policy defined by the user. Here we see a very simple workload which has nine GPU nodes. It's three-way data parallel and three-way pipeline parallel. Depending on the issue, your policy can have different types of remediation. For example, if you have correctable error like a software crash, you can just do an in-place job GPU restart, restore from local checkpoint, and resume the job. If you have an uncorrectable hardware failure, and you have spare capacity, you can do a node hot swap, restore from a peer checkpoint, and resume the job. Finally, if you have no spare resources, you can scale down and continue your job. In this case, you're scaling down by one dimension of data parallel replica from three to two replicas. In the future, when the replacement node is available, you can scale back up automatically to your full capacity and resume training. If you change your scale up and down based on elastic training, you might need to change some hyperparameters for your training job. This is why you have the callbacks available to the user so that you can adjust things like batch sizes or learning rates. This is how you see all the various techniques coming together to put a customizable composable remediation workflow to help reduce your downtimes and maximize your good put as much as possible. In building our solution, we work closely with NVIDIA to integrate with their capabilities, specifically the NVRX Resilience Library and their NEMO framework. And to tell you more about this, I'd like to invite Juhun Lee, the director of product at NVIDIA. Thank you, Juhun Lee. Thank you, Juhun Lee. Thank you, Juhun Lee. So, my name is Juhun Lee. I'm a product manager at NVIDIA. So, today, I want to thank Deepak and the team for giving me a chance to talk about NVIDIA's own resiliency software work. So, I want to start by giving a little bit of background and motivation on why NVIDIA started working on resiliency software. NVIDIA has many research teams and researchers who are training really large scale models. For example, we recently announced NEMOTRON H hybrid Mamba style models. And just this week, we announced LAMA NEMOTRON Ultra model, which can achieve state of the art accuracy with just half of the parameters required in DeepSig model. So, to help these research teams to focus on what they're good at, creating these really cool models, we wanted to make sure that our AI infrastructure is providing goods, very high good put with a good resiliency and full tolerance. So, we started looking at some of the problems that our researchers were running into. And we realized that a lot of wasted time is because they're manually looking at the progress of the model training. And when something goes wrong, they have to manually intervene and look at what's going on and then restart the job. So, we came up with this idea of automatic restart. And to achieve that automatic restart feature, we also had to come up with a set of features to help implement this. So, when we looked at what is required to restart the job, the main time that it was required to restore the job was spent in this checkpointing. So, we wanted to improve our checkpoint features. So, Deepak talked about how we can improve our performance on the checkpoint by leveraging both asynchronous checkpoint as well as leveraging more faster memory, which is local to your GPUs. And then we also looked at how we can accurately detect the faults. So, detecting the faults is the beginning, right? So, in order to take an action, you first need to understand what your problem is. So, we developed the routines that can help detect the GPU that are hang or stragglers. Stragglers are GPUs that are running slower than it's expected to run at. And we also need to distinguish the bad GPUs versus the user errors. Sometimes, the training loop itself was faulty by design. And when something goes wrong, it's important that we don't restart those jobs again, because we'll continuously try to restart and end up with the same error. And also, after you detect the fault, you also need to ensure that the health of your system is still good before you try to attempt the restart. So, we also provide a set of APIs where you can use to query the current status of the GPUs, MB links, and many other components. So, after we developed all this for our own internal researchers and teams, we were able to see great results in improving overall good-put. And we wanted to also provide this benefit to all of our partners and developers who are working on our platform. So, we decided to open source these projects on our GitHub, and we released it as a modularized Python package that you can install using PIP. I would like to cover a little bit more about the automatic restart. So, now I talked about all the individual components that we built to enable this feature. Now we are putting everything together to really improve our restart time. The basic restart is handled by the job schedulers. So, Kubernetes or Slums, whatever job schedulers you're using, they have a basic way of killing your jobs and restarting. However, the problem with that approach is it could take in the order of a few minutes or five minutes. And when you're looking at a scale of 10,000 GPUs running together, that translates into 50,000 minutes of GPU time wasted. So, we really want to reduce this restart time as much as we can. And in order to do that, we actually need to go into the job itself where the application is running. In this case, it's the training framework. And we need to detect the faults and handle it within the application without going back to the job scheduler. So, what's required to do this is an accurate attribution of where the fault is coming from. And by classifying them, we can take a corrective action that's only required so that we can reduce the overall reinitialization time. For example, in the in-process restart, when we detect a transient network flip, we know that all of our GPU CUDA context and the states are still valid. In this case, we can simply roll back to the previous known good checkpoint and then resume the training process again. So, in this case, we can easily achieve this within 10s and 20 seconds range. In some cases where the fault is actually corrupting some of the context, for example, when we encounter an uncorrectable ECC error in the DRM, oftentimes our CUDA context gets corrupted, which means you have to kill those processes and then reinitialize your CUDA context before you can actually reload the checkpoint and resume. So, in this case, it's still much faster than going back to your Kubernetes schedulers and trying to reinitialize everything because we're just killing the running process and then taking corrective action locally. Both in-process restart and in-job restart can be combined with the WAMS pairs. So, if we detect some of the nodes are behaving, then we can simply replace that node and continue with the WAMS pairs. And good news is NVIDIA has integrated all the work that we already did on the resiliency into NEMO framework. NEMO framework is an end-to-end cloud native framework to build, customize, and deploy generative AI models. NEMO also comes with a lot of tools to support you with your training models as well as deployment. We support multi-model data loaders as well as data curation. And you can do a lot of customizations in post-training. You can, for example, do RLHF training with a NEMO aligner. And we allow, we also enable deployment at a scale at anywhere by providing also model evaluation tools and guardrails. You want your model to be safe when you deploy, so we also provide this guardrail feature. We already integrated the first release of NEMO into the NEMO 2502 release. And more features are getting integrated. And we'll continue to integrate our latest and greatest features into NEMO framework. So, what's next for NEMO? NVIDIA, we would like to focus on solving really hard problems. And that's what our engineers are really good at. So, one of the problems that we are looking at is the problem of silent data corruption. So, it's called silent data corruption because it's really hard to detect. It could be a cosmic ray that hit on some of our compute elements and then flipped the bits. And maybe it translated into a NAN or infinity during the training. And oftentimes, you only detect this when you observe your loss curve. Like maybe you see a random loss spikes. And by the time you actually realize something went wrong, it might be, you might already have wasted a lot of time. And this is one of the areas that our team is also very interested in solving. So, we are currently brainstorming a lot of ideas. Also, we also want to focus on what's really important to you. We want to work with a lot of developers and communities to identify the areas of challenges that you guys are having. And then solve those problems together. So, we are now moving our development entirely onto the GitHub first. So, you can see all of our commits and the work that we are doing currently. And this will open the doors for a lot of collaborations. And I would like to welcome any suggestions and feedbacks. You can find me afterwards or please use our GitHub to create an issue and report your suggestions. With that, I'll hand it back to Deepak. Thank you, Jihoon. As you can see, our good put improvement capabilities are integrated deeply with NVIDIA, NMRX Resiliency Library, and with Nemo Framework. We have ready-made good put recipes for Nemo for you to deploy. Internally, we ran recently a case study with 1,000 GPUs for a typical training workload. And we saw good put improvement from 80% plus to 90% plus. That's a huge improvement in good put by applying these techniques. Remember what we said earlier? 1% improvement in good put can translate to a million dollars or more of savings. So, that's a very significant improvement. So, the combination of these capabilities helps us achieve that. This table specifically shows the impact of each of the capabilities which we have measured. With async checkpointing, we can reduce the interruption for training jobs from tenths of seconds to the order of a second. With frequent checkpointing, you can do checkpointing in the order of tenths of minutes instead of hours. This reduces your last productivity when you go and restore to a previous checkpoint. Your interruption recovery time for the training job on a GKE hot-node hot-swap is down to a couple of minutes instead of 20 minutes. And finally, if you have no spare capacity, you no longer have to stall your workload until you get a replacement spare. You can automatically scale down and resume your workload in a couple of minutes. And we have replacement available a few hours later. You can scale up in the same amount of time. So, that's the power of all these capabilities which you can put together to get significant good improvement. Lastly, let's watch a very short video showing elastic training in action. We begin by showcasing our power-way and hot-swapping capabilities in response to an objective failure. After deploying the required home tenants and a brief initialization period, you should see 112 nodes registered as running with the supervisor. These correspond to nodes running the Lama 370D workload. A few moments later, we encounter a post-timeout error. Since three spare nodes are available, the supervisor issues a hot-swap command to replace the faulty node with one of the spares. Fast forward in a few minutes, we see the workload is back to running on 112 nodes, with one failed node being tracked by the supervisor. We can visualize hot-swapping by looking at the cloud monitoring GPU duty cycle graph. The temporary dip in GPU utilization illustrates the recovery time. Next, let's take a look at scale down and scale up. Fast forward into later in the training run, we have no spare nodes left and run into an XIV error. In response, the supervisor issues a scaled-down command to reduce the data replica dimension of the workload by one. A few moments later, we see the workload is back to running, but on 180 nodes, with five failed nodes being tracked by the supervisor. This corresponds to a reduction of our DP dimension, four nodes, or 32 GPUs. After repairing the faulty nodes and reintroducing them into the cluster, we see the supervisor detect a scale up opportunity, allowing us to increase the data replica dimension of the workload by one, back to 112 nodes. After a brief recovery period, we see the workload is back to running on 112 nodes, after a successful scale up event. We can visualize scaling throughout the lifecycle of the workload by tracking world-size and tensorboard. For this figure, we inject a failure every 20 minutes on average to showcase scaling capabilities under extreme stress. So let's conclude by summarizing what we saw. You can maximize your training good put through a set of capabilities, specifically elastic training, which is a customizable, composable technology where you can define a policy for in-place GPU restart, or hot-swap, or scale down and scale up. You can have optimized checkpointing with async saves, fast restores, and frequent saves. And finally, they're all packaged together for easy deployment in a single container, and we have ready-made recipes for you to try out. The recipes are aligned now on the AI hypercomputer GitHub site, and we encourage you to try it out. Thank you.