 Hi, everyone. Welcome to this session on Gemini and BigQuery. I'm Deepak Dhaima, product lead for Gemini and BigQuery here at Google. I'll be joined by two amazing customer speakers, Samir Zubair from Virgin Media O2 and Samjit Srivastav from General Mills. We have a packed session for you. We're going to cover a lot of the improvements we've brought to Gemini and BigQuery over the last 12 months, some of the future-looking changes we are going to bring, and we'll have our customers share their stories of rolling this out within their organizations. So let's get started. So let's take a look at the bigger picture. Many organizations are investing heavily in their AI initiatives. But a significant majority of those are not getting the tangible value and at-scale production rollouts as much as they would like. And this is because AI is not useful without AI-ready data. The reason enterprises struggle unlocking this full potential and having everyone participate in the AI journey is because there are skills gaps, data access restrictions, slow consumption, and tool-specific knowledge that tends to lock up this access. This hinders AI initiatives. This is one of the key motivations for us with Gemini and BigQuery to help customers unlock the value. And as you can see, we introduced at last next the full breadth of capabilities that Gemini and BigQuery addresses, including data pipeline building, discovering and analyzing your data, writing queries more efficiently, and even simplifying your operations. The breadth of capabilities was introduced at last next, and a lot of them have gone generally available since then. Indeed, the momentum has been amazing. We now have thousands of customers using Gemini and BigQuery, and the usage has grown 350% since launch. The high acceptance rates with our SQL and Python code generation reflects the value that it is delivering for data analysts and data scientists. And adoption of AI is not just limited to a certain industry. It is across finance, logistics, retail, and so on. So let's go over which capabilities we are now improving or launching at this next. Data preparation is now available generally. It is one of those experiences which would not have been possible without the power of large language models and Gemini. Data preparation is a low-code, natural language-first experience to building data pipelines. You can create transforms. It suggests in natural language data quality rules and transforms that you could introduce to clean up your data. And it provides them in an intuitive way in previews so that you can apply them with great confidence. Not only that, it monitors for data and schema SKUs to help address them as well. With data validation, Dataplex integration, and with robust CI-CD, it provides a really good data management solution. Many of you have already or may be moving from different platforms onto BigQuery. With Gemini, a lot of the manual code conversion can now be automated. The migration services support for SQL translation is now generally available. To help scale, we also support batch mode as well as API, especially useful for our partners to integrate this into their tools. Now, we know that accurate metadata forms an essential and reliable foundation needed to empower both your people and advanced AI and agentic experiences. But doing so is tedious. And doing so at scale to make sure you can get the insights from all of your data is even harder. So with automated metadata generation, you can dramatically speed up that process and make sure your machines and people are able to discover and use this data more effectively. We're also introducing a knowledge engine module for data insights. With interactive entity relationship visualization, you can now visualize hidden relationships, making it easier to understand how different data entities connect. This solves the problem of manually deciphering the semantic, the schema relationships, and it accelerates data modeling and query design. Now, last year, when we had introduced data insights, it would serve as a library of this natural language questions accompanied by pre-validated SQL, but it was at a per table level. It uncovered a lot of questions you could ask off of your data, something that you probably would not have even tried. Now, we are expanding that to provide data set level insights, doing joins across tables within your data sets, using the data quality as well as usage-based insights to power that. So, this addresses the cold start problem, especially as people discover and use newer data sets. BigQuery data canvas is a workflow graphical tool within BigQuery Studio, and it lets you find, transform, analyze, and even visualize this data by using natural language prompts. It is a graphical interface for workflow analysis. We are now introducing assistive chat within data canvas, and the idea is a seamless AI assistive chat experience. What's powerful here is that it encapsulates the entire workflow. So, even with a single prompt, you could generate a canvas with multiple nodes in it and iterate from there. now, we know sometimes it is pretty hard to get started, especially when you're working with unfamiliar data sets. So, if you're looking at a data set and thinking, where do I begin, it also includes suggested prompts so you can get started with those insights. Data canvas is now also going to go beyond just SQL. You can now embed Python notebooks within a data canvas node. We continue to improve our SQL experience. Over the last year, we introduced a SQL widget, which helps users refine their SQL queries and get to the result they really want. We are now introducing natural language expressions as a feature within that bucket. What this feature does is basically a lot of times when you write your SQL queries, you write it with the business logic in mind. So, in this case, you could just write the business logic in comments and let the feature take care of converting that into a SQL. It oftentimes happens that you have to go and look at a particular function, how it works for Google SQL. You can eliminate a lot of those aspects of writing your SQL queries with this feature. So, let's talk about accelerating your Spark workflows with AI. We now support Apache Spark code generation in preview. Just describe what you need using natural language and specify Spark or PySpark in the prompt, and it generates the code. Second, this is context-aware. So, it automatically uses your actual schemas from BigQuery and Dataproc Metastore. This ensures that the code is accurate and relevant to your data. Last year, we introduced troubleshooting for Serverless Spark, where instead of having to sift through logs that can be in disparate places, it aggregates them and surfaces them as a natural language inside that is actionable. Just today, Gemini Cloud Assist was launched. We now integrate with that feature for investigations. So, we believe a lot of these capabilities should be available and accessible to everyone. So, I'm really happy to announce that a lot of these core capabilities around SQL Code Assist, Python Code Assist, Data Canvas, and Data Preparation are now available at no additional charge to all BigQuery customers. This means you get immediate access to these productivity-boosting features as part of your existing BigQuery usage. So, let's look ahead on how AI and agenting experiences come together. at the foundation, you have state-of-the-art Gemini models for reasoning and planning. Critically, the data foundation is trustworthy. The BigQuery knowledge engine grounds every experience. It uses metadata generation to help you with managing for scale. And it is combined with Gemini-powered AI governance features built right into BigQuery. This ensures AI assistance is secure, accurate, and respects your policies. Building on that government foundation, we also offer specialized data agents for data engineering, data science, and for data analysis tasks. And you can access these seamlessly, whether it's through agent space, through Colab, BigQuery Studio, or via APIs for your own custom apps. So, the result is a powerful agentic experience that's deeply integrated, inherently governed, and available right where you work. We're also working on API that is now in preview for analyzing BigQuery data assets, allowing you to programmatically leverage the intelligence of Gemini to understand, query, and even visualize your data from external applications. By leveraging the agent API, organizations can help democratize data access beyond just platform users. This capability makes insights more intuitive and actionable. Now, here's a feature that is in experimental right now, but I'm really excited. It's called BigQuery AI query engine. The vision here is to embed the power of large language models into your BigQuery analysis. We know that extracting semantic insights can be challenging. asking a question like, provide me all the text descriptions or all reviews that surface safety concerns. That may require maybe building a pipeline, specialized AI skills, and can be time consuming and cost you. So, AI query engine aims to fundamentally change that and make these kind of analysis trivial. the key innovation is allowing you to blend familiar SQL along with natural language prompts, as you can see from this example. And this makes sophisticated analysis easy to use and accessible to your users. Additionally, for those who prefer Python, it integrates with big frames, offering a Pandas-like interface. So, for all the preview features for UpGemina and BigQuery that you saw and may not all be rolled out to your projects, this is where you go ahead and sign up for them. I'm now going to invite Samir from VM02. Wow. Fantastic. Thanks, Deepak. I'm really excited to see the upcoming feature in BigQuery and how it transforms our data journey. I'll start with a question. Raise your hand if you have experienced a moment where you have a slow internet connection impact your professional or personal life. Yeah. Imagine we face this frustration on a scale, affecting millions of people, and this is a challenge VM02 face every day. And we not accept the way we are doing things. We need to transform the connectivity. And today, I'm showing you how. But before we get into the tech of the thing, let me take a moment to introduce who we are, what are the challenges we face, and how can we overcome these challenges. VM02 is one of the UK largest telecommunication company, but we are much more than that. We are the backbone of connectivity in the UK. We serve 45.7 mobile connection, which is nearly two-thirds of the UK population. We serve more than 5 million fixed-line and broadband connection benefit from the ultra-fast internet. And we get more than 515 million mobile contract, which can access to the anytime and anywhere. At VM02, we not provide any connectivity, we reimagined connectivity. Now we have understanding our connectivity landscape. We also have some core business goals. So let's discuss number one. At VM02, data is hard of every decision we make. By enhancing the data literacy, we're not just upskilling our team, but empower every team member to do smart data-driven decisions. so they understand the data, they optimize the data, they can contribute for a better outcome across the organization. Customer retention is more important than customer acquisition. We use AI to monitor customer behavior, identify the customer at risk, and personalize their experience. We help them to reduce churn and improve customer experience. the solution at the right time. At VM02, it's very important that every team have a very accurate, reliable, and timely data on time. We break down the silos, we create a data pipeline, workflow, to ensure that the team gets trusted and good data on the right time. But achieving this goal, we also have a lot of challenges. On the screen, I'm not covering them, I'll cover a few of them, dependency on our data team. We have a non-technical team, heavily depend on the technical team, which slows down the insight and reduces agility. Time consuming process, too much time we spend on the CSV in Excel to get the data. Instead of working on data, we work on data. Data comes from multiple data sources, which lead to conflict insight, and which impact trust and accuracy. To tackle these challenges, we rely on the cross-functional team, our data expert and domain expert. On one side, we have data experts, like data scientists, data engineers, BI developers, data analysts. They are professionals who know the data. They create very cool products within VMO2, but they don't have domain knowledge. On the other side, we have the domain SMEs. They are the performance engineer, radio manager, product owner. They know ins and out of our fixed line and mobile data, but they heavily rely on our data experts. So as a result, data experts need domain knowledge, and domain experts need some insight very quickly. To bridge the gap, we adopt Google BigQuery, and which can support SQL, Python, Spark, data flow, data canvas, all the features to empower our data expert. On the other hand, our domain SMEs use the national language to generate code. So it's just time to show demo. I'll try to show you three demos very quickly. So let me jump to the screen. So in this demo, I have very messy data, and I'll use the data preparation and get some insight from the data. So in this demo, I load the table in the data preparation canvas, and BigQuery gives me all the suggestions on the right side. It identifies first column as a string, and it gives me a suggestion how I convert the string column into data timestamp. I don't do any registration and logic, click of a button, it converts it into timestamp. It also gives me a suggestion to convert another column, which is to string into integer, and it converts simply click of a button convert into integer. Now I need to filter the data by a particular vendor, Nokia. This particular table contains 127 columns. Remember this. I add a step, filter the data, simply write a small prompt, filter by Nokia. Play the prompt, and remember I didn't mention the column name. It identifies the column name which contains Nokia, simply apply, and here you go. It's filtered the data by Nokia. Now I'll rejoin this dataset with another table. I'll add another step. I'll join, browse the table name, radio manager, select the radio manager table, select it. it geminized, is intelligent enough to understand which two columns I need to join, and here you go. I have two tables join. Now let's see the new column available on the right side. And I need to, let's see, yeah. So if you see this column, I have the, there's a underscore 2G customer service suffix there. I need to remove from, I need to clean this column, radio manager. I'll write a simple prompt, remove the 2G customer service feed from the radio manager. It writes the syntax for me. Either I can replace in the same column, I'll create a new column as well. I apply it, and boom, it will clean the data. Now I'm happy with my dataset. So I need add everything into a destination table. I add another step, select the destination table, select the dataset name, give the table name, save it. Now I can see the end-to-end pipeline. Whatever the step I did, do the cleansing, everything, I save this pipeline. I need to schedule the pipeline every day at 8 p.m. I give the schedule name, select the service account, give the time, 8 o'clock. I create a scheduler, let's do a dry run. I go to the scheduler preview, click the scheduler name, and run now. Refresh it, boom. We have the clean dataset available, I didn't write any SQL query, I didn't write any script, and how I have the data available in the table. Now explore more about this data. So I need to auto-generate the insight. So it's very important when I use the insight, I always recommend, based on my experience, use data profiling, because it's removed the hallucination from the data. So I click on data profiling, then I click insight. It took 5 to 10 minutes to generate the insight. I already have a table where I have a profiling ready. If you see the profiling, it will actually generate like a mean, more, median, first quartile, lower quartile, all the context for the data. And if I go to insight, it generates all the auto-generated natural language question. And this sort of question I need to ask from this table. Look how many prompt and SQL queries generated. Quite impressive. Let run one of the query in a BigQuery console. So I copy to the query. It's open a tab. I run it. And I get the result. And it's more than that. Okay? So as I mentioned before, this particular column, table, contains 129 columns. And we need to add the description. So it also generates the description on the column when we generate it inside. So it takes a lot of our time for table description. And always bring human in a loop. So I can change the description if I'm required. And also, it generates the table description as well. So I know the context of the column and I also know the context for description. And it's very important when you use LNM or Gemini in a BigQuery, you need to provide the context. Now, jump to my favorite feature in a BigQuery, Data Canvas, which I love it. I load this table in a canvas. I query it. And I write a small prompt. If you see the prompt, it's clear the daily percentage availability per relative manager. I have not mentioned the column name. I didn't mention the formula. Let's see. Wow. It knows the understand which column we need to take in the SQL query. It knows the context of the formula and it will generate the SQL query and execute it. Now I can visualize it in the canvas. Auto-generate. It will generate the trend chart. And now I generate the insight. So this is the overall feature I need to show. I have another demo I want to show you after this one. I'll go to another slide. Yes. So I will show you one of the use cases which we solved in VMO2 using the concept called 4W. So we have a challenge. So let me start from here. Okay. So I'll walk through our real world demo where we need to investigate the support ticket. And we use a technique called 4Ws. What is 4Ws? Why, what, when, and where? So this method we use to extract the information from the ticket. I'll give you an example for one of the ticket. So back in February, a customer called Team Cruz, not Tom Cruz, called us to our customer services. And I think he's shooting for a movie called Agent Space, 007. And I need to extract this four key information from this ticket. So when the issue has happened, who is the mobile, what's the mobile number? Where exactly has happened, which is the postcode? And what's happened? What service impact otherwise, other data? Or everything overall? So this is example of the ticket. So I'll load the ticket data into the data canvas. Yeah, I'll load the data in canvas. So in here, I use the LLM to extract the information from the ticket. So there's a two step process. First, we need to create the model. It's called the endpoint for Gemini. I'm using Gemini 1.5 Pro 001. So you can create a remote connection in BigQuery. So first, you need to create a connection, create a connection, create a model in a BigQuery, and then call this connection, this model, into SQL. SQL. And there's a specific syntax you have to use, ml. generate text. There's a specific format you have to follow. It's a three-step process. First, you need to call the model. Then you need to write a prompt. And then you need to get some optional, like temperature, token, and all this stuff. Some of you know about it. So I write a very small prompt, which I tell this prompt, write this prompt, can you extract the fall date from the information? And it will extract the date, but one thing you notice that every date is a different format. So I use another prompt, another note, okay, where I give the example from extract me the postcode. I give a few examples to my prompt. It's very important, so each understand the context. So if you see this one, it reads row by row for each of the data and extract the postcode information. So in the previous note, I extract the fall start date. It's a different format. I have another big prompt. It's got big prompt, anyway, which can translate all the date into a specific format for the date. And the third note, which I have it, where I extract the mobile number, I specify the specific format for a mobile number, and it extracts the information, and the last one, I write a prompt, can you identify which service is impacting, are there all data, voice, or SMS? So it gives me information in the first column. I join everything into a single note. I visualize in a pie chart and get in sight. I switch to a live demo. Third demo. Look, this is a script someone given me a couple of years back. I need to, I think I need to put full date to scroll down. Oh, quite a lot. Yeah. Oh. Look like it's not finished. Oh, how many? 4,000 lines. So I need to translate this script into more innovative way. And that's why I love Canvas, where I can visualize my SQL. And someone told me, how can I translate this script? So I bring all these tables into a Canvas. And if you see this one, I can zoom in it well. The same script, I use a GIF. I load six tables in here. If there's six tables here. And I use a method called four-stage method. So each stage do a specific task. So I divided this SQL script into four stages. So first stage do the cleaning of the data, which are called raw data layer. Where I do data build cleaning, like the transformation, typecasting, filtering, and all this good stuff. If you see this one, in this stage, I only do cleaning. And if you note this one, you can do documentation in the same time as well. The second stage, I do transformation, where either I can do join, union, and all this stuff. So I join this two table into a single node. It's quite good. Same script. And the third layer, I do application and calculate some KPIs for the nodes. So I have one single node. I generate three KPIs, like availability, drop code rate, and traffic. Same for the 3D services. I use three KPIs, same. And the last layer, where I visualize this information, I need some trend. How powerful is it? You have documentation, you have SQL, you have staging, and you have, you know, all the trends. And last stage, I generate some insight as well. It's not fascinating that you can do documentation, you can do visualization, you can create everything in a single pane of glass. glass. So this is a demo I have. And I switch over to the slides. Yes. So we do a quick survey in VMO2. By using these services, we expect about 80 to 85% reduction in the malware task, and we expect about 30 to 35% reduction in the data engine workload. So this is my demo. I'll hand over to Deepak. Thank you so much. Thank you. Thank you. Thank you. Thank you. Thank you. Thanks, Amir. That was great. I will now hand over to Sanjit Srivastava from General Mills. Hello. Hello, everyone. My name is Sanjit Srivastava, and I'm going to share with you how General Mills is using AI on BigQuery and Gemini Power to accelerate our analytics platform. While we are doing this, I'll quickly introduce myself. Okay. With me in the room is Ben Sorbel. He's the director for architecture engineering in General Mills, and I'm Sanjit. I lead data architecture in General Mills. Both of us are AI enthusiasts, and we are trying to solve a problem. The problem is we feel engineering is complex. It's hard, sometimes boring, too. Intelligence is fun. You get to get analytics. You get to write reports and graphs. Artificial intelligence is complex and is fun. So what we are trying to do is we're trying to bring the fun part of artificial intelligence into the engineering and development teams. And today I will cover how we try doing it in General Mills and what are the outcomes of that. Okay. A quick intro on General Mills. We are a large organization. We build the food which world loves. We are an organization which serves over 100 countries. Nine of our brands, individual brands, are a billion-dollar brands. But we are not the only company, we are not the company which only makes food. We are a data-centric, data-driven organization. In 2023, Forbes put it as top 10 companies who are going rapidly in AI. Data-driven insights are part of our every decision making in every domain. But before I get to the future tech, I want to share how it all started. Back in 2020, we made a decision of modernizing a data platform. We started a large program pushing over all our analytics assets into cloud surrounded around BigQuery. We also brought along our operational system to take benefit of auto-scaling there. Goal in 2020 was to be the leader in data analytics in CPG domain. We are still striving. We are trying our best to do that, and this partnership of Google and General Mills has helped us a lot. Now, what you're seeing, if I have to narrow down our architecture region of our modern data architecture, you will immediately see we rely heavily on Google services. And these are all modern services. We are real-time enabled ingestion in many of the cases. To do that, we use data transfer offered by Google, by BigQuery. We use PubSubs in many cases. We use DataFlow, right, in many cases to have real-time ingestion. The real metal, the modeling and transformation is done heavily by Airflow and Dataprep. Imagine the power of AI in Airflow and Dataprep. And we are very well supported by industry-leading tools like DBT to accelerate and deliver by speed. We are an organization which believes in data literacy, which believes in tracking our data lineage. So, industry tools like Alation and Monte Carlo are very well supported by BigQuery services like Dataplex and Dataprep to build that 360-degree view of our data literacy and our data movement. And the final layer where rubber meets the road, analytics layer, the fun layer. Heavily invested on Vertex AI and Looker, that drives our innovation, that drives our insights. Very well supported by big players like Palantir and C3 to get our insights specifically for certain business domains. So, you would see there are very, very modern architecture. But we do not build for requirements. We build for frameworks. We build for reusability. And that helps us to deliver with speed. speed. Now, remember I said we want to bring the fun part to engineering. And in the last two years, if you would have seen, the data landscape has changed so fast. So, the fun part to engineering and the landscape change of data is what we are trying to solve by bringing Gemini and AI on data. Now, how has data changed? With generative AI and the trust in generative AI, the demand for data has skyrocketed. It has increased significantly. In general, we see a lot of value in unstructured data. It offers us unlimited possibilities. Data is shifting fast from analytics to operational actions now. So, zero latency is becoming a pretty standard at this point of time. And with agentic AI evolution, we know the data needs to be laid out in a way that you can, the agents can work very smoothly with them and can take right and enable actions. For engineering and a development team, it's a huge shift, which has happened in the last two years. To solve this problem and to bring the fun of AI to engineering team is why we activated Gemini on BigQuery and invested in Gemini models integration in our pipelines. What we quickly realized is that BigQuery already has our data. We're already monitoring our data with AI pipelines. So, our data quality was good. But what we quickly realized was that with AI on BigQuery, we were able to generate the insights pretty fast. With the smooth integration of LNMs on BigQuery notebooks and other pipeline building services, we were able to solve complex engineering solutions relatively well. And that gave us a solid feeling of a unified tool, unified data layer, which BigQuery offers. Now, Gemini on BigQuery. It's a necessary journey we felt we have to take. Our goals are very simple. We wanted to enable AI on data to develop faster, better, and be more consistent. We immediately knew that this technology has certain things which would benefit us. The ability to accelerate the redundant job, right, was a big thing we were trying to do. We wanted to invest around our metadata generation because agentic needs metadata. So, these were the core capabilities we were going along. And we did not want to build that metadata in the old traditional way. We wanted to use the AI for it. We are not there yet, I think. We are not there yet. We are making our way to the fully adoption of that phase. To activate this, sometime in October, we did a closed-door PAV. We had some few partners from engineering team, ML team, and data science, and cloud engineering team to do an evaluation of the technology and find whether it meets our demand or not. Very quickly, we realized that L&M integration on our data can make our world easy and can make our agentic layer a possibility, a reality. So, after realizing that fact, we opened it up to hundreds of our data engineers. It was not a smooth ride. Don't expect a smooth ride. And the reason is very simple. Engineers are like Batman. They don't need help. Definitely not from AI. So, the biggest challenge you're going to face is the change in mindset, the change management aspect of it. But we instantly saw some wins. We saw code summarization, code explainability, troubleshooting was quickly becoming momentum, was taking momentum because of Gemini. We saw the teams which were migrating from different programming languages to SQL or Python were able to make use of this technology pretty well and had good feedbacks about it. Engineering teams which were investing their time to develop something new, they got a lot of acceleration right from the go using this technology. So, Batman was turning around. How did we make this turn around? And we are not there yet. The Justice League has still not been fought. As I said, change management is your biggest piece here. We continuously ran multiple trainings. We continuously ensured that information exchange is happening. We ran multiple surveys to show the wins, to show the opportunities, to measure the wins and to measure the opportunities. We had partnership trainings with Google, with other partners. And now, after running it for quite some time, we have realized that with right data, with right data backups and right peripheral data supports, this technology very easily can save one coding hour for every developer in a day. It's a huge number. It's a huge productivity gain, which we wanted to achieve. It also exposed a huge gap in our data layer, which we felt highly about. We were doing great for reports. We were doing great for our traditional AI models. But the gap it showed was we were not future ready. We all know agents are the future. But agents just don't rely on quality data, just don't rely on reliable data. Agents need much more. They need quality metadata. They need quality tagging. They need quality relationship being defined. We are a two million table assets. If we would have tried solving this problem by traditional ways, that would be a financial nightmare. So we formed a small team, a team which was a team of engineers which were willing to innovate with AI. We already had the background because code assets were enabled. We started building something which we call as intelligent data layer. And we are in the initial stage. But we are seeing some very promising signs. We in General Mills are investing using AI on data and AI for developers. We are transforming our two million data assets into an intelligent data layer by generating General Mills custom metadata. Now I know Google released a metadata generation recently. We will look into that and we feel it will give us a huge acceleration in doing this activity. We are also defining among our tables relationship by using the power of LLMs and tagging them. We are defining standard KPIs with the knowledge of our reporting layers and the knowledge of the LLMs. And together with this quality metadata, quality ontology, we feel confident we will have a quality knowledge graph for agent tech solutions to integrate and avoid hallucinations. We are in testing phase for our metadata agent. So what you are seeing right now is an engineering team built a metadata agent tech solution with the help of code assist and AI for developers. A very small team. It's a complicated engineering challenge. But with the drive and the innovation capabilities in BigQuery, we were able to do it. What you are seeing right now is this is a metadata generating agent framework. We are using a custom built for general mills. We are using a custom built for general mills. We are using a custom built for general mills. Fine tuned and grounded on general mills data cataloging too. The way general mills work. The way general mills acronyms do. And it's pretty simple. Simple because we have code assist now. So we started with our attributes. And we built a framework to identify different attributes which are functionally alike. We use LLMs to do that. We use regex to do that. Sound decks and multiple algorithms to find that grouping. Then we call Gemini models to generate CPG descriptions for them. Now these models are grounded on our cataloging data. They're tuned on our cataloging data and on certain artifacts that general mills only understand. These descriptions based on our experiences are much more tailored to general mills than a general LLM will do. We built this in a pidentic way. We had a critique agent directly interacting with our generating agent. Continuously ranking the descriptions coming out. And we had a threshold defined that if a rank is greater than this, then only this description would be accepted. Our initial test case proposed that we are generating metadata pretty well. Our next vision to do this is to integrate directly into our LLM. And store it in BQ. So that our LLM models have a solid metadata grounding layer to avoid hallucination. Now as I said, this is a huge engineering problem. But with right understanding of code assist and with the ability of bringing solution, the Gemini LLMs on your programs, we were able to solve this in a much faster way. I'll wrap up by saying this. BigQuery data future is changing pretty fast. Data is not intelligence anymore. Data in the future we feel will be making autonomous decisions, taking some reliable actions, driving some very intelligent business operations, and from analytics will shift into operation in nature. With the ability of BigQuery to bring AI on the data and AI for the developers, the power of LLMs, we generally feel that we are prepared, or we are ready to handle this evolution of data engineering landscape. This is what I wanted to share. Keep it over to you. Yeah. Okay. Thanks a lot, guys. Thank you for listening. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.