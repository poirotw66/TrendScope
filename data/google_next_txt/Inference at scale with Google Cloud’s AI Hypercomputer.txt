 RINA LEE- Thank you for joining us here today, especially right after lunch, for Inference at Scale with Google Cloud's AI hypercomputer. My name is Rina Lee, and I'm a Group Product Manager here at Google, building AI accelerator software for Google Cloud TPUs and GPUs. I'm honored to be here today with my colleague, Juan, and together we will share advancements in our Inference software stacks. In addition, we have Kirat and Adithia, who will share as partners to Google Cloud how they use AI hypercomputer to power workloads at Osmos and contextual AI. You've joined at the right place at the right time. We expect 2025 is the beginning of the Inference era. With AI hypercomputer at its foundation, we continue to see a huge increase in Google Cloud's workloads running on AI inference. More and more organizations are choosing to deploy Gen AI to production on both new applications and existing applications. And as you know, the landscape for AI is rapidly evolving. Application requirements are becoming much more diverse and complex. We are starting to shift to a new paradigm where AI agents will proactively both retrieve and generate data data to deliver insights and answers, not just data. Google Cloud's AI hypercomputer provides the best cloud inference infrastructure. It's flexible, performant, easy to use, and scalable. This architecture benefits from decades at Google running large-scale AI serving billions of users. We have listened to your feedback along the way from many customers running many types of workloads, and today we are sharing updates that we have made to the AI hypercomputer stack based on that feedback to power the diverse AI workloads of the future. The end-to-end stack starts with purpose-built hardware. We know that accelerated choices heavily impact both performance and cost efficiency. Google Cloud is the only place on the planet where you can choose between two highly performant, scalable, and widely adopted AI accelerators. The latest in NVIDIA GPUs as well as our custom-designed TPUs. Internally, all of Google's Gemini training and serving runs on TPUs. AI hypercomputer gives you access to that same proven power. And if you're using GPUs, you benefit from our very close collaboration with NVIDIA to offer enhanced reliability and optimized libraries across a wide variety of GPU families. From training the largest models to serving models of all shapes and sizes, we have flexibility and choice for your workloads. LLMs, or large language models, are a revolution for AI. They power a wide range of applications, natural language processing, text generation, translation, and many more. We are offering more ways now to serve LLMs on TPUs. As you heard this morning during the keynote, we're really excited to now bring the power of VLLM to TPUs. If you are already using VLLM on GPUs, this is a simple way to start offering fungibility and seamless flexibility between TPUs and GPUs using libraries you're already well familiar with. In addition, last year at Next, here in Vegas, we announced Jetstream, our state-of-the-art JAX inference engine based on the Gemini serving stack. With both of these options, we deliver competitive performance and community support, both from the open-source ecosystem and also from Google AI experts. Let's dive deeper into each of these options. As you may know, VLLM is the leading open-source inference engine. The explosive growth that you see here continues with over 10x the usage now in the past year. We knew that with this amount of adoption, we want to bring that familiarity to workloads on TPUs as well. So with just a few configuration lines of code, you can easily have the flexibility to move in between GPUs and TPUs with the same code base. You can rely on familiar interface, familiar metrics, and run on either accelerator. We've also worked very closely with both the VLLM community to optimize popular open models so that they are ready to run on TPUs from the start. Now let's also shift gears to Jetstream. As I said, this is our state-of-the-art JAX inference engine. You can see here the stack starting from the client and serving on both TPUs, and in the future, we'll be bringing support to GPUs as well. This diagram shows the architecture where Jetstream handles the orchestration and can leverage JAX LLM like MaxText. It all runs on XLA and is optimized for TPUs. We're excited to share many updates to Jetstream with you today. First, we have expanded the models that we now support to now include DeepSeq R1 and V3, and we're also working now on LAMA 4 support. This adds to existing families of models that we support across both LLMs and Mixture of Expert models from LAMA, Mixtrel, and Google's own GEMMA models. In addition, we're excited to announce that Jetstream now supports long context support up to 256K. This includes optimizations for chunked pre-fill, prefix caching, and page detention. All of these optimizations are designed for maximum performance, and it's all available in open source so that you can get started now in GitHub and build with us. Since we announced Jetstream a year ago at Next, we are investing heavily to continue to improve the performance. Here you'll see that on LAMA 2 70B, Trillium performs at 2.9X better than the last generation V5e. And for Mixtrel, Mixture of Experts model, we now perform 2.X better on Trillium versus V5e. And we will continue this strong momentum as we add more models and add more optimizations to the Jetstream stack. This morning, you also heard that we are bringing Pathways to cloud customers for the first time. Pathways is Google's internal architecture and distributed runtime technology that all of Google's internal AI initiatives are running with. This serving billions of users today and that same power we can bring to you in cloud. There are multiple values that Pathways brings to inference through Jetstream. First, disaggregated serving. Pathways allows for independent scaling between pre-fill and decode so you can offer ultra-low latency serving. And multi-host, as we know models are continuing to grow significantly, allows you to distribute that workload across multiple hosts rather than trying to serve on a single host. Let's look at the performance that we now gain for multi-host inference. On LAMA 3 405b, we now can deliver three times the inferences per dollar on Trillium when compared to the previous generation TPU V5e. This comes down to just a penny or one cent for every thousand tokens served. That cost efficiency would not have been possible without Pathways, and we're really excited to bring that to you now. And when we look at disaggregated serving with Pathways, again, we've been able to reduce the latency significantly to benefit your users. On the pre-fill side, time to first token has been reduced from about two seconds down to 277 milliseconds. And in output token generation, we have improved the latency to be three times faster for time per output token. Now let's put that into context. The human mind can comprehend lag in about 200 to 300 milliseconds. We are now bringing this latency down to that time so that it truly feels almost instant. We invite you to join us tomorrow and Friday for deeper dives on both VLLM and Pathways to learn more about how you can get started and the capabilities available. Now I'm excited to invite Kirat Pandya, one of our close partners and early adopters of both Jetstream and VLLM, to share how they use both solutions and production workloads at Osmos. Thank you, Rina. Thanks, everyone, for coming and hearing us out. So let me start with the problem statement. The problem statement is this. Data ingestion still sucks. Enterprises are spending so much time and energy on top of programmatic tools, manual tools, whatever that is, to bring data into structured data systems that can then be used for driving business value. And so Osmos, we've been around here for a while, around for a while. And our first generation of what we built was AI assisted, which was the idea was humans in the driver's seat and the AI is there to assist. And that went from the pre-LLM era into LLMs being plugged into that process. But what's happening now is the world's moving towards fully autonomous systems. And so what we've built is the world's first fully autonomous data wrangler. The general principle is this. You're busy. Your time is more valuable than cleaning data. You point us to your messy, whatever that data is, right? Thousands of tables, few tables, PDFs, point us to it. Point us to where you want it to be, what you want it to look like at the end of the day, your output tables. And then go away, have a coffee. And when you come back, your data will be ready for the human to review. The human goes from the doer to the supervisor. And so how does this fit in the larger picture for an enterprise? The way it fits is organizations spend an inordinate amount of time and energy going from bronze data, kind of data from systems they've just received, to put it into silver tier and gold tier. It's the standard medallion architecture. And so the OSMOS AI data wrangler fully automates the bronze to silver work so that the human energy can be spent on kind of deriving insights downstream from silver to gold, right? Take the mundane out. So how does this work? This is a honest and true swarm of agents that operates on your data. We go look at the data. We look at the destination. We look at the context of the system. Our agents break up the problem. We figure out, should we be writing code for this? Should we be doing this, quote unquote, manually? And all of that is guided by an inference stack that does Monte Carlo search with process and outcome reward modeling. So it's basically LLMs all the way down, right? And what's really important, though, is now that compute requirement just went through the roof to solve the same problem, right? We moved work from human brains to computer brains. And the way we've built this architecture is our training is done on GKE, on TPUs with both PyTorch, XLA, and JAX. We use MaxText. And then on the inference side, we use both Jetstream and now VLLM as well, also on GKE with Cloud TPU. And if there's one thing you take away, running your own inference is not hard anymore. You should be able to do it. If you can run containers on GKE, you can do inference. So last year, I was on stage. Reena mentioned Jetstream launched. We were one of the early customers. And we've been using Jetstream in production since then. This graph is going to be a little difficult to read. But what I really wanted to point out was this is container uptime. And so the steady climb in the graph indicates container uptime ticking up. So the tallest sawtooth there is like two and a half, three months apart. We had containers running reliably for months that we didn't have to roll over. And it just worked. This is not common in this space. The software is early. So this is really cool to be able to rely on your inference stack as well. Now, what's changed, though, is we were using that for use cases where inference was happening kind of in real time in front of the person. And now in the fully autonomous world, the human's gone away. You have time. You have time to operate. And so the workload changes. You have very long token sequences. You have to balance time to first token with throughput. And because the user experience is kind of assigned work and walk away, you can now start focusing on tokens per second per dollar, right? Your underlying metric for your business. And finally, in all of this, your workload is idle a good amount of time. But then when it shows up, it spikes. And so being able to auto scale your inference infrastructure is extremely important. And so we are now using VLLM for this use case. And the advantages it brings us is we get day zero model support. When the models show up on Hugging Face, we're good to go. We can run it on TPU. We get drop-in API compatibility. We get feature compatibility with samplers and sampling parameter adjustments and all of that. So no code changes required on the client side. And then finally, zero configuration to fantastic performance. You're not sitting there twiddling parameters on pre-fill sizes and chunking all of that. It just works. And at this point, everyone's thinking, okay, give me a number on what is the performance I'm getting. What's more important, in my opinion, is that you work with vendors that are willing and able to move at the pace this market currently is. And what I wanted to highlight was we started working with Google back in October on this. In December, that's the graph from our operations on a single node. It shows about 1,000 to 1,200 tokens per second throughput. And on the bottom right is effectively today. We're approaching 5,000 on average north of 3,000 tokens per second. That is a huge improvement in a very short amount of time. And this is from production systems. So we have this in production today. The final piece I want to leave you with is the metric that matters for your business at the end of the day is your cost of tokens as you scale, at least for a business like ours where it's heavily token driven. And what I would highlight is with TPUs today, with VLLM, you can get per token pricing that matches kind of inference as a service providers with the same amount of simplicity. You're running your back end already on Kubernetes. This goes right in there. And even on-demand rates, it's within bounds to be comparable. Well, what you get is you control your infrastructure, you control your capacity, you control your quantization. Models don't get deprecated on you when you weren't ready. It's just, in my opinion, the right way to build your inference stack if inference is a key part of your product. And so with that, I want to hand over to Juan to talk about diffusion models. Thank you. Hello. Hi, everyone. My name is Juan. I am a software engineer working in Google. I primarily work on diffusion models, specifically MaxDiffusion. And so what is MaxDiffusion? So MaxDiffusion is a high performance and cost efficient diffusion model reference implementation written in Python and JAX. Primarily, if we look here, and let me turn on the pointer. MaxDiffusion sits on top of these JAX core libraries. So it's using the best of the JAX ecosystem to build on top of JAX and basically build the best reference implementation that is open source and can be used. So in here, you'll see that we use like TF data or grain to build performant input pipelines, OPTAX for optimizer state, ORBACs for checkpointing, FLAX, which is what builds these neural networks. And then finally, AQT for quantization. Now, on top, below that, then we have the JAX framework. And then we have, even below that, we have the XLA compiler. And the XLA compiler is what allows us to run on different accelerators, such as TPUs and GPUs. So some of the things when we look at MaxDiffusion and how we wanted to approach MaxDiffusion, we had certain goals in mind. The first one is that we wanted MaxDiffusion to be easy to use. So things like being able to load different weight formats into the repo, into the MaxDiffusion, so no need to run automated conversions to do like PyTorch to JAX. It's all built in within most of the scripts. Being able to load models directly from Model Hub and from Hugging Face, where a lot of these models are uploaded. We wanted to just make that very easy to use. And I'll be showing a demo right after this on how that's done. Another thing is for MaxDiffusion to be scalable. So we wanted to make sure that we can scale across thousands of TPUs. I believe we got up to 8,000 chips on V5P for training of diffusion models. And that this scaling is as linear as we can make it, as we can possibly make it. So that was one thing that we wanted to focus on, being able to just, again, run the model, make it easy to run across thousands of chips, have the best performance optimizations. We added flash attention, and we added some quantization in it as well. And then finally, that this is flexible. So MaxDiffusion being a reference implementation and an open source tool, it's a great way for you to take any model that you might be thinking about building or trying to use with TPUs and use pieces of it to kind of build your own model. And this has been pretty successful. I worked with quite a few customers that when they kind of started the journey, they wanted to try TPUs, they didn't know how to get started. They took their PyTorch models, looked at some of these reference implementations in MaxDiffusion, and were able to get started, sometimes able to rewrite their whole model even within like a few days. So I think this is pretty useful for doing that. Now, when we started MaxDiffusion, it actually started as a fork from the Hugging Face diffusers. So if you're familiar with the Hugging Face ecosystem, such as Transformers, such as diffusers, MaxDiffusion will be similar. It will feel a little bit familiar in that sense, because it did have some of the FLAX and JAX models implemented in it. And those models were primarily Stable Diffusion, like 1.x, which is 1.4 or 1.5, and then the 2.1 and 2.2 models, and Stable Diffusion Excel. Now, we started building on top of this, so I started adding things like training for Stable Diffusion Excel, training, again, across multiple hosts or many TPU chips. But if you're familiar with the FUSION models, a FUSION model doesn't just you build a model, you serve it, and you're done. They have a lot of these different adapters. So they have LORAs, they have Control Nets, people build Dream Booth on top. So I wanted to add those features in, and they are added in, so where you can run different Control Nets to guide the image generation. Or you can load multiple LORAs so that you can change the style of the generation that you're working on. And then Dream Booth, if you want to stylize it or make it into a particular style. Now, something that we recently added was Flux. Flux is a new model, it's a state-of-the-art model. Very good fidelity model, but it's also a lot bigger. I think it's 12 billion parameters. So this was just added to Max Diffusion, the implementation. So we added both Schnell and Dev. And as we look down the road of what we would like to do with Max Diffusion, we would love to add a video generation. So video generation is something that's become very popular in the last six months, I would say. There's been a plethora of new open source diffusion models, video generation models that are coming out. And so this is something that we definitely want to support. So now shifting a little bit and moving into TPUs and what we can expect, and specifically with Trillium. When we did the MLTurf submissions for inference, we can see that Trillium delivers our best inference performance to date out of any TPU that we have available today. And this is demonstrated, again, with Max Diffusion and Jaxx. Our tests have demonstrated about 3.5x better performance than our previous TPU inference chip, which was the V5E. And so these are some results for what Trillium can deliver. And when we're looking at cost efficiency, Trillium is built to be cost efficient by design. And so one example of this is that using Max Diffusion, the cost to generate 1,000 images can be as low as $0.22 when you're using Trillium. And that's 1,000 images for $0.22. This is about a 35% reduction in cost compared to our previous generation, which was the V5E. Okay, let's go back to Flux. And again, as I was mentioning, Flux has recently been implemented in Max Diffusion. Recently, we added it. And this is an image here that was generated with Max Diffusion on Trillium that I wanted to share. And I'm going to show you a quick demo on how to get started. Like, how can you do this? And it is hopefully quite simple. First, you're going to create a VM. So you're going to create like a, in this case, I believe I use a V6E4, which is a four-chip Trillium. And you're going to clone the repo. So again, the repo is open source, so it's available. You just go ahead and clone that. And then you're going to run this installation script. Now, what's nice about this installation script is, again, looking at the goals of Max Diffusion, and we want it to be easy to use, is that this shell script will just install all the dependencies required for you to get started running and creating some of these images. In this case, you see that we have a mode. The mode is stable. Stable means that this is using the JaxStableStackStableMode, which means that all the libraries are compatible. They're being tested. We know it works. There's other modes, such as Nightly, where there is a possibility that because it's a Nightly, it might not work out of the box. And another setting that is not here, that I did not here, is that if we go back to that previous slide where we talked about what Max Diffusion is, you can run this on either a TPU or a GPU. So there is another setting here that you can set if you want to install the dependencies for GPUs so that you can actually run this on GPU instead of TPU. So it's really nice. One code base, and you can run on multiple accelerators. So finally, after you do all of this, you go ahead and run this inference. You have this inference, which is generateFlux.py. Max Diffusion is based on configuration files, which you will always see them under this folder, which is configs and something. In this case, it's the dev variant that we're running. And you can actually change the commands via the CLI. So you can, in this case, we're changing from that Corgi that we saw earlier in a house made of sushi. We're going to change the prompt to create a photograph of an electronics chips in the shape of a race car with Trillium written on its side. Once you run this, you're going to have some compilation times. So the first time that Jax runs, you have some compilation as it's basically optimizing your code to run on that specific hardware that you selected, in this case, a TPU. And that will take a little bit of time. But after that, you can say that the model is hot loaded. It's ready. And then the subsequent times after that, you should be able to run a lot faster. And so in here, you can see the image that was generated using this command. And so if we look at some performance numbers that we get here with Flux, I want to point out a couple of things. So again, there's two variants. We have the Dev, the FluxDev variant, and the Schnell. And we have some benchmarks here. And we also have sharding strategy. So you can do two things. You can either run it in a replicated mode. Trillium has 32 GB of HBM memory. So Trillium Flux can fit on the device as DDP. But Flux also has very heavy text encoders. Specifically, one of the text encoders is the T5 XXL variant, which is extremely large. So sometimes when you're running DDP mode, what happens is you have to offload some of those encoders so you can actually fit in memory. So you have another option here, which is you can run in FSDP mode. And when you run in FSDP mode, you're basically sharding the model across the accelerators that are available for you. And you do take a bit of a hit on the communication time, because now the devices have to communicate in order to generate that image. But now you kind of have more memory. You can keep those text encoders loaded in memory and ready to be used over and over. Okay. So I've been talking a lot about what, you know, we talk about MaxiFusion. We talk about Trillium. But how do we put this all together into an actual use case? So there is a customer, HubX, that has been using this for their applications. So first of all, HubX is a venture builder that develops mobile applications across multiple verticals with over 200 million downloads. They have different applications. Here I have two of them. One is DaVinci. The other one is Nova. And you can see that they generate in total about 400,000 images a day. And they're using Trillium for this. And they're using Trillium specifically with something that's pretty cool too. It's PyTorch XLA. So they're not using Jaxx in this case. PyTorch XLA is a framework that lets you convert your PyTorch code to run on TPUs without doing a full rewrite. Right? So now you have options. It's very flexible. You can either say, I want to take my PyTorch code. Maybe I want to test and test the waters, run it on TPUs. I'm going to just use PyTorch XLA. Or I'm going to do a full conversion into Jaxx. And in this case, we can also see some of the numbers here, some of the benchmarks that the customer has provided. And they are very close to Jaxx's number. So again, this has a lot of flexibility. Because now you have a framework of choice. Again, you can stick with PyTorch and just do that conversion with PyTorch XLA. Or say, I'm just going to go with Jaxx and I'm going to do full rewrites. And after the customer switched to Trillium, they are using Flux again. And they're able to achieve about a 35% improvement in latency. And about a 45% reduction in cost per image, generating about four images in seven seconds. So that is all I have for today. I do want to say, if you scan this QR code in the back, it was going to take you to a Hugging Face Hub space. It is not running Flux. It is running SDXL. But you can basically run your own images. It's running SDXL and under the covers is TPU V5E. And so you can just play around and generate your own images. And yeah, that's all I have for today. I'm going to turn it back to Rina so she can share our latest updates on GPU inference. Thank you. Thank you, Juan. It's exciting to see how much we've improved and what we offer for both LLMs and Diffusion today. So we've shared a lot about Trillium, the latest updates with TPUs. But I want to share about the updates with GPU inference, as that is the power of choice and flexibility with the AI hypercomputer. So as I mentioned in the beginning, our portfolio includes, through close collaboration with NVIDIA, a variety of options across the GPU family. Our most recent additions are A3 and A4 VMs, which now include A3 Ultra powered by H200 and A4 powered by B200. We have customers like JetBrains who are here with us and have been using A3 Mega VMs with H100 to run inference in production today. And they're excited to start moving from A3 to A4 and see that reduction in latency and enhance responsiveness of the IDIs for their customers. These are just one example of many customers that are running on GPU inference today for other customers. In the latest MLPerf submission for inference 5.0, we submitted and published results on both A3 Ultra H200s and A4 VMs with B200 across a wide variety of models of LLMs, MOEs, image models and recommendation models. And for all of these, these delivered highly competitive performance, as you can see in ML Commons. This is a testament to Google Cloud's close collaboration and partnership with NVIDIA to deliver infrastructure for the most demanding workloads out there. And we'll continue to collaborate and grow the portfolio of what we offer. I'm excited to highlight another customer today, Contextual AI, who has been using Google Cloud GPUs in production to bring this powerful platform to their customers. Please welcome Adithia, Chief Product Officer at Contextual AI. Thank you. Thanks, Rina. Hi, everyone. It's great to be here. My name is Adithia Bindal, and I'm the CPO at Contextual. So what is Contextual? Our story as a company really begins about five years ago when Dow Keela, our founder and CEO, was at Facebook AI Research. And the problem they were trying to solve back then was language models are great, but what you really need to give them in order to make them useful in enterprises in real world settings is the right context. Because the language model will not know out of the box your specific domain, your data, your preferences. So the technique that they invented that many of you might be using and might be familiar with now is Retrieval Augmented Generation, or RAG. So Dow led the team that invented RAG, and this was a very powerful idea at the time that, frankly, everyone forgot about for about three years. And then when ChatGPT came out, people started seeing that this could give you some really quick gains. Now, since then, we've published a lot of interesting research around information retrieval, reward modeling, alignment techniques that allow us to innovate in some of the most interesting spaces where our customers have been using our platform. So where do we really fit? If you think about how enterprises get the most value from language models from RAG today, you can think about it on a few different dimensions. One of them is how much differentiated value are you getting? If you're just giving access to ChatGPT to your employees, it can help them compose an email. It can help them do some quick search that might be using web search, web agents. But if you want really differentiated value, you need to make those systems specialized so that when you say, why did my sales go down in Q1? Or what is our bank's recommendation on a certain asset class? You need that knowledge to be contextualized. You need that task to get specialized in your domain, your understanding. So as you get more and more context, you get more and more value because now you're getting more of that specialization. And we see this clustering around three sectors in particular. So technology and engineering. We'll talk a little bit about semiconductors in a few minutes. Financial services is another one. And then professional services where you have a lot of unstructured data. You have a lot of domain-specific knowledge. You have a lot of processes and techniques that are highly specific to each firm. So what we have is a platform that really is designed from the ground up to solve this problem in a holistic way end-to-end. So when you think about all the different modalities of data that might exist in your companies, you might have unstructured data like PDFs, HTML files, Word documents, PowerPoint slides, markdown files. You might have data warehouses and structured data. And then you might even have some services from which you pull information via API. And so when you think about this problem end-to-end, it's not just about the language model. It's about really taking all of these different modalities of data, taking them through a document understanding system that can deal with all the complex hierarchical knowledge, complex layouts, rich media, charts, tables, all the things that make enterprise data so hard to work with. Have a model and a system that's able to understand that in the right way. And then take that through what we call our contextual RAG agent. So we're using a mixture of retrievers where we can retrieve information from all these different modalities using different techniques. Could be lexical, could be embeddings, could be graphs, could be APIs. And then a state-of-the-art instruction following re-ranker. So you might want to say, for example, prioritize all the documents that Mark authored or prioritize documents that were only written in the last three months. And then you want the system to be able to adhere to your instructions and get steered in that direction. And then finally, you want to give that knowledge to an AI system and make sure that it's grounded exclusively in it. It's not looking at its training data. It's not looking at something in Wikipedia or Reddit. It is exclusively relying on your data to give a very grounded answer. And then our customers are taking the starting point and specializing it further. So that's tuning this entire system so that it gets to know their task and keeps getting better and better, giving them a powerful feedback loop. Now, let's talk about how this actually works. So Kirith showed you the swarm of AI agents. And increasingly, enterprise use cases will look like that. It's not just about one query and a language model giving you an answer. You need to do things in this iterative way where the system is saying, what do I need to retrieve here? Is this the right thing? Let me go back and look for something else. Is that the right thing? And then keep doing this in an iterative way until it reaches the information it needs. Now, there's three things that I wanted to highlight that we faced as challenges that Google Cloud is really helping us solve in a scalable way. So the first is elasticity. Our platform has a number of different sync and async jobs. And so by having the dynamic workload scheduler, we have this ability now to provision GPU inference for different types of sync and async jobs based on the kinds of usage we're seeing in our platform. And doing that in a way that gives us both the scalability but also the elasticity makes this obviously cost effective. But then the second part is how do we minimize the startup time? So if we have enterprise customers where we are on the hook with SLAs to make sure that their end users are not waiting too long for tokens, for retrievals to come back, we need to make sure that these models are mounted in a way that minimize the cold start. And so that's where we have an approach that uses GCS to store our models. We have an underlying GKE cluster and then we can augment that with DWS that's adding and removing capacity for GPU inference as we need it. And then finally, we're running these things in mission critical workloads. So there are real enterprise customers putting a hard production dependency on us as a startup, which is a really big responsibility and we take that very seriously. So having the right observability in our system so that we know how systems are functioning. If something stops working, what is going wrong? How do we root cause that quickly? What do we need to diagnose and then communicate? These are all extremely important. And so this is where the managed Prometheus plays a huge role. Now, we talked a lot about the operational side, but I think what also matters a lot for us because we're in the business of language models, of GPU performance, is the underlying performance of the models that we're seeing from Google's infrastructure. So on the training side, we saw a 60% reduction in model training time when we moved from the A2s to the A3s. But then on the inference side, what was in some ways much more critical because this is scaling across every new customer we get is 30% improvement in throughput and a 40% reduction in latency. Now, the key thing to remember here is we keep innovating. We keep using that throughput and latency budget to do more and more. So because Google is giving us essentially a lot of this performance gain for free, we can go and tell our research team, by the way, you guys can try that experimental idea that you said was going to increase latency because we can now stay neutral and we get this benefit for free and we can keep innovating. Now, that innovation has a direct impact on what value we can give to our customers. So if you think about RAG, it's not just about one query and one response. It's a compound system that consists of many different pieces. So when we think about the performance of that entire system, we look at the end-to-end RAG performance and we compare it against the best off-the-shelf models that customers can get today to build themselves. And then we also look at each individual component. So very quickly, the key thing to take away here is one that we have built state-of-the-art end-to-end systems and then each of the individual components, which are the underlying models for document understanding, unstructured retrieval, text-to-SQL, and then grounded generation are state-of-the-art by themselves. So seeing this in action, one of our biggest customers, Qualcomm, has this very unique and complex problem where they have a customer engineering team that helps companies like Apple and Samsung integrate Qualcomm's chips into their products. And these are people with masters and PhDs in electrical engineering. They have a really hard job where if Apple comes in and says, I'm trying to integrate your chip into my camera module, how do I make this actually work the right way? I'm seeing this error message. Now, before they had a contextual agent to help them with these types of questions and issues, there would be about 60, 70,000 documents that had technical specs, circuit diagrams, schematics, going all the way back to the early 90s. And they would need to first find the subject matter expert. Then that person would know where to look. And this whole process would take them over a week. Now, what they can do with the agent they've built in our platform is just ask the question in natural language. We can go through those millions of tokens, and we can figure out exactly what the intent of the query was and exactly what knowledge needs to be retrieved and generate an answer. And that gets resolved now in a matter of minutes. So this is running in production with over 2,000 of these customer-facing hardware experts. And it's all powered by that same inference stack that you saw a few slides ago on GKE and using DWS. And so you can also check this out if it's Monday morning and you want to find out how many ports are in the RB3. You can go to the Qualcomm Linux documentation page, and there's a very small subset of publicly available documents that now Qualcomm has actually exposed. And this is all powered by our agent. So for us, this was a very proud moment because it's the ultimate stamp of trust that it's not just helping our internal employees as this Fortune 500 companies. We're now going to expose it externally, put our brand on it, and all of this is running with Contextual's platform, and that's running on GKE. So finally, I wanted to just call out that we're now available in the GCP marketplace. And with that, I wanted to hand it back to Rina. Thank you. Thank you, all. We've shared a lot of amazing advancements today in inference. And I just wanted to leave you with a few of the key takeaways. Google Cloud is the only place where you can choose between TPUs and GPUs for accelerator flexibility. We shared with you multiple options for serving today on TPUs. Jetstream as our state-of-the-art JAX inference engine, the same one that we based our Gemini serving stack on, and VLLM now on TPUs. So you could bring the familiarity of using VLLM libraries from GPUs to TPU. In addition, Juan shared updates to Max Diffusion and our latest in GPU inference in terms of performance and the latest GPU families that we now support. I invite you to continue your learning journey throughout the conference to learn more about many other aspects of the AI hypercomputer across the end-to-end stack to deliver a strong inference solution, including GKE, storage, and networking. We appreciate your feedback, and we'll continue to invite you to build with us, to try the products, experiment with them, and provide us feedback so we can continue to improve. I'd like to thank my speakers, Juan, Kirat, and Aditya, for joining me today. And I'd like to thank all of you for joining us to learn more about inference. We'll be outside the room in the hallway if you have any questions, and enjoy the rest of the conference. We'll be outside the room in the hallway if you have any questions.