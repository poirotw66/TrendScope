 All right. It's 9.45. The trains are leaving. Good morning, everybody. Hope you are having a great next so far. Happy Friday. And thank you for taking time to attend our session. Now that you've seen all the cool announcements through this week, it's time for you to think about ROI on cloud and how you can think about maximizing your ROI on GCP. And, of course, what that means is, like, you want to use all this cool new tech, but you also want to do it in a cost-effective way. So we're going to spend some time today talking about that. So my name is Gobind Jahar, and I'm a senior product manager on Google Cloud. I spend my time thinking about how DevOps customers and platform engineering teams spend their time doing cost optimization and how we can make that journey a little bit painless to get them to the goals that they're after. With me, I have my co-presenter, Frank, from Major League Baseball. Hi, everyone. I am Frank Dice, principal cloud architect at Major League Baseball. And I lead the cloud architecture and optimization division, and that covers FinOps and cloud architecture, as in the title. So in short, I'm a glorified plumber. I keep the lights on in the cloud and then patching those holes via FinOps where we're leaking the money. Thank you, Frank. So over the next 45 minutes, we'll take you through just why it's so painful for DevOps customers and platform engineering teams to optimize for costs today. We'll figure out how Cloud Hub Optimize and Cloud Assist Optimize, some new products that we're launching today, will help you take that pain away and simplify your cost optimization journey. And we'll drive home this point with a case study by looking at how MLB is already using some of these products and testing them out and how it's helping them in their day-to-day work. Finally, we'll show you a demo where you can see just how easy it is for you to get started with some of these new announcements. So let's jump right in. Why exactly is it so painful for DevOps customers and platform engineering teams to figure out their cost and utilization? One thing we know about our DevOps customers is that they hate looking at cost data. It's a painful task. It's a tiresome task for them to be able to get their costs for their workloads. And that means they have to now cross an additional hurdle in getting things done. Imagine that your finance team is coming to you and telling you that your application is running over budget for the month and that you have to figure out how to bring it back on track. So now you have to drop everything and then go hunt for cost for your workload, utilizations for your workload, and getting all that information together. To track down cost savings, you have to start by collecting data from different data sources. These data sources could straddle cloud billing, logging, monitoring, resource manager, app hub, and probably a dozen other surfaces. To add to that, you have to probably deal with a lot of idiosyncrasies of all the tools that you have to think about to pull data from those data sources. This could be in the form of an API, a SQL query that you wrote, or even a third-party product that sits between you and the data source. Next, when you have collected all the data, you also have to assemble it, and that means you have to now sanitize it, filter it, join it to come up with the insights that you were looking for, and you have to do it accurately, which could be a multiple-day effort, if not longer. What makes all of this worse is that all this time could have been spent working on your application, and now you've spent all this time hunting down costs. Here's an example of a tool that we have today that sprays you with information about how you're spending money on cloud today. The billing data export gives you granular information about SKUs, regions, projects, the granular cost themselves per product, as well as any other information that might be relevant for you to track down your costs. It's a treasure trove of information. However, it requires you to now understand the proprietary billing format of all this information, and that means you have to write custom scripts for that. You also have to now pull this data and then join it with your utilization data manually, which means that you have to now deal with inconsistencies, potentially, between those data sources, and it also means that you now have to be an expert in writing SQL queries as well as visualization dashboards and ensuring resilience in case something were to change in these data sources later on. It looks really easy on the surface, but those of you who have tried this out know just how painful this exercise can be. Let's do a show of hands real quick in this room. How many of you have actually tried to build a cost-tracking system internally for your team? Raise your hands. How many of you spent at least a month? Keep them up. How many of you spent at least a month doing it? How many of you spent three months? How many of you spent six months? Yeah, I think pretty much all the hands stayed up for all those options. So it's pretty brutal, and I think Frank has some experience in this space, too. So Frank, how is MLB doing this? Oh. So let's look at an example billing analysis query. All this query is doing is pulling the list, discounted, and effective pricing by SKU for just the networking resource for a single month. I picked this one specifically because I could make it fit on the screen. So this is pretty complicated, and as Frank mentioned, it's only for one product category, so it's networking issues, and it's for one month, which means that you have to do this over and over again, and you have to maintain the script, too. So to put it simply, it is extremely painful for DevOps and platform engineering teams to get their costs under control because it's time-consuming, it's error-prone, and it can potentially be very, very expensive. So can we do better? Sure we can. After all, we want all of our platform engineering teams and DevOps teams to be working on the apps that they're interested in, not tracking down costs. Let me take you through what we've been cooking inside Cloud Assist Optimize to help you do just that. Cloud Assist Optimization is a new umbrella of tools that we're launching today. It contains three new surfaces, Cloud Hub Optimize, Cost Explorer, and Gemini Cloud Assist Chat Helper Agent. With these new surfaces, you can now get granular cost and utilization information right in one roof. We've already done all the heavy lifting for you so that you don't have to. What do I mean by doing the heavy lifting? I mean that we've literally taken your data from cost and utilization and we've fused it together. This gives you an accurate representation of your cost efficiency across your applications and projects running in Google Cloud. In addition, we're also happy to support App Hub from day one, which means that all your costs are going to show up natively as an application and you can look at it and debug them as an application view instead of a project view if you so choose. And of course, we have a smart AI system that can now answer your cost-related questions in natural language so that we can make that last mile friction go away entirely. Let's jump into these experiences and see what I'm talking about. Let me... Excuse me. Let me... Oh, yeah. Okay. Sorry about that. So you may have already heard about Cloud Hub as a new announcement this week. And if you haven't already, it's a new, simple, elegant way for you to manage your applications on cloud. And if you are interested in the talk, you should check out the QR link at the top so you can see that talk later as well. Inside Cloud Hub, we're now launching a new optimized tab. The new optimized tab now houses all the pertinent information in a simple and approachable way for you to be able to look at your costs quickly and get to action immediately. We wanted to build this in a simple way, so what did we do? We actually ran a quick user study and figured out that DevOps paid attention to the two most important signals when looking at cost optimization. The two signals they look at are what has changed from the prior period of billing or analysis and what underutilized resources they might have that they can go optimize. So we've done just that. We've taken all that information and we've assembled it here for you in this simple dashboard. And with this, now you can quickly see at a glance your cost trend for your application right at the top. You can also see your top five workloads by cost and the cost change over that period. And you can also see your top five workloads that are expensive and underutilized. So this gives you actionable information at your fingertips. Let's take a look at each of these surfaces more closely. Granular cost data. So cloud of optimization already ingests all the granular cost data that you can get access to through billing export. It organizes this information in a simple and consumable way for you to take action. Now remember that example that we started this presentation with where finance is breathing down your neck about saving costs and they're asking you that your application is trending over budget. So instead of wondering where to get started, now you go straight to Cloud Hub and then you can get your answers right there. So here I can see that my app is costing me about $3,000 approximately over the period and that there are some resources that are costing me some money here and specifically I see this Movie Guru cluster seems a little bit suspect where I'm spending about $2,000 on it and that's the lion's share of my spend. So that seems interesting. I probably know where I want to go. It seems like a good optimization target. And notice how there is helpful information right underneath all of these resources. So we give you the information about your product, your region, as well as your project name. This helps you disambiguate resources very, very quickly at a glance so you know who to go talk to in your team. Now that we know that this cluster is interesting, you know, we would also want to click on this potentially and get more information which will take us right to the GKE details page. This makes it super easy for you to get the information that you want without having to go digging further. Another thing that our customers ask us for is breakdown of costs by product and resource type. Now this is important information, however, the way that we do this today is fairly crude because it's based on SKUs and API names in billing. What we've done is we've gone a step further and we've reorganized our SKUs mapping to a more logical product and resource taxonomy so that you can get the benefit of even more granular costs and even more explainability. This is super helpful for our customers. I'll give you an example. So before you would see a big line item under Compute Engine that would have a lot of cost under it, but now we're able to break it down even further into GKE clusters, GCE VMs, MIGs, persistent disks, and even individual cloud networking products such as Cloud VPN and load balancing. This level of information is extremely valuable for you to be able to ask the right level of questions about your costs. And of course, if we cannot categorize a cost, we mark it as other at some point so that you know that these costs are accurate and they cover everything and we're not missing anything for you. So just to highlight here, these four resources earlier would have shown up as one line item on your billing report, but now they show up as four different line items where you can actually track down individual costs and know exactly where to go look for your cost optimization opportunities. Moving on to our second signal, which is utilization. This is one of the biggest signals for DevOps users to figure out where there might be some excess fat that they can trim. And now the finance team is looking for you to do this quickly. What you can do now is just head on over to Cloud Hub Optimize and figure out which ones are your underutilized resources. So in this utilization summary, I already see that my total vCPUs for this application allocated are at about 70%, but my average utilization is fairly low. It's about 3%. And that means that I'm looking at this project and I'm in the right direction. This is directionally super helpful. I know that I'm looking in the right place. Looking at it a bit more closely, I see that this cluster is spending $2,000, but it's also only used about 2.8%. I have this information readily available so that I know that I can actually go and debug this further if I need to. Notice also that Cloud Hub Optimize shows it to you for all the products that offer utilization information. This includes Cloud SQL, Cloud Run, GCE, and GKE. So you don't have to go through each of the individual pages for products and resources to figure out what their cost and their utilization might be. It's all right there for you. So Frank, I recall you mentioning that this is a huge time saver and force multiplier for your team. Is that right? Well, first off, thank you for putting me on the spot. And yes, it's all of these pieces being able to be put together and I'm going to go way more in depth about the actual process a little bit later. Sounds good. Thank you. So let's keep going and let's figure out what does it mean to show these costs through applications. We are all familiar with the concept of projects and looking at our costs through projects. But Google Cloud is now giving you a way to organically organize your applications, organize your cloud around your applications instead of the other way around where you have to organize your application around your cloud. What this means is that you can now look at your costs both in a project view as well as in an application view. So for example, in the previous slide that I showed you, we looked at the cluster itself and the cluster was telling me that it's costing me this much and there's 2.8% utilization. But I don't really have a good understanding of what the workloads are on this cluster and whether they're using my resources efficiently or not. So with the app view now, I can actually get that information right on this dashboard, which means that I can see the individual workloads that are running on this cluster and whether they're using all the compute resources effectively or not. So that was Cloud Hub Optimize. Now let's switch gears into Cost Explorer, another surface under the Cloud Assist Optimize umbrella that gives you even more detail into how to debug your cost and utilization data for your projects and applications. So in addition to seeing all the cost trends and the per product and resource type breakdowns, you can now also see all your workloads including their total cost, their cost change, their P95 utilization, and their average utilization. This peak utilization number is one of the most requested features that we hear from our customers about when looking at utilization analysis. We've made it so much easier that you no longer have to figure out which queries to write and how to get all the Prometheus data so that you can actually process it yourself. And the tree map is a great visualization tool for me to be able to see exactly who are the biggest offenders for my cost optimization exercise. So in this particular case, I can see that the VLLM Gemma deployment is one of the biggest rectangles which means that it's costing me a lot and that it's colored lightly which means that it's underutilized. Bingo. This is exactly what I wanted to get at. And I got at this very, very quickly in a matter of minutes. And notice again that we provide deep links to all the workload details so that you don't have to go figure out how to get more information. You can just click on it and figure out what are all the other properties of this workload potentially. So if you like what you've heard so far, you're going to love this slide. This is all the products that we're supporting for granular costs on day one. That is, you don't have to configure anything. This works right out of the box. Well, you may have to configure permissions. But other than that, this should work right out of the box for you and without having to do any other work. And these are all the resource types that we will support granular costs for from day one as well. Again, no heavy lifting required from your part and batteries are included. So we've seen two new surfaces already, Cloud Hub Optimize and Cost Explorer. But we also want to remove that last mile friction and make it even easier for customers, for DevOps customers and platform teams to be able to understand their costs even in more natural language. So we've actually integrated all your cost data into the Gemini Cloud Assist cost helper. And you can ask it questions like how much did I spend on my project from 2022 to 2024? And it will give you an answer in natural language. You can also ask it other questions, for example, like how much did I spend on this last month or what are my most expensive projects? And it will figure out the details for you quickly and accurately. Notice that none of this requires you to write a single line of SQL or under any of our billing data whatsoever. This is all just done for you. One other thing I'll mention is that there is a deep link to the billing console here for you to be able to verify your costs. What this means is that you can be sure that these numbers are in fact accurate and that the LLM didn't actually hallucinate, which is one of the key problems that many of our customers tell us is bad for looking at cost data with LLMs. All right. So with that, I'll transfer it to Frank, who will take us through his journey at MLB and how some of the products that we just discussed are helping his team. Frank? Thank you very much, Gobind. So first off, let's do a little history of FinOps at MLB. The program officially started in 2023. It was originally just an ad hoc duty of mine. Hey, this costs a lot of money or hey, we're running way over budget at the end of the month and all right, let's go figure it out. And it just evolved from there. Excuse me. So the initial step we took is we had contractors who were able to help us jumpstart the program and chase that low-hanging fruit. This was quite successful overall and as some of you might know, how absolutely terrifyingly hard it is to chase down some dev teams to get changes made. So in 2024, the team currently sits with two part-timers, myself included, and a full-time data analyst. And here's some quick bullet points about our fleet in GCP. We have over 200 GKE clusters, over 6,000 applications, 20-plus petabytes of data, and 700 GCP projects. Also, we have a cloud platform team of five. And we're doing this with a cloud platform team of five. So we take shift left very seriously and that's the reason I'm excited to be up here today. So coming back to this absolute monstrosity, while effective, it gets tedious to do relatively basic calculations from raw billing exports. Now think about adding a metric for this. How about breaking it out by application? I think we're starting to see a theme here. So this brings us full circle as to why I'm excited about Cloud Hub and Cost Explorer. What it does is create that single pane of glass inside the platform directly. It helps the developers streamline within the ecosystem, keeping them in the platform they know, not making them go from tool to tool to tool to tool. I was going to keep doing that but I was told I can't. So with this in mind, we have more than 35 different business units focusing on building applications and they go from brand new to the cloud to the folks that I throw alpha and beta products at. So skills vary across the board. App Hub on day one is also fantastic. It allows that central FinOps team to share the burden of defining each individual team's applications and then being able to report on it. For more information about App Hub, please see another presentation. Also, Gobind promised me there would be an API. I did. So why am I excited? I'll share a quote that comes from nowhere in particular. A rising tide lifts all dev teams. So back to cost analysis, I mentioned earlier, how do we do it today? This is my workflow. And this is more of an abridged version of said workflow. So I'm only going to highlight a few of these steps. We're going to reach out to the team, reach back out to the team for clarification, find some metrics, realize those were the wrong metrics and pull different metrics, realize we now need business KPIs and put those in place, and where does it all end up? In a Google sheet, which we are probably all very comfortable with. So before we get to the live demo, which is really neat, super fun, I want to share a real example of Cost Explorer. So as a few of you might be aware, MLB had their opening day a few weeks back. This means going from no baseball to quite literally all the baseball. So some changes need to be made in the infrastructure. This, in particular, is a snapshot of a single project that's fan-facing. From here, we can see the bumps and what resources have the most spend. Click. So on the same page, as Gobind, excuse me, as Gobind has and will show, it's interesting to note that while we had the expected cloud GKE increase, cloud logging also increased at a much higher rate. And first off, that's an ongoing conversation with the development team as of three days ago. And this insight, which would likely be discovered much later, usually around month-end billing reconciliation, is right here, plain as day. I truly am looking forward to the full release and roadmap features of this tooling, and I'm going to pass it back to Gobind to get into the live demo. Thank you, Frank. So with that appreciation, I can actually share another funny story internally. We were preparing for this demo, and we were using a test project that had already been created back in October. So we were just mucking around and creating our resources, going about tracking our costs, and we pulled up this dashboard, and we realized that there were some suspicious resources that are showing up that we had no idea about. So we started looking back in history and realized that somebody had created those resources and just plain forgot about them. So we, as part of the demo prep, dogfooding our own product, realized that we were wasting some resources, and it was that easy for us to find them, even when we were not looking for them. So this is how easy it is to use this product. So with that said, I'll quickly take you through a little bit of a demo and show you how this is easy for you to get started as well. We have a simple GKE app that does some inference for us. It's a movie recommendation. It's nothing to write home about, so brace yourself for some crappy responses from the LLM. But that being said, the important thing here is that we're using a handful of Google Cloud products, including GKE, Cloud SQL, Cloud Storage, and Memory Store. And then we'll run all of this through Cloud of Optimize and some of the other surfaces we talked about to see if there's something we can do to save money. And once we find that opportunity, we'll apply that opportunity to see the results actually take effect and see if we can actually go back to our finance team a happy camper. So before I jump into it, I just want to highlight a quick thing here so that you can get a better sense of what's running under the hood. We have two node pools running in the GKE cluster. On the left is the green node pool, which is running my front end and the web server that's hosting the application. And on the right, I have the red node pool, which is running my GPU machines and running the LLM model for inferencing. So I have this GKE cluster set up with two node pools, and let's take a look at this application to see what it actually does. Can we switch to the demo, please? All right. So this is a basic movie recommendation app. I'll just ask it, can you recommend some action movies? We'll give it a second. And again, the answer is not important. We just want to see if it works. Okay. Well, I didn't recommend anything, but that's okay. I'll recommend some action movies later if you want to really know the answer to that question. But let's actually take a look at this in Cloud of Optimize and see what we can learn about this application. So I've had this app running for about seven days now, so you can see that at the top in the right corner. This is showing you the last seven days' worth of data. And quite immediately, I can see some vital information about this application. I can see that it's costing me... Sorry. It would help if I actually wasn't the right tab. It's costing me about $1,600 over the last seven days. And then I can also see some information about how I'm using my resources. So here in my utilization chart, I can see that I have about 36 vCPU allocated in this application, and that I'm only using about 10% average on this app. So there ought to be something I can do about this. So looking closely, I see that the MovieGuru cluster is costing, you know, about $1,200. It's used about 10%. So that seems like a good candidate for me to actually check it out. But before I look at that in more detail, let's also make sure that we can go to Cost Explorer, which is also linked here, and figure out if there might be other candidates we want to look at. So this is the same application now pulled up in Cost Explorer, and again, I'm looking at seven days of data. In addition to the cost trends, I can also now verify that GKE is about an order of magnitude bigger spend than any of the other products. So this is really comforting for me to know that I'm looking in the right direction, and that I can actually proceed further with my investigation on that GKE cluster. Notice how I'm doing all this in a matter of seconds and minutes, not order of days and months and years as you might have experienced previously. Now, I also see visual confirmation about my infrastructure, where I can see the cluster itself being the lion's share of my spend visually because it's the largest rectangle, and it's also lightly colored, and it tells me that it's underutilized. So I have all the evidence I need, and I've triangulated that this is the resource I want to go investigate. So what do we do about it? Well, we can click right here on the cluster and jump into the cluster details page in GKE. Again, you don't have to find this information. It's there for you. So you go here in the cluster details page, and very quickly you can see what the problem is. The allocatable CPU is at about 32 vCPU. We're only using about a fraction of that. Great. So I have four nodes. Remember the two node pools I mentioned, the green and the red. And I have this allocated 32 vCPU, but I'm only using this little fraction. So what do I do? Notice how my GPU machines here are both only using four and three vCPU each. So I have a bit of headroom here in the 12 vCPU machines. Seems like an ideal right-sized candidate. Right. Makes sense. But these are A2 VMs. What that means is this is the smallest size I can get from these VMs because I need the A100 GPU. I don't really have an alternative. So the other thing I can think about here is that now there might be an opportunity for us to use this number and see if we can merge some of these CPU workloads from the front end to the GPU machines. What that means is we're now more efficiently using our GPUs because the CPUs on these machines are basically idling at this point. We can just merge these workloads over without degrading performance. So I've actually gone ahead and made those changes in an exact replica of this application. And then now we can actually jump into Cloud Hub to see if we notice the differences that we want. So here we are. Same app, different infrastructure, but now what we're looking for is that has our infrastructure actually gone down in size? And we see it immediately. Before, we were looking at the total vCPU of the application at about 36 vCPU, but now we're seeing 28 vCPU. That's exactly a reduction of 8 vCPU, which is the merging of the green node pool into the red node pool. And look at the utilization. It's also gone up. It was about 8% to 9% aggregate before, but now it's 13.4. So these numbers are trending in the right direction, and the cluster itself has gone up from 10% utilization to 15% utilization. So all of this is very reassuring. But my finance team doesn't care about the utilization itself. What they care about is the numbers that we're spending in terms of cost. So when we compare our cost from before and after, we quickly see that over the week, we've already cut our costs down by about $100 by making that change. This is extremely helpful because we've done all of this without any extra frustration on our part, and we now have an answer for our finance team within minutes without having to write any complicated SQL queries. So at this point, I think we can switch back to our slides. All right. So I think we've already covered this, so we'll keep going. Excellent. So now we have the Cloud Assist optimized product as we presented it to you today. It's the fastest way for you to identify waste in your DevOps journey. It covers a few things for you. It gives you cost breakdowns by GCP products and resources right at your fingertips. It gives you cost optimization opportunities based on utilization. It's ready to use with App Hub applications, and it's integrated with Gemini Cloud Assist so that you can actually start asking cost-related questions to the assistant for instance access. What's next? So we've already shown you some cool new surfaces for you to consume costs, but there's more that we want to do. We want to build a richer chat, so we'll be introducing more features around the chat where you can actually leverage the LLM to ask questions about applications, about resources, about utilization and products, all of that under one roof, so that this makes it even easier for you to access all this information. We also want to build multi-project and multi-app support, which means a lot of our customers are interested in managing several hundreds of projects, and they want to see the same insights, but for multiple projects at the same time. So you want to build that support as well. And finally, as Frank already mentioned, I did promise him an API, so we will work on an API, because our DevOps customers love to consume this data from the command line and API format as well. So with that, I would encourage you all to try out the Cloud Assist Optimize product today. We talked about Cloud Hub Optimize, Cost Explorer, as well as Gemini Cloud Assist Chat. There is a private preview that we're launching today, and we'd love for you to try out our product by signing on to the Trusted Tester program, which has the QR code linked here. For pricing, there is no additional cost to access this. In fact, we give you money back for using this product because you'll be able to find more cost savings, so this is actually a better use of your time. And then for permissions, there's really nothing else required beyond billing and your monitoring permissions. So this should be ready to go for you pretty easily. And with that, I'd like to request your feedback on our session so that we can do even better next time. Please take a moment to give us some feedback. I'd also like to thank Frank for being my co-presenter today and taking us behind the scenes on how MLB is benefiting from our products. And thank you very much for taking time out today, and safe travels back home. Thank you.