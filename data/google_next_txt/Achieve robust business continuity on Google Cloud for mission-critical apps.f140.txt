 SUNDAR PARMESTRANHARANI Welcome to our session today. We're going to talk about mission-critical apps in the context of business continuity on Google Cloud. Let's start with a quick round of introductions. Sundar, you want to go first? SUNDAR PARMESTRANHARANI Everyone, good morning. I'm Sundar Parmestran. I'm part of the product team at Google focused on infrastructure resilience and platform reliability. Good morning, everyone. I'm Arun Julluri. I'm with Charles Schwab, and I'm a systems director. I focus on SRE and automation. Good morning, folks. My name is Tim Isaacs. I'm also part of the product team focused on reliability at Google. And one of our speakers couldn't make it today. I believe he was. Yeah, yeah. I think you've seen the market. It's been a little up and down. So some of our folks had to stay back and look after the systems. So yeah, that's more important, right? So yeah, he couldn't be here, but he helped me a lot with preparing this presentation. So thanks, Sutam. Wonderful. All right, so I'm going to be your emcee for the next 15 minutes, and then we will switch to other speakers and then wrap with Q&A at the end. All right, so we're going to basically cover two main things today. Right? The first is let's talk about the building blocks of business continuity. What are the various architectural and operational elements that go into building a robust business continuity strategy? Of course, the focus here is going to be on mission-critical apps. So we are being a little more scoped to one particular class. So we'll talk about that first. Then I'll hand off to our friends at Charles Schwab to talk about their story on Google Cloud. And really, that story highlights how a lot of these principles are brought into action. So this is a real-world example of something that's actually done and working in production. And then, of course, we'll wrap with Q&A. All right, so let's start with building blocks and what is business continuity? I think everyone here is probably reasonably familiar with what that means. And there are really two things that we should be concerned about here. The first is to make sure that you do have a good strategy. You have to have your applications architected a certain way and primarily architected to be able to tolerate a good number of failures. And these failures could be all kinds. There could be infrastructure failures. There could be software failures. Some of those infrastructure failures could be disasters. You also have to look at how you protect your data so that you avoid data corruptions. So there's a slew of the various things that could go wrong. And what you obviously want to do is architect against those failures. Another very important aspect is architecture is only as good as your ability to put it into practice. And so that's where operational practices come in. And there are many considerations here, but if you had to pick a few, there would be things like being able to safely roll out changes. So changes tend to be one of the biggest vectors of incidents and outages for, I'm sure, for all of you and even for our services in Google Cloud. So making sure that you have a robust change management strategy is key. Testing through the lifecycle of your development process, but also doing things like failure testing is super important so that you catch things before they actually are in production. And finally, you'll also have to implement a good incident detection and response strategy. There's a lot that goes into that. But the more you can make this metrics and SLO-driven, the better you have predictability around how you actually catch these issues when they do occur. All right, so let's start with the architecture piece. But, you know, it makes sense to define what we are shooting for here, right? So mission-critical apps are obviously the applications that are your crown jewels. And you really don't want any downtime here, right? So what does little to no downtime means? You can express that in terms of number of nines. We'll pick five nines because that is, if you will, the gold standard when it comes to mission-critical apps. And that means, you know, you don't really have much of an error budget every year, right? It's just in the order of a few minutes. You also will have very tight RPOs and RTOs. So what sort of a deployment modality or archetype should you be picking? Generally speaking, for mission-critical apps, you should be looking at multi-regional architectures or multi-regional archetypes. And that simply means that you're going to be deploying your application across usually two regions, right? So multiple regions, usually two regions. And the most common deployment modes tend to be either you have a primary region where you're serving out your users, and then you have a hot or a warm standby region such that you can failover if something goes wrong with a primary region. And then the second is active-active, where you're serving out users from both regions. And if there's something bad happens, you're simply redirecting traffic to the other region. So let's actually dig into this a little bit more. So starting with the first type, primary region with standby, if I get your attention to the picture on the right, so here you can kind of see you've got two regions here. You've got your application that's spread across multiple zones in each region. So now you have zonal-level redundancies as well. And then, of course, they're spread across multiple zones in the other region as well. So you can obviously survive. With this sort of a model, you can survive zone failures. You can survive region failures. Since they're two different regions, you have to make sure that you're replicating data between them. A few examples of how you would do this, if you're using block storage or persistent disks, you would use async replication between the regions. If you're using Google Cloud Storage, then there's a feature called dual region with turbo replication. So these are just examples of things you could do to make sure you're actually replicating data between your regions. If you look at the cost and complexity dimensions here, they tend to be on the medium to high side. I mean, this is certainly not the simplest of designs, and there is a cost when it comes to building applications with five nines, right? So that's something that you have to account for up front. Some best practices. I think a common mistake that all of us might make here is you obviously want this to work when something bad happens, right? So that means you have to be testing, and then testing, you know, involves actually doing a failover of some kind and doing a failback of some kind. But now how do you know when to failover? So you need full-stack, you know, health monitoring to make sure that you're only failing over when you really need to. So there's quite a bit of stuff that goes in here. Capacity planning is another consideration. Obviously, when you failover, you want to make sure that you do have enough capacity and that the capacity also spins up in time, right, so that your RTO is not impacted. So this is a fairly common one. You actually kind of see this design brought forward from the on-premises world. In the on-premises world, you would have commonly had two data centers in, you know, if you will, a metro, two different cities, and then you would implement a similar design. In the cloud world, you actually get much more because you are getting zonal-level redundancies as well as region-level redundancies. All right, so that was the first type. The second is multi-region active-active. Now, the pictures look very similar because the amount of redundancies you're getting are, you know, indeed quite similar to the previous type, but because you're serving out your users from both regions at the same time, you're really just traffic shaping, it is a slightly more complicated design, right? You have to, you know, maintain two stacks in parallel, so you're obviously syncing up the two stacks and, you know, all of the time, making sure that you don't have a whole lot of a lag between. But it does make failover easier, right? Because now when you have a failure, you're not doing a classical failover to the other region. You're simply redirecting all your users to that other region, right? So it does simplify that aspect. So it all boils down to what are you optimizing for, right? Do you want to take on a slightly higher cost and complexity and do an active-active and then simplify your life when something bad were to happen? Or do you want to maybe save on costs a little bit and do a primary standby? But your cost will tend to be a little higher when you do have to do a failover. Okay. So if you keep this deployment mode at the back of your minds, then there are a few other considerations that you need to keep in mind. So the first one here is service choices. So what do we mean by that? Well, if you look at Google Cloud services, we typically classify them as zonal, regional, multiregional, global. They're of increasing scopes. And as they increase in scope, they're more built-in redundancies. So, for example, regional services automatically deploy across multiple zones. Now, not all services are available in all variants, right? Many a time, some services are only available as multiregional or global or only available as regional. So depending on your architecture, you're going to be picking maybe a mix of these. And then you just need to consider that, hey, look, in some cases, I'm getting built-in resiliency from the service itself. In some cases, I have to self-architect for that resiliency. An example of that is what I just mentioned, right? So persistent disks, if you're using block storage as is, then you'd have to implement async replication between regions. Second consideration, capacity planning. And pretty obvious one, but there are a few nuanced considerations here. So there's capacity planning in steady state. So how do you plan for steady state loads? Then there's also capacity planning for peak loads, right? So obviously, nothing is ever just a straight line. So accounting for that also is important. But more important is, you also account for capacity planning when something bad happens. If you have a failure of some kind, do you have enough capacity to fail over? Be that to another region or be that to another zone. And again, some services might take care of that for you, right? Depending on how they're scoped. But if you're self-architecting for this, then you do have to plan for capacity up front. There are ways you can accomplish this in your application. You should certainly be implementing things like auto-scaling and traffic shaping and methods like that. All right. Last thing is backups. Again, a pretty obvious one. Everybody should be taking backups of your data. But in addition to backups just being a point-in-time ledger of various different copies, it is also your last line of defense. So if all else fails, right, you still can recover from your backups. So that's where backups, even though it might sound like a pretty boring thing, is actually pretty important. Another key thing here is, I think many people take backups and forget about it. They never really test recovery. Testing recovery is super important because, again, you don't want to be testing recovery when you're in a crisis situation. We have some interesting new capabilities here on the backup side from our Google Cloud Backup and DR product, and one of them is this notion of backup vaults where you're able to take an off-site, immutable copy of your data. There's a session that goes and covers this in more detail. I've sort of called it out out here. So if you're, you know, so inclined, I would encourage you to attend that session as well. All right, so that was architecture. And as I mentioned before, architecture is pretty useless unless you also marry it with some good operational practices. And there are many operational considerations, and I'm just going to pick a few, right? So the first is the notion of making sure that you deploy your changes safely. Changes, as I mentioned before, tends to be the vector for most things, usually much smaller-grained things, but they do, you know, can cause outages if you don't deploy them safely. And then, you know, one might think that changes should only be in the actual rollout phase, but really, if you want to implement a good change management strategy, it starts all the way in your development stage, right? How you're coding standards, how you're doing coding, how you're testing various aspects, and then you finally advance to the right and do progressive rollouts. The second is, I should say, more broadly testing, and I've called out failure testing, because, you know, generally speaking, everybody does testing, right, at various different stages. But the one thing that, if I were to encourage folks to do more of it, would be failure testing. So in a pre-production environment, you want to go off and inject failures in a sort of a controlled way to various different components of your application and then to your full application as well and see whether all your design elements are actually working, right? And, you know, I can probably tell you that there'll be many aspects that probably won't work up front, and you'll have to go off and account for that and, you know, tweak your architecture a little bit. Final bit here is your ability to quickly catch and respond to issues. Many a time, the first thing you've got to do is you've got to understand whether this was an issue on your front, on your side, or whether this was on the provider side, right? If it was Google Cloud or any other provider. And then finally, when you actually set up a strategy here, you probably want to think about a set of SLOs, right? Like, how quickly am I going to be responding to an issue? How quickly do I get to mitigation? How quickly do I get to resolution? And having SLOs on that up front will go a long way in making some of this predictable, right? This is an inherently unpredictable, you know, stage of your life cycle, but some SLOs can actually help make them a little bit more predictable. All right, so let me just punch into each of these a little bit, right? So safe changes. Like I said, a good change management strategy starts all the way on the left, right at your development process. You know, it starts with very basic things like high-quality coding standards, the whole notion of a left shift that, you know, you push a lot of your quality practices to the left rather than finding them later in the life cycle. All kinds of testing, of course, right, at various different granularities. So let's just say you do a good job of your development. The next is your release qualification. Obviously, you don't want to roll out anything unless you're super confident that you've done everything you can to make sure it is safe. Very common practices these days is continuous build and test. And then here's where you start to do more on different kinds of testing, right? There's now, obviously, there's the notion of integration testing where you're taking multiple components and are testing all of them together. There's load testing, and this goes back to, you know, how you also do capacity planning, right? You're going to, you know, test with, hopefully, as close to real-world loads as reasonable, and then that'll tell you whether you've actually planned for your capacity, you know, well. And then we talked about failure testing. We'll hit upon that again. And another key piece here is, so there'll be many instances where you actually do a rollout and you find that, oh, no, it was actually a bad change. Something went wrong. You need to be able to roll back quickly, right? So, again, testing that rollback, not just the capability to roll back, but testing that rollback is going to be super important. How did we come up with a lot of these, by the way, is because this is what we do at Google Cloud for our services, right? So I'll talk about that in just a second, but the final stage here is safe rollouts, right? So your rollouts should be progressive, right? If that's the one thing you want to take away, it should be that, which is rollouts should be progressive. You don't ever want to roll out to your entire fleet at the same time. You want to do it progressively. Obviously, there are many different methods and techniques here. You can do, I mean, you start with canary portions of your fleet. You do what they call blue-green deployments. You can do A-B testing to check to see whether a portion of your fleet is okay with this new change. And then along the way, it's super important to have monitoring and supervision so that you know if anything goes wrong, you're catching it, and then you're doing a rollback. I have linked a paper at the bottom. It's a public paper. It's a simple, you could just do a search and say, you know, Google Cloud's approach to change, and everything that we have discussed in these last two minutes on safe changes is covered in a lot of detail. And that's exactly how we do it at Google for all of our services. So if, you know, you're curious about how we do this, that's the paper to read. All right. So talked about safe changes. Now let's talk about failure testing. I'm homing in on this a little bit because this tends to be an ignored portion of the overall testing framework. And here the idea, of course, is to do controlled fault injection to understand how your app behaves when such things could actually happen in the real world. And there are two dimensions here. The first is you're going to go and fail some component of your application, right? So, you know, failing a compute instance, failing a disk, failing a GKE or a Kubernetes cluster, failing a load balancer, right? So, you know, you're trying to break certain portions of your application. So that's one approach that you should take. The second is you also want to test if your entire application can withstand some sort of failures or disasters, right? So now you want to, in that case, what you want to do is group a set of resources, right? Basically, your entire application or your entire project and try to go off and fail that, right? So that would now simulate a more larger scope failure, like a zonal failure or sometimes even a regional failure. I would encourage that these are done in pre-production and when you get really good at it in pre-production, then you can start messing around in production. The production, obviously, is going to be a super high bar, right? So pre-prod is where most folks do this. All right. Last thing here, incident detection and response is super important. It's a large topic. We're not going to do justice to it here, but I do want to call out one thing. So the first thing you obviously need to do is understand whether it's you or it was the provider, right? So that's like the first, you know, branch in your triage. And we do provide two different surfaces for this, right? So there's the personalized surface, what we call PSH. We also have a session that covers this particular area in more detail. And this will tell you if you're impacted by anything, right? So it's a more personalized approach to telling you if, you know, if there's an emerging incident coming along or if something that might impact you. And the dimensions here typically is, hey, it's a service that you're using or it's a region that you're using. So, you know, there's several different things that kind of personalize it to your context. And then we have the more broader cloud-level dashboard called CSH, which now tells you about failures that are more, you know, larger in scope. So that's probably the first thing you would do when something were to go wrong. Hey, is it me? Is it Google? I can look at these surfaces and make a quick assessment and then, you know, go further into your diagnostics. Okay. So if I were to just briefly recap, we talked about architectural pieces that need to be considered and designed for. And we talked about a few operational elements. I'm now going to hand off to our friends at Charles Schwab to actually talk about how they put all of these principles into action. Thank you, Tim. As they say, what happens in Vegas stays in Vegas. But I think you can all agree that the insights and learning from the Google Cloud Next are something that we want to take back home with us. So what is Charles Schwab, right? It's a leading financial services firm. We have businesses spread across different domains and offer different investment solutions like wealth management, securities brokerage, brokerage, banking, asset management, and financial advisory services. We have about 37 million active brokerage accounts and about 5.5 million retirement plan participants. And we manage over $10 trillion in client assets. To put that in perspective, only two countries have larger GDP than that. So today's topic particularly is about business continuity of a mission-critical application called login or client authentication. So login for any application or any company, it's critical. But for Schwab, it is mission-critical. It is a common solution that we developed across that is to be used across businesses of Schwab. it is risk-aware. It provides uninterrupted and seamless experience to clients. And it also is a supporter of the demand and the sustained and search demand of the logins. As you've seen in the last few weeks, the markets have been very volatile and we could only support with the resilient systems that we have. and we could not have done it without help of a lot of teams in Schwab. Like, there are a lot of teams that kind of help us move to the cloud, like the enterprise architecture, security, networking teams. I know a lot of teams kind of helped us migrate. It was not one single team's effort. Why did we move to cloud? So, our primary drivers, apart from the benefits that we typically get in the cloud migration, which is scaling, agility, flexibility, we also had business continuity, growth and scalability, and business enablement as important factors that helped us move to cloud, that made us move to cloud. So, we want to be more resilient and more available and can gracefully handle Black Swan type of events like GameStop event or a meme stock event, right? Like, you can't, it is very hard to predict those kind of events. So, we had to be agile and faster to scale up and scale down to the market needs. And then we also want to take advantage of pay as you go, right? We don't want to over-provision it or under-provision the system. So, we had to do more better at that. And then we also had business enablement factors considering like the on-prem limitations, whatever that we generally have, and then the advantage of using the ephemeral environments like creating and destroying the environments as you need to do any kind of testing, and then also have a single pane of glass of observability across cloud and on-premises. And another important factor that we, you know, considered moving to cloud was the cloud that enables the faster and power the innovation that we want to achieve. So, how did we prepare for it, right? Because preparation is more important than the actual migration. So, yes, we had our preparation very rigorous and extensive. we spent a large amount of time preparing for the migration because there was no room for error. Some of the aspects of cloud were relatively new operational landscape for us, but it was not entirely new. It was relatively new, so we had to take careful steps before moving to the cloud. And some of the important ones that I will be highlighting in the upcoming slides are like multi-layered resiliency, security posture, safe rollout, scalability, and observability, and the incident response. Well, so, the first step of our preparedness was planning, obviously. So, the planning was meticulous and comprehensive from applications to operations. The modernization of the application started a few years ago. It was not a month's effort or so. It was planned well ahead in advance. Like, we know that target state and we started, you know, modernizing the application stack, getting ready for our cloud. So, we analyzed, we did a deep analysis, and then, you know, with proper strategy and planning, we started making moves. Then we ensured some defensive coding practices are in place to ensure the resiliency and fault tolerance in the application or the workload level as well. And then the capacity needs, as Tim was mentioning, it's very important to understand how your system is going to behave when X amount of load is put on it. And you need to plan for various types of capacity like resources, machine types, network allocations, bandwidth, storage, and, you know, what kind of compute you want to use. like you want to go with the GKE or Cloud Run. All these analysis has to be done when you're doing the capacity planning. And then resiliency, of course, is the front seat of, you know, the whole preparation. We had to do an in-depth analysis on various aspects of system software architecture before, you know, implementing the solution. So, the DR strategies, HA, which I'll be talking about in a minute, and then service availability, storage, and data consistency, which is the data replication mechanisms, and security posture, things like that. And operations is definitely not an afterthought. It is something that we had it in mind all the way. So, very good observability and incident response mechanisms and strategies are really important for resilient systems and then to quickly fail back and fail over to any issues. And then controlled and predictable maintenance schedule for not only your applications or your services, but also the cloud and the vendor products is very important. So, that planning was also very helpful. So, the multi-layered resiliency. The resiliency is something that you'll have to think about like each and every layer of the architecture of the system and the application. solution. It's of paramount importance, as you can see. Like, that's a no-brainer. So, our goal in Schwab was to ensure one single cluster can handle, in one region, can handle 100% of the load. So, we did a thorough and in-depth analysis of each and every layer, like I was saying, to understand what are the breaking points, what are the bottlenecks, what is it that we have to understand to make our systems more resilient? So, the deployment architecture or archetype that we followed was more like active hot standby in cloud, and then we also have on-premises presence, which acts as a warm standby. So, we have multi-region in cloud and multi-region in on-premises as well. So, in cloud, we can survive multi-zone, multi-region failures, and also multi-culture cluster-level failures as well. It's a GKE cluster. We use, our workloads are deployed to GKE. And the data replication mechanisms were also very important, the asynchronous replication between the regions and scheduled snapshot within the cloud, and then again, asynchronous replication between cloud and on-premises. All these are important considerations for building the resilient systems. solutions. And then we also have at the application level some resiliency built in the system, right, like the blue-green deployments that we spoke about, our team spoke about, are, like, very crucial to do a faster and quicker rollbacks. And then when it comes to setting up workload resiliency, we also looked at different aspects of availability, like pod disruption budgets, topology spread constraints, and isolation of resources like secrets, et cetera. So, I wanted to put a slide for security as well. Yes, it may not be directly related, but it is very important for Schwab because we are a financial firm. We have a strict regulatory, legal, operational, and security controls that are put in place by various organizations, so we have to, you know, make sure that our security is of paramount importance. And we don't have any disruptions to the clients. So, we designed an in-depth, stringent security strategy for security. So, a few critical elements of that security posture are, like, cluster security where we have private clusters and shielded nodes, GKE nodes, and then enforced strong IAM policies. Network security is an important thing. We have a colleague of ours who is going to be talking about what how to, I mean, the best practice of cross-cloud security tomorrow. And then, image and supply chain security, like GKE binary authorization to ensure that there's no tampering of the images and workload security, data encryption at rest. And transit, we use CMEK for more control of our data keys. And then, incident response and detection also plays a major role in terms of identifying the security vulnerabilities or threats, basically. So, we use Google's chronicle security for identifying those intrusions. And then, compliance and governance is also the same important aspect that we considered. So, safe rollout. So, besides resiliency and the security of the applications, it is equally important that you follow safe rollout practices, like Tim was also alluding to. So, we do have pretty rigorous and robust safe rollout practices, which were carefully crafted and tested, validated multiple times. So, the rollouts are progressive and controlled. And majority of it, as you can see, a lot of resiliency, a lot of thought has been put into the design, and it adds complexity to the system. So, we had to have a lot of automated end-to-end solutions so we can repetitively do the failovers and failbacks and rollbacks, everything under a couple of minutes. So, that was very important for us. And then, game days exercises are one of the important aspects of our rollout strategy. It helped us. Like, it's more than just a DR drill. It's more like getting people together, like all the partners, stakeholders, everyone getting to sit with you, understand how the system is going to behave and how the team is going to behave. So, this helped us validate resilience, identify weaknesses, and then enhance collaboration between the teams, and then also increase the confidence in the team so they know exactly how to react when there's a situation. So, other key considerations apart from these were, like, scaling, observability, and incident response. Scaling, yes, we have the default, you know, the auto-scaling that comes up with the pods, like the pod auto-scaling, HPAs, and then cluster auto-scaling. But besides that, we also have implemented what we call scheduled scaling, which is something that will prime our systems and make them ready for the market to make sure that it can handle the capacity. That's because the nature of our workload is predictable. We know that it's going to start before the market and then it's going to go down after the market, so we used this strategy to scale our systems. And that's a little diagram over there on how we implemented that. And then the observability, we have greatly benefited from the PSH, personalized service health dashboards, which makes us really understand what is the problem and what is the health of various systems and services that we use in GCP. And alerting with GCP monitoring suite, it kind of enhanced our already strong alerting setup or framework, so it really helped achieve our faster MTTD that we have. And then the unified dashboard or OmniView dashboard spanning across cloud and on-premises is also something that we greatly benefit from to understand the overall health of the system. Then we are also exploring AI-based observability, automatic RCAs, and all that. It's in a beta stage. And a colleague of mine, Ram, is going to be talking about AI in observability in the afternoon today. Please try to add in if you can. And then the incident response. Having, you know, well-thought-after SLOs will really be helpful for accurate detection of any issues and also pinpoint to the systems that are causing the problems, right? And then we have set up well-structured detection, investigation, and then recovering, you know, from incidents using GCP operation suite and other in-house tools. So what are the key lessons learned from this whole migration of mission-critical application, right? So the most important thing that we learned is identifying single point of failures. It oftentimes kind of, you know, brings down the whole system if you really don't identify what is the single point of failure and there's no way of recovering. And so we had some bad experience with a particular service, which was a single point of failure, which later something that we kind of enhanced. So that's very good learning. So just make sure there's no shared resources across fault domains. And understand the cloud design and architecture bottlenecks, because the cloud is not, it's a black box for some of us, and it is a black box of certain services. So try to understand exactly what the service has to offer, what machine types it uses underneath, and what CPUs and memories it can offer and things like that so you can design around it. And also apprehend the maintenance windows for each cloud service and then plan for it and try to utilize the exclusion windows, inclusion windows, and everything. When you plan for currency and patching, that's very critical as well. And make sure you have some kind of rollout of a sequence of rolling out those changes. And the repetitive and frequent failover, failback, and chaos testing or the game day exercises are something that we highly recommend. That's something that we really helped over achieving the resiliency that we have today. And then establish channels with the stakeholders, understand how to get them on board when there's a problem and there's an incident, and how to get them on board. And optimization efficiency, it's a continuous process, right? Like, it's not a destination. It's a journey that matters. And what's next? So we are looking at making more resiliency improvements. We are looking at identifying the fault domains and the shared resources, things that need to be taken care of and enhance the data replication techniques that we have, and then probably use multi-cloud for higher resiliency, and then improve app resiliency as well. So there's some design optimizations being considered as well, like we are looking at Google's rollout sequencing for cluster upgrades. And then FinOps, you know, it's very important that we analyze and keep a track of your operational costs. So that's also something that we are looking at. And, of course, I could not have closed the presentation without talking about AI. So AI enabled observability and investigations are something that we are looking at, which will probably, which will make us, you know, achieve our faster MTTR and MTTD. Yeah. That's it. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.