 Hi, I'm Ishan Sharma, Product Manager in the Google Kubernetes Engine team. I'm here to talk to you about Cluster Director for GKE, optimal performance at max scale. The computation needed for training AI models has grown from doubling every year to more than doubling every six months. While early models could be trained in hours on simple hardware, today's models require long computations, even with tens of thousands of special purpose computers. Therefore, today it is more important than ever for your infrastructure to operate together. For large scale, tightly coupled workloads, such as pre-training jobs, the cluster is the computer. All the machines needed to work for your job need to work together as a single unit, with performance preserved at scale. This is because training workloads are highly synchronized jobs that run across thousands and thousands of nodes. A single degraded node has the potential to disrupt an entire job. To handle these challenges, we built Cluster Director for Google Kubernetes Engine. Cluster Director for GKE enhances the capabilities of our AI hypercomputer. Cluster Director for GKE is a very important tool to use. With Cluster Director for GKE, you can deploy and manage large clusters of accelerated VMs with compute, storage, and networking all operating as a single unit. Cluster Director for GKE is a very important tool to use. It delivers exceptionally high performance and resiliency for large distributed workloads. Cluster Director for GKE lets you deploy, scale, and manage AI-optimized GKE clusters, where physically co-located accelerators work together as a single unit with high performance and ultra-low latency. One of the best things about Cluster Director for GKE is that you can orchestrate all of this through the standard Kubernetes API. And of course, the associated ecosystem tooling. There are no new platforms, just new capabilities on the platform that you know and love, Google Kubernetes Engine. So how do these AI-optimized GKE clusters maximize performance for large AI workloads? Well, let's discuss six key value propositions. Number one, high performance. You can run densely co-located accelerators as one unit with dynamic ML network fabric, helping you reduce network latency and optimize for maximum performance. Number two, mega scale. You can confidently run mega scale training jobs with the confidence that GKE supports 65,000 nodes, the largest scale among managed Kubernetes providers. Three, topology-aware scheduling. You can view infrastructure topology and schedule pods based on network topology, which helps you schedule your workloads with the minimum number of networking hops, helping you optimize bandwidth utilization, reduce latency, and thereby improve performance. Four, ease of use. You can easily set up AI-optimized GKE clusters using our easy-to-deploy UI, or using our configurable Terraform-based blueprints in Cluster Toolkit. You can also use the easy-to-use, non-Kubernetes-based UI for XPK. Additionally, you have access to the GKE APIs and Terraform that you already might be using for your current deployments. Five, 360-degree observability. You have out-of-the-box full-stack active infrastructure and workload monitoring to help you identify bottlenecks. Optimize accelerator utilization and monitor your fleet. Six, resiliency. You can run with the confidence that your workloads are running on the most performant accelerators with the ability to report and replace faulty nodes by gracefully evicting workloads from the node and automatically replacing them with spare capacity within your co-located zone. You can also manage host maintenance so that you can manually start maintenance right from the GKE API or use management information and Kubernetes concepts while scheduling your workloads. The best part of it all, all of this is available through the GKE API. To learn more and get started today, visit cloud.google.com slash AI hypercomputer.