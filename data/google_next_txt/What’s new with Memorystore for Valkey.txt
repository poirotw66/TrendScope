 Well, good afternoon, everyone. Welcome to What's New with Memorystore for Valky. We're glad that you could join us today. I know it's the last day, so everyone wants to get back home. So today's session, some of you are new to Memorystore, some of you are new to Valky, some of you are new to Memorystore for Valky. So it's catered or it's created in a way that you're going to be, you know, very happy with it, and you're going to, by the end of it, know what Memstri for Valky is all about, what changes we have been making, and what's exciting in the world of Memstri for Valky and OSS. Quick introduction to the speakers of today. I'm Ankit. I'm the product manager for Memorystore. We'll be having Rahul Joshi joining us. He's from Major League Baseball. MLB is one of our top partners. We've been working very closely to build this product out, and their feedback has been instrumental, so they will tell us, you know, how they use Memorystore in their day-to-day. And then we'll have Ping joining us. Ping is part of the technical steering committee on Valky. He's also an Uber TL for Memorystore. So without further ado, our agenda for the day. We are going to be starting with an introduction into Memorystore. We're going to dive into Valky, and then we'll be handing it over to Major League Baseball to talk about how they use Memorystore. And finally, we'll also show you under the hood as to what powers Valky, Memorystore for Valky, and what makes Valky OSS so much better than its predecessors. So let's dive in. So if you look at the Google Cloud databases, you know, Memorystore has a very special place. You have all these other databases on the side, and Memorystore is towards the left because it's the only in-memory datastore, which means by default we're not persisting things to disk. But don't get us wrong. Like, we don't just lose data. It's not truly volatile. We have a lot of persistence offerings. What it does mean is that Memorystore can actually complement any database and, you know, be used as a cache. In my team, we usually say, you know, everyone can use a cache, and that is kind of the reason why 90% of the top 100 Google Cloud customers actually use Memorystore. So if you're not using Memorystore today, hopefully we can convince you of two things. A, you need a cache, and B, you need to use Memorystore for Valky. But calling Memorystore just a cache would be kind of a disservice, right? You can use Memorystore for a lot of other things. So if you have large data processing needs, if you need a message broker, if you need a PubSub, if you have a leaderboard that is sorting data in real time, if you want to use geospatial apps, you want to do, you know, hey, what are the five nearest restaurants from this place? All these capabilities are built into Memorystore as a product. So it's not just, you know, a key value store where you can just dump data and retrieve it fast. There's a bunch of other stuff that you can do with it. So now that we have a little bit of idea of, you know, what Memorystore is, let's dive a little bit into Valky and what's been happening in the world of caching. So if you were here last year, you know, it was very fresh in everyone's mind that Redis Inc. changed the license on Redis open source. What that meant is that, you know, it moved to a restrictive dual licensing mode and we would not be updating the Redis engine going forward on Google Cloud. Within a matter of days, Valky was formed. And Valky had a lot of contributors from the old Redis OSS project as well as the power of, you know, Google, Amazon, Oracle, Ericsson, and Snap behind it. And within just a few days, they were able to launch Valky 7.2. We launched Valky 8.0 service in public preview in October. Since then, it's been in preview. But as you can see, this looks a little bit incomplete towards the end, right? That's because we GA'd on April 2nd. So those of you who have been in touch with us already know this, but Memstri for Valky 7.2 and 8.0, both services are GA. So you can actually run your production workloads on us. You know, it just comes with the same 4.9 SLA as Memstri for Redis cluster, and you will see why it's better in a little bit. So like I said, it's generally available as of April 2nd, 2025. If you have been on our website, the preview tags are gone, and you can start using it. So what does GA mean for you? It means that going forward, Memstri for Valky is the recommended engine from Google for your caching needs. So we do offer four engines. We have Redis OSS standalone, which is the non-clustered offering. We have Redis OSS cluster, which is the clustered offering from Redis. We have Valky, which is our recommended engine going forward, and we also have Memcached. I want to be clear. We do intend to support all of our customers, but the engine innovation is going to come to Valky, but we are there for you. When you're ready to move to Valky, we'll be there to help you move. Now, the reason why we're recommending Valky, there are a lot of reasons. First is, you know, we want you to be able to migrate with confidence to an OSS solution. So Valky is a drop-in replacement for Redis OSS 7.2, so if you're already using Memcached for Redis cluster, you can just drop in Valky, the clients will be compatible, and you can start using it. But we also want you to be able to channel the power of OSS innovation. The fact that, you know, Google is behind it, Amazon is behind it, other partners are behind it, means that all of us have been contributing our secret sauce back to Valky. So, you know, we had certain reliability improvements with how slot movement happens. We donated it back to Valky OSS. Amazon has been kind enough to move some of their innovation from ElastiCache back to Valky OSS. So it's really the power of all of these big organizations, plus the OSS contributors who have actually moved to Valky that's making this project move forward. And why you should use Memcached for Valky is that it is Google optimized for the enterprise. We will go a little bit deeper into what are some of the features that work just out of the box with Valky GA, but this is like battle-tested technology. We have been running this for years. Like I showed you, more than 90% of the top 100 Google Cloud customers actually use Memorystore. So all that knowledge and expertise and experience we have put into Memorystore for Valky. Valky 8.0, which was, you know, the version where Valky actually started innovating, has moved way ahead of its predecessors. So it offers faster performance. The number that we claim is, you know, 2x the QPS at microsecond latency versus Memorystore for Redis cluster. We encourage you to run your own benchmarks. We have been pleasantly surprised by certain customers who have been able to achieve more. It also has better storage, like optimized storage. So if you are storing a lot of key value pairs, there's some optimizations that actually reduce the memory footprint, and it also has unwavering reliability. Like I said, we made a lot of improvements running Memorystore over the years, and we have actually put them back into reliability. Ping will dive way deeper into some of these enhancements, but I'm just giving you a sneak peek into, you know, what will be coming up next. Now, you should be using Valky, but why should you be using Memorystore for Valky? So we'll dive a little bit into the features that are available on day zero as VGA. So first is it comes with our 4.9 SLA. That's like four minutes of downtime allowed a month. We don't usually even hit that, but that's the financial backing we are giving the service. How we achieve that is we do automatic zonal distribution. So when you deploy a Valky cluster, what we do is we choose the zones for your primaries and your replicas in a way so that if a zone goes down, you still have the capacity and you still have availability for you. And we do automatic and fast failovers. So you don't have to manage any of these failovers yourself. We are the ones who are actually ensuring, okay, this zone is up. This is where your primary should be moved to. Along with this high availability, we also offer you zero downtime scaling. What that means is as your workload grows, you click a button and you get more nodes. If your workload shrinks, we don't force you to keep the larger cluster in there and we let you shrink as well. There are other services out there that won't let you scale in once you have scaled out. But we do this for you. And we also have an OSS autoscaler that has been built by Google Customer Engineers. You can deploy it on the side of your projects and it can monitor your memory store for Valky clusters and see that, okay, you're running high CPU, you're running high on memory footprint, and you can actually right-size your clusters. We also have, like I said, built-in reliability. This multi-availability zones actually do offer you additional protection during the scaling operations. We're also announcing public preview of vertical scaling. Ping is going to go way deeper into it, but Valky has asynchronous I.O. enhancements that actually allow you to take advantage of, you know, extra cores on your beefier machines. So today we do have an eight-VCPU offering, but Memory Store for Redis cluster is not exactly able to take full advantage of those eight-VCPUs. Because the new Memory Store for Valky stack does, we will allow you to actually move to a beefier machine. So if you're running a two-VCPU machine and you say, okay, you know what, I made more VCPUs, you can actually move to an eight-VCPU machine, and Memory Store for Valky will be able to take advantage of those additional cores. Along with that, you know, needless to say, this comes fully integrated with Google Cloud. So you have centralized access control with IAM auth. You can use IAM auth to control who has access, both to the control plane and the data plane. You can use TLS. So all data within Google Cloud is already encrypted, but if you need additional TLS encryption, we offer you that capability. And because it's built for Google Cloud, it already has audit logging built in. So any operations that are happening on the control plane, they're automatically being logged, so you never have to, you know, worry about who did what and when. We also have multi-VPC access in Memory Store for Valky. What this basically means is, if you have a private service connect endpoint, which is our producer endpoint, and you have multiple VPCs or, you know, clients in multiple projects, you can actually connect to the same Memory Store for Valky endpoint. This has been a much-requested feature for even for our previous services. We had just launched it for our previous service, and on day zero, it's already GA and available for Memory Store for Valky. Single zone clusters. So, you know, I talked about high availability. I talked about how much effort we take to ensure that, you know, if something goes down, your cluster is still available, you don't lose your data. But that's not the use case for all customers. Some customers truly, truly, truly need an ephemeral cache, and all they care about is performance and cost benefits. So if you are one of those, we actually offer single zone clusters. What that means is you deploy all of your nodes within the same zone, and you deploy your clients within the same zone as well. Since same zone traffic is free, you're not paying any inter-zone traffic costs, and you also get faster performance since there's no zonal boundary being crossed by your clients. This does lose a nine in terms of SLA because we are not able to ensure that if the zone goes down, there's a backup available in another zone. But in general, this works well for truly ephemeral caching use cases, and sometimes we do have to put in witness nodes in other zones, and that's a cost that we take as GCP, sorry, as GCP, and we don't pass on to you. Cross-region replication, this is another big one. So we today offer a warm standby topology. What that means is there is a primary, and you can have one or more secondaries. The secondaries are always warm standby, which means is they're ready to receive... They're already servicing reads, so they can always take care of reads, but, you know, you put in a command, and they can actually start servicing writes as well. The advantage is, you know, you can always be ready for DR. So if there is a region that actually does go down due to a natural disaster, you have a full copy of your cache. I have talked to many customers. Trust me, this is the fastest DR you're going to see on your services. Most of the services, you will need hours to get back up. In case of a disaster, this would be ready within single-digit minutes, and it will have your cache all warmed up to another region. Another place where people use it is for, you know, lower-latency reads across regions. So a lot of customers have this requirement that, hey, you know, I'm having this data populated in West US. I need this data in East US as well at a very low latency. How do I make it happen? So, you know, we have had a lot of customers use it for that. Another scenario, which we've been hearing a lot of, is migration. So, you know, you want to migrate from one data center to another, or you want to get another scale unit up, you get a warm cache up, and you get that data in the warm cache on day zero. So we've been seeing a lot of creative uses of CRR. We had another customer who is using this for running their testing environment with the same data as in production. So they actually have their testing cluster set up as a replica of their production cluster, and they're actually running against production data, ensuring that, you know, whatever tests they have, they're actually running against real data. So lots of uses for this, but this is, again, you know, available at SGA on Memes. for Valky on day zero. We also offer one-click persistence. Remember I said, you know, we are not truly volatile. So what you can do is you can actually persist data on a disk. We attach a disk next to your nodes, and any time data is written, you can say, hey, I want high guarantees. You can use AF logging. Any write has actually persisted to disk. Or you can say, you know what, I'm okay with a little bit of stale data. Just, you know, create a snapshot every hour or every six hours. We'll do that for you. This usually comes into play in case of a multinodal failure. If you're running HA, in almost all cases, the HA replica node will take over before this needs to kick in. But if there's a multinodal failure, which are rare but can happen, your nodes actually reboot and come back up with data. They don't have to, you know, start from a cold cache. This can save you some, you know, vital time while servicing your customers. Another question that people ask is, hey, what about maintenance? Well, day zero, maintenance windows are available. You can set the windows during which we can touch your clusters. You get notifications seven days in advance that, hey, there's something that's going to happen to your cluster on this date. And, you know, you can even reschedule it. You can say, you know, I told you Sunday 1 a.m. works the best, but it doesn't work anymore. You can reschedule it in the next 14 days. Again, available day zero. So these are just, you know, some of the features that are available day zero. I can talk for like an hour and still won't be done with all the features. But I also wanted to give you a sneak peek into what's coming up in the next few months. So I peek into what's coming up. We are also offering managed backups. So this is pretty much the most comprehensive backup suit you're going to see in any caching service out there. So we allow you to have on-demand backups as well as scheduled backups. These backups are in Google Managed Storage, and you can set a retention policy on them. You can say, hey, keep them up to 365 days. For on-demand backups, you can say, hey, keep them up to 100 years in storage. And basically, Google manages these backups for you in case you need to analyze these offline. You can export them to storage. If you need to recreate a cluster from these backups, you can do that as well. So this is something that's coming pretty soon. Another thing that's coming very soon is ValkyclusterModeDisable. So you must have seen Redis OSS standalone on the left side. And you say, huh, why don't they have Valky standalone? So, well, spoiler alert, we're not going to have Valky standalone as a separate service. It's going to be part of the same service. It's going to be called ValkyclusterModeDisable. So you can deploy a Valkycluster, a single-node cluster, and run it in ClusterModeDisable. And if you do have a client that cannot talk the cluster protocol, it will be able to use ClusterModeDisable. It's better than Redis standalone. It uses PSC technology, which is, you know, you don't have to use the same amount of IPs. And, you know, it has enhanced parity. So we will not support plain text authentication on ValkyclusterModeDisable. That's the security posture we are taking. So you will have to switch to IAM, and it will have the best, better performance of Valky versus Redis in it. And the other thing is, unlike Memescluster for Redis standalone, which has a three-nines SLA, ValkyclusterModeDisable will have a four-nine SLA. So it will be at par with our cluster offering. We'll also be offering customer-managed encryption keys. So by default, when you're persisting data or creating backups, we use a Google key to encrypt this data. With customer-managed encryption keys, you can say that, hey, don't use your key, use my key. It uses envelope encryption, so we encrypt the key using the customer key, so we can't decrypt it without the customer key. In case your key gets leaked, you can actually revoke that key. We'll never be able to decrypt the data. Just a caveat over here, in-memory data is never encrypted because we are a perf service, but this is for data that's in backups and persistence. So, you know, I have maybe 12 more features that are coming up in the next year or so, but I cannot individually check everyone's NDA over here. So if you do want to know more about that, feel free to get in touch with us. This is just a sneak peek into what we've been working on, what we have launched, and what we'll be launching in the next couple of months. And, you know, these features are great, but how do customers use it? For that, we're going to be having Rahul coming up on stage and showing us, you know, how Major League Baseball has been using Memorystore and why it powers the data that fans long for. So Rahul, you want to come up? Can I run up? Hi, everybody. My name is Rahul Joshi. I'm a principal software engineer at MLB, and I work on the Baseball Data platform team. Today, I'm going to be talking about how we use caching and why it's so important for us. The title of this slide, which is Memorystore Fuels MLB, is a true statement because without Memorystore, we probably would not be able to hit our SLAs. So a little bit of quick background here. We have a service called Stats API, which forms the backbone of our delivery platform. This Stats API is built and designed for a five nines availability. And just to give you a scope for the throughput that this service gets, we can get about 9.7 billion requests per day at edge on the service and about 15,000 requests per second at peak time when the games are going on. So really important service, and right off the bat, caching becomes really important. Just want to talk a little bit about what kind of data types are delivered by Stats API. We have live game data, which contains all the full state of the game that is going on, play-by-play, pitch-by-pitch. We have analytics data, so things like ball position, player position data, pitch speeds, that kind of stuff. We also collect skeletal data, which is like the XYZ coordinates of every joint on a player's body. I think there's like 29 joints that we collect and collect it at 30 frames per second. So we're collecting all this data, and then we're generating metrics and stats on the back end, and those metrics and stats also become available via the Stats API. Last but not least, we've got core data, so you can think about configuration data that is available in the database before the game even starts. Think like schedules, players, venues, teams, and that kind of information. So a lot of data being delivered by Stats API. Who's consuming this data? I'm not going to read all the customers over here, but this is just to give you a sense of the fact that we have a lot of customers. A lot of people are depending on us. Fans, baseball clubs, a lot of internal apps, internal services, everything in the MLB ecosystem depends on Stats API, and it really is a very important service, so we need to keep this thing up and running constantly. So, okay, I think enough said. We all get it. Stats API is important. What I'm going to do next is I'm going to talk about three use cases where we use caching, and in each of those use cases, I'll talk about some metrics related to the caching technology that is used in those use cases. Okay, so the first use case I have here is ballpark caching. So in this diagram, you'll see that anything above this dashed line over here are apps that are deployed in the ballpark. They are deployed in a Google distributed cloud Kubernetes cluster, formerly known as Anthos. We have operator apps, and those operator apps send baseball-specific metadata, things like game ID, play ID information. That metadata gets sent to an active MQ broker, and then we have our tracking vendor, which is a Hawkeye system that sends us tracking data into the same active MQ broker. That tracking data arrives at around 300 frames per second for some data types and other data types at a slower throughput. We have ingestion services that basically merge that raw tracking data and the operator data together and pipe that data into our GCP cloud services, which then store the data in our delivery platform for Stats API to deliver. You'll notice over here that we have Valkey, and that's the caching technology that we're using. In this case, Valkey is not really used as a caching technology, but rather used as a memory store. So as Ankit was saying, it's not always caching, but in this case, it's a memory store. We use it to buffer messages, and it's a way to protect our active MQ broker. We also use it as a way to store any bad messages or messages that were failed to be sent to GCP cloud services. So if there was a network connectivity issue during the game, and for like 15 minutes some of the messages didn't make through, we store them in Valkey, and then we can replay those messages into our cloud services. So that's use case number one. Just want to talk about a few metrics regarding that. We're using the Valkey 8.0 engine in this use case. The writable memory, it's backed by PVC, is about 40 gigs. Any given game doesn't get more than 10 gigs to 15 gigs of data. It's total tracking data, inclusive. I have noticed, I just pulled some numbers, and max memory usage on the Valkey instance was not more than 12 GB during a live game. At peak times during the game, it's about 11,000 commands per second, and the P95 end-to-end latency on those commands is about one to two milliseconds. So pretty impressive. And by E2E latency, I mean it is the latency between the Java application that is using Valkey to get the response back from Valkey. So it includes the little bit of network time between the Java application and Valkey itself. So, you know, right off the bat, you know, the performance is awesome. We have no problems with this. Valkey gives us what we need even without breaking a sweat. Okay. The next use case is live gumbo caching. Gumbo, what is gumbo? It's a grand unified master baseball object. So very well thought out acronym here. For live data feed, think of it as a JSON response that summarizes the entire state of the game. So after every pitch and after every play, this gumbo data gets updated. A few things to note about this use case. The gumbo data size can increase to about five megabytes. Basically, the longer the game, the larger the gumbo data size. Stats API in this use case is a multi-region read setup. So we have set up Stats API in each U.S. East and U.S. Central and users access the data through both regions. So both those regions are delivering the gumbo feed. We're only writing the gumbo feed in the eastern region and so the gumbo feed needs to somehow become available in the central region within two seconds is the SLA. It actually becomes available even sooner than that, within one second, but just to be safe, I've done an approximate two seconds over here. So what's actually going on in this use case? On the left here, you'll see that we've got our ballpark injection apps. Those are the same apps that you saw in the previous slide. Those ballpark injection apps post the tracking data into, sorry, not tracking data, but the live gumbo feed into the GCP apps. Those GCP apps write that data into what I'm calling the Redis gumbo. This Redis gumbo happens to be a memory store standalone Redis. Once that data is in Redis, we replicate that data into the Redis in central. This box over here, which is called the OpJob event, is an oversimplification of our entire replication framework. Basically, we're able to replicate that data into central so that it becomes available within a second. Another thing to note, the users are always going to access Stats API via a CDN. So the CDN provides us the first level of protection when it comes to traffic. Once it gets past the CDN, Stats API will call these GCP apps to retrieve the data from the Redis instance and will hand that data back to the user. So why use memory store? So this is one of those use cases where we're not using a memory store cluster. We're just using a memory store Redis standalone instance, but we have an opportunity to use a memory store cluster in this instance because if we use a memory store cluster, we can use the CRR cross-region replication that comes out of the box with memory store and I'm pretty sure that we can just get rid of our object event infrastructure and anything that Google offers in terms of cross-region replication is probably going to be faster than what we can achieve through our own infrastructure. The other obvious things that we would use memory store cluster for in this use case are reliability and scalability. Ankit talked a bunch about how there is zero-time scale-in, scale-out, and high availability for nines SLA for memory store and so that's a no-brainer. Some metrics around this use case. So currently, the Redis that we're using for Gumbo is open source 6.0. Writable memory is about 16 gigs at any point in time. I think it's about 6 gigs of max memory usage. When games are going on, we have about 1,000 commands per second on this and you can see the end-to-end P95 latencies are in milliseconds again. Sometimes, the H set is actually in microseconds, which is super impressive. 1,000 commands per second is not even close to what Memory Store can do for you. So it's a no-brainer to use Memory Store in this case. All right. The last use case I want to talk about is what I'm clubbing everything into called a semi-live data use case. So this includes all the data minus the live Gumbo feed. And this data can be categorized into various types depending on when that data becomes available to users using Stats API. So some of this data will become available within seconds and some of this data will become available within minutes while some data will become available after the game. So tracking data. Think about for anyone who's seen baseball, when a pitcher pitches and batter hits a home run, right, the broadcaster wants to be able to show things like exit velocities, launch angle. Broadcasters can draw these cool little graphs or, you know, trails, hit trails that show how the home run was happening, shows the distance of the home run. So all these cool graphics can be shown by the broadcaster only if that data is available right after the play is over. And that data is generated on the fly within seconds, right? So that's the tracking data that gets used to generate this metrics data. And that data gets vended via Stats API to the broadcasters. Other metrics can get generated within minutes, things like base running and fielding metrics. Some metrics are a little bit more computationally expensive, so they take a little bit more time to generate. And then there are some statistics like pitching stats and hitting stats or leaderboards and standings that will just get generated after the game is over. Besides all that, as usual, we have the core data, things about players, schedules, and rosters. That information also is vended via Stats API. Okay. Okay. So this is a bit of a complicated diagram. So let's just go through this. Again, on the left, common theme, ballpark ingestion apps that you saw in the previous slide. Those ingestion apps are sending tracking data down to the GCS apps in GCP in East. East happens to be our primary region. So in the primary region, we write that data into the Postgres database. This little box over here, which is called create data op job, so when the tracking data comes in, we create these op job events to create more data. So the metrics that we need to generate and the stats that we need to generate essentially asynchronously create more data in the database. Every time data is written to the database, data will be replicated to the Postgres replica in Central, and we'll also generate these data change events, and those data change events will call cache update services. The cache update services will call Stats API endpoints, which will update the memory store cache for those data types. The same cache update services are calling Stats API endpoints in Central and updating the cache in Central memory store. The memory store used in this use case is a clustered memory store. We are actually using two clusters in each region. One is a heavy cluster and one is a regular cluster. The reason we have this is just to reduce the blast radius. We have noticed that when it comes to hashing our cache keys, we are not so good at distributing the keys across all nodes evenly in the cluster. So we were getting a bunch of thrashing and a bunch of contention on the keys and so we decided to separate that out into a heavy cluster and a regular cluster so that things are a little bit more separated and, you know, the blast radius is smaller. Another thing to notice in this diagram is that you have this little, apologize if you can't read this, but what this says over here is read through cache puts. So I just wanted to call out that the pattern used here is a read through cache pattern, meaning when the user requests the data from Stats API, if the data is not present in the cache, then we will get the data from the database, Stats API will update the data in the cache and then return the data to the user. Why is that important? Because it's important to note that a read through cache pattern means that Stats API is not only reading data from memory store, but it's also writing data to memory store in each region. So why use memory store in this use case? So again, a couple of no-brainers, zero downtime maintenance and zero downtime for scaling in and scaling out. This is something that Ankit had mentioned, and since it's a managed service, we don't have to worry about anything when there's maintenance happening or if we have to scale up nodes because our data size is growing or scale down nodes to save money. Things just work out of the box. High availability via replicas. We're talking, you know, we have, I'll show in the next slide that we have 70 shards for each memory store cluster in here, but each shard has one replica, and so we're protected in a way, and we have a high availability because of that. And then this is, I'm really excited about this one, which is the future possibility of active-active multi-region memory store. A little wink and nudge to the Google folks here. And a little shout-out to Bigtable because Bigtable, if you notice, is right in the center over here, and the reason I've put it in there is because it actually is an active-active setup where you can read and write to Bigtable. So if memory store becomes an active-active setup, basically what would happen is we would just get rid of this entire cache update services infrastructure, which seems simple, but it is really bloated, and we would love to get rid of that. So that is, I'm holding out hope that that will happen someday. Okay. Something about configurations now. So as I mentioned, we have two clusters in each region, a regular and a heavy. We have 70 shards for the regular cluster, 10 shards for the heavy cluster, each having one replica for each shard. The size of the cluster, regular cluster, is about 728 gigs, and this is about 100 gigs. So a lot of data, and we have four of these, right? So one in each region. Some metrics. So you'll notice that the regular cluster average memory usage is about 37%, so we have some headroom over here. We always have a couple of nodes that are running at 100% memory on the regular cluster, but this is, again, going back to the point that I was making about us not being able to yet fine-tune our key namespaces. The heavy cluster is about 75% utilization. Cache hit ratio on both clusters is awesome. The commands per second, at peak traffic, we can get about 200,000 commands per second happening on this memory store cluster, and on the heavy cluster about 51,000 commands per second at peak traffic. So that's pretty impressive. So let's talk about the latencies for that kind of throughput. I have two columns here. One is the server execution latency, which is in microseconds. That's super impressive. And then we have the E2E latency. So this is the Java application, meaning Stats API, how long does it take Stats API to do gets, mgets, and sets on the Redis cluster, and we're talking milliseconds over here. Considering that we get a throughput of about 200,000 commands per second, these latencies are pretty impressive. So, I mean, I have no doubt that we've made the right choice over here with memory store. It's working out great for us. And with that said, I'm going to hand it over to Ping, who's going to do a deep dive into the magic behind memory store. Thanks, Rahul. That's really a great discussion there. And also, thanks, Anke, for the great opening there. By the way, my name is Ping. I'm a Uber tech lead on the memory store team. I'm also a founding member of the open source about key projects. I'm currently sitting on the technical steering committee. This is our great opportunity to introduce some of the deep dive into the engine and to share with you the improvements we have done in the Valky 8.0 engine. So with that, let's just quickly review this full compatibility slide. As Anke has already mentioned, Valky 8.0 is a drop-in replacement for the Redis OSS 7.2. So I'm not going to go through all the talking points here, but the key message here is that if you are currently migrating from Redis OSS 7.2 or memory store for Redis clusters, your application can just run as this without any changes. It's just a simple endpoint change and then you are up and go. With that, let's just take a quick look at the performance. Performance has always been the major focus of Valky projects. So in Valky 8.0, so as Anke has already mentioned, with memory store for Valky 8.0, users can achieve up to 2x throughput at microsecond latency compared to our previous offering of memory store for Redis clusters. Now let's take a look at how Valky 8.0 achieved those performance gains. So what's shown here is Valky 7.2's command processing pipeline, which consists of four key stages. By the way, I'm going to be using Valky 7.2 as the baseline for the discussion after this. So as you can see here, the very first stage here is the EPO stage, which returns client connections ready for read. After EPO, the read stage iterates over every active connection and starts reading commands and parsing them into executable commands, which then are being fed into the execution stage. The execution stage will just execute one command at a time, generating the responses. So the right stage, as you can see on the far right hand, is the last stage, which is currently a stage that can benefit from the I.O. threading. So the key limitation in this design, as you can see, is there is a very strict ordering between those stages. A later stage cannot start running until the previous stages are completely finished. The end result here is that the additional CPUs that you have in the system will mostly sit idle. They're not doing much. Now let's take a look at how Valkyrie 8.0 solved this problem. So Valkyrie 8.0 engine still uses a similar four-stage command processing pipeline, but the similarity pretty much stops there. And rather than having this very strict ordering among those stages, now that the main thread immediately schedules I.O. tasks such as read and write, and then dispatch them to the I.O. threads. And then the I.O. threads will just go off and then start processing those I.O.s and then reading the commands, parsing them into executable formats, and eventually mark the completion of this task. In the meanwhile, the main thread will keep executing whatever the commands that have been parsed in the past and also start looking for the new commands ready to be processed, while all other I.O. tasks are still ongoing. As you can see here, this ends up giving us a fully asynchronous and an overlapped command processing pipeline, making sure all CPUs on the system are doing their best job. Another area that where Valkyrie 7.2 is not very efficient with the CPU usage is in how they manage their cache uses. As you can see here, this illustrates the command execution stage on the main thread for Valkyrie 7.2. So in order for every command to be executed, its related keys and values need to be loaded into cache so that the useful computation can be performed. On Valkyrie 7.2, this memory access and loading data into cache is done on the critical computation path such that any memory access stall will lead to a lower QPS overall and then increase the latency. As a contrast, in Valkyrie 8.0, this problem has been addressed through memory prefetching. The way Valkyrie 8.0 does memory prefetching at a very high level is by grouping commands into smaller batches and then for each batch, the engine will analyze the keys and then the values being accessed by that particular command and then issuing those memory prefetching requests ahead of time before any execution can even start executing. So this gives the CPU a very high likelihood of getting the data in the cache when it is needed, completely eliminating or reducing the memory access latency significantly on the main thread that executes these commands. So this is the reason why Valkyrie 8.0 is super efficient and then that gives us the 2x QPS at the microsecond latency. So beyond improving CPU efficiency, Valkyrie 8.0 also brings in improvements in the memory management area, allowing more data to be safe with less memory. And as we probably all know that most common workloads that we have seen today uses key and value pairs that are in the range of 100 bytes. So which means that any memory management overhead will be much more pronounced for workload like that. So in Valkyrie 7.2, we identify two areas for improvements. One is in the cluster mode. So on Valkyrie 7.2, on any Valkyrie 7.2 cluster, when a key is injected into the, is inserted into the database, we need to create additional pointers just to link those keys together to the owning slot, which introduces memory overhead. Another area for improvement is that in Valkyrie 7.2, the key string itself is not saved as part of the metadata structure called dictionary entry. Rather, a separate allocation is performed and then another pointer is introduced to link that key string back to the key entry. And here is a picture that illustrates how this looks like in the Valkyrie 7.2 server. What's shown here is a slot with three different keys, one, two, and three. And then you can see different colors of blocks here representing different types of pointers I just mentioned in the earlier slide. So the end result is that, as we can see, for every key value pair, we are using four pointers in order just to support key enumeration for a given slot or just tie the key string back to the dictionary. So with four pointers, and then we know that on a 64-bit platform, we're talking about 32 bytes, which is quite significant for any small key value pair workload. Rather than storing, using a single main dictionary to store all the keys and then maintaining this per slot doubly linked list on the site, in Valkyrie 8.0, we solved this problem by giving every slot its own dictionary. And then also, we start embedding the key string into the dictionary entry itself. So together, we eliminate those four pointers. Now obviously, every dictionary has its own overhead. However, we made further improvement to make sure that only when the slot has a key, will a dictionary be created for that particular slot. Now as soon as you have three or four keys, the overhead of the dictionary metadata itself is going to break even quickly. Now, we talked about the performance improvement, memory efficiency improvements, but there's only so much it can get out of the single node, right? So most of the time, workloads grow over what a single node can provide. So this is where the challenge comes in that we would like to have a very reliable horizontal scaling process. And then, let me show you how that this is achieved in memory store for Valky. Here's a little bit of quick overhead, like a quick overview of what a cluster, of some of the cluster basics. So a Valky cluster has a fixed number of slots. In order to scaling out in a cluster, we need to move the slots across different nodes. And then this is where that we are going to talk about all the reliability improvements. I'm going to quickly go over this very convoluted process here. The key takeaway here is that the slot migration involves managing multiple states across multiple nodes. And today, in Valky 7.2, those states are not managed in a highly available way. And then we can see there are a bunch of issues that are currently present in Valky 7.2 from not replicating those important states to other replicas, us to not having automatic failover. All these issues have been fixed in our memory store for Redis cluster. And then we are also open sourcing them, bringing them to Valky 8.0, making them available to the entire OSS community. So obviously, there are more improvements in our pipeline, so we are happy just to keep you guys posted when there are more updates from our site. So with that, I think that's pretty much concluded the deep dive for today's session. I know there's still a lot of things that can be covered, but our time is up. I would like to thank you guys for attending this session and also give a big shout-out to Rahul and the MLB team for their great partnership, their feedback, and then the insight is what makes our build this a better product. Thank you. Thank you.