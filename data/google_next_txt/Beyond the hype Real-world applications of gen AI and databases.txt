 Once upon a time, it was easy to build database applications. And today, there's a lot of hype. There's Gen.AI, there's agents, MCP, MCP, MCP. So today, we are thrilled to go beyond the hype and talk about some real-world applications of Gen.AI and databases. My name is Silesh Krishnamurthy. I'm VP Engineering in Databases at Google Cloud. I'm thrilled to share the stage with Amr Sardar, Director of AI at EchoStar, and my good friend and colleague, Yanis Papakonstantino, who's a Distinguished Engineer in Databases at Google Cloud. I want to start with just a quick introduction. Google Cloud offers a highly differentiated data and AI platform from the foundational compute storage network infrastructure to world-class models how it all works with an open ecosystem. Now, we are simplifying complexity for customers with a unified data platform across four pillars, databases, analytics, AI and ML, and BI. And what this helps is it supports customers on their data-to-AI journeys. And, you know, in order to make this happen, we are bringing unique automation and intelligence into the data platforms themselves. What all of this allows is for customers to modernize their data states with agility and flexibility. And if you double-click on the database portfolio itself, you go from in-memory through relational, key-value, document, and analytics. Across the board, we are supporting vector search natively. It's a key enabling technology for Gen.AI applications. But going beyond vector search, natural language to SQL is becoming important. AI models in SQL and AI query engine, all of this is becoming key technology. And so it's not just this functionality inside the database, though. It's how the database can be used to connect to AI models using, you know, our native integrations with Vertex AI, but also the MCP toolbox for databases that we've introduced, which also lets you integrate into the wider ecosystem. Now, which, of course, leads to the question, how should databases and AI actually work together? So, you know, we've all seen, you know, new kinds of user experiences. We've played with Gemini and ChatGPT and all these consumer tools. It lets you do so many different things, you know, generate content, have, you know, conversational experiences. But what we believe is for enterprise Gen.AI applications, there's a gap between what the foundation models provide and, you know, what you have, what enterprise Gen.AI applications need. And your databases, your operational databases, are the key to bridging that gap. In fact, you can think about the evolution of these enterprise Gen.AI applications. A couple of years back, it was all about fairly simple productivity tools. You, you know, perhaps have chat interfaces on corpora of private data. And from that, you've gone to agentic architectures and workflows. And so agents is a complicated term, perhaps overused term, just a way for LLMs to decide what to do. They often retrieve information from various kinds of systems. And a key system that agents will have to work with is your databases. Now, you know, there's a lot of hot takes going on, right? Everyone's vibe coding. SAS is dead. Applications are dead. Everything's an agent. Next step will be vibe manufacturing because, you know, that's the headlines of the day. So with so much going on, it's really useful to try to pick apart what is actually different, what's real, what's not, right? And also an interesting question as well, there's a lot of experimental prototyping happening with greenfield applications. What about your existing estate, your brownfield applications? So I'm thinking about this from the lens of internal and external workloads. So your internal workloads are mainly for your internal knowledge workers. They have, you know, access to a large amount of data, lots and lots of documents, typically, and spreadsheets and other kinds of things. Typically, internal workers have access to a lot of that data, not untrammeled access. And really what you often need is to shrink that data to get the right context to answer the questions you want. In contrast, your external users have a tiny slice of data relevant to their work, and they may need to expand that to get some contextual relevance. Let me give a couple of quick examples. So for instance, in my role, I can find documents about, say, feature requests. These are called technical deal blockers that my customers, many of you have filed, with our account teams. And so suppose I want to ask a question, which is, in a particular region, not just look at these, you know, find all the deal blocker documents for customers who spend at least some amount of money on my product, Cloud SQL, and have more than five support cases. Now, that's much harder, because financial information is not, you know, easily available to everyone. While, you know, I may be able to see financial information about my service, I'm certainly not allowed to see, for instance, cloud storage, it's financial information. So how can you answer these kinds of questions in a secure and flexible way? Similarly, again, real-life example for me, last week I went to my health equity plan. I wanted to look at, you know, my FSA and LPFSA, HSA and LPFSA data. I could see outstanding claims by individual in my family. I could kind of see, could make some projections based on when payments will happen, you know, if a deductible is met. But I was actually trying to answer a question, which was, can I use these funds for some particular expense? And of course, it wasn't easy. The website pointed me to some complicated IRS document, which then I had to interpret to see if, you know, interpret with respect to my data. So if I had only a way to take that slice of my data and expand it with that IRS document to actually answer my question, that would have helped me a lot, right? So if you look at these two trends, what you're seeing is a world that goes from search, information retrieval on the internal side, and expanding to databases, whereas in the external side, you have a world that, you know, goes from databases first, structured data first, and is then expanding, you know, to search. So as it turns out, you know, for both these kinds of workloads, you need to be able to combine your structured and your unstructured data with modern agentic techniques. And so this is the problem at the moment. What we're going to show you today is how Google Cloud's databases will enable your classic applications and your agentic applications to process your structured and your unstructured data using the power of modern AI. And now, to see what this means for databases, Yanis will talk to us about the AI-ready database. All right, folks. So let's go into, we have seen now that applications come in many categories. Some are internally facing, some are externally facing, as I less suggested. Some are brownfield applications that will be modernized with AI, and some are greenfield applications that will pursue new opportunities. And Amar from EchoStar will show you great examples of both kinds. But all of them need a new kind of database, the AI-ready database that simplifies the application stack. What AI-readyness means and how it simplifies the application stack has two branches. The first one is that the AI-ready database brings AI to the data in the sense that the SQL and the associated APIs of the AI-ready database feature powerful AI capabilities so that you do not have to spend your time integrating them at the application layer, and you can rather focus on innovating on your applications. AlloyDB is the spearhead of AI-readyness in Google Cloud databases, and there is three broad AI features that it brings to its alphanumeric data, its unstructured text data, its images and videos. All of them deeply integrate with Vertex AI and third-party LLMs and bedding models and other AI functionalities. They optimize for AlloyDB, and they save you from work that you would have to otherwise do on the application stack. The first feature along this was vector search. Its adoption went seven times bigger in 2024, and we have great customer use cases, like, for example, target.com now uses AlloyDB to improve their online search experience with SQL queries that combine structured data and vector search all in one query and enhance the accuracy of search by 20%. If you want to find out more about this, target is presenting tomorrow. But what is driving, technically, this adoption and these great results? First, the deep integration of vector search into the PostgresQL of AlloyDB guarantees that the search results are always up to date, so you don't have to worry on the application layer about synchronizing data. Second, it's not just vector search. Your SQL can combine filters and joins over the structured data with vector search, thus biggest of use, but also a lot of optimizations, performance optimizations, that you will not be able to easily or at all do at the application layer. If you want to find out more about this, I recommend Friday's AlloyDB AI deep dive session. Finally, it's always good to bring you great Google technologies and in AlloyDB, we brought you the scan index, which is the one behind YouTube, ads, and many other Google properties and has basically 13 years of research behind it. Now, the vector search was really the first step into increasing semantic understanding of queries and first step in semantic search. The new AI query engine currently in experimental preview state enables your SQL queries to unlock deep semantic insights in the enterprise data by operators like AI.if and AI.rank that draw their power from their real-world knowledge and their reasoning capabilities of the latest foundation models that are sitting behind them. The use of them is pretty simple. For example, look at this query that looks for inexpensive, well-known skincare brands in order of review according to the positivity of the review. The AI.if on the fourth line is basically bringing in through the LLM world knowledge on which one is a well-known brand for skincare. The AI.rank at the end is bringing in knowledge about positivity. And we see a lot of cases now, customers asking for re-ranking that basically allows you to combine within the database great quality and great speed by, for example, if you want a top 10 results for something, rather you use vector search to get top 50, then you are using one of our ranking APIs, and I strongly recommend the recently released Vertex AI ranking API that has topped basically all the benchmarks on speed and on quality. to basically get the top 10 among the 50 candidates. So, combined in single query, no application integration, quality, and speed. And you can also do the same stuff now with multimodal embeddings. The third pillar is natural language. But there I'd really like first Amart to show the great cases language that they did in Echo Star with natural language. Good afternoon, everyone. It's nice to be here at Google Cloud Next. So, my name is Amart. I'm a director of AI at Hughes Network Systems, part of Echo Star. Our goal is simple, to create a more connected world. And we do this by creating a lot of cutting-edge solutions that you see here, from 5G to in-flight connectivity to cybersecurity. And that's how we connect people, enterprises, and governments across the world. But today, I'm here to talk about how we use AI. So, this is an overview that we apply AI across various modalities, speech, text, vision, documents, and structured data, using various AI techniques and models, machine learning, deep learning, and generative AI, across various functional areas, like field services, customer services, quality assurance, and also providing the fault management system in the in-flight connectivity. community. And if you see here, these are some of the metrics. We already have 12 applications running in productions, and more than 10 are in development, and we have able to scale fast by creating some frameworks, and we processed billions of tokens already, and we processed around 2.5 million calls, generated a lot of images that we used to train traditional computer vision models, and then off late, we are trying to generate a lot of videos as well. data. And the way we are able to scale is by creating reusable frameworks. So we have created frameworks to analyze data of different modalities, for call analytics, for searching our documentations, creating virtual assistants, and also to analyze our documents, and also for building agent-AI frameworks and observability, and also LLM ops, which is at the center of that, right? And we know that there are a lot of risks associated with generative AI. We know with LLM attacks, prompt injection, copyright issues. So to mitigate all of this risk, we came up with a very strong governance and strategy across different layers, starting with the data privacy and security, and also governance, and we have also incorporated the responsible AI into our applications, and you'll see that in the coming slides, and we also have worked closely with the legal team for legal indemnification and IP protection, right? And here is a snapshot of our observability dashboard. On the top, you can see the various guardrails that we have created, which analyzes the response of the LLMs and the inputs real-time, helping us detect data loss prevention early, and also you can see we are measuring the performance and quality of the responses from the LLM, and also collecting the human feedback at the same time, and also keeping a track of the cost and the number of tokens that it has processed. used. And diving into some of the use cases that we have been building with respect to field services, so we are empowering our technicians with some of the tools, wherein they're using video analysis to do the site surveys, and also to audit the installations. We are using computer vision, using image audit, and also we have provided them with a virtual assistant with which they can ask any questions that they have. In the backend operations, we are helping the operations team with call analytics and summarization of the service order activities and selecting the right ordering template and also analyzing a lot of contracts that we signed with our other partners. And AI has also played a significant role in our quality assurance with respect to understand our customer sentiment, wherein we are analyzing the calls, surveys, and also the cases that the customers created for better retention. We are also looking at the installer notes to help us optimize our truck rolls and collecting the feedback from the customers on the service that we send them out regularly. We are also auditing the sales agent to check if they are following the script that has been given to him so that they don't deviate from it. And in customer service, for the customers, we have provided a self-service chatbot with which they can get answers to a lot of questions related to billing, technical questions that they have, and in the backend, we are providing the agents with a lot of tools with respect to the knowledge based search, automated call summary, so that they don't have to type the case notes after the call is done, and we have seen tremendous benefit on that use case. And also, while the call is going on, we can real-time detect the intent of the call that the customer made, and hook it up using the knowledge basis and the tools that we have, and real-time assist the agents. This is a textbook use case of Agentic AI, and then we are working on that as well. And AI has a significant role in security operations, and we have rolled out an application for the security operations here, which does threat detection, remediation, and then we correlated with intelligence integration, and with which the security team is able to generate a lot of reports, and we have used, again here, Agentic AI. Now, we have seen tremendous benefits already with all these use cases that are running in production, and calculating ROI is one of the key areas that we spend. So the way we do it is by measuring a unit of work pre-Gen AI and post-Gen AI. As you can see, in the customer service, we are almost saving 30,000 hours annually just by summarizing the call, right? Similarly, in the service order activity summarizer for the fields operations team, we're increasing the efficiency by 20%, saved around 5,000 hours. And with the sales call audit, we had manual auditors listening to these calls, spending a lot of hours, and we automated that entire audit using AI, and we saved a lot of time and money there. So what we have seen is there is a role of AI in every single process within the company, and then we are seeing the benefits of that. And Google Cloud is where all these applications have been orchestrated, and LIDB is one of the key components of it. We have a lot of applications of LIDB, but today I would like to focus on these four, which is a call analytics, how we are able to train the computer vision models faster using Vector Search, and also how we are using LIDB as a long-term memory for agentic AI, and also how we are able to generate text to SQL and also text to Waze. But before I jump into the next slide, I would like to play a demo of the call analytics. Every support call contains invaluable hidden insights. Our advanced AI platform uncovers these and enables our field services team to precisely identify root causes of issues and implement targeted corrective actions. Pose your questions using everyday language, and our platform converts them into database queries using NL2 SQL. Meanwhile, Vector Search finds meaningful patterns and connections that would otherwise go unnoticed. Need visualization? Our platform creates comprehensive dashboards and moments, delivering instant, insightful analytics displays precisely tailored to your specific requirements. As you saw in the demo, using the natural language, the final output was a visualization, a dashboard, and we have used LODB NL2 SQL and Vector Search to be able to come with that. So, we are using the call analytics for mainly four things to get advanced call insights into this call. What's happening? What are the issues that are being faced on the field? What are the resolutions? How can we find the root cause? And also, to measure the quality of our agents, right? How well they are performing on the calls. Correct? This is also helping us to enhance our knowledge base and also using it, we are able to train our agents faster, right? So, let's look at the flow. The way it works is we take the raw audio, transcribe it, apply LLM to it, get some key insights which we store in LODB, but a lot of this has a descriptive output. That is where we are using vector search to basically get better insights, right? And so, once the data is in the LODB database, any user can come here and query it, and that's how we are able to gather actionable insights. Now, I'll jump into the second demo which shows how we are using LODB to train our computer machines models faster. Our AI vision analysis is making perfect satellite internet installations a reality. In the field, our installation apps give technicians instant feedback on site quality. See that obstruction blocking a clear view to our satellite? Our AI segmentation models, running directly on the technician's phone, instantly identifies it, providing a clear site survey in real time. But it's not just about a clear line of sight. Our AI also audits the installation quality of the setup itself. This means fewer errors, fewer repeat visits, and more satisfied customers. So, as part of the field services operations, the installer on the field is required to take a lot of images, videos, and for that, we have to train a lot of conventional machine learning models that run on the edge device, right? So, what we have done here is using generative AI, we have created a lot of synthetic images, and also intelligently labeled them. And that is one of the pinpoints in training this computer vision models. A lot of effort goes into creating the training data set by annotating them. And this is one example. If you have to train a computer vision model to detect what a sky versus a street is, somebody has to sit and annotate that image and then label it, which goes as an input to the traditional competition model. But we thought, with the power of vector search capability in LOIDB, if I have this original image that you can see, using the open source SAM library, I can detect all the masks inside it, and then for each of the masks, I can generate embeddings and persist it in the LOIDB. And then I know what is my region of interest, right? Here is the radio. Then I can generate embeddings for it. And then with that, I'm easily able to create a training data set, which will be used to basically train the edge models that run on the device. So with this, we are really able to train conventional AI models faster, and it's saving a lot of time and increasing our productivity. I'll jump now to the last use case that we have, where we have used agentic AI as long-term memory. Our AI-powered operations assistant transforms incident response for SRE teams using agentic AI, eliminating tedious manual troubleshooting. Seamlessly integrated with GCP cloud APIs, databases, and code bases, it leverages real-time context and continuous learning from past incidents. It autonomously detects alerts, examines issues, and pinpoints root causes, delivering actionable insights through an intuitive conversational interface. With our operations assistant, free your engineers to focus on innovation. So a lot of time is spent by the SRE team to diagnose, find out the root cause, do code fixes, and based on that, take some recommended actions. So this is a textbook use case for agent TKI, wherein we can provide the agent all the tools that an SRE team has. And memory plays a key role here, the short-term memory and the long-term memory. And that is where we have decided to use the inbuilt capabilities of LIDB like natural language to SQL, vector search, and the inbuilt LLM. If you look at how the long-term memory is classified, it's classified into three segments. It's the episodic memory that in this case is about the past incidents. Semantic memory is about all the architectures and runbook information, and procedural is about the workflows. So by using the inbuilt capabilities, we are really able to make this agent TKI application more intelligent. And lastly, I would like to conclude with some core principles that have really helped us build this scalable AI applications. is to keep the architecture simple. Most of our applications are using serverless components, and also bring and use reusable frameworks, right? You must have seen in the demos. And then start small and build on top of it. And governance is a key part to it. And plug and play. What I mean here is if LIDB already has these inbuilt capabilities, so we can use them out of the box. So, and thank you for your time. APPLAUSE All right, folks. Thank you, Amar, again for this great showcase of AI and DB applications. So now I want to give you a glimpse on how to set up yourself the AlloyDB natural language interface. And you will see that you can achieve chatbots and related applications like what Amar showed without having to deal hands-on with Gen.AI frameworks. I also want to give you a glimpse of what is going on under the hood of the natural language interface. And this will be useful to those of you who want to innovate further by putting your hands on a Gen.AI agenda framework, use lower level interfaces from AlloyDB and also other Google Cloud databases and connect these lower level interfaces with the agenda framework using our recently released MCP toolbox that Syles will elaborate further five minutes later. So, not surprisingly, AlloyDB AI NL has an under-the-hood agent. The task of the agent is simple. Get the natural language user question, use developer-provided context. You may think of context as broadly info about the database and then come up the agent with a SQL query that answers the question. So, the key questions here is what is the context that you need to provide and what are the methods that the agent uses to turn the natural language question into SQL. SQL. The first method utilizes an LLM to create the SQL. In this case, the context includes metadata, comments about the metadata, sample values, and examples. The highlight of the method is the extreme flexibility. Basically, it can answer any query over the schema that you choose to publish. And often, it is also very accurate. Actually, Amar hit almost 100% for certain classes of questions. And the big reason for accuracy is that providing a rich context raises the accuracy and we help you, the product helps you to automate the production of rich context. Notice that theoretically there is a security danger here by probed injection where a malicious attacker can convince the LLM to use someone else's data. But we solved this problem in 2024 using the so-called parameter iSecureViews and Syles will discuss it on the security part of the talk. Now, the second method, the NL choose SQL, is perfect for the case where you want virtually certified and explainable answers for predictable and critical questions such as show me my current checking account balance. In this case, the context includes, in addition to some values, query templates that is basically pairs of parameterized query and indent, indent, and the reason it achieves virtual certifiability and explainability is because you have blessed the queries and the intents ahead of time. Now, if you want to combine flexibility with certifiability and productivity, we also have the faceted query template, which is basically templates with combinable fragments, and use case to think about there would be something like imagine a retailer's product search page that theoretically has dozens of product properties, far too daunting for a screen-based interface to list all of them. Now, with the faceted search natural language template, you're essentially setting up one fragment per property, and then your application with just a simple search box, it can have the natural language interface synthesize queries that basically may be discussing combinations of these properties. And we're also facilitating the automated production of it. Now, a word of caution, at the end, the end users of your applications will many times ask inherently ambiguous questions, right, that nobody can answer. In these cases, the AlloyDB natural language will ask follow-up questions, and there is some interesting cases of ambiguity that are rooted deep into the data, and the database is the best case for solving them. For example, a question may refer to John Smith, but there may be two John Smiths in the database, or maybe also a John with J-O-N, either because of misspelling or because he's real different person. So, AlloyDB concept types allow you to set up search for particular concepts, persons, addresses, and so on, and the AlloyDB values index facilitates the agent in finding out among the relevant entities and their concepts which one the natural language question is talking about. All right, and so with this, we are finishing the first part of what is an AI-ready database. There is a second part. the AI-ready database integrates with authentic applications. So, you have heard of agent space and AlloyDB is integrating with agent space and thus, now, you can have chats where you get the knowledge of your enterprise data combined with the knowledge of the web through agent space. Let's see a demo for this. So, here, I have a retail database, right, and I have connected my AlloyDB retail database to agent space and I'm going through agent space to this question. Top five accounts that use Google Cloud platform. natural language question, the thing that we discussed is going to turn it into a SQL. The result of the SQL will go to agent space, it will verbalize like that. Now, agent space is asking did any of these appear recently in the news? Notice two things here. When it says this, it brings in the chat context, right? Number two, it's combining the AlloyDB data with the web search that agent space goes out to do to find out news about these companies. And also, agent space allows you to know code with zero code build your own agents that may use AlloyDB as one of the many sources. So, for example, here, we have a database that has S&P commodities data, and we are building the oil and gas analyst agent that, whose task is basically going to be able to answer questions about oil and gas commodities. So, I'm going to skip this, the details, and now go to high code. What if you want to build your own authentic application in your favorite Gen.AI application framework? And Syles will walk you through it. Okay. Thanks, Yanis and Amar. Those are some amazing demos. We saw the power of using an AI-ready database like AlloyDB for building Gen.AI applications. And as these technologies become mainstream, a new application stack is emerging. And so, this new stack has agents and databases, data more generally at its center. I think initially agents will be developed and deployed as standalone prototypes. They will first augment existing applications. Over time, they will absorb, we think they can absorb more and more functionality. But the core enterprise database, right? The core enterprise data is still central to all of this. And in order for all of this to work correctly, if the AI has to go access data directly, context and security are critical, as we've explained. And we think of these databases that can fully unlock the power of agents as this AI-ready database. Now, all of Google Cloud's databases, and indeed those of other databases in the industry, they're all in this journey of becoming AI-ready, but they are at various stages. So the great thing about the emerging application stack is you can use it to leverage all your databases, no matter where they are in this journey. And the key idea is to combine agents and data. So what is an agent? Well, agents are really a software component that are used in an LLM orchestration. They go through multiple phases, thinking, reasoning, and then acting. And acting produces more information that feeds back into this loop. Now, the interesting thing is the acting stage. In the acting, the agent uses a tool to go and take some particular action. And so, while an agent itself is the system that uses the LLM as the engine, it gets agency by talking to these tools. And the actual process of tool usage is that the agent provides the name of the tool and the parameters, and then you go call the tool. So for instance, in this particular case, you have an example. Maybe you're trying to have some kind of customer service use case. You want to return, you want to be able to return, you have a user who's trying to ask if they can return something, and the agent workflow is a little complicated. You want to see, is this user a premier tier member by looking at the loyalty program, and maybe you want to find the list of actual orders from that user, and then you want to go and take some action calling an email gateway to send a return authorization, an RMA. And all of these things, I think the interesting thing is all of these things can be done today, and all these kind of ambiguous workflows you can actually start deploying right now. And there are two kinds of tools very broadly here. Those that package SQL, where you don't necessarily need any very fancy AI-ready databases. You can do them today with your existing databases, where the LLM will provide parameters for a query that was written and pre-approved ahead of time. Maybe something as simple as look up what time a user's flight leaves. And then you can have a more complicated natural language generation, tools that generate SQL, where you need context, and where you may be trying to ask a more open-ended question, right? Now, there are many challenges of deploying these kinds of Gen AI tools. Managing them is complex. You're going from a world where all your application code is in kind of a monolith to a whole bunch of tools. Each of them have a lot of boilerplate. You need a degree in literature to write the descriptions to make these LLMs work well. And so, one of the key things here is security. So, I'll just spend a little bit of time on security. It's not all roses in this land of agents, right? Because it's very hard to fully predict and control the behavior of the agent. So, in this particular example itself we just talked about, you could have the same agent as before, but the user Alice is trying to trick the agent. It's basically saying something like, hey, I want to know what my friend Bob ordered. So, if this kind of tool is written in a fairly naive way, you could imagine a query being generated that lets Alice go see the result of Bob's orders, which would be kind of a bad thing to do. So, we've introduced the idea of the MCP toolbox for databases that makes it much easy to develop secure database tools. Now, in this particular case, the key idea is instead of relying on the AI to make the right decision 100% of the time, we instead move the most critical information, that is the identity of the user, out of the AI's control and instead have it follow a deterministic path. So, in this example, we can use the MCP toolbox for databases to separate the critical parameter, in this case the user ID, and get it from an authentication service that is independent of the AI. Now, it gets more complicated with SQL, that's why we have things like parameterized secure views. Now, agents are disruptive because I think things are changing very fast. Agents can be engaged in many different surfaces of the application, and even the whole concept of what an application is or isn't, I think is going to change. And so, we are barreling headlong into this future of agents talking to databases. You could have a conventional application, the application UI on the top of the backend, all the way to agents augmenting this, and eventually perhaps a case where agents themselves could be replaced. I mean, apps themselves could be replaced by agents. It's hard to tell right now exactly how all of this would work, but it's, I think, very clear that we are living in extremely interesting times where the agent and data stack is likely to disrupt enterprise applications. Thank you so much. We have a bunch of other sessions in this next talking about more of these kinds of technologies. Thank you. It's a great opportunity to people with happy conservation 잡아 experience of mentoring. Employee alles association peux aj ANDERI se vê professionals and women understand lovers tend tu enough that great toért