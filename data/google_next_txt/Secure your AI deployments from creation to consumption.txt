 Thank you for joining our breakout session here on securing your AI deployments. My name is Jürgen. I'm the vice president at Mandiant Consulting here part of Google Cloud. Joining me today is Archner, Senior Director of Product Management, as well as our special guest, Guillaume, Engineering Manager here at Global. We want to take the next 45 minutes to cover three main areas. I'll kick things off with an overview of the threat landscape, specifically as it relates to AI. We'll share some of the insights from Mandiant Consulting's unique perspective gained through our work at the front lines of cyber incidents. Archner will then take us through how Google Cloud is empowering organizations to securely adopt AI, as well as providing us with a brief demo. And for the last part of the agenda, we'll have our special guest, Jürgen May, join us, provide us with his experience around secure AI adoption. Now, before talking about the AI threat landscape, I want to briefly provide an introduction on Mandiant Consulting, explain who we are and how our work and expertise provides us with this unique perspective and view into the threat landscape. Mandiant Consulting, part of Google Cloud, has been providing security consulting services for over 20 years. With about 1,000 consultants across 30 countries or so, we're helping enterprises and governments alike across the globe during and after a cyber breach. Our portfolio of services is divided into four main areas. First and foremost are incident response services. This is what we're probably the most well known for. This is how we help organizations investigate, recover from critical cyber incidents. We answer questions like, have we been breached? What data was potentially taken? Who was the threat actor? But we're also helping answer questions around preparedness. Our strategic readiness services is all about helping organizations understand risk and more importantly, how and where they can reduce risk. Our technical assurance capabilities is about validating and testing the effectiveness of your security controls. This is going to help you understand how well your critical assets are protected and give you insights into how we can further harden your system and environment. And the fourth category is all about building cyber defense, cyber resiliency into your environment. Building a cyber defense capability that helps you detect, respond and contain future incidents, as well as, of course, security services specifically targeted around AI. Now, whenever a new technology is launched, threat actors will try to take advantage of this technology. So it shouldn't come as a surprise that Iran, China, North Korea and Russian Nexus threat actors have been and are using AI for their own nefarious benefits. Through our work at the front lines, we've identified four main use cases that threat actors are leveraging AI for. First and foremost, it's all about social engineering. They're using it for mass production of phishing lures written in proper localized content, using audio and video to make phishing lures even more convincing for their potential victims. But they also use it to help them crafting the tools of the trade they use. They use AI to help generate malicious scripts and malicious code with speed and agility. But AI is also being used as part of the initial research that threat actors are doing. They use it to understand potential targets, infrastructure, hosting providers, identifying potential attack paths into the environment. And once that they're in the environment, they will use AI, of course, to research those post-compromise activities, activities such as how to evade detection, escalation of privileges, internal reconnaissance, etc. Now, in addition to using AI, the attackers are also directly targeting AI. So threat actors have attempted to actually jailbreak foundation chat models and other AI products through the use of publicly available tools. Now, they do that with the objective of having the AI leak potentially sensitive information or course the AI into generating malicious content for them. Now, two main takeaways when it comes to the attackers' use of AI. First and foremost, if you look at what the attackers are using AI for, it's nothing really new. They're using AI for speed, for scale, but they're not coming up with any new techniques so far that we have seen. The second point is AI represents a far bigger advantage for the defenders. And I want to take a look now at how the good guys, how we at Mandiant Consulting, are using AI in our engagements every single day. And there's a very long list of examples. I'm only going to touch upon a few of those. But in investigations, speed is critical. You're in an arms race against an attacker trying to achieve an objective. AI is helping us with speed. It helps us build detection rules, threat hunting rules on the fly. As we find new pieces of evidence, we can use AI to help us build these rules. We also use it in our proactive engagements. If we have a large data set of compromised data, we can find, for example, if credentials are potentially stored in this data set. We can use it to identify target systems. Now, just like the threat actors, we also use it for social engineering attacks when an organization is asking us to see if we can breach their defenses as part of a red teaming engagement or social engineering engagement. We're also using it, of course, for workflow automation, documentation generation. But we've also created a use case where we have an AI assistant that helps our junior consultants be more effective during their initial engagements. It's been trained on our methodology and will work with the consultant during these first engagements. We use it, of course, also when we have to analyze large sections of source code to identify if there's potentially malicious code within those environments. Now, besides, of course, leveraging AI, we're helping organizations with identifying critical vulnerabilities within their AI implementations. We're helping them achieve more secure AI implementations. We have a number of different offerings in that space. And I want to give a couple of examples of some of our recent work here. We recently were hired by a financial institution that had enabled one of their banking applications with an AI chatbot. We realized that by injecting a forged document, we're able to bypass the banking chatbot and we're able to obtain a 200-month 0% loan, which is great for consumers, you might argue, but probably not so good for the financial institution. Application wasn't in production yet, so no harm done there. We also found related to that some applications using the chat history to maintain context. We injected new prompts and manipulated the chat history to trick the bot into believing that it had accepted or agreed to certain propositions that it should have never agreed to. And tied into that, we also saw deviation during longer conversations, policy violations potentially, without any explicit jailbreaking having to take place. Now, in addition to this, of course, we also see the classical web vulnerabilities within those environments, whether they allow remote code execution or direct database exit, you have all of the classical problems, of course, that have to be addressed as well. So what are the lessons learned from the front lines? Here are a few key takeaways. Being proactive and anticipating issues is always an important defense. Validate security controls, remediate them and revalidate them. Assume that your systems will be targeted. Security best practices have helped us for many, many years. Also applying here, of course, prioritize security during the integration. Consider strong authentication, authorization, evaluate how and where you share data. Layered defense is not a new concept, but it also applies very much to the world of secure AI enablement. Think about secure coding guidelines, input sanitization, output validation, middleware, use of leak canaries. Logging and monitoring are critical components. Now, more uniquely around AI is, of course, also the vetting of RAC data sources. You got to make sure you understand what sensitive data potentially could be exposed to an LLM. You have to understand the data flows throughout your systems and applications and make sure you have the necessary controls here to avoid any potential sensitive data exposure. Whenever a new technology comes out, it is obvious that it introduces new risks. This is no different than when e-commerce originally started, mobile payments started, when the web shifted to mobile devices. It brings new risks. For enterprises, it is critical to understand this risk, to build good governance structures around it, and to find ways to mitigate those risks. I want to leave you with a couple of thoughts on how, ultimately, organizations need to think about securing AI. Some of the critical observations we've made through our own engagements here. First and foremost, it's all about visibility. Make sure you have complete pipeline visibility. Map out the entire process from start to finish, models, tools, dependencies. And secondly, build a good governance structure. A lot of organizations forget this critical parse. Establish clear accountability guidelines and risk mitigation strategies. Security needs to start right when you start to model things. You implement security measures through access control, encryption, vulnerability scanning, remediation, and retesting need to be part of your overall model security. And don't just think about security at the application layer, at the AI layer, which is what many organizations do. Think about security across the entire stack. That also means think about how you secure the cloud platform, the containers. Conduct security testing also at the infrastructure level with the same validation, remediation, retesting process. Ultimately, it's all about being proactive. Continuous monitoring, threat intelligence, modeling different threat scenarios, performing red teaming, and continuous validation, of course, are critical components about a proactive security program. Now, as I leave you with these recommendations, I want to now turn it over to Archana to talk to us about how Google Cloud is empowering organizations with secure AI deployment. Thank you. Thank you so much. Thank you so much, Jirgin, for joining us today and talking to us about the latest threat intelligence and findings from Mandiant. I'm Achna Ramamurthy. Thank you so much for joining us today, and good morning to all of you. I'm the Senior Director of Product Management and Cloud Security, and my day-to-day job is to make sure that we're building the right tools and strategies in order to protect AI resources. And we're at a very, very pivotal moment in technology. How many of you have already played with Gen.AI tools or are actively using Gen.AI tools within your environment? Pretty much everyone, right? The FOMO is real. Everyone wants to use Gen.AI. It's helping us transform our businesses. It's actually helping us come up with more innovative strategies and products. And we're also seeing customers getting more and more ingrained and using this for their day-to-day activities within their organizations. But with all of the threats that Jirgin mentioned that are looming, the question to all of you and even to companies like Google is, how are we going to continue protecting all of these AI assets that customers continue to build? And how can we help customers continue with their transformation journey, utilizing capabilities provided by AI? So this is a very recent McKinsey global survey on AI, which actually highlights very similar concerns. You see that security, privacy, and compliance are ranked among the top five biggest risks that are associated with generative AI usage within companies. And this actually highlights the critical need for actually having better solutions to safeguard your AI workloads as you think about your journey in the AI space. So at Google, our mission is to ensure that you as customers are able to use the generative powers of AI in a secure, private, and compliant manner. And this mission has always been at the heart of all of the AI products and technologies that Google comes up with, including the ones that you all saw being showcased at the keynote yesterday and at all the sessions that you probably attended prior to this. How many of you folks actually remember when the first version of chat, GPT, or the first version of BARD came out? Can anyone shout out the year, maybe? Maybe? 2023. 2023. I heard 2023 somewhere. It was actually at the end of 2023 that the first version of chat, GPT, was available. And it was in early 2024 that BARD was actually available. So this actually shows us this technology is very, very new. It's still evolving. It's still changing. And one of the things that Google did was we came up with the SAFE framework when we started evolving the AI technology. Now, SAFE is basically a framework that allows customers to think about the right set of processes and structures to put in place when they're thinking about deploying AI within their environment. I would highly encourage you folks to go take a look at it if you haven't looked at it. But it actually provides mechanisms through which customers can think about how they would design AI applications, how they would actually securely deploy an AI application within their environment, how they would actually think about incident response, for example, how they would think about mitigating threats. So all of these at least gives you the foundational benefits of thinking about security hand-in-hand when you're actually deploying AI, because security cannot be an afterthought. It has to be a foundation when you're building out your AI applications. Now, our approach to securing AI at Google Cloud has actually been deeply rooted in the same SAFE principles that I just talked about. So what we did in 2024 with the rapid development of AI was we basically developed a set of security capabilities and products that you could actually go in and use today to secure your AI applications. So these actually cover all layers of the product stack, as Juergen mentioned, starting from infrastructure, going up to applications, to the data, to the models. So you get that end-to-end protection with each of these capabilities. But that being said, 2025 is very different. Now we're getting to a stage where AI is no longer just in the boardroom conversations. Everyone's deploying AI applications. You're starting to use AI in critical paths within your development environments and also within your production environments for your downstream customers. This means that we need to start thinking about a risk-centric approach to intelligently sort of combine these insights that we get across the stack and provide you with a single pane of glass that allows you to actually understand the risk across your portfolio when it comes to AI. We're very, very excited to announce AI protection. So this is a product that actually brings together all the various tools that you saw on the previous page, providing a risk-centric view and a single pane of glass for customers to utilize in protecting their AI applications. Now, AI protection helps organizations manage risk holistically. And some of the things that customers always talk about when we have conversations with AI is the first one which Juergen mentioned already, which is, look, I do not know where all of the data that's feeding my AI systems are. I don't understand if there are specific models being used within my environment. Sometimes as a security professional, I don't actually have that insight because there are developers in different parts of the organization that are using AI for different use cases. So how do I actually get a full view of the usage of AI within my platform? And that's exactly what the first pillar provides, which is the ability for you to discover the usage of AI within your environment. The next thing that I hear from customers is, how do I actually secure these assets once I know where these assets are? How do I make sure that the right postures, the right protection mechanisms are put in place so that I can protect my AI assets across the lifecycle? So for that, what we do is we actually inject all of this information, this discovery information, into a holistic risk engine that actually provides customers with a view of the top risks that they see within their AI portfolio and that allows you folks to actually put in the right control mechanisms in place to protect your runtime assets. And then lastly, customers have expressed that current solutions actually lack tailored rules and intelligence to detect threats across their AI portfolio. And here we're actually integrating with our SecOps tools, so with our threat intelligence and response engine, and it actually extracts AI-centric detections beyond traditional security signatures, and it actually provides runtime detections for customers. So this allows you to have a very holistic view, which is delivered via security command center. And there's one last thing that I want to point out. A lot of times, AI security is a shared responsibility within organizations. I'm hearing that folks have governing bodies within their organizations. It's a combination of folks that are in the AI space and the security space sitting together to figure out how to actually protect AI resources. So with this in mind, we've also designed AI protection to inherently be seamlessly integrated within the constructs of all of the Google assets so that your journey can start as a developer in protecting your assets, and you can still, as a security professional, have this holistic view. I think I've talked enough. So let's actually go to a demo where we'll showcase the powers of AI protection so you can have a full view of what's happening in this dashboard. The new AI security dashboard within security command center, and this is the single pane of glass from which you can actually secure all your AI assets. Now here you see that there are 14 projects that have AI usage, and there are about 18 instances where foundation models like Lama or Gemini are being used for inference. You also see there are 21 custom models that are being trained on 27 datasets, and out of these 27 datasets, 18 of them are actually sensitive datasets. Now all this rich AI inventory is actually fed into SEC's risk engine. It actually performs virtual red teaming to determine the toxic combinations and the attack paths that can be used to compromise sensitive AI systems, and then what the system does is it actually provides you with a prioritized list of use cases and risk issues that you need to be focused on within your organization. And one of the things that we do with this is ensure that customers have a very clear view of the top risks that you should be prioritizing, and it also gives you a mechanism through which you can pick the highest one. So here you can see that the top risk is a publicly exposed dataset that actually contains tuning data, and this tuning data is being used to actually train certain models. And if we click into it, you can actually get the details of what models are being trained on this dataset, and that gives you an idea of how far or how the model can actually be compromised. And we also use sensitive data protection behind the scenes to say, within that bucket, what kinds of sensitive data are actually present. So you can see to the right of the screen that you have specific sensitive data that's being presented within this bucket, which means this data, when compromised, could alter the way in which the model actually performs. Now you can drill in even more to get details of the specific endpoints and also recommendations of how do you actually fix this issue. So from one place, you not only get the details of the highest risk attacks, you also get a mechanism through which you can actually go in and fix the permissioning and the scoping to ensure that your data is protected the right way. Now we've come back to our main dashboard. And then what we're going to do here is we're seeing that there are about 18 foundational models, but you see that red box there which says four of these models are not protected with any kind of LLM guards. So this means that the prompts that are coming in and the responses that are going out are not being protected by Model Armor, for instance. And Model Armor is the ability for you to have safety and security filters at the prompts and the responses. So now let's pick the first use case. This is a Gemini 2.5 instance that's running. And within this Gemini 2.5 instance, clearly it's not protected by any sort of LLM guard. So here we're saying we allow you to configure Model Armor from within the same interface. And it makes it much simpler because once you click into it, it actually shows you how to configure this. And it protects against malicious URL detection, prompt injection, and jailbreaking. It prevents sensitive data leaks from happening within the organization. So some of the use cases that Juergen mentioned before where a chatbot could be tricked to say and do certain things, you can prevent such kind of attacks by putting an LLM guard in place. And as a CISO, you actually have two options. You can either put it in an inspect mode where all the traffic that's coming in through Vertex, and we have a native Vertex integration, which is the first in the market. It allows you to actually have the inspection run on all of the applications that are running on top of Vertex, whether it's 1P or 3P. And then you can also determine if you want to block some of these violations directly. So you can run it in detect mode or you can run it in detect and prevent mode. So it gives you that option as well. So once you have these datasets enabled and Model Armor turned on for your ecosystem, it actually allows you to have this protection capability across the board. And this part of the dashboard also shows you what happens after you've turned on Model Armor. So it starts showing you all of the sensitive data content and harmful content that was protected in the process of turning on Model Armor for your applications. So this view sort of gives you an idea of how the detection gets done, how you can actually put in some of the protective capabilities. And lastly, we also have a view of all of the AI threats within the organization. Here you can see on the top right-hand side that there was a possible LLM hijacking attempt that was made. And there was also another one, which is a jailbreak attempt that was made. And you can basically click into each of these, and this integrates with our SecOps frameworks and tooling, so it allows you to actually have a very seamless, integrated, end-to-end experience when it comes to discovery, when it comes to protection, and when it comes to detection of AI threats within the ecosystem. So with that, we will move on to the next slide. So now that you've taken a look at how AI protection comes in together, I wanted to double-click on Model Armor. Now, this is a capability that's already available in GA for customers to use, so you can go and try and play with this capability. But as I mentioned before, Model Armor is basically the ability for customers to put in a safeguarding mechanism in place for LLM prompts and responses. So this basically helps with the non-deterministic state of Gen AI, and it allows you to have the filters and capabilities to avoid malicious URLs from being put within the context of a chatbot. It allows you to have control over jailbreak attempts. It allows you to make sure that sensitive data doesn't get input into LLM engines that customers use. This is multi-model, which means you don't just have the ability to use this with Google models. You can also use this with OpenAI models. You can use this with any model in the industry. It also works very closely in integration with any of the clouds that you can work with. So we deliver this as part of an API. So you can use this API as part of an AWS instance where you have an OpenAI model running on top of it, or you could use it as part of Google where we have native integrations with Vertex. We'll have native integrations with GKE, Apigee, and several places, all the way from the networking layer up to the secure web proxy and also as part of an API. Now, the second product, which is also available for you to use already, is SDP Discovery. So this is a very critical capability for AI protection, for you to be able to get a very good view of all of the data within your ecosystem, and then which data actually feeds AI systems. And it also allows you to have a very clear understanding of identifying if sensitive data at any given point of time is actually being used to train an LLM model, for instance. So this capability is also something that you can make use of today. And as I said before, we're announcing the private preview of AI protection, and the QR code on the right is for you to sign up if you want to be part of the early adoption program. So it's something for you to take a look at. But this actually provides you with that experience I talked about and demoed. So it gives you that integrated AIP dashboard. It has model armor integrated as part of it. It has sensitive data protection. It has the AI-related toxic combinations and a high level of all of the risks within your environment. So turning this on in detective mode on day one will give you a very, very good idea of all of the risks that you have with the usage of AI within your environment. So it's something that a lot of our customers have been asking us for, and it's something that's going to be available next month for you to start using. Now, this is just the start, right? As I said, this is an ever-evolving field, which means we expect this field to change quite rapidly. As you saw yesterday, there's a huge momentum on the use of agents, for example. So the next evolution for us beyond this current release is to ensure that we're actually bringing in more capabilities to protect AI resources, whether it is for agentic AI or whether it's for deeper integrations into this Google portfolio or expanding it out to actually provide you with more capabilities around compliance and audits around AI, especially with the changing regulations. So this is an area of active focus for us. So we're always looking for customers that are interested in being part of design partner groups with us and are interested in partnering with us on specific use cases. So feel free to reach out to me after the session if you're interested in being part of any of those. Now, with that, you know, it would be great for us to hear from customers on how they're using some of the products and capabilities. So I'd like to call up on stage Girmay from Global. Thank you so much for joining us today. Thank you. So maybe could you take a moment to tell us about Global and about yourself and your role? Yes. So Global is a media company in Brazil. It's the largest one there. And we produce a lot of content and distribute it in various channels. We have a lot of digital products and TV channels. And we produce a lot of very popular content like telenovelas, sports shows, entertainment, and even a fantasy game that we call Cartola. And we use, we have more than 130 million users in all digital products. And in this year, we have like, bring it to home an Oscar for I'm Still Here movie for international feature. And we have a very large company there. The RTV channel Global is the largest one in Latin America. And in my role there is, I did some big data initiatives where you collect data to understand better our users and improve our products with recommendation, AI, analytics, insights, and everything that's so related to creating platforms and do for that. Thank you so much for joining us. And you know, it was fantastic to see you folks being highlighted at the keynote yesterday. And you know, with media being one of the specific and more important areas where AI is getting used, we all saw the Wizard of Oz, you know, changes that have been made with respect to AI. How are you folks thinking about the kind of applications that you want to build with Gen.AI? Yes. In the keynote, they talk about one of the initiatives that we have to improve our recommendation. And now we are trying to use Gen.TV AI in a lot of initiatives. One of them is creating more conventional interface for our users. And today, we are working on one form of recipes, cooking recipes, assistance to this kind of conventional initiative. But we use a lot of these internal tools, like for advanced analytics, for like a data analyst, using a conventional application, and for creating content, like we're helping create stories and everything in the supply chain of creating a content. Yeah, I think conversational AI is something that a lot of our customers here today are looking at. We've seen several customers, you know, use AI for chatbots, use AI for document creation and generation within their interfaces. But as you're going through this process, what are some of the concerns or challenges that you have encountered in protecting your Gen.AI applications? Yes, this has been a challenge, because today, in the beginning, we're most focused on internal tools. So for internal users, this has this kind of problem, but it's less worried than when you have a user-facing conversational interface. And now that we are starting to move for having more user, for the final user, you have a lot of worries about how this can affect our brand, how this is, how you prevent the conversational, be like a human specialist and not respond to anything that's out of top key or that has some malicious content on that or some leakage of data and everything that is related to responsible AI. And, you know, we've been partnering together to work on AI security with you folks. How are you using and, you know, thinking about products like Model Armor and how have you made the integration and has it made a difference? Yes, it's very useful for our POC because our security teams are not used to work with Gen.AI and understand how harmful things that can happen in this environment. So the partnership and work with Google to understand and better know what responsible AI is and applications how you do this by designing our products was very, very helpful. And today you are using Model Armor to prevent the, to receive any query that can be harmful for our content and that's not for the specific users but for that conversational application. And what are the other tools you're using within your ecosystem for security in general? And, you know, you mentioned a few concerns specifically around, you know, conversational AI but just zooming out a little bit, what are some of your top security priorities and risks when it comes to AI? The responsible AI topic is getting more attention every time because we have to understand from the beginning what are the implications that using this, mainly the conversational application but another AI in the, in the, all the path of the applications. And the, we're trying to do this for how, however, developing team knows how, is the, the best practices and frameworks to use for the security of the, of this, these interfaces today. Do teams often agree and adhere to those processes that you put in place? What are some of the guardrails that you have to make sure that teams are actually indeed following the rules that you set? Today we're done, we are trying to work with the governance teams to enforce, enforce now that the best practices in the guardrails, but we're not using a specific tool like you present before to look all the models, everything else together. That's, that's still in the path how to improve our, our ecosystem. And how are your teams sort of structured? Do you have your security teams separately and your development teams for AI separately? Or how does that work? Yeah. Then we build like multi-disciplinary teams. So the security guys are inside, come inside our teams when we start producing these new products. And then they help us to do this in the beginning by design, how you are, how you implement the security, all the product path. Got it. And then what is next for you when it comes to AI security? What are some of the areas of investment for you? Today we're trying to build in a platform like a multi-agent platform and are looking for many things that we saw here in the conference like AG Space and everything else. And then we're looking how we improve in our ecosystem, this platform to have built-in security in place. So for every project that we build and I think the chief don't have to worry about implement everything and model everything again, architect everything again. And within this product that we build and there is the cooking insights, we use model armor and we did a lot of evaluation how it's working. And it was very good results. We have like 85% of accuracy and have some other issues that we got. It's a lot related about the way the Brazilian Portuguese are speaking differently in every region. So the more colloquial way to say things like poison was not getting by the model because it's not something that is in... When you do a broader question, it's a problem. It's a problem because it's a cooking insight and then use any kind of poison in any query will be a problem. Yes, that's a very interesting aspect that you bring up because a lot of times when we at Google are building products like model armor, we can fine tune the individual LLMs and classifiers behind the scenes that we're using for jailbreak detection or for malicious content detection based on 90% use case of the language. But I think this colloquial usage of language and usage of words that's very different from what you would expect in other countries. So I think you brought up a very good example. And those are specific blacklists that you could possibly have within your ecosystem for individual use cases. Yes, and that's exactly what when we were working with you guys at Google, we managed to organize and have like a deny list of words before we send for a model armor. So that's making the accuracy of the protection very high. No, that's absolutely right. And maybe you can give the audience your point of view, especially given the place where you are in this journey. Any advice do you have for customers who are just starting out this journey or even in the process of integrating Gen.ai into their applications? Yes, I completely agree with what was said before in your presentation that you have to start with security by designing the beginning of your initiative projects because if you build everything, you can put in production without that because it's too easy if you're not using any security measures to protect the queries that you send for the Gen.ai or for the models. It's very easy to go out of talk with them. So it's very important that since the beginning you look for that so we can prevent even the internal tools of using the protections for that kind of stuff because you can try to use for different data leak and everything else even the internal tools. So that's really, that's been my thought about that. Yeah, thank you so much, Guillerme, for joining us. It was a pleasure having you here with us. And with that, we wrap up our session. Dat Talk