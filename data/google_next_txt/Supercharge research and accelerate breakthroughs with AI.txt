 All right, everybody. Thank you so much for joining our engaging session on AI and high-performance computing. I'm Leigh Palmer. I lead technology strategy and delivery for Google Public Sector. Hello! The gallery's over there. Thank you. I really, really appreciate everybody being here at NEXT. Hopefully you heard the keynote and you got really energized by the announcements that we made. And I really appreciate everybody being here at both the conference and our session. I'm just going to spend a few minutes kicking it off in that you will not be at this conference without hearing AI several times and a new era of innovation and where AI is taking us. AI presents unique opportunities for us in economic growth. And it's also enabling governments to get more out of limited resources. Harnessing the power of AI can help governments become safer, stronger, and serve our citizens better. At Google, we believe our government should have the same access to cutting-edge technology and innovations as the commercial sector and not be constrained. We took a different approach to how we deliver technology to our government customers by certifying our commercial cloud, which means that we can generate services from commercial to government faster. Same infrastructure, same compute, same model, same security. We're really excited to talk to you about that this week. And so you'll see multiple sessions around talking about our approach to commercial cloud for government. This should look really familiar for those of you who were at the keynote. This is our full stack. What's unique about Google is we deliver a full stack approach to AI, starting at the infrastructure. The TPU announcement was pretty exciting for those who saw it. To world-class research to include Nobel Prize-winning research. To the models and tooling for developers and the products and platforms like Agentspace that run on top of it. Today, you're not here to hear me speak. You're here to hear our esteemed panelists. And we have a really exciting conversation around what we're doing around AI for the HPC community. And we're deeply invested in it. And I would like to, I'm going to turn it over to my panelists to introduce themselves and have a really interesting conversation. So with that, I'm going to start with you, Bhavana. Why don't you introduce yourself and tell folks all about what you've been doing? I'm Bhavana Tripuluri. I'm the Senior Director for Technical Services and University IT at Stanford University. University IT is one of the special organizations within Stanford community that focuses on providing support to the faculty, alumni, students, and everyone in providing services that they can then leverage. So the central IT organization that kind of promotes and helps Stanford's mission. And Stanford being one of the R1 institutes in the country, it really elevates and supports the Stanford's mission. Mike? Hello. Mike Kennedy. I'm the Deputy Chief Information Officer at the University of California at Riverside. So, see, I have my cheering session over there, right? I don't think my mic's working. Cheer sessions for everybody. All right. Also from a central IT organization at UC Riverside. The areas that I really focus on, of course, research, first and foremost. I mean, that's an important part of our mission at an R1 university like UC Riverside. But there's a lot of other areas that kind of work together to serve that mission that my teams work on. Of course, data, right? So I think one of the things I'm looking forward to talking about today is kind of what we're doing around data. But additionally, lately, my focus has really been as an AI evangelist, generative AI in particular on campus, getting people excited about the potential, both in the research side, but also on the kind of administrative enterprise side as well. And Doug. Yay. Fan. Fan. Fan. Fan. So I'm Doug Downey, senior director of the Semantic Scholar Research Group at the Allen Institute for AI. The Allen Institute for AI is a nonprofit AI research institute of about 300 people, mostly scientists and engineers. Our stated mission is building breakthrough AI to solve the world's biggest problems. We do a lot of work building fully open language models, and I'll talk a little bit about that a day. The particular group that I direct is aimed at natural language processing and human computer interaction research to accelerate science, so building tools to help scientists do their work. All right. Thank you so much, you guys. And we'll just get to it. And I'm going to start with you, Bhavana. When we were prepping for this panel, I heard a lot about the Stanford AI Playground. It's providing researchers and students secure access to advanced AI models. So why don't you talk a little bit about the playground, what it is, why it's important, why you're excited about it for this audience? Absolutely. One of the things that I want to kind of highlight is AI has been in conversations in the forefront of how Stanford has been looking at it for quite some time. It's not something in the last couple of years that we have focused on. If you look back, it's back in 2019 or so. There's an institute for human-centered AI that Stanford invested in and established. It's an interdisciplinary institute focused on identifying where AI research can be focused on and how we can promote AI. And that's one of the outcomes, I would say, that led to providing some of the guidance that we used that was the catalyst that kind of encouraged us to start looking in that direction. Last year, when we had our new president and provost who started at Stanford, they had prioritized as focusing on leadership and data-driven discovery and AI. That was one of our primary top priorities. And within university IT, one of the key strategic values that we focus on is innovate with purpose. And we saw a potential where we can leverage AI in multiple operations and teaching and research that we focus on. And this kind of led the path for us to evaluate and see where all can we leverage AI. There are a number of initiatives within Stanford University that we try to take advantage of within university IT as well. If you can show. Yes. That's a lot. There's a lot going on there. There's a lot going on. And there's a lot more going into it. Stanford AI Playground did play a very critical role in this whole engagement because the purpose of this was to provide an equitable platform for people to start learning about AI, start experimenting with it. But you'll see there are a number of areas where we've tried to leverage based on our understanding of the playground and take advantage of it to expand on it. So that's essentially some of the, yes, some of the petals in the flower that you see are the initiatives that UIT was focused on. One of the key advantages that I found for us, especially on the AI playground, was promoting that education and research initiatives for the faculty, for alumni, for students, and for staff members. And this is where I think Google played a really important role for us. And I don't know how many people know the whole Google search engine originates from Stanford. But really for university IT, being the central IT organization and supporting all the enterprise applications, it kind of laid the foundation relationship that we built with Google where most of our authentication systems are built on GKE. And it just seemed like a natural progression when we thought about building this AI playground, which is essentially a place for us to experiment and try out different LLM models. So we used Vertex AI. We used GKE to build our infrastructure, giving a safe space where people can experiment and try out, compare different LLM models. And the model garden definitely was the advantage that we gained where we get access to not just Google-provided AI models, LLM models, but also third-party models. So one strategic path that helped us build the AI playground. And there are a lot more initiatives that spun off from that. Well, I'm sure we're going to talk about them. And I know that we're doing a lot with the model garden with AI too, but we'll get to that in a minute as well. And hopefully you've been playing with Gemini and the big announcements there. A lot, yes. So I think I went question. So before we get to Doug and what we're doing with model garden and AI too, let's talk Mike. So UC Riverside, pretty innovative as well, doing a lot to break down data silos and enable more access for researchers. So can you talk a little bit about how you did that, what the challenges were, maybe some lessons? Because I think there's probably other people in this audience that also have data silos in their organization. Yeah. So our journey started, I would say, back in 2022. So I think before the rise of kind of this recent conversation around generative AI. But really that provided us an opportunity, right, to help us with our data strategy. To really say, hey, you want generative AI, we need to continue what we're doing already. So some of the challenges that we were seeing on our campus, and again, I don't think it's unique to higher ed, certainly, is a variety of systems. So, for example, our student information system is one thing over here, banner. Another system is Canvas. Another system is our Oracle financial system. Another one is our HR system that's in the cloud. So systems everywhere, a variety of different protocols and technologies to be able to pull that data together. Meanwhile, we have the campus asking questions like, from their perspective, simple questions like, can we find out what the net revenue per student is? Right? So that seems like a simple question, but at the heart of it, that's pulling data from our financial system. That's pulling data from our banner student information system to answer a simple question. And so what would happen is my staff would spend a lot of time trying to answer these one-off questions for any particular person who asked that. And then maybe two weeks later, we get a similar question, but worded a little bit different. Right? So we could really see right off the bat that people had a need to answer questions about data, but in their own way, and things that made sense for them for their mission of their specific department on campus, which really says a lot about self-service data analytics. And so looking at kind of the technology spectrum across the board, we really focused on Looker specifically, because that gave us an opportunity to build those self-service data analytics that really our campus was asking for. And of course, a big challenge, right, that goes along with that is how do you build trust? So we saw across campus that there were many folks who had these shadow data systems. So for example, a spreadsheet sitting on a desktop somewhere. Never happens. Right, never. In a variety of different technologies, people who were starting jobs at the university needed to answer questions that they needed to find. Where's this data? Well, you need to talk to so-and-so who's been here for 20 years, because I think they know what hard drive that's on somewhere to be able to answer that question. Oh, by the way, it's out of date. So, and then the bigger challenge, right, is these same departments, because we're highly distributed, they're reporting these answers to questions that are coming from our university office of the president, from maybe the federal government, from the state government, and they're answering it based on their own view. And so sometimes someone in a department would ask to respond to an inquiry from our office of the president. They would go to our controller and say, hey, this person said this. And we're like, what the? Where did they get that information from? Well, it turns out it's some spreadsheet sitting somewhere. Right? So really this posed a big problem that we wanted to solve with kind of our journey. So, of course, using Looker and then other technologies behind the scenes like BigQuery has really enabled us to accelerate and be able to build trust around the data, be able to give people the ability to answer questions in that self-service way. And again, this is not something that we are done with. I think we've seen right off, and I think one of the lessons that we've learned is this is a continuous improvement. Right? And so we're continuing to build data literacy on how to use these tools in the right way, where to go for the right data, how to, if you have a new data set, where to put that data, how to connect with IT, how to use these tools additionally. And I would say really at the base of that, the biggest lesson learned is the need for a very clear data strategy and data governance process. Ultimately, that is really the backbone of any tool. BigQuery is excellent. Looker is excellent. It really enables us to do a lot, but it's really built on a foundation of good data governance that we brought to the campus. I'm sure people in the audience can resonate with some of the stories that you said there. So we'll explore that a little bit more later. So, Doug. Last but certainly not least, AI2 is doing some really exciting things in the scientific research community. And you've been really focused on democratizing AI with your ULMO models. So can you talk a little bit about that and how our infrastructure is helping you accelerate that AI research and what you've got going on over at AI2? Yeah, absolutely. So this was a recent collaboration with Google Cloud where we trained and released our ULMO 32B large language model. It's our best performing pre-trained model that we've released. And it's the first fully open model that outperforms GPT 3.5 and 4.0 Mini on our suite of benchmarks. When I say fully open, I mean that not only the weights of the model are shared so that people can use it, but also all of the training data and the methods and code needed to train the model are also shared. So those resources are going to allow people to study and build on the work and reproduce it in a way that just open weights doesn't necessarily allow. And I can come back and say a little bit more about that in a second. But I should also call out some of the especially helpful inputs we got from Google Cloud. So in addition to, you know, world-class hardware, this is the best performing and largest pre-trained model. We've trained and released. We trained it on six trillion tokens. We needed super-performant hardware, but we also got help from Google Cloud support engineers to really level up the efficiency of our training. So we weren't leveraging network topology as well as we could. Google gave us some pointers on that. Tooling to detect underperforming nodes and swap them in with better ones is something that came out of that collaboration. And then finally toward the end of the run, Google delivered a new driver that we think is going to give us, like, in future runs a lot more options on batch size. And as I'm aware, these are all tools that if they're not publicly available, they, you know, will be soon. So Google's working on them. And I think that's, you know, an attraction to Google Cloud for anybody who wants to do large-scale, large-scale AI work. And that was definitely an asset for us. I'll say just a little bit more about the, you know, the value of having a fully open model and the AI that it, AI research, it enables folks to do. So we released a tool today called Olmo Trace, which allows you to see what parts of the training data seem most similar to a model generation. And this is something you can only do if you have the training data that a model was trained on. But for a lot of, you know, even open weight models, you don't have this. And so that's one of the values of a fully open model. And just to highlight one question. So there's a really fundamental question. There are a lot of these. But one of them is, you know, to what extent does a model's ability to recall a fact vary with how frequently that fact is explicitly expressed in the training data? And people have studied these, you know, for a handful of models where data is open. But for the really state-of-the-art models, we don't know the answer. And, you know, that could have big implications for sort of what the ceiling of performance is on large language models. Like, to what extent do they actually need the statements they can recall to be explicitly written down by humans in the training data? It's an important question. So that's one of the things that we're hoping this Olmo 32b model that came out of our collaboration with Google Cloud will enable people to look at. Thank you. So now that we've talked about perspective that our panelists have around playground, data silos, open models, and doing research, now I'm going to mix it up a little bit. And we're going to ask the same question of all of you and just kind of riff off each other. And the first I think is a really important point, and you hit on it a little bit, Doug, is trust and transparency in research, right? So how you, it's critical and how do you build that trust and transparency in what you're doing and enable that trust to faster have breakthroughs. So, I don't know, why don't we start with you, Mike? What do you, you know, this is how I keep them on their toes. They don't know who I'm going to start with. We're going to start with Mike, but everybody's going to kind of weigh in on this, on this subject. So. Yeah, I think, you know, for us going back to the, to the kind of the data topic, a big part of that is a strong, is our strong data governance that we put in place. Part of the reason why a lot of our campus is building these, these kind of data silos or these data, shadow, shadow data systems, if you will, is because they don't really know where the data is that they, they have available to them. So they're kind of building their own view of the world. So, as we build these tools built on these strong data governance, I think it's really a lot of conversations. So, what I spent a lot of time doing is meeting, going from department to department, talking about how to use these tools over and over again. I think someone told me once you have to have a conversation five times before someone really. I heard it's 90. 90 times. I think it's way more than five. It's probably a spectrum. There's certainly some on our campus. None of them in here, I don't think. We have to have that 90 conversation. But, but it's just that constant conversation, right? And especially in the higher ed environment, faculty in particular are so key to being part of that conversation. So I spent a lot of time talking to faculty, talking to their grad students, talking to their postdocs, helping them understand how this could help them, where the data's come from, how we're using the data, ultimately in the service of just getting this incremental progress towards trusting these tools and moving people towards this new direction that we want to go. Vanya, you want to weigh in on this subject? And then Doug will have the final word. Yes. So for us, it's more with the lens of AI in mind. So I think Stanford has been lucky with the fact that we had HAI in place. So there's an institute for human-centered AI. Apart from that, there's actually a special committee that was put forth by the provost last year, just primarily focusing on giving guidelines on how best to leverage AI. So they actually, as part of that advisory committee, they were trying to make sure, giving us guardrails on how can we leverage, how the university can take advantage of AI in research, in teaching, in administration and everything. And one of the key things, I mean, most of folks do know and understand that you have to be cognizant of how you're using AI. Data security, privacy, transparency, making sure that you have data controls in place. Yes, the human factor involved is the key thing. But the ultimate, one of the key things that they highlighted was the AI golden rule, which continues to be one of the things that we take advantage of as we start building AI and using AI. And the AI golden rule is along the lines of use and share AI outputs as you would want others to share AI outputs with yourself. Right? So that keeps the lens of making sure that you are cognizant of how best, where you want to use AI, and the focus being how are you supporting people and how the human operations that are involved, right? How are you elevating their needs? So that transparency, I feel like, is kind of incorporated in our operations model and having it, taking advantage of those guidelines in every step of the way increases the faith of people and saying, okay, now I want to engage more. I want to try and take advantage of AI in different facets of our operations. Yeah, I like that golden rule. That's easy to remember. Doug, transparency, trust. So, yeah, you know, the Olmo 32B model and Olmo Trace that I mentioned are, you know, those products are aimed at providing transparency into AI models. So that's, you know, something we're really enthusiastic about. I think what both Mike and Bahavna mentioned were, you know, some of the challenges in trying to get users of AI systems to trust those systems. So, you know, Olmo 32B and Olmo Trace are more about AI researchers, right? But what about somebody who's just consuming, you know, an AI system? So one thing I'd like to share along those lines, we built a QA system, qa.allen.ai, encourage people to try it. This answers complex scientific questions. And one of the design principles in there is, you know, like good answers to scientific questions, we cite literature. And then also when we cite literature, you know, in a retrieval augmented generation kind of setup, we also provide, in cases where we can, when the text is open, excerpts from those papers that we're citing that support the claims in the answers we're making. So then you can check and actually, you know, not too infrequently, disappointingly frequently, the claims that are made in the answer actually aren't, you know, as supported by the text as we would like in, you know, the systems that we build. So this is a feature that some other folks have, so Elicit is a startup, answers scientific questions, has a similar feature. But I think that kind of thing is really important. So to what extent can you tell users what, you know, what answers are based on when you're answering questions? Provide them the raw support data. I think that's a principle that is really helpful. So I'm pretty excited about this next question. Two of you are in a university setting and I'm sure you have a lot to add. So I'm going to start with Doug, which is about the workforce, right? This is new technology. This is, you know, a new work, training a workforce, right? How do you get a workforce prepared for the future with AI? So I'll start with Doug because it'll be hardest. The university, I'm sure, has a lot to say about this. Yeah, so I do want to hear from the university folks on this. I guess what I would say is, like, I have a lot of respect for this problem. I think that, you know, the way people should be using AI and helping people understand, build models of what AI can do, what's appropriate to ask it to do, how you need to work with it, is just a really important and open problem that's going to last for a long time. So, yeah, curious what the other folks have to say. All right. No pressure. Mike. Yeah, so I think I look at this, and I think university looks at it from a lot of different lenses. One of them is just our staff, right? So I'm not talking about faculty researchers, not talking about students. There's so much potential that AI, generative AI in particular, has to improve just the working conditions. The amount of work that smaller staff are being asked to do a lot. AI has a lot of potential to improve that. So honestly, like I said before, it's a lot of conversation. I spent a lot of time basically doing road shows, demoing, taking tools like a Gemini or a Notebook LM or these other amazing tools that Google in particular have developed and showing them how it works for their specific use case. There's a lot of light bulbs I see turn on in the room when it's beyond just maybe kind of the, you know, what's at the keynote to say, look, you take that, let's see how we can solve your challenge in the communications department or your challenge in student affairs or your challenge in the school of business. In some cases, it's just giving people permission to use AI in the first place. We find a lot of people, you know, we drill into them, only use these set of tools because we have the right terms and conditions. We have the right regulatory framework in place. We've vetted these tools. So sometimes there's the opposite effect where people are afraid to use anything, right? And so being able to sit down and explain, no, this is, you can use this. In fact, here's how to use it. People are really excited about that. Additionally, on the faculty side, it's similar in many ways, but we found that a lot of faculty have come into university using AI tools that maybe they've been trained on from their training at another university. And for a faculty member, they just want to hit the easy button. They don't want to spend a lot of time learning something new. But what's been great about something like a vertex AI is that we're able to show how easy it is to incorporate their current workflow just kind of into the workflow that's built in. And that's a really fun conversation to have there as well as we're helping to really improve their, you know, what they do. So again, it's a lot of evangelism. It's a lot of talking through. We have a lot of courses that we put on as well for our faculty and staff to help them learn how to use these tools. We partner a lot with certainly our partners at Google to help put on workshops and town halls and all sorts of things. And again, it's this constant conversation to get people excited. If you're interested, just go out and try it. Let us know what you're doing because we want to know and share that out. 90 times, right? 90 times. All right. Vada, bring us home on the workforce preparation. Yes. So for us, Stanford Playground, AI Playground was built with that very intent, the core functionality of opening it up for students, faculty, staff, everyone. So you get to experiment with AI. It's a safe space where we already have the vendor agreements. We know the data is internal. It is not being used to train the model. So it is something that's accessible for folks who have heard about it but may not have the financial leverage to actually take advantage. So that was the intent of the playground. And we've seen usage. Probably it's been six months or so. And we have 38 billion tokens already used. There's faculty adoption. There are faculty members who are encouraging people to use the students, to use AI Playground in classroom activities. There's a history professor whose students and he went through and scanned through their periodicals and documents to generate summary reports. Staff is leveraging. They get to experiment and they are evaluating how best can I incorporate it into my operations. There's departments that have taken advantage of the playground. So the intent of the playground was to give them a space where they feel like this is where I can experiment without having to second guess whether or not it is safe to do it. It goes through our security office review. We have teaching them on how to do prompts, encouraging them and helping them understand what hallucinations are, what to expect from the output. So it really served the purpose. And I can relate to what Mike was saying. It takes. And that's one of the reasons where AI Playground is built with user feedback. So we hear that there is a new model out there that people are interested. We incorporate that model into the playground. So they get to experiment. We see not a high usage of a specific model. We take it out the playground. So it's a continuous improvement, keeping the conversation going. I should have. I'll go off script for a minute and should have said, I think it takes a lot of conversations when you're my age. I think it takes less. My daughter is a freshman in college. And I think it takes less conversations when you're her age. So I'll just really quickly. She had, she's taking Italian and she had an Italian test coming up. And I was asking her, how's it going studying for Italian? Oh, don't worry, Mom. I put a test. I had AI write a sample test for me. And I've been taking tests that she's using AI to generate to practice for her actual test. So she's adopting it, I think, faster than others. So super exciting. Okay. We're going to switch gears for a little bit and say future. I'm going to ask you guys to put your crystal ball and look into the future and the future of innovation and what's next for your organization. So, you know, what are you most excited about, you know, in the realm of possible? I know this is really hard because things are changing so fast. So you can define future however you want. Three months from now, six months from now, or ten years from now. So I'll start with you, Vivana. Future. I'm going to start with something that just happened a couple of days back. And I'm going to take you. That's past. That's not future. I see a future for that. So I'll start off and then you'll realize the connection. So we released the AI playground and just earlier this week we had AI API gateway released, which is again expanding on you've now experimented with the playground. You now have an idea of the AI models. You can take the API route and start experimenting and incorporating in your applications. So that's again a safe space for our community to start leveraging it. There are a number of AI agents, AI bots that we are building on different websites. It's supporting our different departments like the Office of Sponsored Research to evaluate proposals. So they are going down that path. We see faculty members incorporating it. So there is an outcome of experimenting and then taking advantage of in which operations, which departments can we take this forward. So there is a lot of work that's going on that we see people have started to gain ideas around. So I see this potential, I can probably use it to spin up SQL, write my code, and then shorten the timeframe associated with it. So across the board, this probably six months from now when we meet again or one year later, there will be so much. All right, we'll go straight down the line, Mike. Future. Yeah, there's a lot. We're thinking about the future here. Probably the area that I'll touch on is specifically in research, of course. So one of the vision that we have for our researchers is basically for them to focus on their research, right? And have their concerns about these, bringing up the infrastructure as kind of just behind the scenes for them. Generative AI, I think, provides a really big opportunity in this way. So, specifically in security, right? In trying to meet the regulatory frameworks of some of granting agencies, for example. There's a lot of opportunities in the past that we missed out on because we didn't have infrastructure in the right place, and we didn't have the right controls in place. Generative AI will allow us to create an environment where a faculty member can describe what they're trying to accomplish, their research. With the advent of things like machine actionable data management plans, being able to bring that in as well. And then behind the scenes, have a tool like a Gemini generate the Terraform scripts and other things like that, that would build out the infrastructure and make it available almost immediately so the faculty can get to work and really reduce that time to science to be able to actually submit a proposal, feel confident that it will be accepted. There's concerns about meeting requirements of like CMC, CMMC, FedRAMP, other things that are really coming very fast to our faculty researchers. That's where I think there's a lot of power in kind of where I'm looking for our future. Additionally, just where we can help out, and I'm looking forward to Doug's answer on some of these, because building tools to help with just the whole research lifecycle, being able to look for grants, look, build out a proposal, fill out all of these silly little forms that the NSF, for example, requires around your current and pending support, and all these other things that I think a faculty spent a lot of time on that I really think we have this opportunity to improve that and just get them doing the real work that's important. All right, Doug, future. Yeah, and so a lot of what Mike just said really resonated with me about trying to use AI to help scientists focus on just their work and remove a lot of extraneous stuff. If anybody's gone after federal funding and has had to fill out a collaborators document, that one in particular, I feel like I really want AI to handle. It's like a giant list of, you know, names of people you may have written a paper with in the last five years. But I think that, so one really exciting area of the future, in my opinion, that is of this flavor that sort of like helps scientists do their work, removes some of the like monitoring tasks that scientists have to do, scanning for work that's related to what they do. So, you know, especially in AI research, new papers are appearing on the archive all the time. Some of them are relevant to what you're doing. Some of them give you a new tool you could use. A lot of them are irrelevant. And scientists spend a lot of time just kind of monitoring this data as it comes in. So I think it would be wonderful to have AI agents that could take over that monitoring task for us. So they know kind of pragmatically what scientific innovations are going to have an impact on our projects, right? They know about the projects you're doing and they know about what's going to help or maybe hinder your progress and what you need to be aware of that's going on out there in the world. And then you can just depend on that agent to alert you to new things that are out there. And you don't have to scan it. You know, there's an interesting insight from a paper. Eric Horvitz is the senior author. I apologize. I don't remember the first author. But it makes this analogy to aviation and kind of plays with the co-pilot metaphor. It makes this point that human beings actually really don't like monitoring tasks. And this is part of why, like, say, air traffic control can be a difficult job because there's a lot of monitoring in that. But so imagine if you could have an agent that takes over that, you know, this tedious task of, like, making sure you've seen all the recent work and they can just surface that to you when you need it because they know about the results that are relevant to your work. That's a tool I think it's going to actually take a lot of development to build something like that. But I do think it's on the horizon and it's something I'm really excited about. I can think of a lot of things I'd like agents to do for me. I mean, my driver's license, my passport. There's a lot of stuff. All right. So the audience can't see this annoying countdown timer that we have. We're almost out of time, but I promised the panelists a lightning round. So this will be our Jeopardy! lightning round. Final piece of advice or lesson learned for the audience. And we'll go in reverse order starting with you, Doug. Final piece of advice. This is, so, yeah, based on the 20 years I've been in the field of AI, I think we've got a lot of work to do. I know it sort of feels like the pace of progress is incredibly fast and, like, maybe all the problems are going to go away really quickly. I think that happens at points in the history of AI. I've never seen progress this strong, though. But I do, I think, you know, don't confuse the pace of progress toward getting 80% of the problem right with how hard it's going to be to get the last, you know, 10 or 20%. And that was maybe a little more than lightning, but I'm done. That's all right. Mike, lightning. Lightning round. I would say engage broadly. Have lots of conversations. That certainly in a higher ed space where shared governance is so important, you really have to bring everyone along. Now, obviously, there's going to be people toward the latter end of that adoption curve, and that's okay. Focus on the ones that are really excited. Have them help you tell the story. Demonstrate kind of the stuff that they're doing, the cool stuff, right? Get people excited in that manner. Bring us home. Three things. Three things. Innovate with purpose. Human-centered AI. And don't forget the AI golden rule. I like the golden rule. I like the golden rule. That's a really great way to end it. Thank you very much. I have a couple public service announcements, and I'm going to thank my panelists here. Please visit the Google Public Sector Hub on the show floor. We have demos of all of this technology in action. There is a spotlight session with our CEO of Google Public Sector and the CEO of AI2. At 5 o'clock today, I'm sure it's in your program. I don't have the location here, but 5 o'clock today, we would love to see you all there. And please help me thank our panelists for this really engaging conversation. Thank you so much, you guys. Thank you so much, you guys. Thank you. Thank you.