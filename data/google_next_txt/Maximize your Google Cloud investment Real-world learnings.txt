 . All right, let's get started. I didn't trip coming on stage, so off to a good start so far. I'm Chase Platoon. I'm a senior staff technical program manager at Shopify, leading e-commerce platform and empowering our merchants. Marissa Lewis, who you all scared off, my Google Cloud account lead is sitting right here. So it's just me today. And today we are going to talk about Shopify's use case of two things, community use discounts and the Google Cloud pricing calculator. A quick question before we get into it. Raise your hand if you have been using community use discounts. Raise your hand if you just started hearing about it this week. Okay? So about half of you all are somewhat experienced, so maybe even more experienced. So we're not here to say that we're an expert, but we are here to share some of our experiences, our practices, and maybe you might learn something. We'll probably definitely learn something from you all as well. All right. So no presentation is good without the obligatory quote. I'm a big car guy. And as this is Google, we had to do some F1 quotes here from Andrea Stella. I couldn't find a good one on efficiency or competitiveness with Ferrari. So we had to pick McLaren. Sorry. But really, just to improve our competitiveness, we must focus on efficiency in our process. Every detail matters in delivering the best performance. I couldn't pull off a good Italian accent, so that's the best I can do. But really, cloud spending, as most of you all know, it's one of the largest expenses your company is probably facing, probably second to headcount. So efficiency optimization is so important, at least to get the best ROI out of your cloud investments. So let's first talk about community use discounts, cuds. So Google's community use discounts, they offer significant savings on cloud resources. You have community use discounts, which are resource-based. You also have spend-based. Resource-based, especially for GCE, you're committing to a number of cores, RAM, for a certain machine type, family, and a specific region. Spend-based, which is also applied to GCE as well as other services like Dataflow, Bigtable, you're committing to a spend value, usually it's dollar per hour. Shopify uses both. What do cuds offer? They offer some predictability, right? You're committing to a certain amount of resources, you know you're going to be at least at that amount. I'm jumping around here. Flexible commitments, you can choose from various different commitment levels. There's no upfront fees like AWS. If you're familiar with AWS resources, or sorry, commitments, there's partial upfront, there's all upfront. Google, there's no upfront. So you're paying for the commitment, but no upfront costs. Long-term alignment, this is also important, especially if you're trying to have a better understanding of what your forecast is. So you're committing to resources, you know you'll be using those resources over a given period. Cuds are usually between one and three years. So it adds more predictability to your environment. All right. This is a preview to the second part of our discussion here. But I wanted to show and highlight, I have a laser pointer. Look at that. The cost difference between different, between on demand one year and three year. And this is really truly to highlight the cost impact that cuds can have, especially on GCE. Now, Shopify, we scale up and down. We host a lot of merchants. And I think at one point we scale up to 2.4 million cores at one point. Especially during BFCM, it's very high for us, right? So to keep that cost down, to optimize our resources, we rely heavily on committed use discounts. You can see here, you have on demand versus a three year cut. Now again, this is the Google Cloud pricing calculator. This is list price, by the way. So don't worry, I'm not sharing you any specific discounts that Shopify gets. This is what everybody gets before any, whatever commitment you're making. But at least this will show you the impact. But one year, three year, three years, obviously, the least expensive. It's a longer commitment. One year, a little more flexible, but definitely, you're looking at some significant savings around 30% almost. We'll come back to the Google price calculator here in a bit. So this is how we use cuds. We use GCE, cuds specifically, majority of our infrastructure. We use one to three year. And it usually depends upon the workload. Our more stable workloads, workloads we know aren't be fluctuating, we put those on three years. If we have a team that is experimenting, they're not really sure of their forecast, they're not really sure of a, they're testing out a new region, we might be extra conservative and start out with a one year committed use discount. We use batch purchasing. So we will purchase in small batches. We don't try to keep large purchases all at once. And we try to maintain that. We do some laddering purchasing purchases. So what that means is we will have some significant, sorry, some initial purchase of a community use discount. And we will actually continue to layer on that as we see sustained growth. We do, if you're new to cuds, most likely you have a lot of on demand. So most likely you will require a larger upfront purchase to cover that on demand cost. In which case then you can get into the habit of layering on after that. I actually just learned yesterday, speaking with Prosperops, who I think they're sitting right there, they had a report earlier this week. They actually, from the customers that they interviewed, 60% only use community discounts and the 40% actually don't use them at all. That was mind blowing. Amount of wasted, or not wasted, but amount of savings that are on the table. And I was going to mention about flex cuds as well. Flex cuds are offered also by Google. They are not dictated by a machine family or region. So they are a little more flexible. The discount is slightly lower, and that's why Shopify personally, we stick with the resource-based cuds. They are bounded by region. They are bounded by machine family. So less flexible, but higher discount. And that's, oh, did I go back? No, we're still on the right track. Also calling out spend-based cuds. I mentioned this earlier. Spend-based cuds we use on services like Bigtable, Dataflow. Again, you're not purchasing or not committing to a resource, so you're not committing to, you're committing to a dollar amount, dollar per hour. Same thing, one year versus three year. But we've started using that as well, and that's proven also very helpful for us as well, reducing our on-demand cost. Just know it doesn't apply to all the skews for Dataflow or Bigtable, only some. So be mindful of that. Happy to talk with you offline. But there are some nuances with the spend-based cuds. So some best practices I want to call out here. I probably should have mentioned. So one thing that I should have mentioned from the very beginning. I lead our cloud FinOps team at Shopify in conjunction with finance engineering representatives. Together we come together to make sure we help manage the use of our community use discounts, the purchases. And that's one thing I wanted to call out is process. Committed use discounts are great, but it has to be centralized. And I can argue with this all day long, but if you have team members who are purchasing cuds without your knowledge, you will eventually run into the case of unused committee use discounts, especially if you're not being shared globally across your platform. We also make sure that we're purchasing our committee use discounts centrally so that they could be shared across the entire platform, not by project, but by organization. That allows you, so let's say a team decides to scale down, right? Other teams can then benefit from those committee use discounts. If you are purchasing at a project level, if that project does scale down, you are still paying for those committee use discounts for the remainder of their term. So they might think they're winning by optimizing and reducing their cost. Great. You're still paying for those committee use discounts for the rest of that term. So the more you can do to centralize those committee use discounts in your organization, the more likely you will have less wasted cuds. And that's the enabled cut sharing. One thing I also want to call out, that's the second topic there, stateful versus optimized workloads. I want to really emphasize this, because this is a common misconception about committee use discounts. A lot of people think that committee use discounts will enable immediate savings. Yes, they will, but you have to be having your workload optimized first. And this is kind of going back to if your workload is not optimized and you're applying a bunch of committee use discounts, let's say you're covering 100% of that. Two months down the road, they realize, oh, we can reduce our, we're only using like 30% of our actual CPU utilization. Uh-oh, you're trying to scale down. Now you're left with a bunch of unused cuds. Cuds should only be used only when your workload is already optimized. And I can't stress that enough. Your team will then argue like, hey, we have a optimization opportunity here where we could scale down by 50%. To them it's a win, but then you're looking at it still as a loss. You covered that entire footprint with committee use discounts and you're still going to be paying for it once they've scaled down for the remainder of that term. So again, can't emphasize enough, if you walk away today here with anything. Cuds should only be applied to optimized workloads. Let's see here. There we go. So some strategic usage Shopify uses today. Calling out wider coverage strategy, spreading out our commitments. The further you spread your commitments out, the less cliffs you have. So, let's say you do make a large purchase of a three year cut, right? Of intus and US central one, for example. You're fine for three years, but three years come up. It's a large commit you have to manage, right? The more you can spread it out and use that ladder approach, the more flexibility you have, especially when newer machines come out. Google announces new machines almost every year. So the more you can spread out your purchases, the more flexibility you have in making changes. This is what Shopify tries to utilize as best we can, as we always like to be on the latest and greatest technology and get the greatest price performance, especially the newer machines. Merging and splitting. So, merging commitments, you can consolidate multiple commitments into one. So let's say you have a bunch scattered over a course of time. So, I have a great visual here coming up, but I'll start explaining it. Let's say I have a commitment coming up here next week. It's a one-year commitment. And I have a one-year commitment, same region, same machine family, and the same expiring in six months' time. Let's say my team's not ready to migrate off of the intus machines. They will be ready in that six months' time. If you merge that commitment to the future commitment, as long as it's the same machine family, same region, it will adopt that later date. You just extended your one-year commitment from those six months. This is where the laddering part gets really interesting, right? And so what Shopify does with that laddering, we actually put anchor points in the future that keeps some of our commitments alive and give us that extra additional flexibility. Splitting the same thing, let's say we have a commitment coming up here, a one-year commitment expiring next week. In our footprint, we have a team saying we can scale things down. We can scale our resources down quite a bit substantially. And I also have an anchor point in the future for the rest of the team who wants to merge and keep a footprint alive. I can keep a portion of my existing commitment alive and merge only that portion to the future. You're right-sizing your commitment with these anchor points. So, let's say I have 10,000 commitments that are expiring, sorry, 10,000 cores, 10,000 gigabits of RAM expiring next week. I want to keep only half of that alive to the next anchor point. You can do that by splitting off an amount, let that die off, and keeping off a portion to merge that future date. So, you're right-sizing your footprint as you go along. That's where these anchor points really come in handy and are extremely beneficial. And this is what Shopify has been utilizing the last couple years. This is all documented by Google, by the way. This is not some secret Shopify came up with. This is actually some practice that Google actually does say you should be doing. This is just a visual from, there's a documentation there. It is referenced. But this is showing just the merging process. This is through the code management and the console. You can do this yourself. You'll select two or multiple commitments. It will adopt the later end date. You can also rename that commitment. We also like to make sure we label these commitments. We want to make sure that we can be traced back. So, we track a lot of our commitments in internal process. We'll reference that ticket. We'll open a ticket. It's vetted by engineering, vetted by finance. Again, this is going back to that process. You want to have those checks and balances. You don't want to be purchasing these cuds in a vacuum, right? You want everyone to be aware of what's happening, what's being committed, so everyone can be aligned. And we actually are labeling for those specific tickets so that we have the history there. We know what we can go back to. But this is actually just showing, like, what happens when you merge. So, it adopts a total number of the CPUs in memory and it keeps that lighter end date. Don't worry. I'll make sure we have lots of times for questions. I also notice the clock hasn't started, so I'm going to guess where we're at right now. And we'll keep going. So, I also want to talk about the Google Cloud Pricing Calculator. And this isn't anything new. This has been around for some time. What is new, well, first of all, let's call it what it is. It helps estimate Google Cloud services for you. Now, I may not be able to estimate everything, like network traffic or how much you're going to be caching or the amount of storage you're going to be accumulating over the years. But if you're trying to price out your infrastructure, this is a very useful tool. Now, why is it important? It helps for your teams, your engineering teams to start forecasting what their info, their new app, their new feature may look like. Definitely helpful with forecasting. Obviously, we want to increase predictability. But what is, why am I bringing it up? And it's really this little feature there, link billing account. Now, this was released late last year. Now, everyone here, I'm sure, if you have an enterprise agreement with Google, you probably have some unique discounting. It may not be across everything. You actually have at the SKU level. One of the biggest challenges we had was to provide our teams with accurate estimations of their workloads. Because we were pulling pricing from our price sheet, doing unit pricing, a lot of Google sheet work, and we're putting some estimates together. By enabling this alone, when you're signing with your credentials, your discounting will be enabled at every SKU level for all Google Cloud services. So you can therefore get accurate pricing on all your estimations. This has been super helpful for us. Previously, our FinOps team was tasked with a lot of engineering teams coming to us saying, hey, can you help me price this out? Can you help me price this out? This is empowering your teams to come to you with accurate information to give you that forecast, to give you that level of predictability. This has been hugely powerful for us and has even allowed us to get some better insights into regional differences. As much as you know, regions do vary by price. Some regions are less expensive than others. You can then make informative decisions with your team saying, hey, this is how much more expensive this region is to this region. So very powerful. It's also allowed us to better understand spend-based cuts. Spend-based cuts, because the discounting is stacked on each other, it is a little more difficult to calculate. With your pricing enabled, this allowed us to actually get very accurate pricing and really clearly show the benefits of spend-based resources. I think I also mentioned just no more having to use a price list. The price list can get very long. There's thousands of SKUs. You have to go through that, especially if it's a new service, right, that you may not even have pricing for. You have to go through all SKUs and search for that. Very difficult, very cumbersome to your team. Again, this is a very powerful tool to enable your engineering teams to get accurate pricing. So that's what we're doing. Let's see how we are on time. Actually, we're blowing through it pretty quickly. And we are at the end. I really want to spend a lot of time on questions, Q&A, share our experiences, kind of get your concerns. This is actually perfect. And I think there'll be a microphone passing around. If you want to raise your hand if there's any questions for me specifically, I can speak to our experiences. I do have some Google reps here for later afterwards to speak more to the services themselves. But yeah, thank you all. And I'll start taking some questions here. Hold on. They're running with the microphone. So I'll give a chance. You had your hand up here first. So I'll point to here in the hat in the second row. Still coming. So it's a super nice summary. And we use it a lot. But the reality is when we talk about optimizing workloads, that is an iteration and it's a constant workload. And then Google doesn't provide like future roadmap about when they release new instance shapes. So how do you manage to align the constant iterations of roadmap of teams that you don't know or sometimes they don't know different optimizations versus the difference versus FlexUD versus not FlexUD? Keeping in mind that also the release of VMs is not publicly shared in advance. So the question was how does Shopify anticipate for future changes new machine types? That's a great question. One, we want to stay really close with our account team. We are proactively having road map sessions with them. But we also all there's the cost benefit of migrating. There is effort. There is time. Sometimes our services have to be spun up in parallel to migrate. So what's the ROI? Is this machine going to provide us that price performance that's worth that migration? So we make conscious decisions when we do that. It's not often. So we try to make those decisions that make best make most sense for our fleet in general. And we try to we do want our teams to use the best machines that are out there. But we also want to be cognizant of not every machine is required to be moved to. So there is some, I don't want to say governance, but we do provide some guidance on which machines we like to use, especially when we want to maximize our price performance and go with the machines that have probably the highest discounts. I saw your hand up next. You have a microphone there. So what is your all's coverage of the cuts look like? How much of your actual spend are you covering specifically for virtual machines with your cuts? That's a great question. We set a goal of having 80 percent of our fleet covered by community discounts. Why not 100 percent? Well, this gives us, Shopify does scale. There's weekly scale, there's seasonal scale, especially with our merchants and shopping patterns. So we tend to stay at 80 percent and that allows, that's really, we found that to be our optimal level. That's our goal. Now, we might be in some regions higher than that, but it's really to avoid the unused commitments, right? And it allows you to flexibility scale up and down. So that's where our goal is to always be at 80 percent at least. And that's after the fact that we've confirmed with the teams there in those regions that they're optimized, they're not going anywhere. The other thing that's really good to calculate out is your break even, right? That's the actual price of the one year, three year versus the on demand. So one year, you're there at least seven months, six months. Typically, for a break even for a three year, it's 13 months. So if your teams can say they can commit to staying in a region on a specific machine type for seven months at least, one year cut, if they can do that with 13 months, that's when you're saying, okay, we can do at least a three-year cut. I think he was, or up here, sorry, or there's one, okay. So you spoke about laddered commitments. Is there any tool, or how do you plan for those ladders throughout the year? And another question on the resource-based versus flexible commitments. What that ratio typically looks like? Okay. So I'll answer the first question with laddering. We tend not to do it too often just because of how large our fleet is, how quickly we can move. So we try to put resources in there on a quarterly or monthly basis. That gives us some flexibility to move, perhaps we'll have some time to plan up ahead. This is still a much manual process. There are tools out there that do automate this, like ProsperOps. We have not used them, but they have had some few talks out there where they do automate this to some extent. For us, it makes sense for us to be manual at the moment. It allows us to be more strategic, but there are tools out there. Same thing with spend-based cuts. We tend to do that quarterly, especially if we're still using it. We want to get an initial purchase first and then ladder on top of that. So that's still a quarterly laddering as well. I think, yeah, you're next because you were waiting. Yeah. Hello. Hey, I love that you talked about how teams should optimize their application or service before actually committing to these cuts. How do you build a culture of cost optimization or put processes in place where people aren't optimizing just when leadership says we're out of money? That's a great question. How do you get everyone else excited about saving money? It's hard. It is a cultural shift because a lot of times our teams want to ship fast, ship quickly, get their product out, right? Ship whatever it takes to ship it, right? That's where the visibility of cost comes into play. That's where the Google pricing calculator really helps illustrate that. Like, hey, this is going to give you your, how much is this actually going to cost me? Because sometimes that's not really there or available to your team beforehand. So one is reiterating the consequences of the committee use discounts. And that's why I wanted to reiterate it here with you all because it is a bit of an educational process. A lot of teams, our engineering teams, are aware that committee use discounts do bring immediate savings. So it's always like this is the easiest thing to bring down our cost. But you have to mention those caveats to them. And that's really education. That's where that cultural shift comes from. Yeah, I think there was a mic back there. For a company that is just in the early stages of a multi-year migration from on-prem to GCP. Oh, so you're over there. I'm sorry. Thanks, Matty. So for a company that's just in the early stages of a migration from on-prem to GCP, that will be an extended migration over probably a year or two before all of the VMs are migrated. How far into that migration do you recommend before committee use discounts are starting to be used, particularly when we're still trying to figure out the very different machine types and performance compared to what we're used to with our current environment? That's a great question. So at what point do you decide to start using committee use discounts when you're going from on-prem to the cloud? And I've done this before. I used to be at Mandiant before I was at Shopify. They were partnered up with FireEye, and we were moving from on-prem to AWS. It's really difficult because you don't know what to optimize. You're still playing with different machine types, and depending on your initiatives, your teams might be all over the place with the machine types they're using. So it depends. If you're, from a fleet-wide perspective, if you are narrowing it down to only a couple different machine families, I would recommend spend-based cuts, but starting off layering, right? Do an initial investment that you're comfortable with, that you know, like, if we were to optimize, we wouldn't be below this level. And then you're incrementally laddered on top of that. Now, if you don't know what region you'll be in, what machine types, that's when I would say you probably want to look at FlexCuds, right? FlexCuds, it's by a dollar amount still, but you have the ability to, it's applied to any machine family in any region. So that helps reduce your risk, especially when you're just going from on-prem to the cloud, and you're not really sure, like, where that's settling. That's what I would, the advice I would give in that sense. All right, there's, who has a microphone? Yeah, I'm over here, yep. There we go. You talked a little bit about labeling Cuds, and I wanted to understand, is that a combination of naming the Cuds in GCP and then labeling them in an internal system? And if so, what does that look like using key value pairs? How do you keep track of that, and what kind of... Sure. So part of our process is whenever there's a request for a new committed use discount, we open a ticket. It can be any ticketing system we use GitHub. This allows us to pull in teams to do their analysis. We have our data science team, our engineering team, finance team, make sure everyone's aligned. When we are purchasing a committed use discount, I want to make sure I'm tying it back to that ticket so you can see the history of that dialogue that built up to that specific commitment, right? So the commitment name, we're tying it back to that ticket. We're tying in the region, and that's usually about it. If you have a lot of commitments, like Shopify does, seeing a bunch of commitment just by numbers, it's difficult to kind of go through that list, especially in the console. So if you can do anything to help make your life easier and just knowing what this commitment is and knowing the genealogy of it, super helpful. Especially when you get into the merging and the splitting, things get too convoluted, right? So once you split or merge, that commitment does get renamed. So it's always good to name it so you can track back a genealogy of that commitment, really. So that's how Shopify kind of takes care of that. Yes? Oh, sorry. Whoever has... Hi. Do you guys do any form of parking? If you do, how do you balance that with CUDS? Sorry, can you say it one more time? Do you guys do any form of parking, you know, turning systems off when they're not being used? Yes. And how do you balance that with CUDS? It's a great question. So do we park, do we turn... We obviously scale a lot of our resources, and there are some projects that do scale completely down, like over the weekends, for example, right? That's where we have that 80% mark, right? But it's also understanding what our optimal level is. And so our data science team will actually go in through... We have an internal dashboard that lets us understand, okay, what's the optimal point of which we want to cover? Sometimes it's that 80% mark. That's like our watermark. But sometimes it can be a different varying number. And that would cover... Making sure that we're not dipping below that mark of where that one project is scaling down to. That's why we also purchase all of our CUDS at a global level, so they're shared with all the projects and not by a single project. So we really have to be mindful of when those projects do scale down to take consideration, and we're not having too many unused CUDS. I hope that helped answer your question there. I saw microphones there now. Cool. Do you use any FlexCUDs for projects that are... You know you're going to spend the money doing exploration. How do you think about that? Because you know you're going to need resources. It's going to cost money. That's a great question. So for projects that are doing exploration, we personally haven't used FlexCUDs as of yet. When it comes to exploration, we really try to understand... And this is really preference and also the discounts that we see with the resource-based codes. We try to maximize that. But if someone's doing exploration, we know what the break-even is for a lot of these committed use discounts from a GCE standpoint. So if a team can say that they know they're going to be at a certain level of usage in a given region for a new machine that they're testing out, all right, if the break-even and for a one-year cut in four is seven months, I would say, okay, let's cover this X amount with a one-year cut then. But if it's like something like other services like Dataflow or Bigtable, again, it all comes down to that break-even. We haven't really explored FlexCUDs just because it doesn't make sense for us at the moment. But it is extremely valuable if the discounting is there for that additional flexibility. Hi. So, yeah, I see most of the questions here around the cuts. And we have been on Google for about two years now. And we have kind of mastered our cuts. Like, you know, we are pretty much 95%, 98% on our compute. But we still struggle with things like other services like file store, backups, or inter-region replication costs. So do you have a way to optimize and monitor those costs? Because that's the thing that, like, you know, we struggle with. And I'm sure once people go to GCP, that's where, like, the majority of their cost would be, after cuts. On backups, specifically? Not just backups. Like, you know, backups, file stores, inter-region replication, those kind of costs. Yeah. So we definitely are tracking all of our costs. We also look at any week-over-week anomalies, also daily anomalies as well. We try to optimize. Recently we started using Anywhere Cache, which was recently GA'd last week to help bring it down some of our network egress costs. It makes sense for some, not all, especially if you're caching a lot from a small volume. But if you're caching from a lot, it doesn't make sense. So we explore other options, such as Anywhere Cache is a great example. And that just comes to mind because that was recently in GA'd, and we've been leveraging that. I have to explore that a bit more. So I will say I don't know. I'm not going to try to give you just an answer just to give an answer. Yes? Do you use TPU cards? Sorry? TPU or GPU? GPU, yeah. And it wouldn't be a Google talk without talk of AI. We are exploring it. The world of GPUs is constantly moving, constantly, and especially with capacity constraints. TPUs especially. So with the capacity constraints, with the pace at which things are coming out, or new GPUs are coming out, it is becoming extremely difficult to argue for committee use discount for these workloads. There are some cases where we have a baseline use of GPUs. Where we are starting to explore community use discounts. We do. We do use cards for GPUs right now, today. But it is for a baseline that we know that that workload is not going to be going away. And we're, again, it's all about that break-even. Are we going to be there for the next seven months, 13 months? And that dictates our decision-making on that. Thank you. And another question. Do you use prioritized attribution on the cards? Or the... We don't use prioritized attribution. It's spread across our entire fleet. So that way everyone can benefit from it. However, if we do scale up, you'll see that attribution gets shifted slightly. Thank you. Yeah. Is there any other questions? Oh, a couple here. Thank you for your presentation. Yeah. You mentioned about managing the cards in the central team. And I assume that's your team, right? Do you see any challenge when you charge back those costs? That's a good question. As long as you have a great taxonomy, that's where it's super helpful in understanding where your GCP projects are, what they're attributed to are. So labeling is key, right? Specific to cuds and the investment you make, because there is a dollar amount associated with purchasing those cuds, right? You're committing to a certain dollar amount. Attribution, I would say, attributionally just... Sorry, proportionally attributing that back to those projects or teams is probably the best way we would go about doing that. We don't have a specific method of attributing back cuds to projects, but I would assume as long as your taxonomy is robust enough and you're using proportionally attribution of cuds, that would be the model you would need to follow based on your spend. Yeah. Hi. Thank you for doing this presentation and taking the Q&As. You had mentioned earlier that you were using... Everything was kind of manual and that that was intentional because right now you're still kind of learning and thinking about the strategy for how to deploy it. Do you see a future where you might automate some of that and how would you go about doing that potentially? Absolutely. Definitely with automation. It's part of the Phenoms process, right? You're identifying opportunities, you're providing the observability, and the end goal is to always automate. Definitely see opportunities in automating anchoring, setting anchor points in the future, setting laddering purchases, smaller purchases. But yeah, that's the end goal, right? To reduce that manual work and to reduce the having to go and check when's the right time to get an optimized level. So that's what you want to be. If you're at that point, great. But that's the place you want to be at is to get that level of automation. Hi. Regarding the taxonomy comment over here. All right, raise your hands. Okay, thank you. Sorry. I was curious if you have any best practices on how you keep those labels up to date. We actually have our taxonomy at the project level. And so each project is uniquely identified and we're able to know what teams are in there. There are some teams where we have some, I know my finance team is not here, but we do have labels within those projects to attribute that cost to. Those are cost owner labels. And with our larger projects, those become definitely more needed because if it's a project, a shared application across our company, like we have to be able to charge that back to the right department. So with the larger projects, yes, but from smaller projects, it's really at the project level. We just have a lot of projects. Yes. Hi. Quick question. The FinOps team plays a critical role, obviously, in the optimization of costs and stuff. Does your organization have the FinOps team driving controls that make it through the, you know, the infrastructure of the code or the pipeline where these controls have to be met before you deploy resources? Have you reached that level where the FinOps is, apart from the security controls, is your FinOps stream driving those controls and do they have a, you know, seat at the architecture review board of sorts? Because that means you're putting your foot down in. It's a great question. Controls, guardrails, definitely key from a FinOps standpoint. Shopify is starting to put in quotas for resources. So teams have or given a certain level of quotas that they have to manage. This actually was driven by our CTO, and the way he described it was really appreciated. You can put controls in by budgets, right? But if you're limiting someone by their cost, they could be hacking things away just to keep the cost down. That's not necessarily promoting good engineering. By limiting their resources, you're building the best possible thing with the least amount of resources. So Shopify is introducing quotas for different Google services. This is like internal quotas that we're enforcing, but we see that as the best possible method of promoting efficient engineering rather than just a dollar amount. Got a few minutes left. Anything else? Oh, got one up here in the front. I saw one in the back there. Hi, thank you. Given recent market volatility and global instability, does that sort of inform how you look at your one-year to three-year cuds? Are you looking at any external factors when you're making decisions? That's a good question, especially with tariffs you're talking about, yeah. I haven't seen, nor in the conversations I've had, seen any impact that we would dictate us to do any knee-jerk reaction. Same thing like you're investing in the market. If you're jumping around a lot, you probably just want to stay with your investments. Think of commitments as almost like an investment, right? If you're moving around a lot, you're actually probably hurting yourself or exposing yourself to more losses. So staying stable with your commitments is what I would recommend. The capacity is there, right? So I don't see that being an external factor. Now, I'm saying that now. The news changes every day, but I don't see that being a factor that is something we consider. We do kind of keep that in front of you just to make sure we're asking the right questions at the right time, but it is not a factor that would affect anything at this point. Thank you. Yeah. I saw one there. Yep. Hello. Can you talk about your FinOps team, like, makeup and size? Yeah. Good question. And this is not, like, a standard thing, so don't go back to your boss and say, I need this many people. No. Our team is actually pretty small for the size of Shopify. It's made up by operations, so I'm on the operations side or technical program management side, finance, and our engineering. We also have procurement represented as well, and really it's a team of 10 to 15 people. Now, I've spoken with other companies. I'm not going to say names, but it ranges. You have, I've seen teams of 30 to 50 people, so it depends on the amount of investment that teams make, and not saying that Shopify isn't investing in FinOps, but we find it efficient with the team that we have today. So it's preference. It's does your team have the right resources to make those decisions to be impactful? So a number of people doesn't necessarily mean greater impact. It's having the right people in the room. We always want to make sure we have engineering representative, finance representative as well, procurement representative, because we look at commitments as little tiny contracts. Procurement needs to know about it. Finance needs to know where we're investing. Engineering needs to ensure that, yes, we are optimized in that region, right? So we have an engineering representative that can vet that those commitments are the right levels, and that those teams there are optimized. So I think having that makeup is key more than the number of people on the team itself. With a recent comment from your CEO. Are you over there? Okay, I want to make sure I'm looking at you. Yeah, that headcount needs to be justified by AI before headcount increases. How did that actually manifest in the company as far as your guys' rules, policies, and things? You say that one more time, sorry. So the recent comment by your CEO, 2B Luke, about headcount needs to be ruled that AI can replace it before it can be justified. How is that manifesting? I mean, the way I look at it, it's an opportunity, right? AI is a tool. And if you can leverage that tool the best of your ability, it's the more successful you're going to be. So we're exploring how to use AI more so now. And the possibility that they're endless. Especially when it comes to, like, what's our optimal level? What are our patterns? That's where I can see AI being more helpful in the FedOp space. I see it as only a positive opportunity to leverage to, how can I make my own job easier as well? How can I continue to expand my impact? And that's how that comment has been viewed, at least, at Shopify. At least, sorry, I'll speak for myself. I don't want to speak for all of Shopify. Do not quote me on that. But it's definitely an opportunity. And it's pushing us to leverage it more. AI should be used as a tool, not as a threat. Or seen as a tool, not as a threat. And it's definitely an opportunity for us to leverage it more to maximize our impact. All right, we got, I think, time for one more question here. Yeah. How do you guys manage the Gen I-related workloads with respect to cost? The Gen I workloads? And AI-related workloads? And this is kind of back to an earlier question. Because they are so sporadic, right, it is difficult to do, especially with what we're leveraging or trying to manage our GPU costs. So, again, it's finding that baseline, right? And that's where you're really trying to maximize your savings. Once that baseline is determined, that's where you're going to be able to maximize your savings on that. But if it's spiky, it's really, then you're looking at, okay, from an engineering perspective, is this optimized, right? Are we using it to the most optimal level? All right. Again, thank you, everyone, for your questions. Appreciate your time. Hopefully this was helpful. Yeah, appreciate it. Thank you. Thank you.