 Music Good morning, everyone. Welcome to our session. Today in the session, we'll be talking about building and serving the next generation AI models with Jax. My name is Rajesh. I'm a product management lead here at Google Cloud, focusing on ML infrastructure software. I've been clicking off the session by giving an overview of Jax's stack on Google Cloud. First off, what is Jax? Jax is a high-performance ML framework that is compiler-oriented, modular, and completely Python-based, which makes it highly performant, scalable, and easy to use. As a result of these qualities, Jax has become the framework of choice for foundation model building everywhere. Here you can see a high-level view of the Jax stack we offer on Google Cloud. The Jax stack we offer on Google Cloud is built for performance and scale on both TPUs and GPUs. At the bottom, you can see both TPUs and GPUs tightly coupled with XLA, which is the compiler, and the core Jax framework. On top of that, what users add are common libraries in the Jax AI stack, including FLAX for neural network building, RBACs for checkpointing, and OPTAX for optimizers. On top of that, what users add are other libraries for other use cases, such as for data loading. You can use Grain for deterministic data loading for training. TF data or HF data, depending on where you're loading your data, as well as libraries such as AQT for quantization and Jetstream for high-performance inference. On top of this, we have MaxDiffusion and MaxText, which are performant reference implementations for various models. So MaxDiffusion is for diffusion models, and MaxText is for LLM and MOE models. On top of this, if you're using GKE, which is Google Kubernetes Engine, we have a tool called XPK, which allows ML engineers to use ML semantics to create, manage, and run jobs on GKE without knowing Kubernetes. In addition to the stack, we're also offering things for better developer and onboarding experience in Jax. One of the things we're launching today is Jax AI images for GCP, which are Docker images and artifacts that package the Jax framework and Jax libraries, along with other tools to provide a strong foundation for running Jax workloads on GCP. On top of these Jax AI images, we also provide optimized recipes for many MaxText and MaxDiffusion models, so you can do easy benchmarking and onboarding onto Jax on Google Cloud. We also offer developer tools, including for profiling, monitoring, and logging. Today we're launching in preview a Cloud Diagnostics XProf library to enable easy ML workload profiling on GCP. We're also constantly expanding support for new use cases, including multi-host inference, host training, and reinforcement learning. One of the things I'm very excited to announce we're launching today is Jax AI images for GCP for TPU training and GPU training. As I mentioned previously, the composability of the Jax ecosystem is great for speed and iteration, but this creates issues with customers with dependency issues between different Jax libraries, Jax core, and other tools. So to solve this problem, we are providing Jax AI images as Docker images and artifacts that package the Jax framework, Jax libraries, along with other tools. Very well tested so the customers can have a strong Jax foundation for their production workloads on GCP. On TPUs, these images are available for Trillium, and for GPUs, these images are available for A3 Ultra and A4. Some of the use cases where you can use these Jax AI images, so you can use the Docker as is for a custom workload. You can use the build recipes to build your own Docker container to integrate with your custom workload. If you're using Max Text or Max Diffusion for full fine-tuning, as the customers today are using, you can use this as a foundation under Max Text or Max Diffusion. And also for the Max Star optimized recipes for reproducer benchmarks, these AI images are pre-integrated and used as a foundation under the hood. Deep diving into the Max Star optimized recipes, these are optimized recipes for Max Text and Max Diffusion models that allow ease of benchmarking and onboarding on TPUs and GPUs and GCP. You can find these recipes on GitHub under AI Harper Computer, and some of the recent benchmark models we've released for Trillium include LAMA 3405B and Mixtral 8x22B. And on the GPU side, some recent benchmark models include LAMA 3405B and 70B, with more coming in the future. In order to improve developer experience, specifically in the area of ML workload profiling, I'm very happy to announce in preview of a new cloud diagnostics profiling library as well as a newly updated X-Prof tool. Both of these together enable a seamless profiling experience for ML engineers to performance-optimize their ML workloads on any XLA-based framework. This is including JAX, PyTorch XLA, and Keras TensorFlow. The Cloud Diagnostics X-Prof library has been built to enable a seamless, self-hosted experience for ML workload profiling on GCP. The main features of this library include easy setup of infrastructure for X-Prof using TensorFlow as host, profile sharing to enable collaboration within user teams and with Google, and on-demand profiling to capture profiles during your workload run. Check out this library on GitHub under AI Hypercomputer Cloud Diagnostics X-Prof. The core profiling tool X-Prof has also been newly updated. Previously, you may be familiar with the tool TensorFlow Profiler. We have put the entire code base under the OpenXLA project and rebranded to X-Prof. So now we can support all X-Prof frameworks used in the market with the same consistent experience. With this updated X-Prof code base and rebranding, we are now bringing the same tool views internally inside Google and Google DeepMind to optimize ML workloads out to Google Cloud customers and the rest of the ecosystem. In this talk, our customers will be talking a lot about using JAX on TPUs for LLM and MOE model training, but the JAX stack on GCP is also built for GPUs and other model architectures as well. So on GPUs, you can see the benchmark here showing we get state-of-the-art training performance for LLM A3-405B using FPA training on A3 Ultra, which is based on NVIDIA's H200. As you can see in the graph, we're able to scale near linearly at high performance up to 1,024 GPU scale. We use a metric called EMFU, which is effective model flops utilization, which is a ratio of utilized FPA flops divided by peak BF16 flops, and we achieve greater than 80% EMFU, which is state-of-the-art performance at this scale. We're able to achieve such performance at scale due to our close partnership with NVIDIA, as well as the core tenets of JAX, XLA, and Maxstacks, which allows us to easily performance optimize. You can check out the recipe for this model in our Maxstar Optimize Recipes GitHub repo. In addition to training for LLM and MOE models, we also have great inference performance for diffusion models with max diffusion and the JAX stack. Here you can see relative to V5e, we get nearly 3.5x better inference throughput for stable diffusion XL for both offline and server inference on Trillium. In addition, from a cost perspective, you can see that using SDXL in max diffusion, the cost to generate 1,000 images is as low as $0.22 for Trillium, which is a 35% reduction in cost compared to TPU V5e. I would now like to hand it over to Minho and Nayun, who are from Kakao, and they will explain how they are developing LLMs with Maxstacks and TPUs. Welcome. Thank you. Thank you for your introduction, Rajesh. Hello, everyone. I'm Minho Ryu from Kakao, and this is my colleague Nayun Kim, and today we are sharing our journey with Maxstacks and TPUs for developing large language models. So we started as a GPU-based team but extended TPUs, and I'll tell you why we did it, how we adapted it, and our achievements. First, a quick introduction to our company, Kakao. We are the company behind Kakao Talk, which is used by 93% of South Koreans. That's 49 million people. Think of it as Korea's equivalent to WhatsApp or Facebook Messenger, but even more deeply integrated into daily life. With such a massive user base, our LLMs need to be both powerful and efficient, and they must excel at Korean language understanding. Kakao Talk isn't just text messaging. Our users communicate with over 700,000 different images and stickers, and they make voice and video call right in the app. These real-world communication patterns influence how we approach our model training and why we needed a flexible, customizable framework like MaxText. Hakao also has grown beyond messaging into a full ecosystem of services, from finance terms to location directions. Having AI that works well across all these services is a strategic priority for us, which is why we are constantly pushing to build more efficient language models. I'd like to introduce Kanana, our family of language models. What makes Kanana truly exceptional is its efficiency. As you can see from the graph, our models deliver superior performance while consuming substantially less pre-training compute than competitive models in the field. Our strategic investment in TPUs further enhances these efficiency gains, providing significant cost advantages that aren't captured in this visualization. Then from now on, I'm going to share how we implemented LLM training with MaxText and TPUs. This journey took us from initial deduction to practical implementation with many lessons along the way. We had to rethink some of our established practice, learning tools, and customized components to fit our specific needs. Let's begin with our additional point last year, whether to continue GPUs or embrace cloud TPUs. Option one was the safe choice, get more GPUs. Our team was familiar with them, and we wouldn't need to change our code, but we were hitting power and budget limits. Option two was cloud TPU, more computing power for the same budget, but with a learning curve. We would need to adopt new code base and learn new tool chain. After a deep consideration, we decided to adopt TPUs for a better cost-performance ratio, recognizing that short-term discomfort would yield long-term competitive advantages. Then, I'd like to walk you through how we adapted the Jax stack for our needs. This wasn't just about using off-the-shelf tools. We had to carefully integrate and customize several components to create a cohesive workflow. XPK was straightforward, making cluster management easier. We adopted Grain's data pipeline because it is deterministic for long-running LLM training. Our key focus was customizing the data pipeline for training flexibility and compatibility with Megatron LLM on GPUs. Now, I'll dive into the data pipeline for multi-source blending. The standard max text code assumes you are working with a single pre-processed corpus. In our training, we need to carefully balance different types of content, web text, code, math, and so forth. Our solution is to use Grain's mix function to dynamically blend these sources with configural weights. For example, we might use 70% of web text, 20% of code, and 10% of math during one phase of training, then adjust these ratios for different training stages. What makes this powerful is that we can experiment with different blending strategies without reprocessing terabytes of data each time. This flexibility was crucial for our iterative research process. Another key customization was our token processing approach. This technical adjustment is to align with Metron LLM implementation and to make training more efficient. The original max text code shifts input by padding the force token. While this is functional, it effectively weighs a token portion in each sequence. Our custom implementation adds an extra token per batch and shifts via truncation instead. The diagrams illustrate the difference. Finally, I will share our training results with max text pipeline. We successfully trained two significant models on TPU-5E. First, we trained Kanana to 0.1 billion model from scratch. The graph demonstrates the training compatibility between our TPU-based approach and GPU methods. It shows how our max text implementation achieves compatible performance in Metron LLM at each training stage. This compatibility is crucial because it validates our TPU pipeline can seamlessly replace GPU workflows while effectively training models from the ground up. Second, we also perform depth upscaling with continual pre-training from our existing 8B model to 9.B architecture. Depth upscaling is a process to improve model performance by stacking more layers. This graph shows constant, consistent improvements across various benchmarks. The key takeaway here is not the numbers, but the fact that we successfully complete these training jobs using customized max text pipeline on TPUs. This proved that our transition from GPU to TPU was successful. Furthermore, we got an opportunity for early access to Trillium. Using SPK, we simply changed a few parameters in our cluster and workload configuration for a transition from V5E to Trillium. the performance improvements were immediate and substantial. Across both our test models, we saw throughput increases of 2.7 times. What's even more impressive is that Trillium also provides better cost performance efficiency. That means we are not just getting faster training, we are getting more computation with the same budget. based on our experience, we believe that the Jack Stack and TPUs offer compelling advantages for anyone training large language models. SPK simplifies management of accelerator with simple commands, making TPUs accessible even if you are not familiar with Kubernetes. TPUs deliver exceptional cost performance efficiency, letting you train models with less budget. And Max Text, built with pure Python and Jax, provides a clean, functional approach to LLM training that's both powerful and customizable. We are excited about the possibilities that this package opens up for the entire AI community. Thanks for listening. I will hand over to Nayeon Kim. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. And from now on, I'd like to share about some of our experiences using Max Text on TPUs to train mixture of experts models. And recently, there's been growing interest in MOE models such as DeepSeq V3, or QN MOE series, or the latest LLM as well. And our team is also very interested in building inference efficient models that still deliver a strong performance. So a big part of my recent work has been experimenting with ways to build MOE models effectively. And thanks to Max Text's simplicity and flexibility, getting started to this framework was quite seamless, even though I was more used to GPU-based frameworks. And thanks... And so in this section, I'd like to first briefly go through the experimental setup and results of our MOE experiments. Then I'll explain the modifications that we made in Max Text to support MOE model training, along with a few additional changes that we are currently working on. And finally, I'll wrap up by summarizing what we found to be the strengths of Max Text. Okay. So there were two main objectives for this MOE experiment. Our first objective was to explore whether we could expand the model capacity by upcycling our existing dense models into a mixture of expert structure independent of the training data. And the second objective was to evaluate if the TPUs and Max Text frameworks are suitable for MOE training, especially in terms of the training efficiency and potential advantages that Max Text would bring. Since the efficiency of TPUs has been covered earlier, I will focus more on the Max Text aspects. Okay. So for this experiment, we built our MOE model by upcycling our Kanana nanobase model into a structure with total 64 experts with eight of them are active per token. And this results in a total of 13.4 B parameters with 2.3 B active. And we trained the model with 800 billion tokens using the exact same data that was previously used for training the original dense model to only focus on the impact of the MOE architecture itself and avoid any effect from the new data mixture. And the training was carried out on three slices of B5 EF TPUs using the Max Text framework with fully sharded data parallelism. Okay. Since we didn't introduce any new high-quality data in this experiment, we couldn't expect a dramatic boost in performance. However, the results were still enough to show that it's possible to expand the model capacity by upcycling a dense model into an MOE architecture even when the number of active parameters remained roughly the same. And we validated across a range of benchmarks covering English general knowledge and Korean general knowledge and coding and math. While we saw improvements across all tasks, the gains were significantly the most significant in code and math benchmarks. This suggests that the MOE architecture may be internally specializing, which means forming domain experts that can handle certain type of tasks more effectively. Okay. So, interestingly, under MVP benchmark, which evaluates the coding ability of the model, our MOE model achieved a score that was quite comparable to our Canada and SN8 BDance model. With this performance results, we were able to meet our first experimental objective, validating the potential of expanding model capacity by upcycling a dense model into a mixture of expert structure. We also met our second experimental objective as implementing MOE trainings with MEXTX turned out to be quite seamless. First, Megaboss kernels, which supports the optimized MOE features like grouped gem, were already integrated into JEGS about a year ago. And this actually happened earlier than many GPU-based frameworks. On top of that, MEXTX is built on flex with scheduling and checkpointing obtained by Optex and Orbex. The code base is very simple and intuitive, making it so easy to understand and customize both the model architecture and the training strategies. These kinds of flexibility are especially useful for MO experiments where you often need to run a wide range of operations for the model architecture itself and the training strategy. Next, I'll show you a few examples of how we actually took advantage of flexible MEXTX framework during our experiments. This slide shows the model configuration we used for our MOE model. And as you can see, it summarizes the model structure in a very simple and intuitive way. Our model is based on the mixture architecture but we modified it into a total of 64 experts and eight active ones. And here, you can see key configuration details very clearly like tensor sizes for model layers, the activation function we used and the additional settings such as dropout, tight embeddings and whether the pre-soft MEXT routing is enabled for the expert router. And this is one of the customized learning schedulers we use for training. We defined three separate schedulers with the Optex library which were warm-up, constant and annealing schedulers. Then, using the Optex join schedules, we were able to simply combine them into a single learning rate scheduler. And this demonstrates how Optex-based scheduling in MEXTXT allows for highly flexible training schedules which is super useful for experimenting with different training strategies. The last modification we made for Kanana MOE experiments was enabling the load balancing loss from the MOE block in the mixture decoder layers within MEXTXT. Originally, this load balancing loss was only implemented for the dense MEDMOL operations and however, since we were using Megablock's kernels with sparse MEDMOL, we wanted to extend this support of load balancing loss for our settings as well. This is the permute function we use for sparse MEDMOL within the MOE block. And here, the green highlighted line shows the modifications we made to calculate the load balancing loss directly from the router logits. And what's interesting is that this addition was quite simple which only required inserting a single line of code at the point where the router logits are used to select the expert outputs. And one last thing, I'd like to briefly share some of our additional modifications that we are currently working on. Inspired by DeepSick V3 and other open MOE models, we are experimenting with incorporating shared experts as well as replacing the first K MOE layers with a dense layer. But unfortunately, we don't have evaluation results for these configurations yet since we are still working on it. For the first one, adding the shared experts, we introduced a customized shared MOE block that includes both shared experts and the routed experts. We customized the shared MOE block by integrating the red highlighted routed experts and the yellow highlighted shared experts defined from the MOE block. And the blue highlighted shared expert gate producing a sigmoid output is introduced to control the shared experts effect before merging it to the outputs from the routed experts. And this demonstrates the simplicity of customizing the decoder layer to meet specific needs. in addition to adding the shared experts, we also replaced the first K layers with standard dense layers in our MOE model using the simple implementation based on the DeepSeq 3.3 approach that was implemented in MaxText already. So, to wrap things up, here are some key advantages of using MaxText. So, first of all, MaxText is implemented with JAX, so we can get just-in-time compilation via XLA so smoothly. And it has flex-based code structures, which is very intuitive and clean. And it's optimized for single-program multiple data training, which is highly advantageous for large-scale parallel computing. And also, MaxText supports for the fully charted data parallelism, which ensures the efficient training across the distributed systems. And lastly, MaxText delivers fast updates for new features. For example, when DeepSeq 3.0 was released recently, many of its features have been quickly integrated into MaxText. And these strengths make MaxText a powerful and flexible framework for training the LNMs. And thank you so much for listening to us. And with Kakao, we'll be always excited about new updates to TPUs and the related works. And we really look forward to utilizing them. Thank you. And back to you, Rajesh. Thank you. Thank you, Minho and Nayoon for sharing your experiences with MaxText, JAX, and TPUs for Karana models. I would now like to introduce Dr. Ian Campbell from Children's Hospital of Philadelphia, who will share how he's doing using MaxText and TPUs for building a true pediatric medical assistant. Thank you so much for that introduction. My name is Ian Campbell. I am a pediatric medical geneticist. I see children that have suspected genetic diseases. About 15% of my job. And the rest of the time I run a research lab that trains large language models. But as a physician working for a non-profit, we have strict ethics reporting requirements. And so I need to tell you that Google Cloud provided credits and complimentary services which partially supported this work and that I am a named inventor on a patent that the Children's Hospital of Philadelphia has submitted. But first, a quick bit about Children's Hospital of Philadelphia. It was founded back in 1855 and was the first freestanding pediatric hospital in the nation. Our non-profit mission is to improve child health not only in Philadelphia but also around the world. We saw about 35,000 inpatient visits last year and 1.6 million ambulatory visits. But in addition to providing world-class clinical care, we also are a research institution with over 1,100 principal investigators like myself that are trying to change the world through research. And so I want to ask you, have you ever felt like your doctor doesn't know anything about you? And I hope that none of my patients feel that way, but the reason that some of you may feel this way is because there are a lot of time pressures on physicians with a lot of different tasks and, you know, there's a lot of information about patients stuck in the electronic health record where it's very difficult to get. And so a lot of us use, you know, LLMs to do coding, for example. Wouldn't it be great if there was an AI medical assistant that could help physicians better understand their patients? And when I say an AI medical assistant, I mean an AI that could answer any arbitrary question about a patient that's sitting in front of them. For example, what antiarrhythmic medications has this patient been on? And so the traditional approach used by our EHR vendor is to query a proprietary closed-source foundational LLM to ask the question. But obviously that model doesn't know anything about the patient. How could it? And so the only way to fix that problem traditionally has been through retrieval augmented generation, passing along with the question all the required medical history, medication history, things like that. But the problem is that embedding-based RAG systems fail to retrieve relevant context in the medical setting. And LLM-based RAG systems are very expensive and very slow. And worse yet, the model never learns anything about the patient. It just, you know, has a brief understanding in its context about the patient. And so here's an example of the failure of a RAG-based system in a clinical context. Here, a clinician is asking, you know, what antiarrhythmic medications has this patient ever been on? And then we have a complicated RAG-based system that sends the current medication list to the model and also does some semantic search and provides some chunks of documents along with the question. And so the model answer is no, this patient has never been on any antiarrhythmic medications. And that's despite the fact that there is a document chunk that talks about metoprolol, which is an antiarrhythmic medication. Now, to be fair, this patient has tens of thousands of document chunks, so it's kind of understandable that that might be missed. But we thought maybe there's a better way. And so instead of primarily relying on retrieval-based systems, instead, we pre-train the model as much as possible to know about the patient in advance. And then instead of relying primarily on retrieval-based systems, we instead identify that individual patient to the model that it already knows about. And so the idea of a model that knows and so a benefit there is that the model learns from years of patient-specific information and also learns from the other patients that are in the health system. And that's a little bit scary, a model that knows about all the children in a children's hospital. And so we take privacy and security. Those are our top priorities. And so the work that I'm about to tell you about is supervised by our institutional review board, which is, you know, there to protect human subjects. It's also governed by our hospital's AI governance committee to make sure that this is an acceptable use of the technology. And all of the training that I'm about to talk to you about occurs within our highly secure, HIPAA-compliant environment within Google Cloud. And so we started off with a reasoning-based model, DeepSeq R1 Distill LAMA 3.370B, and then we continued pre-training using essentially all of the electronic health records of all the children at Children's Hospital of Philadelphia. That's 1.6 million children and 146 million clinical notes. And we took advantage of 256 trillion TPUs, and we used the MaxTax and Jax frameworks to pre-train our model. And as you'll see in a moment, we fine-tuned the model to induce patient-specific reasoning about our patients. And in the future, we hope to do some reinforcement learning to improve performance on additional tasks. At the end of that, we've hopefully arrived at an institutional AI medical assistant proof of concept. And so there will be immediate uses for our researchers at CHOP to better understand their research participants. There are also operational use cases like metrics that we submit to governing bodies and rankings organizations like U.S. News and World Reports. But as a clinician, I'm really excited about a future in which there might be an AI medical assistant that can help me take better care of my patients. And so here is an output of our model about a real patient. And so here we're going to ask a question about why did this patient have a liver transplant? We're not providing any additional context. The model has to use what it knows about that patient identified by the medical record number, name, and date of birth, which I've obviously blanked out for privacy reasons. And so this is the first question I ever asked one of our reasoning models. And because it's our model, we can observe its reasoning stream. So it says, okay, I need to figure out why this patient had a liver transplant. Let me start off by reviewing what I know about the patient. From her medical history, I remember that she was diagnosed with propionic acidemia, which is a rare genetic disorder that affects the body's ability to break down amino acids. And it goes on to remember that this causes liver damage. And then it remembers that she was hospitalized for multiple times for metabolic crises and that she had a liver biopsy that showed liver damage and that she needed the liver transplant to prevent further complications and improve her quality of life. And then it uses that reasoning to answer the question. And this is, as I said, the first question I ever asked this reasoning model. And I was like, oh, maybe that's the future of medicine. And so the benefits of this approach are that it's extremely fast inference. There's no RAG system to wait on. The model can access deep context about the patient and potentially about all the patients in the entire health system all at the same time. It might be able to discover new insights from the training data. It also critically learns the culture and practices of the health system that it serves. It starts wanting to implement CHOP treatment pathways when you ask it medical decision-making tasks. It also learns the styles and preferences of individual clinicians. It's read every note I've ever written as a doctor. And so going back to our failed RAG example in the past, we can ask the same question about the same patient using our reasoning model. And so this shows an emergent behavior of our model taking clinical notes across hundreds of different context windows in its training data and it recreates a linear history of the patient. And so you can see it thinking about 2013 and 2014 and then it says, oh, in 2021, I remember she was started on metoprolol. Wait, metoprolol is a beta blocker and that's an antiarrhythmic. And so it sort of has this aha moment that this patient has been on antiarrhythmic and then it correctly tells us about that. And so you might be wondering how can we induce patient-specific reasoning? In our model. And so it turns out that the process is actually fully self-supervised. And so we provide a bunch of notes about an individual patient and then we ask the model to come up with a question. And so it came up with a question on its own about what genetic testing was recommended. So then we ask the model a second time using the same context to answer the question. And it generates a reasoning stream and talks about, okay, I'm going to go through the notes provided and in the first note I remember this and in the second note I remember that. That's not helpful though because it's using the notes as context. We want it to use its internal knowledge as context. And so we simply force the model to reframe its knowledge, its reasoning stream, into the internal context. And so you can see that it's changed to be about what I remember about the patient or from what I remember and then add a follow-up at 12 months instead of, you know, talking about a second note. And then we use this data, this to fine-tune our model to have patient-specific reasoning that I showed you before. And so MaxText and JAX make it easy for researchers like us to make such advances. And so six researchers, many of whom are here in the audience supporting me today, can use this tech stack to train on clinical data. And we made a lot of the same decisions that our friends at Cacao made using Grain and MaxText. And we had to change just 36 lines of code to be able to train on our specialized clinical data. And it was easy to control parallelism across different accelerators and platforms and trivial to train on eight or a thousand accelerators. And we're looking to move on-premises to unify our code base. Also, Trillium TPUs allow, you know, very low-cost flops per dollar, allowing a nonprofit like us to be able to train on such large scales of data. And so what's next for us? Well, we've, you know, done some fine-tuning, but we're really excited to use some reinforcement learning to improve performance on other tasks. We've had success expanding our models from eight billion parameters to 70 billion parameters dense models, and maybe we'll be looking at mixtures of experts like our friends at Cacao. But I'm also really excited to ask, how can we help other institutions train their own models to better understand their research participants and care for patients? So if you want to learn more and see how you could contribute, we're also hiring, you can visit our website or just Google Chop Campbell Lab for more information. And I'm going to hand it over to Kyle, a product manager at Google Cloud, to finish it up. Thanks. Thanks. Awesome. Thanks for giving that speech. And thanks, everyone, for being here. So one of the fun things Rajesh and I get to do is work with awesome customers like this and help them to adopt Jax, help them to use Max Text, and build these amazing things. So definitely, after the session, we'll be outside if you want to talk or speak with our customers. We'd love to talk about what you're building and how we can help. So just kind of summarizing what I learned from those sessions, I mean, one really cool thing is how easy it was for these customers to adopt Jax and Max Text in very different use cases and build something quickly, in some cases, with small teams and also with people who weren't familiar with Jax before. A lot of people are coming from PyTorch GPU land, and so we're trying to make it as easy as possible to adopt TPUs and get that improved performance and better price performance. I'll talk about how we at Google Cloud are trying to make it easier and easier for customers and users to do this with things that Rajesh mentioned, Max Text, Max Diffusion, and I'll also mention Pathways, which is something that we launched this year at Cloud Next. So what is Max Text? We've talked about it a lot. It's an open source reference implementation for the best way to get the best performance with Jax on Google Cloud, whether it's TPUs or GPUs, and because it's open source, you know, we collaborate with everyone out there, and we're always trying to add the latest features, the latest models, and to give an opinionated opinion, opinionated way of doing the best thing possible and getting the best performance. So for example, we show how to use, you know, mixed raw on TPUs. We also show how to use 40 different pods of 256 chips of Trillium to train mixed raw. So there's small examples. There's extremely large examples because we're trying to push the envelope here on both ends of the spectrum. Something that we're excited to announce today is that we have added support for some of these latest models. So that includes Gemma 3, DeepSeq v3, the March 24 date version, so like the new version, and also we just added support for Llama 4 Scout, I think yesterday or this morning. So we're trying to stay as up-to-date as possible to give everyone out there the tools and the latest capabilities so that if you want to build something on Jax, whether it's Google Cloud or anywhere else, you have that at your disposal, you see how to implement it and how to get the best performance. Historically, MaxText has been very focused on pre-training and as some of our speakers talked about, we demonstrate all these different ways to do sharding, data parallelism, FSTP, TP, EP, pipeline parallelism, all these different techniques and we also have maybe what we can call features. So we're showing how to do things like host offloading and utilizing the large VM memory as well as the TPU memory. So how can you improve the overall performance, good put, MFU of your training runs? We also have custom kernels like the Kakao team talked about and we're also, I think, very excitingly expanding into post-training. So not just pre-training on MaxText but also post-training. So we've recently added SFT to MaxText and now we're working on GRPO and DPO which should be here in just a few weeks. So again, happy to talk to everybody afterward. We want to hear from you. We want to hear what techniques are you using? What would you like to see implemented? How can we accelerate your workflows? Because this is an open source project and that's exactly what we want to be able to do. Also very exciting are all the integrations that the team's talked about. So we're showing how to use all the JAX AI libraries. We're also showing how to use XPK which is a really easy way to get started that the Kakao team is using. And then announced at Cloud Next this year we have multi-tiered checkpointing which is going to speed up your training runs, improve the good put of your training runs because it'll be much faster to resume state. And also pathways which I'll talk about on a few slides which is a really cool Google DeepMind way of doing training and inference and interactive supercomputing. So all of this is stuff we're adding to Max Text to make it more cutting edge and to demonstrate how to do these things on Google Cloud. As for Josh mentioned we also have Max Diffusion which is basically the same thing but for diffusion models. So here we're showing you how to train models, how to pre-train models, how to post-train models, and also how to serve models. So in the same way we're adding all these capabilities, whatever's the most popular open source diffusion model, we want to add it. We want to give our customers the easiest, fastest way to implement that, to train that, post-train it, and then serve it on Google Cloud. And so here's just a few of the techniques that we already have in Max Diffusion today. And again, we'd love to hear from you, hear what everybody's working on and how we can work together to make Max Text even more valuable to all of you. This is the last slide here. I just want to mention Pathways. There was a session this morning that was recorded so I encourage everybody to go take a look. Pathways was developed internally at Google over the course of many years. It's what DeepMind uses to train and serve Gemini models and it's something that we're now making available to Google Cloud customers. And it allows you to do a lot of different things but three maybe of the most exciting things here that I want to call out are resilient training. So during your training runs you can do things like have hot swaps. You can do things like scaling the training as capacity becomes available or actually loses availability. So it allows you to continue to train even through all these potential hiccups with hardware. Multi-host inference is a really exciting thing that we're demonstrating and you can go and take a look at this today on Jetstream. Jetstream is like Max Text but for serving. So we're showing how to do all these things in JAX on GCP today. And the last one is super exciting interactive supercomputing. So this is probably the most natural way for developers to interact with TPUs. You can do everything from a Jupyter notebook. You can be interacting with a couple TPUs or hundreds of TPUs and it feels like it's this native local development. So in terms of iteration speed it's really comfortable, it's really fast and it allows people to build more quickly. So on behalf of the team I want to thank everybody for coming today. Please reach out if you have questions. A lot of this is open source. I think the customers were all happy to talk to you so please stick around and let us know if you have any questions. Thank you. Thank you. Thank you.