 Hi, everybody. This talk is titled Let's Talk About Thinking. Test, time, compute, and reasoning. Thank you all so much for coming. I know it's Friday afternoon. Thursday was party night. Friday, we'll try and keep things very mellow. I am a researcher within Gemini. My name is Jack, and I'm also the tech lead of thinking within Gemini. We feel like thinking is a super exciting new capability of Gemini, and that's what I'm going to talk about in detail. A bit about myself, I'm very passionate about making our models smarter and more capable. I've been working for a long time at Google DeepMind, a lot of time mostly spent on pre-training. More recently, I've been really focused on thinking. And I'm going to break down the story of thinking into kind of four kind of sections. One is talking about kind of a motivation from the research side, which I think is actually an interesting motivation that everyone should know about, which is about a compute bottleneck that was holding our current models back in terms of how smart they were and why thinking is our solution to this. I'm going to talk about just pragmatically what is thinking, how do we train it. And then I'm going to talk about a little bit more and put myself in your shoes. Why should I care? What do I get? And the two main themes of this section will be more capable models and steerability. And then I'm going to end on kind of a spotlight of 2.5 Pro, which is our most recent model with thinking inbuilt, and just show some kind of cool results and some demos where kind of maybe we touch upon thinking, but it's just nice to see the model use these things in action. So what is the compute bottleneck? This section is really going to have a simple message that for our model development thus far, until late last year, intelligence has been bottlenecked by test time compute, and our solution is thinking. So let's unpack this. Traditionally, and by traditionally I mean throughout the whole of history until late last year, we've been training models to respond immediately to requests. So if you have a question, you have an API request, the model, the transition between the end of that request and the beginning of the model's response or answer is immediate. It has one single, what we call forward pass through the model, and then it starts having to answer. From an intuitive aspect, it's not easy to come up with a really good answer to a question, if someone asks you a question, on the spot immediately. We intuitively like to pause and ponder before we respond, and it's no surprise that this ends up helping our models. But let's talk about this in a bit more detail. So when I say forward pass, what's really happening behind the scenes is we have a large language model. Gemini is built up as a large neural network, and it spends compute at test time, so at the time of your request, on your particular request, and we can think of the compute that it spends as a proxy for how much it thinks about your specific problem and how much intelligence it can apply on the spot. Now, what actually happens when we talk about a forward pass? We can think of a neural network as a collection of layers, and these layers have a kind of a width which kind of dictates how much parallel computation can be happening within the neural network. It also has a stacked number of layers which allows it for some iterative computation. And when we talk about a forward pass, what happens is your request is fed as input into the model, and then it's turned into activations, and these flow up through the neural network in one forward pass until it then starts writing out a response. So the transition point is the compute of this single forward pass between your question or your request and the response. Now, actually, it's kind of amazing what the model can do in a single forward pass, transitioning from your question to answer. It can recall key pieces of information from its weights. It stores information in an incredibly compressed and interesting way across its model weights. It can combine existing knowledge, maybe through its activations or through its attention mechanism. It can perform intermediate calculations. Maybe you want to summarize a document and you need to aggregate a bunch of numbers. It can often hold many of these in its activation space and aggregate them in parallel. And it even can actually explore a limited number of multiple ideas. It can hold them as some kind of interesting juxtaposed kind of activations in its neural network, considers multiple possibilities, and collapses them down to one answer. So it's kind of incredible what it can do with a single forward pass. However, what if your problem requires just a lot more test time compute than that single forward pass? What have been your options so far? Well, the main option that we've had is to be able to use a larger model. So if we have a larger model, it has more layers. It has a kind of greater width. It can do more parallel computation. It can do more iterative computation. And we see this even in the Gemini offerings where we have kind of tiers of model size. We have Flashlight at the moment, Flash. We have Pro Series models. And that's one way to get around the problem of if this specific model you're using is just not able, it's not capable enough to do all the things that you wanted it to do, then just try a larger model, and maybe that larger amount of test time compute will help. However, this, you know, has some limitations. One is that then everything is still kind of predefined. You still have to choose a fixed number of potential available models. And also, you know, maybe the range of intelligence that you need across your applications is really large. Maybe you need the model to be incredibly intelligent for some small number of requests and maybe be 10,000 times more intelligent than the average model request for some subset. So you want a very large dynamic range of intelligence potentially. Now, right now, we actually have something that's pretty cool. I guess we can proxy the computation, the test time compute with the model cost in the API. We have a bunch of Gemini models. We can kind of draw two orders of magnitude in cost from our kind of cheapest model to most expensive. So two orders of magnitude is cool, but maybe four or five orders of magnitude would be even cooler. And that's going to be hard to pull off with just this approach of having different size models predefined. So to kind of wrap up that kind of chain of thought, users might want to be able to think 1,000 or 10,000 or maybe even a million times harder for very challenging or valuable tasks or a very kind of relevant subset of queries to solve harder and harder problems. And also, they may want it to be a lot more adaptive, the model to spend less compute for simple requests and more compute for harder requests. So in summary of this motivation section, intelligence can be bottlenecked by the amount of test time compute. And I'm going to talk about our solution to this, which is thinking in Gemini. So what is thinking? So very practically, you probably have already maybe seen thinking in action right now because we have models with a thinking capability in the Gemini app, on AI Studio, in the API, on Vertex. And just very practically, what you see is when you give the model a request, it can emit some tokens which we denote as thinking tokens, so some text that it emits before it then commits to a final answer. So the way we can really map that back to my comment about test time compute is actually what is happening within this thinking section. This thinking section is a period of time and compute that we give the model to be able to run many forward passes. Each forward pass is a single token. It can generate up to tens of thousands of forward passes if it really believes that it needs to. So 10,000 times more test time compute during this thinking stage before responding. And the thinking stage we really can think of as a relaxation of this test time compute bottleneck such that we can just inject a lot more test time compute and then hopefully be able to come up with a better response. And how a lot of people ask, how do we actually build this? What's happening behind the scenes? What is it? Is this some kind of templated thing? And actually, it's a very cool story, really. We train these models originally at their inception via pre-training to predict the next token. And this is pre-training. I'm sure many of you are aware of it. And then we have a reinforcement learning stage where we take the model and we train it to solve different types of tasks. And we train it with reinforcement learning where it gets a positive signal if it completes the task correctly and a negative signal if it doesn't. And during this reinforcement learning stage, what we found is you can just allow this thinking stage to be present. You can allow the model to spend compute during its thinking stage and purely just get a learning signal from the general reinforcement learning recipe which just says solve the task. And what we found is that actually that was enough already for the model to learn to use thinking in interesting and novel ways. So we train it with a very general recipe. We find the model learns lots of strategies. Here's an example of like, I think probably the first time within the Gemini research group we saw an interesting thinking strategy which really clarified to us that we didn't need to make thinking an incredibly templated or structured thing with human priors, but we could try and learn it end to end with reinforcement learning. This was just an example of a particular kind of math puzzle as kind of a predict the next number. And what we thought was interesting was the model kind of had a suspicion of maybe the underlying formula but then once it decided to kind of self-verify, it started to apply this to the existing numbers, it found actually that the formula didn't hold. So it says it in token space. It says it as text that you can read. This formula doesn't hold. And then it says let's try another approach and it explores a different path. This was very exciting for us because it wasn't super clear whether or not we would have to be enforcing a lot of structure in terms of branching when the model should try different paths or have a lot more kind of like intervention with how this works. But just a very general recipe, solve useful tasks, and allow the model to think during reinforcement learning we were seeing some emergent behavior. In fact, right now within Gemini we see a lot of interesting thinking strategies and you can also see them when you use our thinking models. The model can often decide to break down problems into subcomponents, explore multiple possible solutions, kind of like we saw before where it tried multiple approaches. It can, for example, with code, draft fragments of code and build them up compositionally, which is something that we like to do as software engineers. It's a very useful strategy. So modular kind of composition of thoughts and also just do very like kind of pragmatic things. Perform intermediate calculations. If you're summarizing a very large financial document and you need to aggregate numbers from many, many different places and you actually might want quite a lot of tokens to be able to grab all the relevant pieces and compile them and perform all the intermediate calculations. And crucially, also use tools. So the model, this is an area of active development, but we want the model to be able to execute code and core search, potentially to ground its answers during its thinking stage in order to become kind of more correct, more factual and things like that. So crucially, we train the model end-to-end with reinforcement learning. We find the model learns thinking strategies that are quite interesting and non-trivial, but crucially, it's all about using more test time compute to give smarter responses. And what does this give you? So I've kind of been alluding to it already quite a bit, but I think, you know, capability is always something which people appreciate. If we can improve capability, reliability of the model, this is always useful. So the first thing of why it's interesting, more capable models. We're finding thinking is enabling a real breakthrough in capability. So, and I actually really want to point to not just thinking, but actually the composition of paradigms, the main paradigms that we've seen working and that we're actively scaling within Google and within the Gemini team. And just want to make a point that there are several paradigms, and they're actually all multiplying together right now for what is truly an amazing time to watch AI unravel, because they're multiplying together, and this results in faster and faster progress, which is very relevant to you all, because if you're going to invest in using these models, you might want to know, are things going to plateau or speed up, and this is one angle. So scaling more paradigms. I think 2019 was really the advent of pre-training paradigm kicking in, where scaling data and model size was really starting to work, and people started to get a sense of how does this improve model capability, and what's the kind of pace of improvement. I think there was an uptick in usefulness when we could combine that very effectively with post-training, and especially scaling the quality of human feedback and using reinforcement learning from human feedback effectively, such that models could follow instructions. This really led to kind of, initially kind of what you could say is like the chatbot era, but is otherwise models that actually have decent instruction following era, and that has been very useful. And we feel like thinking is a new paradigm, where scaling, test time, compute, stacks on top of all these three, and we don't really feel like there's a kind of an end in sight at present with any one of these three. We're actively working on them as fast as we can, and I guess the exciting thing is, yeah, they multiply, and as a result, we are seeing models improving capability faster than we've ever seen before. So within research, we often target academic benchmarks, and I'll move on to some more real-world benchmarks later, but we like to track core capabilities in coding, math, science, and many other capabilities. But just to kind of comment on the kind of rapid progress in reasoning capabilities in particular, we saw that for a bunch of benchmarks where the model was kind of like a C or a D grade student back in December, so that might be 2.0 flash, which was one of our flagship models, and December wasn't very long ago in real-world time. And some of these benchmarks have kind of been idling, especially things like the math AIME 2025. The model is starting off not exactly from random, but it's really failing the test, and that's been the case for all model development up until that point. And then we kind of see a takeoff in performance, and it's not just helping math, but it's also especially helping code, which we're very excited about, as well as other topics. So that's kind of just a visual of the rate of progress. But actually, another way we could transform this plot is by actually having the amount of test time compute on the x-axis. So it's not marked very clearly, but the main point is it's just an x-axis, which is log scale, which is the amount of test time compute of these models. And actually, behind the scenes of this progress, we're seeing a lot of the investment is from scaling up the amount of test time compute available to these models, which translates to an almost kind of scientific phenomenon of a kind of a log linear relationship between test time compute and model performance in these benchmarks. So I've talked a bit about capability. I think another thing that's just very pragmatic is steerability. So we want to empower developers to be able to steer the appropriate cost versus quality of their models. You know, so going back to just the prior state of having a selection of model tiers, that allowed you to be able to pick a particular model that has a particular cost and a particular capability, and you can iterate through this kind of finite set and try and choose something that's appropriate to you. However, what if actually the sweet spot for you would be kind of between two model tiers? Or what if the best model tier just isn't strong enough and you would, if you could have, like to have spent more to see if you can kind of try and tackle a real-world use case that is eluding current model capabilities? So thinking is actually very useful in this state, and this is something that is coming soon. Thinking can and should allow the user to have more fine-grained control over cost versus quality. Right now, the model is trained end-to-end with reinforcement learning to think to solve a task, and it decides how much it wants to think for a given task, and you'll have probably seen already two things. One, it does spend a bit less time thinking for simpler tasks and more time thinking for harder tasks, but it generally overthinks, and also this isn't something you can control with a budget. So we want to, and this was also announced by Sundar, but coming soon, we will have steerable thinking budgets that will allow developers to kind of set kind of maximum thinking budgets and thus be able to have a slider over cost to quality. Oh, I think I just said really what the, yeah, so, you know, coming soon, there'll be, you'll be able to switch thinking off with any model. You'll be able to enable it to kind of think automatically and dynamically depending on the problem a little bit as we see today with models like 2.5 Pro, but we would like to be able to have a steerable thinking budget. So this is coming very soon. Oh, oh, sorry, people are taking photos and I skipped, sorry. Can I go back? No, I don't know how to. Because the steerable thinking budget isn't launched, so we didn't have like amazing demos yet, but I'll just give a flavor of kind of, you know, an example. So it's same model and we can, you know, set different thinking budgets on this particular cryptic crossword. Cryptic crosswords are very popular in the United Kingdom. I don't know actually how popular they are elsewhere, but in this case, it was just sliding the thinking budget allows the model to explore this very simply defined but kind of elusive cryptic crossword where it has different strategies in its thoughts and it just turned out that sliding this up to 16K, the model, was eventually able to figure it out. That's just grounding the example of thinking budgets, but honestly, it's going to be much more exciting when we launch this so we can show a lot more kind of real world use cases. So I've talked a bit about, you know, why did we want to invest research in thinking? How does it generally work? What should it bring to you? And I'll just talk a little bit about 2.5 Pro because 2.5 Pro is our latest model and part of the reason it's so brilliant is because it employs thinking to solve tasks. You know, I'm going to just go through some highlight reels of why it's a great model. You know, we find that it's very strong compared to competitors on a bunch of academic benchmarks, but, you know, everyone can evaluate on these benchmarks. You never really know exactly how held out they are and things. We also find that they do very well on a bunch of, like, externally verified, more real world benchmarks so this one is a really good one, Ada Polyglot, which is like a code editing benchmark that we don't run and we don't have access to, but they run it. We actually, to our surprise, found, you know, 2.5 Pro generalize very well to real world coding tasks. It's not all kind of math or competitive coding or logical puzzles or cryptic crosswords. We scale, do a really nice set of these held out evals. They call it the SEAL leaderboard. We found 2.5 Pro is really, really good across things like tool use and also this humanities last exam dramatically named, but it's kind of like a extremely difficult kind of fact-finding, question-answering task which may require some quite specific scientific knowledge or mathematical knowledge or it may require specific, like, you might need to do a bit of a literature review before you even can figure out how to solve the question. And also, actually, in some things which are kind of in the core focus area of a lot of the researchers like math capability, we saw kind of signs of, like, a new kind of step change in performance for a new era of, like, even harder math tasks. So, the USA Mathematical Olympiad, this is the first model which is getting a non-trivial score. And, you know, it's still 25%. It has a long way to go to 100%. But, as you've probably seen, you know, usually when you can get a line going up to the right, it keeps going up to the right. So, we feel very excited about this kind of models which can actually approach truly expert level in technical domains. And here is another kind of real-world leaderboard that, you know, it's held out and it's run. This is the dev in agentic code. So, thinking and agentic performance is something we're very excited about. You want to think deeply about what maybe problem you want to solve but you also want to combine this with rich agentic interactions to be able to actually go and do and use tools and manipulate code bases and things and actually do more open-ended tasks. So, this is an incredibly strong model for that. And, yeah, I guess I got some kind of like examples you know, using this in AI Studio in this case. This is like creating some collab code to kind of create a visualization of some data specified and, yeah, I guess it kind of decides where to get the data and what libraries to use and then writes a bunch of code which is kind of one-shots a large amount of code and then if you paste it into collab, you get this kind of nice data analysis visualization. So, this is a cool one. I find myself spending a lot of time trying to leap from an idea of what kind of data I want to visualize to actually like writing plotting code and things. Having a model which can one-shot it is very useful. Another example, this one is not a video but I thought it was very cool. It was from some people within Google DeepMind playing around with a model but they found you could pass the model the DQN paper which is kind of a seminal deep reinforcement learning paper from DeepMind and kind of fun fact actually, the last time I was in a casino was in 2013. I went to Tahoe and I went to New Rips 2013 and I saw DeepMind presenting this paper and so I guess that's 12 years ago which now we have a model which can kind of probably, you know, this task it may have seen code bases where they implement some of the algorithms but crucially you feed it the paper it implements the algorithm it runs a training curve and it can even run a little environment where it plays Atari as well so you can see the model playing Atari and this kind of it does I think with five kind of vibe coding interactions so it's pretty much just like from paper to the algorithm and the whole demo which would probably take a regular research of weeks if not maybe even order months to get all that right is kind of incredible and then kind of a you know, it's not always about coding and math it was just kind of a funny one write a poem about Jeff Dean but you know in the style of a villanelle a villanelle is like a very technical poem for those that like their poetry I thought it was just kind of funny you know, like we don't really train these models to write poetry with reinforcement learning but you know it spends a lot of time thinking what things should I write the poem about for Jeff like should I talk about Bigtable and TensorFlow and then I'll try a few ideas on what should rhyme or like what the structure should be and it's kind of just funny seeing a lot of this stuff generalized to creative tasks as well we're actually part of the reason we called it thinking within Gemini is because we don't want it all to be about reasoning we do want these models to be more and more useful for actually maybe long form useful creative text generation and things and it makes a lot of sense that you might want to plan out what you want to write before you actually commit to it and then one final demo this one is quite an involved web app it's like a cake design visualization kind of web app I actually I don't actually know how long this takes to write the code because it's quite involved so I'll say something interesting but in general I think right now people are very very excited about this notion of one shotting or like few interaction vibe coding quite complex web apps especially web apps often have a lot of like boilerplate code and it takes me like a very long time to re-familiarize myself with all of the different like up-to-date libraries and things like that so it's actually often very useful even if you know you eventually do want to take the reins and actually steer it to exactly what you want to build to even just have something one-shotted at the beginning that gives a very rich template of something and then you can like retweak it or like build upon it or even maybe go through another few interactions to keep defining it I thought this was a really cool demo and I didn't know why Sundar didn't choose it but I'm now seeing maybe why because it's a very it's really spending a lot of time writing a lot of stuff so actually I guess one thing that I'm going to do as a preview one thing that we do really care about with thinking going forward is we want these hooray it made this thing so it's kind of like a complicated app I mean once you start to go through it it has a little like 3D view it has a like little different pricing things I think it scrolls down at some point it can like estimate the pricing based on a bunch of criteria yeah so this seemed like something that would take me quite a long time to build unless I was like super familiar with this so I thought it was kind of a cool example takes a while but hey a while these days is kind of like a minute or two if I had to live code it now you would be here for weeks yeah and then it's kind of fun you can like kind of further interact with this and update it and I don't know what it's doing oh there we go and it's going to go off thinking again to be honest I may just say goodbye to this cool one goodbye cake design visualize more cool stuff happens but I'm going to just say goodbye to it if I can so yeah what's coming next now we do want these models to be you know using compute and using like your API credits as efficiently as possible so we want actually I'm going to start from the bottom and go up we want thinking to be more efficient that's really important you probably have if you've played with a thinking model right now you may have seen it kind of overthinks it kind of sometimes goes around in loops a bit it feels a little bit wasteful so trying to get the capability for better efficiency that's a general theme within Gemini we've always been at the forefront of kind of cost to performance Pareto Frontier we do a really good job on that but we want to drive it and make it a lot better especially in thinking okay going up steerable thinking I already mentioned we want it to be more user controlled this is coming soon and we would like that to be the case for all models and all models in future really in an ideal state you have to think about thinking as little as possible they won't be separate Gemini this versus Gemini thinking it will all be natively built into the one model and you can enable it if you want you can switch on thinking you can set a budget if you want or you can switch it off and then crucially what's next for thinking we we're very excited as researchers to have these models spend more and more inference time compute to discover levels of capability that pull us into the future and make these models be able to be even more useful and amazing things so that's definitely a large priority as well more capable models I think you should expect our Gemini series to just be rapidly improving this year it's going to be quite amazing to see and that will be an ensemble effort from everyone within Gemini but especially in thinking we want to help this thing be better at reasoning and also branch it to more kind of open ended tasks great so that's the end of my talk there's a thing where you can give feedback I didn't add this slide but I am very appreciative of it because that feedback is always very welcome thank you applause to dwell thanks to thereby coming to the endפ pedalsarina isn't enough to hang over there's not anybody gone gate that's the bottom door you right I amuks albo that's bad me you you you I am3000 I am a good level I do I amtha