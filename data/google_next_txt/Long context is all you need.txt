 This session is a recap of my presentation, Long Context is All You Need for Google Cloud Next 25. So to give you some background, what is context? Context could be a current prompt itself, or it could be prior interactions with the user, or data provided by the user like PDFs or videos, or it could be background information like Wikipedia web pages. The context is essential for the model to understand the user's intent and generate relevant responses. A context window measures how many tokens the model can process at once. The challenge that we covered in the main session was the challenge of missing context. Without context, LLMs rely on in-weight memory based on the training data. So that's basically the knowledge that they were able to memorize during pre-training from the internet. But in-weight memory has limitations. One limitation is obsolete information. While some information may be correct at the time of training, it could become obsolete or incorrect at the moment of inference. Another challenge is rare or private facts. In other words, information not well represented in training. And long context is a solution to those challenges. It enables users to easily provide as much background information as they want. So this removes the need to cherry-pick which information to include and allows the model to rely on up-to-date web search or corporate knowledge with appropriate access controls. As a result, LLM can give more relevant and helpful answers as opposed to hallucinated or generic ones. On this slide, I'm covering a range of models that we are providing. These are not all the models, just the most important ones. So I'm showing four models, Gemini 1.5 Pro, 2.0 Flashlight, 2.0 Flash, and 2.5 Pro. All of the models except 2.5 Pro are in GA, which means that they are generally available. And 2.5 Pro is currently in preview. We normally serve Pro models with 2 million tokens and Flash models with 1 million. 2.5 Pro is a bit of an exception. Currently, we serve it with 1 million. But 2 million is coming soon. The token output limits were about 8,000 before 2.5 Pro. And for 2.5 Pro, it's currently 65,000. The speed of sampling is normally medium for Pro models and fast for Flash and even faster for Flashlight. And the context caching is supported for 1.5 Pro model and for the others, it's coming soon. It's not only the length of the context that matters, but the actual accuracy. On this slide, I'm showing the comparison of 2.5 Pro with respect to four strong baselines like O3 Mini High, GPT-4.5, DeepSeq R1, and Sonnet 3.7. There are four benchmarks. MRCR, LOFT, LongBench V2, and Fiction LiveBench. So the results for our model are in the first column. And the higher, the better. And for the others, I'm going to highlight the best out of them on each benchmark in red. So you can see that there is quite some gap in performance between 2.5 Pro and the best out of the baselines. For example, for Fiction LiveBench, we are going from 64% for the best baseline, which is GPT-4.5, to about 90% for 2.5 Pro. If you haven't yet tried our long context models, please do in AI Studio at no charge. And please head to ai.dev for that. And get API key there. Thank you. Thank you.