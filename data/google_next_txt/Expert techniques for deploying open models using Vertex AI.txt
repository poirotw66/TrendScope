 . Open models are on rise. Deploying them can be challenging when you are building your generative applications. Today I'll be talking about expert techniques for deploying open models on vertex AI. Hi, I'm Rajas Thalam, Solutions Architect from Applied Agingering Team, part of Cloud AI at Google. I'll share with you how Yahoo email went about setting goals and making progress with open models on vertex AI as well. So building with open models starts with model discovery, exploring numerous choices of models, and model selection involves evaluating models based on needs, capabilities, and support. And finally, the model deployment requires running models securely, efficiently, and with monitoring and observability capabilities. And vertex AI model garden offers choice and flexibility with Google models or open source and third party models, which are accessible through Huggy Face integration, and also you can self-host as well along with the serverless APIs. vertex AI prediction is the backend engine built behind our model garden designed to reliably host your ML models with built-in security and intelligence. I'll share a few techniques with you today on how you can deploy them effectively and run efficiently on vertex AI. So fast deploy is our new feature to deploy popular models in less than two minutes. And we do this by caching models on high performance storage and pooled clusters, giving you a better developer experience and faster iteration. And you can deploy these models to vertex AI from model garden using CLI or UI or even SDK using the familiar G cloud commands or vertex AI Python SDK. And most of our optimizations today are baked into the inference containers like VLM, which really boosts the performance on vertex AI. So think about faster loading, less waiting, and up to 50% throughput with parallel data retrieval and smart caching techniques behind the scenes. And hex LLM is our super optimized container for cloud TPUs with continuous patching, smart memory management, and parallel processing, all for that top efficiency. To make LLMs run faster, we use two main techniques, prefix caching and speculative decoding. And prefix caching reuses computations from the previously generated text, which speeds up initial responses, especially for similar requests. And speculative decoding uses a smaller, faster draft model to predict next tokens than the larger model verifies them. And both these methods boost speeds without losing any accuracy. And they are included in our VLM and hex LLM containers. To make an open model of your own, fine tuning is the way to go. And model garden has recipes using tools like Hugging Face and Axolotl for efficient tuning on vertex AI training. And you can sell these models using optimized containers like VLM. And today we can bring you your own new feature where you can bring your own custom model weights on vertex AI, where you can just upload them to cloud storage bucket and call model.deploy using model garden SDK. And for monitoring your deployed models, vertex model garden has a pre-built dashboard in cloud monitoring, showing the usage, latency, and errors, which makes it easy to spot any problems. And you can troubleshoot any issues like capacity or errors pretty quickly. And for security, model garden's organization policy provides a central place to control any actions and model access, allowing you to fine-grained access control on the models and allowing the users to use only approved models in your organization. For cost optimization, spot VMs offer discounted compute for prediction jobs by utilizing excess compute engine capacity, providing you a cost-effective option for fault-tolerant AI workload and experimentation. And GC reservations and committed use discounts, sharing enables you to access and use of reserved GPU capacity across multiple vertex AI products, which maximizes your GPU investments. So with that, let's see how Yahoo! Email used open models and some of these optimization techniques in their favor. So Yahoo! Mail's mission is to simplify daily tasks. So Mail TLDR provides short summaries of long mails, enhancing mail management. The key consideration in their modeling journey included meeting Yahoo! Mail's scale and meeting near real-time latency and cost requirements. Initially, they started their experiments using GPT-4 zero-shot prompting, and then shifted to Mistral's seven million fine-tuning with LoRa for optimization and cost savings. And further exploration included exploring Lava and Mistral models for synthetic data generation, and QAN models for task-specific modeling, and finally, Gemini 1.5 Flash from Vortex AI for Model Garden, which they saw a huge improvement in both cost and quality. And for TLDR extraction, the hardware selection has been very critical. So GPUs offered the best balance for that, and L4 GPUs specifically provided optimal cost-effectiveness for the use case. And for inference tool and platform selection, VLM was selected for its simplicity, open-source support, and hardware compatibility. And for inference platform, selection focused on ease of use, maintenance, deployment, and order scaling, monitoring, and resource availability. So Vortex AI Online Endpoint offered the ideal managed solution, which would use the operational overhead and focusing on the model deployment or development. So the architecture, the near real-time TLDR processing architecture, involved emails arriving in an event queue, process-by-email process, which was sent to Vortex Endpoint for inference, and post-process them, and then store them in a database. So this setup handles like a 1 billion events daily, emphasizing the simplicity, scalability, and the cost-effectiveness through hardware, inference tool, and performance choices. So wrapping up, to build with open models, start with Vortex AI Model Garden for discovering and deploying Google, open-source, and third-party models, and utilize Vortex AI prediction for robust hosting, auto-scaling, optimizing containers, optimized containers like VLM, and inference optimization techniques for inference on GPUs and TPS. And Vortex AI is enterprise-ready out of the box. You get security and other capabilities right built in. So I'll leave you with the resources here, with you to navigate building with open models and continue your journey. Thanks for tuning in and taking your time. So I'll leave you with the resources here. Okay, thank you. Thank you. Thank you. Thank you so much. Thank you so much. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.