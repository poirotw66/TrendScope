 . Please welcome GitHub Chief Product Officer Mario Rodriguez. Hello, hello everyone, and thank you for being here. I want to maybe start with a reflection. The way that I think about things. You know, the person who created the steam engine wasn't thinking about rocket ships. The person who created the transistor wasn't thinking about electric cars. And the person who created the first computer wasn't thinking about AI native software development. But as I reflect on that, I think you should be. Because it's here. Hi everyone again. My name is Mario Rodriguez, and I'm GitHub's Chief Product Officer. And today I want to talk to you about the future of the SDLC. At GitHub, we're the premier open source developer platform, and we believe that software is at the center of human progress. Medical research, space exploration, education, communication, agriculture. I mean, software touches it all. And the only way we're going to accelerate human progress is by giving our community of devs, 150 million strong, the tools they need to innovate. So, with every product launch, every upgrade, and every new feature, it's with the goal of meeting your needs today and tomorrow. Software development is currently undergoing the most significant transformation since cloud computing. And I mean it, it's significant. And we're committing to building that with you, not just for you, but with you at every single step. I probably don't have to tell you this, that this space is evolving at the speed of light. You're seeing it every day. You're feeling it every day. I mean, for me, it's every time I pull out my phone, there's something new and something more for me to explore overall. So, these shifts, you know, they used to center around where code runs, like at least in our industry. You know, if you think about the cloud transformation as an example, right? Well, today is actually all in one. It's about where code is written, how it's maintained, where it's running. It's kind of collapsing all in one. So, essentially, every single part of the developer life cycle is evolving today. So, with this AI revolution comes new opportunities, right? And new challenges for us as well. If you think about dependency management, about the integration complexities that we deal with, about scalability, the list goes on and on usually on this. And the only constant right now is that change. So, what are we doing about it? Where? We're here to make your code easier to write. Not harder, easier. And here's how we're going to do that. By going all in on AI native. But before I get to that, I want to share with you GitHub's world view during this transformative moment. Essentially, we believe, number one, that in the future anyone, and I really mean everyone, can create software. From an eight year old who wants to build a gaming app, all the way to a mother of five who has only a few minutes a day to spare. Overall, to bring that idea, something from an idea to production. And now, we believe that programming language shouldn't be a limitation for these people to enter and create. Number two, the software needs to be scalable and high quality and secure by default. You probably have heard a lot about by code before. Well, if you do that and your software doesn't have high quality and everyone can go and hack it. Well, that's not software development, right? It needs to be secure by default. And number three, that AI would increasingly play a role not only in how software is built, but also how it works. And all of this through natural language. So, in other words, software itself will become AI native. Now, I know that the term AI native has been thrown along a significant amount. So, let me just break it down for you on what it means to us. So, I'm a proud girl dad. So, if you take my daughters, there's Thea is at the top, Lexa is the one, the little one. So, they're ages nine and seven. They still love for me to cuddle in bed and all of those type of things. Now, my girls are native English and Spanish speakers. From the moment they started babbling, they began to oscillate between English and Spanish. Like, it was kind of second nature to them. Now, if you fast forward, my girl now, like I said, are ages nine and seven. And recently, I've been teaching them Python, more to the older one. And now, this is a new language for them, right? It's foreign to their growing minds. They learn it all, but that doesn't mean that it comes naturally for them. It kind of takes a lot of work. So, we need to think about AI almost in the same way. You know, in the past few years, we've become accustomed to things like Siri. Like, and I will call Siri an AI-infused system. And just like many people go in and to be able to learn software development through YouTube, as an example, these AI-infused systems use AI to augment the existing functions that you have. And don't get me wrong, it's helpful to us, but it's learned and it's not organic, right? So, there's another layer to explore there. So, you know, in Siri, we used to play music to send texts. And sure, you know, back in 2011, it was very much like, wow, but now, now you just expect it. And it just works. There's nothing else left to your imagination, right? Now, on the flip side, I think AI-native, not AI-infused, AI-native systems are like a native speaker. AI is at the very, very core of the system. So, it's inherently more fluent and capable of predicting the executing tasks. So, it kind of gets, you know, goes to the next level. And we haven't even scratched the surface of what it could do with natural language. This is the beauty of what we're living today, is that with just with natural language, you could achieve things that were not possible before. So, as we step into this new era, we're working to transform GitHub agentic experience then from that AI-infused, like I was talking about, to that AI-native. Something that is native to GitHub, and it's native to the SDLC overall. And this is where it gets really cool for us, right? We're thinking about GitHub not only on that collaboration platform, but across the entirety of the SDLC. It goes into creation tools and what we're doing with in VS Code, in IntelliJ, in Eclipse. It goes in that collaboration platform across issues, PRs, code spaces, discussions. And it goes all the way into cloud hosting as well. So, from creation to deployment, we're envisioning an SDLC powered by an agentic layer that spans that inner and outer loop. Where agents kind of work synchronously and asynchronously with you as active collaborators every step of the process. Now, what makes this AI-native is that rather than you navigating to all of these features, agents are going to start meeting you where you are. So, wherever you're going to go and execute a task, an agent is going to be there with you. It's going to be proactive and it's going to be reactive. And it's going to react at the right time. Here's what I mean, too. So, if we fast forward, look at this set of developer experience. Sense making, writing code, debugging code, writing test documentation. You know what I'm talking about because you do this every single day. We aim to make each one of those experiences agentic from the moment they start. So, earlier this year, we unveiled the preview of GitHub's co-pilot new agent mode. And last week, we rolled out agent mode with MCP support to all VS Code users. Now, in agent mode, co-pilot iterates not just on its own output, right, but the result of that output. And it will iterate and iterate and iterate until it accomplishes the task. So, you can start by just saying, do this application for me as if it's something very simple. And it will go and say, you know what, to do that, I'm going to have to install these dependencies. I'm going to have to go update a set of environment variables. And it's just going to walk you through it. You don't have to go and define each one of those steps. But it's there. And agent mode is there at every single one of them helping you. So, at GitHub, we're also about developer choice. So, in agent mode, you can choose models that you already use today. So, for example, you can use from Anthropics, Sonnets, all the way to Google, Gemini 2.0. And I think just recently, we just got access to 2.5 Pro. So, thank you, Google, so much for that. I think that's a great model for coding, specifically in the back end. So, we're very excited about that. But instead of me telling you more and more about agent mode, what about I just then show it to you with a quick demo? Is that good? Is that good? Now, all of this is going to be live demos. So, and you will see. Are we here? Perfect. So, I'm actually going to start in Spark. And this is an application that we launched last year, still in preview, that just lets me do very quick games. The other day, my daughter woke up and is like, Daddy, I want to go. And I think it was play Nintendo or watch some TV show. I'm like, no. Like, we need to do something instructional. It's too early in the morning. So, I did a quick app for her that was a math game. And she played with that for like around 30 minutes, as an example. And then after that, she's like, okay, I'm done with this. And then one of her friends was already at the door. So, I was able to not have her spend time doing other things and doing something instructional overall. So, you can see it over here for create math game. And if I click on it, it's very simple. Actually, this was the one for the little one. I did the other one after that. You know, 44 minus 41. So, you can just say three. And it says, yeah, you got it right. Now, another thing that I also do, and since we're in Vegas, I actually teach my girls how to play poker. And I know what you're thinking. I should not be doing that. But let me just tell you something. Poker is a great game to teach kids because it's all about, you know, what are you going to do about prediction? And what are the odds of things happening? And what is luck? And are you lucky or unlucky? And all of those type of things. So, I did a very quick five card draw poker game for them as well. So, you can see it over here. What should I do, you think? This is a pretty good hand, let me tell you. So, maybe the eight and the ten. And draw. So, there you go. I win. The ace is the high card. And then after that, the queen. So, there you go. So, they just play this. If I was going to do this and wait for a developer to do it, like there's many apps in the app store, but they all have, you know, these ads and this complexity that it adds. And I don't want them seeing that. I don't want them to become gamblers, right? What I just want them is to understand how to play a simple game and understand luck and also probability at the end. Now, we also have a little bit of fun, too. I'll show you one more. I just recently did this one. And they're very much into Mario Kart and Mario Bros. So, I did a Mario World for them. Again, something that I cannot just buy in the store either. And you think about Spark as an agent that allows me to do these apps. Even though I am not a professional developer, I do mainly product for a living. But, you know, I could code. Let's see. Maybe let's say happy. I got one of them, right? Let's see. Let's test your these tiny friends with mushroom hats offering Mario on his adventure. You know what that is? I know what that is. What does it start with, the answer, you think? I think it's Toa. I think it's that. There you go. So, again, this very simple applications that you could end up doing and then have fun. And then from an educational perspective, it works pretty well. So, the other thing, and this is more into what we want to be doing now, is I travel a fair amount. So, what I wanted to do was build a quick app then. And since we are Google's conference, we wanted to do a photo location calendar app. It's just a plain Flask and JavaScript application that takes a photo and captures my current location, saves that to Google Photo or Google Drive, appends a file name with it, creates a Google Calendar event, and then adds the photo to Google Map. And then at any point in time, if they want to see where that is, they could just go into the calendar and then click on it and then see a picture of me. Again, something that I'm not going to buy in the App Store, but it's pretty fun to build and share with them. So, let's try to actually build this. So, if we go here, what I want to do is... So, this is VS Code. Whoops. Let's restart it really quick. Hopefully, the Internet is going to be okay with us. Okay. So, what I'm going to do here is I'm in a code space. So, this is a cloud development environment, again, in the cloud. So, it's not running locally. And it just allows me to do very, you know, without having to pollute my machine with all of the Python stuff and all of these packages. I could just do that in the cloud very easily. And on the right-hand side, what you're seeing is our Copilot functionality. And let's go into agent mode. And what I want to do is just tell it to create the application. So, I'm going to tell it, create the application and subscribe in the readme. Use it as a PRD, which is a product requirement document. Please keep it simple. The map and open the camera is the most important item. Make sure to install the dependencies and run the app at the end. I could not have to say the entirety of that, but for this demo. So, now what Copilot is going to do is it's going to analyze that readme file. And it's going to go and figure out what is the next step. So, it's search the code base for a Pi Plus application. It probably is not going to end up finding that. It's setting up my requirements. And you can see how that is now a new file. And here is everything that it's using. So, I'd really like this. So, let's keep it. Now, it's going to think a little bit more and try to figure it out. Okay, now that I have a set of requirements, what else should I create? And, you know, this is non-deterministic. So, it might go ahead and create the application itself. Or it might choose to go in and install some packages. So, it's going to take a little bit of a second here. And I do want it to go after this one. There you go. So, it's going to create the application now. And it's going to generate those edits. You can see in app PY here. I could keep it as well. And now it tells me I want to create this directory. Again, I didn't tell it that. But agent mode decided this is the logical next step. I'm going to continue. And it's going to go and execute that. Then after that, it's going to use that template and start creating my application together with the HTML and with the JavaScript that goes around with it. And I wanted to get past this one for you to see it. And then I'll go to one that's already built. And then we'll take a picture together. What about that? So, this is the thing with agent mode and a live demo. Okay. There you go. It's creating the index.html file. This one actually goes very quickly as well. I just wanted to show you, like, I'm not lying to you. This is kind of live. It's going to now create the environment. We're almost there, team. We're almost there. And then after this environment, then we install some packages. And then we're done. But let's come back to that later. Here's one that is already done. And you can see how I'm live on it in the photo calendar app. Let's actually take, and it's already saying that it's in Las Vegas Strip. So, if you all want to say anything. Woo! We took the photo. And we're going to go ahead and save it. Now, it's saving that into my Google Drive. Successfully save. And then if I go into my family planner, then you're going to see right here the photo that we just took. And then maybe some of the other ones that I have been doing. Maybe we'll check out this one here. Actually, we can do that later. But you can see how it kind of takes the application and then kind of renders this in a very rich map as well. Let's come back into the other code space. And by then, like I told you, it's going to want to install the requirements. I just want you to see how this ends up finishing, too. And now it should ask me. The last step of this should be run the application with a plus command. And then that's it. There you go. Continue. It should open up a port now in Codespaces. After it runs the application, here's the port. And I'm going to open in Browser. Now, probably I don't know if it will work in just one shot. Sometimes you have to iterate on it. But let's give it a try and see what it ended up doing. And there it is. It does have already for me to be able to take that photo and then save it up. So maybe we do this. There you go. There you go. It works. Let's go back. Now, that was just one example on how you can use agents interactively. But what about tasks that we want Copilot to solve, let's say, asynchronously, right? And that's where Project Padawan comes into play. We're also building an autonomous suite agent. And the way that it works is you just go to an issue and you assign it to Copilot and Copilot takes it from there. Again, with Project Padawan, what you have to do is there's an issue with what you want to build. I could have done a PRD document of that application. You assign it to Copilot and you produce a fully tested pull request at the end. Once that task is finished, Copilot will kind of come back to you and say, I'm done. And then you could go on a loop on addressing feedback. Maybe you have some more feedback in that pull request and it will react to that, too. So in a sense, what we're doing with Project Padawan is you're bringing Copilot as a contributor to every repository on GitHub. So just imagine that. It's a fully personalized, proactive, and ubiquitous agent embedded into every step so you can spend more time doing what you love. And what you love is building. So that's where we're headed. Now, we aren't just talking about the future here. Like Project Padawan is something that is coming. But right now, we also have some other agents that are already in production. And one of those is Copilot Code Review. We introduced it last October. And what it does is it gives you instant feedback on your pull request. Many of us who are developers, whenever we do a pull request, we have to wait for someone else to go and review that. And sometimes it could take, you know, minutes, hours, or even days. Well, with Copilot Code Review, it automatically analyzes that pull request, spots bugs, enforces best practices, and then it provides you actionable improvements, suggestions as you work. So this frees you out of time. Again, like it's all about that productivity. It frees you out of time to focus on higher level tasks and problems. So in essence, Copilot Code Review stream slams your workflow, but also helps you deliver more robust and maintainable software faster. Let me just show it to you really quick again in a demo. And this one will just kind of click through it. Let's go back into it. So over here, what I have is a repo. This is where we created the application. And you can see a set of pull requests as well. I'm going to click on number three. And you're going to see one thing. You're going to see Copilot saying, hey, this is, I went and reviewed the code. This is what I think the application is. But more importantly, it gives you this. It gives you suggestions. So in this case, login request headers and body makes both sensitive data in production. Please remove or restrict this by login detail. Right? I may have missed that. So now I have someone giving me instant feedback on how I should be doing this. Not only that, it also telling me, you know, running the Flask app with debug 2 can expose sensitive information as well. Something that many times people miss. So, again, you're getting this person that is giving you actionable feedback at every step of the development lifecycle. I'll show you one a little bit more complicated as well. Actually, before we do that, I'll do one that is about security. In this case, you can see over here, we were doing a Travel Guy admin page. And we had Copilot review this code. And one of those things is by our advanced security product or code scanning product. And in this case, it actually found a security vulnerability. But it's not only about finding them. Like, we want to play this game of found means fix. So we also allow you to understand how to fix the problem by telling you a suggested change set into it. And just with commit suggestion here, you could just commit that back into the repo. You don't have to waste time, you know, doing a clone and then figuring out how to resolve the vulnerability. And this is another one. I really like this one because it gives us an ability to see best practices. So in this case, one of the â€“ this comment was generated based on a coding guideline of this repo. And in this case, we don't want 60. It's not clear. So we want to specify the unit specifically. So in this case, seconds. So we're going to be doing things like don't use numbers. Use a constant as a meaningful name. Or things like this, which is the variable last name is used before it's initialized. And just finds bugs, finds coding styles, and it's there constantly helping you. Now, that's just the tip of the iceberg. I could probably spend all day today showing you kind of these new agentic tools and features to help you build faster, smarter than you ever thought possible. I know I use it every single day, and I'm getting better and better at prompting AI. But I don't want you just to take it from me either. So with that in mind, it is my pleasure to welcome Verily's former head of engineering platform and current and distinguished engineer, Stas. Welcome to the show, Stas. How are you doing, sir? Thank you for having me. Okay, so Stas, tell us a little bit more about Verily and what you do. So Verily is an alphabet company. We're a healthcare company. We're kind of a blend of technology and healthcare with a lot of AI and data science thrown in. We have multiple products. I'll just mention three. A platform for clinical studies. We test wastewater to detect viruses such as COVID or avian flu. We also have a platform to manage diseases for people like you and I, regular people, for example, diabetes. And all of that runs on a common technology and data platform. Yeah, I mean, I love this. I mean, talk about accelerating human progress. So I'm sure everyone is probably thinking, okay, where does GitHub fit into this? These are amazing products. So what's behind this? So we came out of Google, and we had to choose a tool set for our developers that gives them flexibility, gives them productivity. We also needed to stay compliant because we are in a highly regulated industry, so compliance is paramount. And so we chose GitHub. That was a few years ago. We've been very happy with our decision. GitHub gives our developers the freedom they need to innovate. Yep. Now, I think I heard that you have an announcement to make today, and I already kind of know what it is, and I think it's pretty cool. So what about we let them know? Yeah. Well, so that's right. It's the Verily Developer Assistant, VDA or VIDA. So VIDA is a VS Code plugin that allows developers to invoke commands from within the IDE. So from within the IDE, you can perform commands that invoke applications and get to various data sources outside of it. And it can also invoke Copilot. And we're not using Copilot for the capabilities right out of the box, such as code generation. We use it for very specific commands that require our very specific context. Yeah. So it's super highly contextualized then at the end. That's right. Exactly. So, I mean, like with Copilot, that's what we envision, right? We want it to be ubiquitous across the development lifecycle. We want it to be conversational with the power of natural language, but we also want it to be personalized as well. So tell me a little bit more about the motivation or what's the spark that ignited the vision of VIDA? Yeah. So most of our post-merge activities are automated, and we're realizing the productivity and efficiency of that. Now, we wanted to get some of that goodness in the pre-merge world. And we started thinking about it. And as you know, developers spend a lot of their time outside of ID, looking up information, looking up playbooks, getting data they need, making decisions. Some of those are deterministic. Some of those require a lot of human judgment. And so the idea was that let's try to keep developers inside of the ID. Let's reach out to all of those data sources, get the data we need, right? And we started calling that commands. And then let's string them together into playbooks. And it only took about one second to realize that we could also call into Copilot to make some of those decisions that require human judgment. 100%. So, I mean, at GitHub, we have this phrase that says, memos, not demos. So what about we see a demo of it as well? All right. So I have to explain what we're about to see. So I recorded this last week. There's going to be three videos, three demos. The first one is quite simply we have a requirement specified in the JIRA issue. And from that requirement, we're going to get to an implementation plan using Copilot for this purpose. Let's roll. Let's play the first one. Okay. So we're going to specify the JIRA issue. There's some other niceties we do here, but let's ignore it right here. So workspace is created. And on the right, you'll start seeing Vida. So we're telling it to start. It loads the issue. And we are seeing the implementation plan. It's going to be about eight steps, including make changes to this Terraform file, add this DNS entry, which I guarantee I would forget to do. And now if you click on one of those steps, it takes you directly to the file and it tells you where to make changes. So that's pretty cool. That is indeed. It takes you a lot of time with all best practices just included. Yeah. And all you have to do is kind of get a JIRA ticket into it. That's right. So the second demo was a real scenario. As we're working on Vida, we got a request from security saying, hey, can you tell us which permissions this Terraform stack will establish? And if you work with Terraform, you know that it's not an easy question. It's a beauty. It's a beauty. It's not an easy question to answer. But we're like, hey, we have Vida. And Vida is friends with Copilot. So let's roll. Okay. So we overloaded explain. And with this, we generated this prompt, super simple prompt. But within just seconds, we're going to get our answer. And there it is. Okay. There it is. Okay. So my third and final illusion. No, this is not illusion. This is real. Is, again, a very real use case. I mentioned we're a highly regulated company, which means we need to have a lot of documentation. And we store all of the documentation in GitHub. And that includes requirements, test protocol, test themselves. Also internally, we just kind of got into a habit of storing a lot of stuff in GitHub, including our architectural decomposition. It's all described in GitHub. So that's thousands of various files. They're all structured, but they lack schema. And so it's easy to make mistakes. So the idea was, let's generate a schema. We can use it to validate. And we can use it for some UI niceties, such as tool tips. Okay. So let's see that in action. Okay. So in this case, we're looking at our architectural decomposition file that has our systems. So we're going to ingest some stuff from the web. And we're going to feed our files to LLM to generate a schema. And we're going to validate it. And there it is. That's awesome. Thank you so much for that. All right. Now, what comes next then for Vita? So you're using Copilot. Those are amazing uses. What comes next then for your product? So we started on this project about two months ago. So now we're starting to roll it out to development teams across the company. We believe so. We're on the journey to make people productive. So we believe that we're going to be implementing a lot more commands. And they will cover more and more of the developer workflow. And then in a couple of months, we're planning to open source the underlying plugin so that anybody can use it. And you too can use it to provide it your specific context and invoke Copilot with it. Okay. Awesome. So I don't know about you all, but I'm super pumped up about Burley's vision and what they're actually doing in here. So everyone, give it up for Stas and Burley. And thank you so much. All right. Thank you. And we'll have you back in the Q&A. All right. Thank you. Thank you. Thanks. You know, the truth is that this session was not about what I could do when you saw me do there for my daughters. It's not what can GitHub do either. It's not what Stas and Burley can do. And it's really about what are you going to do. You know, we're giving you these tools and we're iterating on them very, very quickly. And we want you to create with them. Because I know that we haven't, like, even what you are imagining hasn't been even created. So go ahead and play. The way that I think about this is we're the fuel. You're the rocket ship. So I wanted to leave this session with one thing. Think beyond. Think anything is possible. Because the future is coming. And it's coming fast. Now, before I depart, I want to just tell you, please go and visit our booth. 1640 is pumping right now. We have amazing swag and stickers and games in there. Take a peek of agent mode as well. And, of course, grab some GitHub swag from us. So I hope to see you there. Now, we wouldn't be GitHub with our community of this. So I want to also thank you for building with us today. And we wanted to reserve. I think we have 10 minutes for Q&A. And I know developers are never shy about asking questions. So we have a couple of people with microphones that can go to you. So any questions about anything, agent mode or anything related, to GitHub and the platform. Thank you so much for spending your time. Thank you. Stas, you can come back. Is this me? Yes. This is you. Hello, hello. Hello. I have two questions for you. One, you said you want to meet developers where they are. I'm old. I'm on the command line. Can you meet me there? Can you meet me there? Yes. We have command line support. We even have VIM support as well. Do you, though? We, it's a good question, yeah. So Emacs will be like the next set of things that we have. But, yes, we're trying to make it ubiquitous overall. And we want it to be productive wherever you are. If I need to put, you know, Copilot in a TV, you know, like Netflix is installed in every TV, because you expect it there, that's where it's going to be. Excellent. So we're working on it? Or it's, because it's not there yet, right? There's VIM support today. We could probably do other things that would be great. For the agentic mode? Not agentic mode. That's what I'm talking about. Oh, I got it. I got it. You know, you're pushing me to cloud code. I want to stay with Copilot. We got you. We got you. Awesome. The second question is about VS code compatibility with the Copilot extensions, because now at our operation, we're running Rails 7, which is now no longer compatible with VS code 1.99, and I've got to roll back to 1.98, which means now the Copilot chat extension no longer works, so my VS code is bricked, so I need terminal support even more. Okay. Makes sense. Here's my shout. Yeah, it would be great to kind of talk to you offline and try to see what we can do for that one. Love it. Love it. But I hear you. You're going to get it in the terminal. We're going to make you productive there. What other question? We got one here. Thanks. Thanks. Thank you. Great chat. Thanks. We've been playing around with Copilot, and one of the things that we're noticing is maybe the debugging side. Like, how would you rate the current state of the art when it comes to, in the development cycle, when it comes to the agent actually debugging issues? Yeah. So the question is about debugging and what is kind of the state of the art on those capabilities right now. I mean, I would answer that in kind of three phases, in my opinion. One is, we have been seeing these models get smarter and smarter at great speeds, right? So if you think about a year and a half ago and the capabilities of the models to what they are today, it's night and day, in my opinion. So it's exponential how smart they're getting. The second thing that I would also say is the context windows are getting larger, and the ability for some of these models to utilize all of the context windows appropriately, because many times, again, a year and a half ago, people would say a million or two million, but then you pass it a million tokens and it would get very confused. Nowadays, it's getting better and better with the new technology. So I think that's also helping. And the third thing that I would say is, in today's world, context is king. So the better context you pass to the LM, the better the answer you will get. So that's a way of articulating, if you actually step back for Copilot to get better at debugging, there are three things that must continue to happen. And right now, I think, depending on the programming language, going back to answer your question specifically, at some places like Go and Python, I think you will see it be very, very good. And in some other places where the tools are not passing the right context, then it's not getting the fullness of what it could do. So agent mode goes and gets confused and then tries to do other things. I think with MCP tools, this will get better, too, if you have played around with model context protocol. I think there's innovation that is going to happen around that that will get better in debugging. But my expectation is we've got to get better at passing context and design a new set of APIs that are LLM first, AI native, instead of human or tool first. If you think about it, a lot of debugging today was done in tools made for humans. And those tools expect an API that was made for the tool specifically. We don't want that. We want to create a brand new set of things that are LLM native into how you're passing the data. And then you're going to see an exponential on what agent mode can do for you overall in those cases. And again, with better tools and more context, I feel pretty good. But if you have specific feedback, find me later, and then we can kind of talk about that. But that's a great question, and thank you for that. We have one here. And then one at the back as well. Sorry, you're probably going to excuse my English, not a native speaker. I have two questions. One, I've been using the copilot out of description in the pull request. It's great, but I would like to tweak the output to internal standards. Is there an upcoming feature to customize the actual layout and the content? And the other one is we're using copilot for code assistance, but we would like to tweak it using internal guidelines and just narrow down the suggestions to make it more accurate for our internal call base. Is there something to do? Yeah. So the question is there are, it's mainly about personalization and customization of copilot. One in the pull request phase and another one during development phase in the IDE. So I'll answer the second one first. Right now there are two ways that you're customized, in my opinion. Number one is we introduce this concept of custom instructions for copilot. And those are empty files that live within the repo or they could live at the organization level as well. And you then just through plain natural language can explain what you want copilot to look at. We're also working in the future on what we call contextualization to bring in best practices, not only from the web, but from your internal best practices into copilot too. As well, there's a feature called knowledge basis that is the start of that as well. So I would say, please use custom instructions for the repo, not for personal. And start utilizing a little bit more of knowledge basis because eventually you're going to be able to pass that into context as well. On your first one, yes, there is a way today in the template for you to specify a little bit better. But we're working on more ways to allow you to do that so it's even personalized to the user at times where you want them to show. So that's also coming in the, I would say, you know, maybe this quarter or the next quarter after that is you're going to see copilot get significantly more personalized and customizable within your code base and your organization and your team. Thanks. Hi. So copilot's great. VS code plug-in's great. This is awesome. Thank you for all your work. Love that you let us switch out models. When can we switch out for our own models? And also, what about OpenAI's deep research? Can we have that for integration? Yeah. And finally, can you please tell the Codespaces people to give us GPU? Gotcha. So, three-part question in there. The first one is when can I actually use my own model? I think that's what you were asking in there. I have some good news and then some later good news. So, the now good news is with the latest release of VS Code 1.99, you're able to actually use any model as long as you provide a key to that model. So, that allows you immediately to use in agent mode and kind of in other modes as well in chat, the ability to bring your own model right there. So, that's kind of like today what you could do. So, we call it BYOK, which is bring your own key and then you just kind of specify that. Now, we only support OpenAI, Gemini, et cetera, et cetera. So, like the ones that we have, like you cannot just go and talk to something that we have never seen because the APIs, we don't know how to talk to that model in those APIs. But any of the major providers, then we're going to be able to do. The second thing that we're working, that we will be working on is what we call bring your own model instead of bring your own key. And in bring your own model, you could kind of have it so you don't have to have keys in every developer machine. And you could set up that at the organization layer and then we will go and proxy that request appropriately in there. So, that's kind of that overall. So, that's the first question. The third question was about deep research, I believe, right? So, deep research. Yeah, I think that's super interesting. I just got, we do a lot of experimentation internally. And I just got a demo of deep research for code, essentially. And we're building this kind of new code graph that is very much about LLM. It could talk LLM language, it would be the way to say it. So, deep research is going to be able to utilize that and be very creative as an example. It could scan all your code and say, all of the code bases and say, you know what? You have a lot of duplication across all of these repos. Maybe you should create a library for those things instead of having code duplication across them. Or, hey, you know that there's an open source thing, not open source, but an inner source library that you should be using instead of calling this. So, we're going to be able to do a lot more. So, I'm very excited about it. Now, it's experimental right now, so I cannot tell you a date. But it's definitely, you know, I'm starting to see demos from the team on that. It's not the same as the open AI deep research that you're talking about. But it's the same vein of it at the end. And then, question number two, remind me again. GPUs. GPUs. Code spaces and GPUs. I will pass it to them and I will tell them you have a million bucks waiting to give us. And that they should get going. So, but yes, I know about that ask and it's something that we want to get to for sure. Hey, quick question for Verily and full disclosure, I'm a hubber. I'm used to firing off Copilot from GitHub issues. I noticed in your demo that you got it to pull data from a Jira issue and use that to feed into agent. And I'm wondering how you did that. Just using APIs. Very simple. All right, thanks. So, he's going to integrate GitHub issues very easily then. That's what you heard from him. All right. What other question? Oh, we only have 15 seconds actually. So, probably no other question. Thank you so much for being here. I know it's five o'clock and you're getting to dinner and drinks. So, go and enjoy the end of day one. Thanks. Thanks. Thanks. Thanks, guys.