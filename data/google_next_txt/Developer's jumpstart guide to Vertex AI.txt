 Welcome to this very special and unique session. Have we got developers in the audience? I said, have we got developers in the audience? Woo! OK. Have we got anyone from marketing? Ooh, no, OK. This is going to be a very special session. As you can tell, developers, developers, developers, this is going to be all about proper, real tips told you by real developers showing, and I hope this is OK. I know some people might not like this, but we're going to be showing a lot of demos. Is that OK? If we're doing live demos? OK. So there's going to be a lot of live demos. And I'm actually going to go through my whole slide deck right now. OK. So that's all the slides. So we're going to start with I want to kind of bring some people up from the audience who want to show something really cool. And I'm going to start by showing something I think is really cool, which is the live API. So let's get right into a live demo. Does that sound OK? OK. Let's go. So we're going to switch over. So this is the live API. And I want to show and talk about not only I'm sure a lot of people have seen it in action. I've been using it and building with it for the past month, month or two, building different apps, trying to find different use cases. And as a developer working with this API, I have a couple of tips and tricks to sort of allow you to accelerate one building with it, but also how to better identify the best use cases for it. Because although it is a cool thing, you can speak to it, it's kind of like a chat thing, I think there are some specific sort of areas of problems that this is really good at, and some other areas of problems which this is not going to be the good tool to use. So I want to talk about the most common thing people are going to use this for, potentially, which is to do things like customer service, customer support things. I want to quickly get that demo out of the way so I can show them more fun stuff. So here is my customer support agent. It's audio to audio. So let's see it in action. Hello. Is audio coming through at the back? I'll try it again. Hello there. We'll try it one more time. Let me just make sure the audio is coming out. Hello. This is live demos. So what we're going to do, I believe it's potentially this audio device. We're going to try this again. Hello. Hello there. Yes. OK. Whoo. All right. We're off to a good start. Let's refresh this because it has like a little sort of script. Hello. I have a problem. Hi. You've reached Symbol AI. Tell me what you're trying to figure out. So I got a new router and it's not able to connect to the network. So close. Hello. Did you hear me? Oh, hello. Can you repeat that, please? Oh, OK. I got a new router and it's not able to connect to the Internet. Open the main doors now. What was that? What was that? OK. Was anybody in the session I did earlier? OK. I know I promised you it would work this time. OK. We're going to skip this one. I find a really good advice when using and building with a live API is to have internet. That is really useful when using it. I'm going to try a different use case, even though the use case I'm about to try is more internet intensive. But hopefully it may work. OK. I'm going to go back to the server. Hello. What can you see? I can see the user interface of Blender. In the workspace, there is a wireframe model of a bakery with shelves that contain pastries and other objects. OK. Has anyone tried to use Blender before? Did you find it immediately intuitive? It's a lot of this sort of complicated software. I think a lot of people will maybe set a weekend off to learn it and then find after a few months you're finally able to create a cube. So what we found is with these complex interfaces, we can use the live API to screen share to Gemini and we can almost have Gemini like our little software expert sitting next to us. So the most common thing we want to do usually in Blender is to actually see the thing we're working on. So let me ask, where would I click if I want to see the rendered version or the rendered view of what I'm looking at? OK. I understand. You are looking for the quickest way to see a rendered version of your scene in Blender. Here are a few options. Render image. You can click on the render menu at the top of the Blender window and select Render Image. Alternatively, you can use the shortcut F12, Viewport Shading. In the top right of the 3D viewport, there are a few spheres. Click the rightmost sphere for render view. Material Preview, the second sphere from the right, will give you a material preview using the EEEEEEE render engine. Is there anything else I can help you with regarding rendering or another topic? No, thanks. That was perfect. All right. That worked. Yay. OK. But this is genuinely what I think is one of the most useful things or potentially use cases for this. So the complex software like this. Look at the icons in the top right. How hard would it be to genuinely work out that that's what you're supposed to click on? However, the fortunate thing about this, we can share our entire screen and without flipping back and forth between documentation or feeling embarrassed to just Google, how do I see the thing I'm working on, you can have this almost like pair programmer who's actually looking at the dev environment. And it doesn't need to be code. It can actually be more visual stuff like this. So I actually see a lot of applications in terms of like AI, live AI being used to do like software tutoring, upscaling, or even just like trying to do the thing without having to learn the whole suite of tools. So genuinely for this kind of messy environment where it's what do I click on? How do I click on this? How do I do this thing? The live API with screen share and then feeding that to the complicated software I think is really powerful. And thing to note, I did not teach this about Blender. Fortunately, Blender is a very popular open source software. So there is already loads of sort of information online which the live API already knows about. So that's what it's leveraging. However, if you're a business creating software applications that maybe struggle with a steep learning curve, you can feed in the detailed instruction manuals into the live API's instructions so that you don't need to worry if the live API understands what your application is just by how it looks. You can actually give it the instructions of here's how to use this application and now we can hand that over to the user to ask those back and forth questions to see how to solve things and how to do things with your software. So genuinely, I have used this a few times with different pieces of software to let me sort of learn quickly how to do different things. And it's very useful. I think in the future, this is going to be a very good way to kind of upskill people very quickly. And it kind of is like a sort of stop gap between that sort of ideal agent flow where maybe I can say, do this thing for me and it will do it for me. I think a quicker, more practical approach is tell me how to do this thing given this complicated software. Because that way we don't have to build those complicated integrations and all those APIs. You can effectively retrofit the live API to accelerate how users are learning new pieces of software. So I think genuinely that is a really powerful application. Another powerful application is the combination between the live API and something that Snap is creating. They have created these amazing spectacles. And what I kind of imagine here is Mitchell from Snap. He's going to set up the demo. If you saw him in the earlier session, you know he is an absolute hero. And what we're going to do, we're going to set this up. And the really cool thing about these glasses is it's genuine AR. So there is no like filming through a camera and then little screens on the inside where it blocks your entire view. It's actual glass. You can actually see the real world for what it is. And then it's got the overlays, the spatial sort of anchoring and the spatial mapping to sort of enhance what you're looking at. And we spent a lot of time working last week to get the live API embedded in it. So we almost have like the Tony Stark experience. So we can look at things. We can ask questions. And what's more is leveraging things like function calling. We are able to get the live API to call specific tools that can actually interact with the 3D environment. So we're going to see if that is going to set up. Here is the USB-C. You plug that in there. One second. Oh. Get this set up really quick. Oh, yeah. You'll get that set up. So something very exciting about this. We actually have a couple of extra pairs of spectacles. And I'm not going to give them away. It's not a giveaway. But at the end of the session, we're going to almost create a mini sort of demo space up here. So if you want to see how it actually looks, because genuinely, it is like magic. It is the most exciting piece of hardware that I've experienced in the past couple of years. So at the end of the session, please, if you want to have a go, come up to the front. We'll have a couple of pairs. You can actually see it in live, because we're going to show it through the sort of live capture. But really, the live capture does not do it justice, because the mapping, the real-time, the hand tracking is a thing of magic. Oh, I'll do one mic. Sorry. Just have to stop. OK. No worries. No worries. There's a development kit. It's going to be very interesting. So one more thing. Ah. Talking about, oh, yeah. Keep doing the thing. The live API. So as a developer, I've been working on creating it. If I was to give advice to myself before trying to build stuff with the live API, the main thing is learn what PCM audio is. Who in the room knows what PCM audio is? A couple of people. Oh, there's a couple of people over there. Everyone needs to know it if you want to use the live API. It's kind of funny. It's basically an MP3 file without the definition of what the audio is. So it's used a lot in real-time streaming. Like, Google me. Anything where you're doing live streaming audio through WebSockets, they're using PCM audio. Unless you are very familiar with bit rates, et cetera, it's kind of tricky to work with. It's very easy to get it wrong. On my GitHub, I've actually created a couple of applications that helps you debug PCM audio. And it genuinely saves you so much time because you can validate that you're sending it the right type of data. You're receiving the right type of data. So at the end, if you're interested in building with LiveAPI, check out my GitHub. And there will be, like, PCM audio debugger and PCM audio generator. And it allows you to get started really quickly with generating PCM audio and testing with it. Because if you get that bit, working with the LiveAPI is actually quite straightforward. The rest of it is base64 encoded images and WebSocket stuff. We're going to switch devices. We're having a couple of issues with this one. Okay. Well, you know what? So we're going to switch devices. We're going to do that later on. And then I'll call you back up later on. No worries. But again, hero for building a lot of stuff last minute. Give him a round of applause. He's going to come back later on. We're going to now switch to a new topic. So there's a couple of things we want to discuss. We want to discuss generated media, and we want to discuss agents. But I'm going to let the audience decide what we discuss first. Who wants to talk about agents? Agents. Oh, someone's very passionate. Who wants to talk about generative media? Okay. Generative media is going to have to wait. Do we have anyone in the audience who wants to come up and talk about agents? You want to come on up here, sir? We've got a surprise guest. He is making his way at a reasonable pace. You sat very far away for that bit. It's okay. That's okay. Okay. So welcome, Chris Overhold to the stage. All right. He is going to connect up his laptop, and you are going to take it away telling us all about agents. I'm going to get this set up. Okay. Oh, I'm going to do more improvising. Yes. Thank you. Well, I think agents are great. Agreed. My definition of agents, because it seems to be everyone makes up a new one, I think an agent is just a system that has tools and decides when to use those tools. Does anyone disagree a lot about that? No. I think it's tool, and it decides when to do it. I think a lot of people build agents when really a pipeline will do. I had one person who was calling Gemini once and then called it an agent. I was like, you're really stretching the term. If you're building a system which has multiple tools, and you feed it data, and then the system itself decides which tool to do, I think that's an agent. But you know who's going to tell you more about agents? It's Chris. Take it away. Awesome. Fantastic. Thank you for setting it up with these complex workflows. I like giving agents hard problems. I'm not impressed when they order groceries. It's kind of funny. I remember seeing a tweet that said, why are generative AI models writing poetry and making images and I'm still washing dishes and doing laundry? So, I spend most of my time trying to figure out how to make agents do really boring things, so I don't have to, so I can think about creative things. So, I want to take you on a journey today. I spend way too much time with agents. And in fact, you might have heard, did anybody hear we released in the open source agent framework yesterday called Agent Development Kit? Pretty awesome. Has anybody pip installed it and like built an agent with it? I see a couple of hands. Awesome. That is super cool. So, I work with agents so very much and usually when you see an agent demo, it's a chat bot. And so, I want to tell you today a few interesting points that I reflected on in the last two years of all the Gen AI madness that we've all been doing. So, why chat? So, I want to say why chat? And then I want to show you all there's modalities beyond chat. I think we're all kind of stuck in this loop with chat because that's where all we're seeing all the cool things. But think about other ways of triggering what you're trying to do, right? So, I kind of joke and I think about like where are we at in agent capability? And I think we're at like dishwasher or microwave where you have to initiate the sequence of actions. And, you know, when you do that, it's going to go through a number of steps autonomously and then end, right? So, an example of that. This is a chat app. And I'll show you the repo for this at the end. You can get this, download it, run it in one click, change one line, and talk to your data in BigQuery. So, whenever I work in DevRel and whenever we build stuff, we come with a polished demo app like this. And chat is very human, right? So, it's very easy to demo and show, but it's very hard to do work with this, right? So, here, like when you think about if I chat with this and say, hey, what percentage of orders are returned? You know, it's doing things on the back end and translating to SQL and coming back, right? So, I like demo apps, but if somebody shows me one, I actually want to see behind it. Like what's going on behind the scenes? I want to see all the messy stuff, right? So, it's no surprise when you use Gemini app at gemini.google.com or Google AI Studio or Vertex AI console, right? Those are all chat-based interfaces. So, when I think about frameworks, has anybody used other frameworks like LangChain, LangGraph? Raise your hand, yell it out. What frameworks have you used? LangGraph, LangChain, ADK, a few users. Any others? Agno, AG2, there's so many, right? And so, like any good engineer on the back end, I always think, let me choose the thing with the back end that's really, really simple, solid. It matches my mental model. And then I can make the front end look like whatever it wants to be. So, there's Agno, great back end, AG2, ADK, try them all out. And what I would say is, like when we start to look, so there's frameworks, right? Like if you're going to build a chat bot, you're probably going to build with Streamlit or Gradio. These are all really good frameworks. But as you're building those out, ask yourself, like you might be debugging and testing with the chat bot, but try other entry points, right? Like let's say when I hit this physical button, this is going to initiate, or when I make a commit to Git, it's going to think about those triggers. And I'll show you a couple of things, a couple of frameworks that will help you think that way so you don't have to pull yourself away from code to do it. So, Firebase Studio came out yesterday. Has anybody tried that? That looks awesome. So, like I want to build an agent back end and then something just create an app on the front end. So, this brings us all to one really, really, really cool tool I've been using. Has anybody used N8N? I'm not affiliated with it anyway. It's open source. It's amazing. You should try it. What it does, it's a visual way to build agents. And they've actually been like a lot of tools out here. I'm looking back at things like Airflow and Prefect and seeing like how are they looking at agents. So, here's where we are. My call to you is build the chat bot because it shows really well. But pack it behind an API and build other triggers. That's going to be really, really interesting because you'll start to think about am I always in front of my laptop with the chat prompt with the website loaded or maybe I yell a message at my watch and then like things kick off. So, this framework, I wanted to show two things. Number one is when you build, so this is an actual instance of N8N that I use every day in anger behind the scenes, behind the demos. And so, if you go into one of these and create a workflow, what I really like is it asks you what triggers it. And so, now instead of like I'm building a chat bot, that's just one option. And so, now I'm thinking of like oh, I can actually like trigger this manually, do it on a schedule, so I can pack all of my like scheduled jobs here. This is really cool. This is really cool. And of course, in open source, it hooks up to Gemini 2.5 Pro. You can build really, really cool workflows. I'm going to show you behind the cool demo app. There's things like this. I wrote, you know, a pester bot to pester me for issues I'm not paying attention to. Just a chain of sequences. Like this I can get. Like drag and dropping, I can iterate quickly. And you know, we have tools, Firebase Studio. We have some visual tools as well. There's a lot of lovely open source tools. So, try that out. I challenge you, the next thing you build, put the chat bot, because you need it, but put another way of triggering that. And to find ways of triggering that, I'm going to give you all this as I end here. Take a, grab that QR code, because I want to leave you with one link. It is our generative AI repo on GitHub. It has 250, maybe 300 notebooks now that are all like demo app, sample app, sample notebooks, radio, streamlet. Take them all, and we've done all that part for you. And then make other fun ways of bringing that agent into your day-to-day life. So, I have a question. Earlier, some folks raised their hand. Could you raise your hand if you've used ADK, pip to install it? I saw a few over here. One, two, three. If you're here, Dave, do your thing. We have ADK shirts. We'd love to have it. Who has used ADK? I saw three hands in the back that have used it. Thank you all. I'm going to hand it back. Let's talk about generative media. But please go on, build chatbots, but build other things that are not chatbots. Thank you very much, Chris. Cool. Yes. I think the market is a bit saturated with travel planning agents. So, yes, I agree. Next thing is we're going to move on to generative media. So, this is a topic that seems to be less popular than agents. But I think there are some sort of fun, actual useful use cases within it. Is there anybody in the crowd who wants to talk about generative media? Hi, Chris. Come on up here. Thank you. And you're moving a lot faster than Chris was last time. Excellent. So, please welcome Katie to the stage. Who's going to talk about practical use cases and maybe show some live demos with generative media? Yes. All right. Thank you. Thank you, Zach. Thank you, Chris. I do want to say my feelings were a little hurt when everyone picked agents, but I've since gotten over it. I get it. They're cool. We have shirts. See Dave after. But who is excited about Imagine and Vio, image and video generation? Did anyone see the keynote? Yes. Thank you, Chris. Did anyone see the rock commercial video that was made entirely with Vio? Yes. We've got some enthusiastic yeses. I'm going to show you more of those features and the code behind them today. Let me get plugged in here. Ah, okay. We did it. So, Vio, the very first latest and greatest feature, which I am happy to announce as of yesterday, is entirely public GA'd. So, everyone who is a Vertex AI user can go online and generate text to video. You can try this all out right now. This code is all available in a repo. But basically, all you have to do is provide a very simple text prompt and you're able to get a really astonishing photo realistic video. So, how many people played with the puppies? One. One, that's it. One. Two. Everyone should play with the puppies. The puppies heal my soul. But, to recreate that, I generate a video of a puppy in a ball pit. If you need to see it in real life, please do. It's wonderful. And we have wonderful other ways that we can create videos from very simple text prompts. So, a puppy playing in a ball pit. That's all I had to do. Or sculpting a bowl on a pottery wheel. Another great video. But, we want to let you generate the videos you want to see. So, raise of hands. Who has a subject that they want to see generated? Dave can run with a mic. Don't be shy. Anyone. No. Shout them out. Not all at once. Someone making your pizza. Thank you. Thank you. Thank you very much for the audience participation. Alright. A person. Making a pizza. Very live demo. You can see the latency and the generation that it takes in real time. We have two different aspect ratios. You can go 9 by 16 or 16 by 9. You're able to save all of your videos to your Google Cloud Storage bucket. Which makes it really easy to take those videos and integrate them into other applications for real world use cases. You know, we have marketing. I know we don't do marketing. Zach pointed out. If you did do marketing, this is a really great use case. We have retail. We have social media. We have a lot of other really fun avenues that even if you're just doing a presentation, you know, any kind of added video or visual element is really exciting. So, the video is finished generating. You can see the latency on that was incredible. And while we're waiting for it to copy over and display. I thought it would load right when I said that. But we'll give it a minute. There we go. So, you can see just from a few words and an audience idea, we have a person here making a pizza. We have another real world use case if you want to combine the two generative media models. So, we have video and image. So, let's start with image. Let's say that we're in an e-commerce or retail space. And let's say we're loading this stock image that we already have of a suitcase against a white background. And that's kind of boring. So, maybe we want to imagine the suitcase in a more real world scenario. So, we're going to say, okay, put it in an airport. Right? Because that makes me want to travel. So, right here, we're editing the background of the image using background placement with imagine3 editing. And you can see all we're doing is prompting with a busy airport. And you can see we have our suitcase in an airport now. But I'm still not intrigued. Maybe I need to see movement in order to be really convinced to buy this suitcase. So, let's turn it into a video, which brings us to our second video feature, image2video. And so, the great thing about image2video is you don't have to provide a text prompt. I know sometimes thinking of prompts and ways to get the best output can be a little bit daunting. So, with this, all you have to do is supply an input image. And then the output is going to be a video with natural movement based on what the model kind of sees and interprets from that initial starting image. So, this is a very live demo. Very real, as all of ours have been here today. It's already done. We'll wait for it to come back. A little bit more on the VO videos. They're all 720p, ranging from 5 to 8 seconds at the moment. 24 frames per second. And whatever you're doing, keep doing, because this is the best internet we've had all day. But you can see the suitcase is moving by itself. That is how miraculous this product is. And finally, to round this out, I know if you saw the keynote, you saw some of the new experimental features that are really awesome. I've pre-generated these just for the sake of time, but I'll show you what you can do next with the model. So, let's say we have a starting image of this woman. You can tell I like dogs. They're everywhere. They're running across my screen. I'm with them in the ball pit. It's all good. So, I have this starting image of a woman petting a dog outside of her house. And I would really like to kind of pull out of the image like in a very cinematic kind of way. And rather than figuring out the prompting for this, we have now camera presets. So, all I have to do is supply the initial starting image. And we're just saying add this one line here to the code. And this is the generated video. Again, no prompt. All we had to do is specify that camera. So, extending videos is also really popular. So, let's say we have this eight-second video of a flower also generated with VO. Blooming over a time lapse. And maybe I want, I'm terrified of bugs, but maybe I want a bug to fly into frame and land on the flower. So, I said a ladybug flying in and landing on the flower. And so, this is the original eight-second video stitched together with our little ladybug that flies in, lands on the flower. I'm okay with this one. This one's cute. So, we're happy with this video. But, and last but not least, one of my favorite new features is that you have a first and a last frame that you can specify. And then VO is going to fill in all of the exciting content in the middle. So, let's say I have the starting image of a plate of chocolate chip cookies. And maybe at the end, I now have the milk glass next to my chocolate chip cookies. And I want a hand to reach in from out of frame, drop it down, pull it out. So, all I have to do is prompt and say to VO, I want a hand placing a glass of milk next to the cookies. And then you can see this is the video we get. We put it down. We really want to make sure it's centered. Good. Okay. And that's how we end with our frame. So, I hope you've enjoyed it. I know it's not as cool as agents, but very cool stuff in this field. Back to you, Zach. Wow. That is bold, asking for audience participation to generate stuff. I've seen that go very wrong in the past. One thing, one note I want to make about generative media is I found when we first brought out, like, image generation, I was like, meh. I mean, I can generate images. Cool. But whenever they came out with image editing, I was like, okay, that's actually useful. Like, I can actually use this. I've got images and I want to edit them. And I had the same exact sort of rollercoaster of emotions with video generation. I was like, video generating? Cool. That's a random video that's been generated. But genuinely, that last feature that Katie just showed, the image starting, image ending, like, if you imagine that in a video editing thing, like, maybe you've filmed something and, like, the actor didn't quite do what you needed them to do or it's like you need to change something to do. Like, you could fully generate that video in between. I think there's going to be a lot of really cool applications with that that are actually useful, where you're editing your own videos in a quick and fast way that actually saves you from going into ugly video editors, where you'll have to use the live API to guide you through it. So, really, really cool. So, the last sort of demos we want to talk about, well, they're not really demos. They're more of how you can build demos. And I want to share, like, one thing that I have been doing lately to generate the best code I've ever generated in this whole AI sort of landscape. And I realized, like, so, I'm a software developer, mainly a web developer, build a lot of demos. And what I've started to do, I think, as an experienced developer, when you start developing code, I'm copy and pasting a lot of other people's code. That's how I started. Did anybody else start coding doing that? Can we all admit that we all started just by copy and pasting other people's code? And I realized the more experience I got, the more copy and pasting I did of my own code. Does anyone agree with that? I find, like, when I'm rebuilding stuff, I'm not, you know, checking Stack Overflow again. I'm actually like, oh, I built that before in a different project. Let me go and, like, copy and paste from that. And a lot of my demos are actually, they look like, they look similar because they're copy and pasting a lot of the code. I found a really fun technique. I call it my, like, platinum examples approach to code generation. So rather than relying completely on the model to generate code from scratch, every time I do that, it kind of generates code in a slightly different style. What I've started doing is I created a little script which allows me to just specify a folder where I have code from previous projects put. And then I say, okay, build me this application, but use the code in this folder as your sort of go-to code examples. And I even built in a little function to copy and paste files directly. So if I built a library file, I don't want it to rewrite a new library with slightly different syntax. I want it to copy and paste that other file over. So we haven't got, like, official support anywhere for this. I'll probably put it on my GitHub sometime soon. But if you're looking to generate code and you actually want it to generate code like the code you write, I found this approach of curating here's the code samples that I actually like using and generate based on this. And on that, a great place to get code samples. I think a really useful way to end this session is to talk about where we can get the best resources to build demos, applications, et cetera. So if we've got anyone in the crowd who can... Oh, he's already up. Okay. Come on up. We are going to show you where you can learn about things. And he is setting a new record for people making it to the stage. Please welcome Holt. Okay. Gosh, the microphone on this is making my pants want to fall down. So I'm going to adjust myself and not expose everything. Oh, dear. Okay. So let me open up my screen here. Okay. You can show the stuff on the screen now. Okay. This is already mentioned a couple times. Chris stole my thunder because this is going to be the big reveal. Oh, my gosh. There's a GitHub repo that has everything. But anyways, here it is. And just because everyone's going to be looking for it again, here is the QR code. Once again, if anyone forgot to before. Okay. I'm going to do a little bit of exploration of this GitHub repo and talk about how you all can contribute to it as well. Okay. You can also go to goo.goal slash gen dash ai dash GitHub if you, you know, don't want to access it on your phone because, you know, it's funny. We give QR codes because it's an easy way to share this stuff out, but it's not really easy to use on a computer where you're probably going to be accessing it. So I like the short links as well. What is happening here? This is a very loud air conditioner or something. Okay. Whatever. This is fine. This is all normal. Okay. So we have a little bit about this repo and kind of what all is in it and also how you all can contribute to it. So we have a bunch of different examples on it. This repo has been live for about three years now, I think. So pretty much since the beginning of the generative AI boom in 2020, 2022, 2023. And so we have examples for audio output like the speech to text and text to speech API. We have stuff for speech recognition here using the speech to text API and the chirp2 model. We also have a few examples in here that are using the latest audio generation models, which are called chirp3 HD voices. One example that I like in here that I made a couple of years ago, I've actually tried to make this in a few different ways. I originally tried to make this using document AI. If anyone has used that product before, I made this in like 2021 or something, and then I've readapted it for the Gemini era. So for example, we have some stuff here, narrate a multi-character story with Gemini in text to speech. So what I have it do is I have it create a story using Gemini, and then I assign each character to a voice, and then I use the text to speech API to assign a different voice to each character in that story. And then I put all that into one big audio file. So I have it using a couple of different libraries in here, and then it goes through all of this stuff to actually generate all of this. And I have it generate a play for it as well. I think in this case I use like, what is it? I've seen a few different examples. If you give it a very ambiguous prompt like this, Gemini will just kind of, it'll decide something for you. Sometimes I've had it do Shakespeare plays, sometimes it's done children's stories. I usually turn up the temperature as high as it'll go on this because it makes it the most interesting. Some of the really fascinating examples, it came up with one called Macbeth the Sitcom. That was a really fun thing to listen to whenever it happened. Okay, there's also things like live translation in here. Lots of good options for it. There's stuff for embeddings. I'm not that much of an expert on embeddings. I know kind of what they do. But there's a lot of better examples in here than anything that I have ever come up with. So we have stuff for similarity visualization. There's stuff for, there's an introduction to text embedding and vector search, multimodal embeddings, tuning, all that kind of stuff. In the Gemini folder, this is where the vast majority of stuff is in here. We have things for agent engine, custom agents. There's a sample application here that was actually used in another next session called AutoCal, batch prediction, chat completions. By the way, if you're using OpenAI currently for anything, did you know you can easily switch to Vertex AI or the Gemini developer API with only one or two lines of code changes? How many of you actually knew that? Only the people from my team knew that. So, hey, clearly we've not done our job right to let people know about that. Most of you are probably using Gemini anyway, but if you didn't know that, there is a notebook right here, intro to chat completions with Gemini. And so it shows you how to call Gemini using the OpenAI library. It's pretty cool stuff. There's stuff for code execution, a really cool feature of Gemini 2.0, context caching, control generation, evaluation. This folder is also massive. It has a bunch of examples showing how to do all kinds of evaluation stuff with Gemini. And also open source models. Speaking of open source models, we have some stuff in here from open source models. And, or actually, sorry, that's outside the Gemini folder. If you are completely clueless of what to do, getting started is probably the place to go. We have examples for every model that exists for Gemini. There's some examples for how to use it in chat, in chat types of applications, stuff for curl, the express mode. If you're brand new to Google Cloud, I doubt that you are if you're at this conference, but if you somehow snuck in and, you know, have never used Google Cloud before, Vertex AI Express mode is for you. And there's also stuff for the new generative, the GenAI SDK, if you are still using the Vertex AI SDK. There's stuff here for the global endpoint. There's stuff for grounding, which I showed some stuff in my earlier talk today. There's stuff for model context protocol. Or, sorry, I forget what this is actually called. MCP is the abbreviation for it. And, actually, an example was just added here literally two hours ago. I've been doing a lot of reviews and stuff for this repo as we go. I think Zach is about to come kick me off the stage because I've been saying too many things. But there's a lot of examples in here that you can go in. And we are open for pull requests. There are currently, there have been 1,600 pull requests since this repo was started. So, please go in. If you're going to make a contribution, please read the contributing guide. You know, a lot of people wrote this to make it easier for things to get going in here. There are code quality checks that happen to make it so that everybody who's reviewing your code can, you know, they can save a little bit of time on all of that. So, please read the contributing guide. But go ahead and make a pull request. You can fork the repo, add whatever you want. You can add demos, you can add notebooks. We're very happy to see whatever you come up with. Just make sure it's using Gemini or something used with Vertex.ai. So, excellent. I'm going to go ahead and invite Zach back up to the stage to show the last little part of his demo. All right. Thank you, Holt. Okay. So, funny thing about the repo. I've contributed to it. How I did so is I wrote some code for a demo I got. And then I wrote some code and then I got, because you saw how well structured those notebooks are, very well structured, I'm not going to spend my time structuring a notebook. So, I took my sample code and then I got Gemini to look at the repo and look at all of the code in the repo and say, based on this repo, turn my code into a notebook for that repo. And if you go into the repo and look for the Lego quick build or not, Lego AI quick build, it is all generated using that approach. But we've got a few minutes left. So, what I want to do is I want to invite back up Mitchell from Snap so that we can see the lenses in action. And instead of coming up front to try them, we're going to go out into the hallway because we'll be out of time. So, hopefully, fingers crossed, and you just demo it because you know how to do it. Okay. And then I'll kind of put the mic on you again. So. Sorry to the AV team for doing this. I'll actually be on this side because it comes out of this side. So, you're going to speak and I'm going to move it whenever you need to. Okay, sounds good. Fantastic. It's a demo on the screen right now? Yes. Cool. So, I'm going to launch our Gemini demo right here. Hey, how's it going Gemini? It's going well. Thanks for asking. Is there anything I can help you with today? Yeah. Can you tell me where I am right now? I see a large room filled with many people seated in rows. There's a table with water bottles and screens in the foreground. It looks like you might be in a conference hall or auditorium. Yeah. Well, I'm demoing Gemini to these people. Can you show me my schematics so I can fix my computer right here? Okay. So, looking at this, it looks like I need to connect JILV to C22. Can you make me a sticky note to remind me to do that? Okay. Thank you, Gemini. You're welcome. Is there anything else I can assist you with? I think that's good. All right. Give a round of applause for that. Thank you so much for that. That's amazing. No worries. So, this is, as you can see, the way the sticking everything appears. I think the screen shirt does not do it justice. So, if you're excited to try it out, we'll have demos out in the hallway so you can see it. Hand tracking, everything is amazing. And you can see we had a schematic up on the screen. And I think, in terms of where this is super useful, is anywhere that's not in front of a computer screen. I think. When any time you're out in the field doing things, this kind of interface, where you're able to see things in the real world, is incredible. And the really cool thing is these glasses. So, you can see straight through them. They are the development kit. The form factor might change. Or may not. May or may not. They're cool. You'll try them on. And you can see they're actual glasses. So, unlike other devices where there are screens and then cameras, this is, I feel like, fail safe in sort of field activities. So, if you're in a factory environment and you're using this to display schematics, et cetera, if these maybe lose power, you can still see the real world. So, it's not replacing the world. You're seeing the world as it is, plus extra. So, really cool technology. Make sure you try them out in the hall. But thank you so much. Mitchell, give a big round of applause to Mitchell. And the rest of our demoers today. We will switch back to the slides. And that has been everything. Thank you all so much. Please scan the QR code and enjoy the rest of the conference. Thank you, everyone.