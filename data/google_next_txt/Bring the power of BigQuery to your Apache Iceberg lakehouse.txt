 My name is Gaurav Saxena. I lead product management here at BigQuery, and I'm joined by Pawan, who will be showing an exciting demo. And then we have Edward and Philippe from Spotify Data Platform, who will be sharing their own journey of building iceberg-based lake houses. It's probably not a surprise that lake houses are really transforming data management, and we see this. A lot of you have told us that you really value the interoperability of what OpenTable formats provide and the openness that comes along with the flexibility to run any query engine of choice. And in this space, we have really seen, in the last few months, Apache Iceberg really emerged as a de facto industry standard. It enjoys broad community support, and now there are platform integrations really available across the industry landscape to really make the most of your Apache Iceberg lake house. And we continue to see that evolve and be an active participant in that journey. For a long time, we have offered the enterprise and fully managed storage that BigQuery Native Tables provided. About two years ago, we started to support our customers on their iceberg journey by providing the ability to query iceberg tables using BigQuery. And this is what we call our self-managed iceberg tables. In the last two years, we have seen many customers build production use cases by leveraging these capabilities and using BigQuery's native query processing capabilities to run performant iceberg workloads. As we go down and continue on this particular path, we see that a lot of you are telling us that a lot of you are telling us that open and managed are still two different worlds. What that really means is that for a lot of our customers who have really enjoyed the fully managed nature of BigQuery storage, the ability to really scale up, support high-performance streaming and advanced analytics use cases, some of those are really challenging to do when you are trying to manage your iceberg deployment. And if you don't get that right, that means you have to pay the penalty in query execution and just make it really hard to implement streaming solutions. And we think, you know, this is what we are hearing from a lot of you, that an ideal lake house should not really make these two different choices. It should provide the best of openness what the iceberg has to offer while providing the real-time, highly scalable, metadata-powered, fully managed enterprise storage that you have come to love with BigQuery storage. And this is exactly the direction we are pursuing. This is why we are combining the best of open and managed with BigQuery tables for Apache Iceberg. That brings in the openness of Iceberg while delivering the price performance and the enterprise-grade features that BigQuery native tables provide. And really accelerating Iceberg workloads with the full manageability of BigQuery native tables. In addition to providing high-throughput streaming support in an open and interoperable way, BigQuery tables for Apache Iceberg also are deeply integrated with GCS on a platform level. So they do provide an integrated experience in terms of soft deletes, auto-tiering, and integrated automatic NL generation insights while providing a pricing model that does not require you to pay for ops separately. All of that comes neatly integrated with our catalogs that provides built-in finery and access control and governance. To tell you more about it, I would like to invite Pavan, who is a distinguished engineer at Google Cloud, and will share a demo with us. Thanks, Gaurav. So, as Gaurav said, we are trying to bring the rich, scalable metadata management and advanced capabilities in our managed storage to Iceberg tables. So we want to enable the deep, unlike many other query engines, BigQuery's metadata management is deeply integrated into the query engine. That's where we get the power of the scale and performance that you see with managed tables today. We want to bring that same functionality to Iceberg tables. We also want to enable advanced functionalities like advanced auxiliary data structures, like materialized views, search indices, and all of these things, and high-throughput streaming on Apache Iceberg tables. In summary, we want to bring the complete feature set of BigQuery to Iceberg tables. A few words about our vision. Later this summer, BigQuery and GCS together will be releasing open catalog, which is conformant to the Iceberg open standard spec. It will allow you to, armed with that catalog, you will be able to access your data from GCS, from any engine that you can run on Google Cloud. You get seamless access to Iceberg tables, both from BigQuery and from any other engine with the data sitting in situ in your own cloud storage buckets. Finally, with BigQuery's advanced infrastructure, we are going to provide some differentiated capabilities on Iceberg. You will see some demos in a second. So what does a modern lake house in Iceberg look like? You want to do high-throughput streaming into your lake house. You want to replicate your operational stores into your lake house. You don't just want appends into your Iceberg tables. You want high-throughput streaming mutations at hundreds of gigabytes per second scale. You want fine-grained mutations because you want to reduce the write amplification and the cost of running your mutations. You want multi-table transactions so that you can work with two Iceberg tables or N Iceberg tables together in a consistent way. You want atomicity and transactionality. Finally, we want to enable continuous analytics on Iceberg data. Iceberg is typically considered a slow-moving data set, Iceberg tables, but we want to bring the power of real-time to Iceberg data. So now I'll switch to the demo, please. Thank you. All right. I'll demo three scenarios. The first one is I have this table, which is a large fact table, which is typical of data warehouses. This fact table is an Iceberg table sitting on GCS at this location. This is a GCS path. I'll show the path in a second. Its schema is inspired by the TPCDS catalog sales table. If you're familiar with the TPCDS benchmark, it's an analytic benchmark. Its schema is inspired by that data set. It has about two terabytes of data. The second table that will be involved in this is a BigQuery native table. It has about 44 million records. Its data is sitting in BigQuery native storage. I could have chosen an Iceberg table here, but I chose a native table to show the interop between the two tables. So what I'll do in this demo is I'll take the staging table, think of it as these are changes that your operational store is producing, and you have staged them into this table. And you want to apply them to your target table, which is your large fact table. Essentially, you have sales data that you want to bring from your operational warehouse into your final table. So now, before we go, let me just show the state of the table in Iceberg. So this is the data. This is the large fact table on which we'll do the mutations. This is the data and metadata folder. And if you're familiar with the Iceberg format, this is the metadata JSON file. Essentially, I just created this table, pre-created this table for the demo. This JSON file was added as of that time. So now let me switch back to the query UI, and let me first show the number of records in this table. There are 828 days worth of data, 70 billion records here. So now let's first look at what is the source table trying to, the staging table trying to bring into this table. What are the changes that are staged in this table? So you'll see that there are, there are 10 days worth of data from the dates 0105, so on up to 0114. And you'll see that there are about 4.5 million record orders per day. Almost all of them are in shipped state. Essentially, there is a shipped date column on this table, and the shipped date is set. And a small fraction of these orders are unshipped. You might want to remember these three dates because this will be interesting later in the demo, 12, 13, and 14. Now let's look at what the target table contains for especially those dates, because those are the dates for which the orders, the staging table has orders that is trying to mutate into the target table. All right. So it has, it has 10, it has, this was again the, yeah, so that was the staging table, this is the target table. This one has data from 0105 to 0111. All of the orders are unshipped. So what the staging table will do now is we'll perform an operation that will bring the changes for the staging table into this target table. So in order to do that, we'll use a merge DML statement. A merge DML statement is sort of a poor man's transaction. It can do multiple mutations in a single transaction. Multiple mutation types. For example, you can do deletes, updates, and inserts in the same transaction. So what we are trying to do is, I'm idiomatically, this is trying to say merge the target table from the source table. And this is the, this is the key to match in the two tables. And if there is a match, copy the ship date into the, from the target table, from the source table into the target table. And if there is no matching record, essentially you found a new order in your source table, insert it into the target table. As you can see, this merge statement finished and added 45 million, affected 44 million rows. If you remember, this is exactly the same number of rows that are in the, in the, in the staging table. Essentially, it brought all the changes into the target table. Now, let's make sure that it actually did what we expected it to do. So let's query the target table to see whether the changes made it into this, into this table. You can see that it has data for dates 0105 to 0114. In, previously, the target table only had data up to that date. These three dates are net new that came in from the source, from the source table. And you can see all of the orders are marked as, as shipped. So this way you can perform, you can perform your periodic ETL from your operational store or any other staging data set into your iceberg lake house table. So to show the interop with other engines, let me, let me go to a Spark notebook and query this data from a Spark notebook. We just ran the query, the merge just happened, and you can see that this data can be queried in situ from your Spark cluster. So let me go back to the GCS bucket and show you actually the changes that the merge statement made. As you can see, there's a new metadata JSON file added. It was just added 11.19 a.m. as when I ran the merge statement. And as a result of this metadata being consistently maintained, you are able to query this from your Spark notebook. And you'll see that, if you recall, these are the same, this is the same result that you saw from BigQuery. So this shows the ability of using, our ability to enable the use of iceberg for interop across engines with strong consistency. In my second demo, I'll demonstrate a slightly more advanced functionality. So I have two tables again here, but this time I'll demonstrate two advanced capabilities. I'll do a multi-table transaction between your source table and your target table. The target table is again an iceberg table. The target table is here. Again, its location is in this GCS bucket and stored as iceberg. And the source table this time, I have made it a iceberg table as well. Essentially, what I'll do is I'll do a consistent transaction with asset semantics between these two tables, between the source table and the target table in a single transaction. Additionally, I'll do one more thing. These mutations use what are called fine-grained mutations in BigQuery. It was previewed last year and now we are bringing those two iceberg tables. Fine-grained mutations work in the following way. If you modify data in your table and your modification is touching one row per file, previously BigQuery would rewrite entire files, essentially increasing the resource cost and the latency of these operations. Now, with fine-grained mutations, if you modify one row, BigQuery will only rewrite that one row, allowing you to get orders of magnitude improvement in latency and cost. So this transaction will run with both of these features on and let's look at it now. So the target table of this transaction, which is our large fact table, has about 70 billion records. And let me show the state of the source table before I run this. So it has a single day worth of data. I made it simple for it to be easy to understand. So there is 450,000 records, orders in this table, and most of them are shipped. A small fraction is again unshipped. And then let's look at the target table, what it contains. The target table is the large sales table, which has the 70 billion records. But we are only looking at one day because that is the day that the staging table is trying to modify into this target table. As you can see here, there are 4.5 million records. All of them are unshipped. All of them are in unshipped state. Now, we will use the capability of running multi-statement transactions. I have defined a transaction here. And I'll run this transaction that will atomically mutate these two tables in a single operation. So it is atomic. It follows acid semantics. And it supports what is called snapshot isolation consistency level. So between the two tables, so the first one is again a merge statement. This time, it's matching the orders into the source table and copying the ship date from the source into the target. And once done, as part of the same transaction, it's saying all the rows that have been applied to the target table, just delete them from the source table. This is so that if you retry the transaction or if you run this again, it's an idempotent operation. It doesn't produce duplicates. So this allows you, essentially, most warehouse applications, what they do is the source table is continuously getting changes, the staging table. And periodically, someone will come along and run a transaction. So what this allows is this transaction can run concurrently while changes are being staged into the table and it will consistently truncate the table from the head, the source table from the head. So this transaction has run now and you can see that the merge and delete statement both executed. So it modified 450,000 rows. If you remember, this was the number of rows in the staging table. And let's look at the staging table. We removed all the rows from it. So let's make sure that the transaction is exactly did what it's supposed to do. Let's query the target table again. We should now expect to see that on that date, most of the orders are shipped. As you can see, all of the orders at the staging table, 450,000, it had them shipped, marked as shipped. They are shipped in this, marked as shipped in this target table. The other rows were not present. The other orders were not present in the staging table. And so they were left untouched. Finally, the source table has no data because we truncated it. So I showed the power of SQL and multi-statement transactions to perform ETL. Now I'll show one more capability. How you can perform this ETL, this kind of operation, using streaming. So in order to do that, on the same table that I've just been operating on, on the large fact table, I picked three orders. The item numbers are these three. And what I have done is, I have, my goal is to mark this order as deleted and these two orders, I want to change the quantity to 200 and 400. So as you can see, these three orders are present in the table. What I'll now do is, into the same table, without having a separate staging table, I will stream mutations into this table using the BigQuery high throughput streaming API. In order to do that, let me open a notebook, a Python notebook, which I don't know what happened here. My Python code is missing. Yes, it's right here. Okay. So, in order to run this mutation, I'll use the BigQuery storage write API, and I'll run these mutations, apply these mutations, using the storage write API into this table. It's the same table that we have been operating on. The mutations are described in this JSON file. The storage write API uses binary format, but for the purpose of this demo, I used a JSON file, and the application converts the JSON into a binary format and sends the request to the BigQuery storage backend. So, there are three mutations in this file, and corresponding to the three orders that we want to mutate. It has two operations, three operations that we are performing. I want to delete this order. I want to insert, which is update or insert this quantity of this order to 200, and the next one to 400. That's what I'm trying to do. You'll see two special, these are the primary key columns, these three, but there are two special columns here. These are, the change type is a verb that tells the system how to treat this row that you are sending to the system, and the change sequence number allows you to specify external ordering. So, for example, if you have an application, the arrival time of the rows into the system can be different from the order in which you want the rows to be applied, because if you have a distributed application, rows can go out of order into the system, and you want to describe, the application wants to enforce an order in which BigQuery will apply these mutations. So, let me run this to, so I streamed these three rows to the table. It's already done, and what we expect is this order, should be deleted, and this order should be updated to 200, and the next one should be updated to 400. Essentially, if you see the difference between the previous one and this one, this one is truly managed in the sense that you don't have to have a staging table. You can just send your mutations to the target table, and BigQuery will automatically apply those mutations in real time. So, that's the, and even though I showed a demo with only three rows, you can do this at hundreds of megabytes per second or gigabytes per second into your table. So, this is powered by the BigQuery streaming real-time infrastructure. Finally, let me just show that it actually did the mutation that we expected it to do. As you can see, we started with three rows. Now, we have two rows. The first one got deleted, and the other two got updated to 200 and 400. You can query the same table from Spark, and you would get the same results, and this is how you use the high-throughput streaming power of BigQuery to replicate your operational store into your lake house in real time. Thank you, and I now call upon Philippe and Ed from Spotify to share this. Thank you. Hey, everyone. We are Ed and Philippe, product manager and staff engineer from Spotify's data platform studio, where we look after all of the data collection, processing, and data management capabilities at Spotify. And our team, in particular, looks after the data processing capabilities and the integrations with processing engines at Spotify, and we also manage the resource capabilities of processing engines such as BigQuery. So, of course, Spotify needs very little introduction. We are one of the largest music streaming providers in the world. We have essentially transformed the way that people listen to and access music across the world. We have over 670 million monthly active users, over 260 million subscribers. We're active in over 180 markets, and we have millions of tracks and podcasts and other content on our platform. And, of course, that means there's a lot of data. And we're very proud users of Google Cloud Platform at Spotify. All of our software at Spotify runs on Google Clouds, and we also use Google Cloud for our data processing capabilities, such as BigQuery. And data is important for everything we do at Spotify. So, whether that's for understanding the track consumption to calculate the royalty payments for our artists, whether that's leveraging data to recommend to users exactly what they want to listen to at exactly the right time. Also, for optimizing the monetization of the advertising that we offer on our platform, for leveraging data for metrics and insights that we really, really need to make the right decisions to drive the user experience forward on our platform, and for building exactly the things that our users really want. And other things, such as improving fraud detection. And so, this is just a glimpse at the Google Cloud-based data engineering stack we have at Spotify. So, our primary kind of data engineering stack is based on Dataflow using Apache Beam and our own open source Scala SDK for Apache Beam called Chio. And those are the kind of primary tools we use to power our data engineering pipelines. And that data gets output in the large part to Google Cloud Storage, our main kind of data, primary data warehouse at Spotify. On the other hand, over the years since we've been moved to Google Cloud, BigQuery has become a really important core part of our data processing capabilities. We use BigQuery a huge amount for data transformation. We've seen a huge rise in scheduled SQL workflows at Spotify based on BigQuery. and you can see this kind of hockey stick growth that's really kicked off since 2017 when we moved to the cloud and we've got thousands of scheduled SQL data transformations running at Spotify on Google Cloud products every single day to power the data that we need. And this is great, but we've seen a kind of divergence over time with our data ecosystems. So, we have on the one hand the Beam workflows, running on Dataflow outputting data to GCS. On the other hand we've got the SQL workflows using BigQuery, but they're both using very different storage layers. So, on the one hand we've got GCS, on the other hand we've got BigQuery using BigQuery native storage for storing data. And the trouble is that our SQL consumers using BigQuery until now haven't really had access, direct access to the GCS data. So, we've had this process over time where we've actually had to copy data across, loading data across from one storage layer into the other. And this is a problem. This has caused some negative consequences at Spotify. It's led to a kind of poor consumer experience. So, different sources of truth to work with for our consumers depending on the type of technology they might have to use different tools to work with that data. And there's delayed data set delivery due to that duplication process. And it's a huge waste of cloud resources and cost for us at Spotify. So, we reckon that at least one-third of our data in BigQuery is actually just pure duplication from GCS to achieve that querying capability and access to that data. Over 80 megabytes of data costing Spotify over $1 million in terms of our monthly storage bill. And we also use significant BigQuery slot resources to actually power that loading process. Over 6% of our BigQuery slot usage, is actually just used up for that process. It's an extra burden for our data producers, so they need to configure and maintain the workflow to do that loading. If they do a backfill of their data, they have to do multiple backfills in both places. And for us as data platform, we have a maintenance cost. We have to maintain these workflows. We have thousands of workflows running every single day to do that loading. So we want to work towards a new world, a new vision where we have this common abstracted storage interface for reading and writing data at Spotify based on an open standard which is interoperable for all of our data processing engines and tools at Spotify. That's really important to us. The goals of reaching this vision are improved productivity for data producers and consumers, cost efficiency, interoperability, and we need to ensure that it's integrated properly with our current Spotify stack. And now I'm going to pass over to Philippe who's just going to talk to you about the first steps that we've made towards achieving that vision using BigQuery's latest capabilities. All right. Thanks, Ed. So this is a bit of a journey so bear with me a little bit. So trying to push this vision forward and knowing that BigQuery is a fundamental part of our ecosystem, we took a look at BigQuery external tables. However, we found a key limitation with this approach. Our partitioning strategy, which is the bottom example slide there, it looks a bit weird and it doesn't follow the above example which is a Hive-style partitioning strategy which is expected by BigQuery external tables. Now then we kind of looked into several other solutions but we always ended up finding the same key limitation which then led us to kind of consider moving copying and restructuring our data. We have nearly a thousand petabytes which are distributed across thousands of distinct GCS buckets and they are owned by hundreds of teams which made this really impractical. And on top of that, we still deal with more than 50% of our data in our format, which is record-based file format and it doesn't offer like the performance requirements we need for our analytical workloads. Now, ultimately, whoops, oh, this one. Ultimately, we ended on a combination of three main technologies. To no surprise, we decided to further invest more on the adoption of Parquet. and being a columnar storage, it does offer a more efficient compression and it does support predicate push down, which not only help us reduce the storage costs, but also improve IO efficiency that we desperately need. Then we bet on Apache Iceberg as our table format. Iceberg being an abstraction layer on top of the Parquet data file, files, it actually decouples metadata from the actual data files themselves. And a good thing about this, Iceberg really keeps track of all data files and allows flexible data file location. And coupled with the column projection feature, it actually allows us to leverage our existing data sets without needing to come up with a new partition strategy, which is pretty good because we have, remember, we have nearly a thousand petabytes. And on top of that, Iceberg brings important features like time traveling, data compaction for data files, and hidden partition feature, which enables us to easily do partition management, and it supports automatic partition pruning, which not only helps on speeding up the query planning, but also query execution. And the bet on Iceberg turned out to be a really good bet because we've seen a massive adoption within the industry, and our friends at Google were also working on having support for it through BigQuery Iceberg external tables in BigQuery. And this, offering the capabilities that we've seen before through Gaurav and Pavan, but also offering the same kind of feeling as standard BigQuery tables. And that's good because we can bridge now the gap with our BigQuery infrastructure. Now, building this solution, we wanted to minimize developer impact and avoid changes in our production workloads. we wanted to avoid disruptive migrations as much as possible. So we came up with a retrofit approach for this. That can be quickly summed up in three main points here. So we added post-processing capabilities to our workloads to generate Iceberg tables, and we keep consistency of those Iceberg tables with partition and schema updates. And lastly, we create BigQuery Iceberg external tables pointing to the previously created. Iceberg tables. This is good because now we provide a unified data access layer for both our batch processing workloads and our new SQL analytical workloads. It's perfect. But one thing that we realize at the same time is that we need to optimize our data sets. The data that we have right now, it's well-structured for the batch processing, but it doesn't work that well for this new paradigm. It leads to very inefficient queries and high resource usage in BigQuery, and we need to keep queries fast and costs down. So for that, we need to apply a couple of things. We need to make sure our data is sorted according to common query user patterns. We need to do data compaction, as I mentioned before. We need to optimize the file size so that we avoid the small file size problem. We need to tune our Parquet data files, adjusting the row group size and the number of row groups. And when it comes to Iceberg, we actually need to optimize metadata as well. We need to do snapshot coverage collection. And we are doing also a partition per manifest file for efficient query planning. And this turns out to be quite well. But we know that data and use cases change over time, right? So we need to continuously monitor and benchmark our data and processes for further tuning. And for that, because we do have large infrastructure, we need to leverage automated optimization processes for this. The result of applying these optimizations, because this is just a sample and there are a few more, we do see that BigQuery iceberg external tables perform exceptionally well for large and complex queries. And when it comes to resource usage, sometimes it actually outperforms standard BigQuery tables, which is surprising. Now we have the solution, we have the approach. We still need to kind of seamlessly integrate this into the developer workflow. And for this, we leverage our internal developer portal Backstage, which acts as a central hub unifying diverse ecosystem of tooling, service services, technical documentation, and pipelines. Backstage is actually open source and is a cloud-native computing foundation project. And for companies actually that want to get up and running really quick with it, Spotify has launched Spotify portal, which essentially packs all the core features of Backstage into a nice package. package. Yeah. So when it comes to Backstage data experience, we can see here two screenshots. On the left, it's the internal version we use, and on the right is the Spotify portal version, which looks very similar because it's built upon our internal experience. And this data experience supports a wide range of data sources, including BigQuery and, by extension, BigQuery iceberg external tables. And through this interface, we can see that our data scientists and software engineers can quickly search datasets using familiar terms, view compressive metadata like the source, format, inspect the schema. Then we have, you know, they can preview sample data and even access, directly access the data through their preferred workbench. I think this is very nice because it offers like a smooth integration with this new paradigm. One thing to bear in mind is that we are still kind of building this, so more features are coming to Backstage as well. Yeah. So this has been a journey, and it's been a journey that counted with a strong collaboration with Google, of course. And we worked very close with their engineering team, even having, like, face-to-face meetings. And through this collaboration, we actually found two bugs in BigQuery, which we also addressed. And then out of 14 feature requests, we made nine of them critical, have been already released. Yeah, this partnership has been kind of invaluable because we overcome these roadblocks, and we also helped out shape the future of these new technologies. Now, back to Ed again. Thanks. Okay, so just to, this is the kind of first part of our journey towards this Iceberg integration, but we've already had very significant impacts with our project so far. So, as Philippe was explaining, we now have our own internal Iceberg catalog service, which is already powering 195 BigQuery Iceberg external tables, which are very, already have very significant use at Spotify. You know, just for example, 30,000 queries executed against those tables in the last 30 days. 213 production workflows powered by those tables. So, we've really, through the initial steps in this project, we've already really proved that this is a production-ready technology that integrates really well into our data ecosystem. On the other hand, we've been doing a lot of collaboration with the large dataset producers at Spotify to try to start migrating the biggest sources of the duplication that I talked about earlier across from BigQuery native storage into Big Lake tables, external tables. We've seen a lot of deletion already of duplicate storage through migrating to these external Iceberg tables. So, this is a conservative estimate. We've already saved at least $100,000 a month on that migration process through migrating some of these big datasets. We've reduced the BigQuery load jobs already, so we've already saved over $10,000 a month on removing some of those BigQuery load jobs. And what we've also been working on using some other Google technologies is we've introduced a Gemini LLM-based automation process for migrating the downstream scheduled workflows across from the native storage-based tables across to the new external Iceberg-based tables, which has been proving very scalable so far in terms of automating that migration process, and we're really pleased with the results so far in that. And looking a step further ahead, the next steps towards this Iceberg-based future that we see in our data ecosystem. data ecosystem. So you'll see there, we see very much Iceberg as being at the center of that data ecosystem, and there's a few steps in terms of our Iceberg strategy that we're looking to work towards. So firstly, we want to continue to drive that Iceberg adoption using the BigQuery Iceberg external table solution that's explained to you. So essentially, these external tables have unlocked the adoption of Iceberg at Spotify for us, which has been a really great breakthrough for us. And we'll continue to drive adoption of those tables via internal guidance, recommendation, targeting marketing and communication to data set producers at Spotify. So that's like the top left diamond there in that diagram reading by BigQuery. We've kind of solved this problem now. But we want to move further with this strategy in terms of step two, in terms of an Iceberg-first based approach for our data engineering pipelines. So what we want to start to do is to introduce Iceberg as the new storage interface for our data flow workflows, for reading and writing data in those data flow data engineering pipelines. So we want to start leveraging the new Iceberg I.O. connector for Apache Beam in order to do that. to start using Iceberg as that interface in those workflows. What we also want to do in terms of a third step is to trial BigQuery tables for Apache Iceberg. And you've heard about those from Gaurav and Pavan today in terms of being able to actually not just read Iceberg with BigQuery, but actually to start to write Iceberg tables via our shared-to-deskware workflows. So using the latest tooling such as dbt to actually start producing Iceberg as our output from those scheduled SQL workflows. So you see that we have some like very clear strategic goals there to start to achieve this this Iceberg-based future. And that's what we're going to continue to work towards in the coming months. So I hope that's been a really helpful insight for you into our Iceberg journey at Spotify and how we're integrating Iceberg with BigQuery. There's a link there which in terms of if you want to get started yourself with using external Iceberg tables with BigQuery. BigQuery. And yeah, we'd love to hear your feedback on this presentation, both from our perspective and from Google's perspective. And yeah, feel free to complete the session survey. I think we're out of time in terms of, you know, the time allowed today. But if you'd like to ask any questions, either to us or to Google, then please feel free to come up afterwards and have a chat with us. Thank you.