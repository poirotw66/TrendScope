 Hi everyone, thanks for coming today. My name is Lille. And I'm Callum. And we're spending a lot of time with our automotive customers and partners, as you are probably too. And we're really excited to talk to you today about what we're building to bring AI into the entire automotive user experience and automotive user journey. I'll give you a quick overview of what we're seeing in the industry overall a little bit to start our conversation. And then Callum is going to talk to you about what we've actually built and what we're launching. And yeah, we're excited to walk you through. We'll take questions at the end. So what we're seeing overall in the industry and what you've seen today as well and yesterday in the keynote is that the way that consumers are interacting with applications and devices is changing completely. Right? Like we're all expecting better ways of interacting with applications. For example, you want to be able to ask questions in natural language or maybe get a personalized summary of something. You might want to have like an agent that completes an entire task for you. So what you've seen yesterday in the keynote, for example, when we announced with Samsung, there's a lot of work that we're doing around autonomous assistance that can do things for you. So the way that users are expecting applications to work is changing and they're taking what they see in Google, for example, and they're taking that to other applications. Like, oh, if I can complete this here in this way, why doesn't it work this way in the other application? Right? So users, for example, want to ask questions with natural language and ask full text questions. So it's changing how we're interacting with applications. Now what's really important is that all of these assistant experiences are improving across a ton of different surfaces. So everyone's trying to own the experience across a lot of different applications. Right? So there's different connectors across different platforms. You want to use one app to connect into the other application. Super super important that this is changing how we're completing entire tasks and entire ways of doing things. So these assistants are becoming new surfaces that users interact with and Gemini and Generative AI is helping users really change how they interact. What's important here is that for automotive, this is a huge chance to become one of these surfaces that users use to complete a lot of different tasks. Right? So, of course, right now we're starting with smartphones and everything, but the car can become one place where you can complete a lot of productivity tasks, talk to whoever you need to talk to, complete things that you wanted to do. So becoming one of these surfaces is really important. Then you want this to be AI driven. Right? You want to be able to have the same user experience that you have in other new applications now in the car as well. For OEMs, this is extremely strategic for two different reasons. Number one is brand loyalty. So if you're thinking about building these new AI features and how users would interact with them, these features can become very sticky. So users get used to being able to say, oh, I want to be able to complete a task in my car when I'm driving. And as they're getting used to that experience, they will want that experience in the next car, the next vehicle that they're purchasing. And they get used to the brand voice that interacts with them. Right? So owning that brand voice and owning the user experience is really important for OEMs to be able to not get disintermediated with other application providers. Everything should be in the car, build your own brand, be able to talk to your customer in your own way. And that will drive brand loyalty. The second reason this is really important is monetization. Because there's a big ecosystem that OEMs are trying to tap into for new user experiences. For example, to upsell additional services. So let's think about the OEM entire ecosystems that they own, for example, like maintenance or upsell services, insurance, et cetera. Super important that they are able to monetize and own that entire purchase experience. But also maybe to open new monetization ways to tap into other applications and other ways of partnering with a pretty large ecosystem and AI can help them really do that. What's important here is how this is built. So this should be implemented in a way that the OEM owns the experience. The OEM owns the voice to the customer. And it's not a third-party application that is either in the car or on the device. There's a direct channel to the user. So we're really excited to talk to you today about our automotive AI agent, which essentially enables OEMs to build all of these new experiences now. So we've built this product specifically for OEMs to build these new user experiences. And we're combining a few different key things. So for example, from Google, we're combining key insights from our foundational research, from our on-device speech recognition. So for example, what we've done with Google Home, Google Pixel, et cetera. And then to do intent detection and understand, like, what is the user trying to do? How do we complete the tasks? And then actually do the entire completion and execution of the tasks. So a lot of research has gone into that. And that all is coming together in the automotive AI agent. What's really important as well is that Google's been investing a lot in our multimodal capabilities. So if you're thinking about how a user will interact with these experiences, this is not only a voice experience. There can be a text, maybe in a car manual, or videos that you want to showcase to the user, or pictures or something. So building these new ways of interacting is important both for the input and also for the output of what the AI models understand. And then lastly, we've built this product in a way that it really allows for feature customization by the OEMs. So we're not just giving everyone a black box and then here you go. It's more like here are the different use cases that you can build. And I'll walk you through those in a bit and Calum will explain how you'll be able to customize those to how the OEM wants to interact with their users. I'll walk you through a few experiences that we think are really key for the users. Everything's going to be symbol branded, which is our demo brand. But of course, you can imagine your OEM brand in there. So the first one that we're thinking about is imagine the user is in the vehicle. And there's a few tasks that the user wants to compete in the vehicle with natural language. For example, let's roll down the window. So in this example, we're saying it's really warm in here. What does that mean? Right? So the assistant on device has to understand what does it mean is it too warm? Could we turn down the temperature? Or maybe it's a nice day today. I actually want to roll down the window. So being able to then check back with the user on what the actual intent is, is super important. And then it can execute that. Everything can be done with natural language. The other thing that's important here is we're giving an example of media entertainment. So for example, let's say you're playing your favorite album already. It's recommended to you by personalized based on you entering the vehicle. And then you're saying, play me some chill folk music. Back and again, mean a lot of things to different users. So you want to be able to play exactly what the user is asking for based on the context that you have about the user. Maybe from other applications they've been using, history from their device, et cetera. Super important as well, productivity. So being able to do productivity tasks, meetings, content, et cetera, from the car. What's important here is that we're thinking that building the entire new experience that we're presenting to you enables us to really tap into a lot of different features that you have on your phone as well. So you're not just limited to what you can do with a Bluetooth connection, which would just be a text message. And then lastly, there are some really cool general knowledge interactions that you could have, for example, like telling me a story or call mom. So knowing what the user is already doing is becoming more and more important as well. And then we can connect it to all the different Gemini capabilities that we have here to have an actual conversation. People like to do therapy in car, I heard, as well. So it's interesting for that. The next use case that we are thinking about is maintenance, right? So let's say the user is driving, they're already listening to their favorite album, and now they're thinking about, oh, there's a light that's blinking in my car. What's going on? The way that we have designed this is that we don't think you will complete the entire maintenance in the car as you're driving. There's another step where you want to exit your car and then be able to schedule your maintenance appointment, maybe on your device, once you've exited the vehicle, right? So if you're driving somewhere, you might not know if you're free next Tuesday at 4pm. Again, this is important as well because you can digitize the entire car manual, save that from producing that. And then have what we call an interaction here. So you could have a Q&A interaction, what's the problem, where does this come from, and you can ground that both in the content from the OEM as well as the general knowledge of Google if you want to, right? So based on the question that a user is asking, maybe the OEM doesn't have all the answers. We're going to go into more depth on this one later. The next thing is planning your day. So this is important to make the car become really a surface that you're using to plan your day and to establish the car in the user's everyday. So in this example, we're suggesting the user already, like maybe a few places to go to, based on what we know about the user that they're doing already. They can obviously ask to go to other different places. We're planning a route for them, and then the user can set a reminder as well. So let's say, oh, I know I need to pick up flowers when I'm in the car. Complete that in your application, and then as you're entering the car, it surfaces those reminders for you. Pretty important just that there's a connection between in-car and outside of the vehicle. We're seeing a lot of OEMs being really excited about that use case. The next one is very much connected to the multimodal capabilities that I just talked about. So this is the can I park here use case, which actually is interesting both from like spatial understanding, leveraging all the sensor data that you have, camera data, and then also complex reasoning on the location where you're at. What's the, do you have to pay for parking here, et cetera. This is really an interesting in-car use case for also voice interaction, but it does really leverage content from a lot of different parts of the vehicle as well as just general knowledge, understanding of what does it mean to park here, right? Like, or maybe this is like a restricted zone or there's an event happening, et cetera. You can build that complex reasoning in. And the last one I think is really important for, especially in the EV space, there's a lot of new regulations that are actually coming up right now. But there's a lot of work that OEMs are doing around battery health and battery lifetime as well as charging costs. And really making sure that users can understand that and can ask questions about this and get proactive recommendations is becoming more and more important. So we don't just want to have an app or an interaction that you're doing once you're in the car or once you open the app. We want to be able to be really proactive and say, hey, we know you've not been using this feature. You should use this to improve your battery health. Or there could be another thing where you're recommending ways to improve your battery charging cost or something. All of this can be personalized and generated with Gemini. So that one's really exciting too. And there's a few regulations that are coming around the battery passport in the EU as well that kind of will make these things more interesting and mandatory as well for OEMs. The way that we're thinking about this one is this should be proactive also that you can connect it with other consumer tools. So let's think about Google Calendar, for example. You could connect with your Google Calendar and then say, hey, I know you have a long drive tomorrow. Do you want to charge your car now? It's not charging currently, right? So really establishing the vehicle as a more useful tool in the user's day-to-day life is becoming really interesting. And then obviously you should be able to do the same question interaction in the car as well. So I just gave you an overview of a few use cases that we're showing. But there's obviously a lot more that we know that you're all thinking about. And everyone thinks about these use cases in a few different nuances. So that's why we've built a product that ensures every OEM can implement these use cases in the way that they want to. What is important here is that we have the experiences. Sales and marketing is becoming important as well. So talking about insurance, maybe upselling additional products, experiences. All of this can be done in the app and in the vehicle. And then what's also really interesting is the entire area that we have over here, which is transferring context from outside the vehicle to inside the vehicle and being able to tab into all the different applications that you have on your phone in your vehicle, right? So not being limited to Bluetooth, as I just said, but being able to link your accounts, log into everything you have on your phone in one click in the car, for example, detecting who is entering the car and what experiences do they want. This can be more and more personalized based on which users entering and what applications they're currently running. And then there's the last one that is customer support and servicing. All of this obviously ties into scheduling a maintenance appointment, but then maybe also going to the point where you might have someone call you back and say, hey, I know you've been having this issue. Do you want to have someone come over to help you? So giving the customer service team also the context of what's been happening in your vehicle and the interaction that you've been having with the brand is becoming more and more important. I'm going to hand it over to Calum to actually talk to you about what we've built and how it all works. Thanks. Thanks, Lilith. I put a water there for you if you want it. Which one? This one. No, that one's mine. This one's yours. Thank you. Okay, I have the clicker now, which is good because there's a lot of animations in these slides, so try to bear with me. I'm Calum. I work with Lilith on Automotive AI Agent. And as Lilith mentioned, this is our new product that we've created in Applied AI here at Google Cloud to focus on how we can help automakers bring generative AI and assistant experiences to market. And there's three components of that that I'm going to take you through. And then ultimately, I'm going to take you through a worked example of one of those use cases that Lilith was mentioning. So you can get an idea how these agents actually work, how we're taking advantage of Gemini and its tool usage capability, how we're bringing that between multiple surfaces, and everything that you can take advantage of if you try to build this as an automotive OEM. Hopefully, some of the things are useful for those of you that don't work for auto OEMs, too, but are maybe thinking about other agent journeys. But with Automotive AI Agent, everything starts with what we call our pre-built agent. So this is an out-of-the-box set of pre-functioning capabilities that do everything that you would expect an in-vehicle assistant to do. So Lilith just talked about a bunch of them, all those use cases that you saw up there. But being able to, you know, do everything that you would expect it to do without having to add on to it. And we also have what we call our design time tools, which is the tools that our customers, the OEMs, can use to make that agent their own. Whether that's extending use cases by adding on additional features and additional functionality, or just orchestrating how the system works between the vehicle, the server, and the smartphone all at once. The design time tools are not meant to replace other tools that you've heard about this week, like Vertex or other agent building capabilities, but to extend them and allow you to bring those same agents you're building elsewhere into this vehicle and hybrid surface context. And then finally we have an integration layer, which is essentially our server-side APIs and SDKs that we make available to all customers that help bring together all of these things and power these use cases. And ultimately the goal of this is to enable a seamless experience of having one single agent that represents your brand and your brand identity, and helping your drivers and users that's available to them everywhere. So of course that starts in the vehicle by having an on-device agent that can run in the vehicle and that you can talk to while you're driving, regardless of whether or not you have a smartphone connected or even have a smartphone with you. But then we use our same integrated back-end so that you can extend that same experience, like what Lilith was just talking about, to the smartphone and to the companion app that you have for your vehicles that customers can take advantage of. And having this all be one agent means, one, you only have to build these experiences once, which for those of you working on agent building you know is valuable, but two, it also allows for seamless transfer across something that starts in the car and maybe you finish later in the app. And of course it makes it easier to have better integrations with things outside the vehicle. So as I'll show, the SDKs also let you take advantage of everything that your phone can do while you're talking to the car. Here's a little bit more detailed version of the architecture to help you figure out what I'm going to talk through in the more technical portion of the talk here. But most of the blocks that you see in blue are components of Automotive AI agent. And this is what we make available to our automotive customers to build with and leverage. And then in, but you can see in yellow, you still have your opportunity to build and integrate it into your existing applications, your brand identity, your UX. But the important thing to note here is that we'll talk about before is you can see the on-device and in-vehicle SDKs that make it possible to run completely end-to-end, even while you're disconnected from the internet. So if your vehicle is in a location where there is no internet or you're going through a tunnel, something like that, you still have complete functionality of the system. And that's important for robustness while offline. But in user experience testing, we've also found it's very important for latency reasons. Even if you do have an internet connection, you might not have a very good internet connection. So if you want this to feel very responsive and very human-like, like you're just talking to another co-pilot, it's important to have those little latency experiences. Then what we also have, and what's new this year at Next, is our companion SDK. So the ability to take an SDK that you also embed in the mobile application that you make available to your drivers and users, and connect all that information on the back end. So now I'm going to deep dive on one of the use cases that Lilith was talking about earlier, which is resolving a concern in your vehicle and kind of try to show you how all this comes together and how we take advantage of Gemini's reasoning capabilities and tool use capabilities to make it possible to help users across all services. So we have this example use case. You're driving, you see some light on your dashboard. Maybe there isn't enough information about it. You don't know exactly what it means or why it's there. In this case, maybe let's say it's something related to braking. So you ask your car, you know, hey, Symbol, what's this light mean, right? I don't know what's going on. So starting at the very beginning, what happens here is we have a wake word engine that's part of the AAA in-vehicle SDK that detects this hey Symbol wake word and it starts recognition immediately. This goes to our in-vehicle hybrid speech SDK, which does two things at the same time. So assuming that the vehicle is online, it's going to start streaming audio directly to the server so that we can take advantage of the latest Gemini multimodal and embedded audio capabilities if we do need to resolve this query on the server. But at the same time, it's also leveraging our complete on-device ASR stack so that if you aren't connected to the internet or you end up resolving this query in the vehicle, for example, if it was something very simple like set the AC to 72 and not a complex question about why my brakes don't work, we might be able to stay completely in this vehicle context. So we do both at the same time. In this case, it's going to be a little bit more complicated. So we make a determination on the device that, hey, this is a fairly complicated query. We want to be able to take advantage of both Gemini's reasoning capabilities as well as everything that we have access to when we're online, calling the various data stores that we have that might link to what this error code could be. So what happens is we decide to send everything online to our larger Gemini model that's running on the server side. And when we do that, we do two things. We include, of course, the completed utterance, this thing that the user said earlier. But we also package up a bunch of information from the SDK that we have on the vehicle about the device context. So this could be things like pulling that error code of what that light is, the user's location, what they were doing before or after that. Maybe this is a hard-breaking scenario. Things that the model could use with its various tools to determine what's going on and make a recommendation to the user. Now, moving on from this same box here of being on the server and trying to figure out what's going on, we now determine that we probably need to use some capabilities that take advantage of all the information we know about the car. So we're going to route this query through orchestration to our car FAQ agent. And the car FAQ agent is designed to take and use tools from a variety of sources of information about the specific vehicle. So this could be external OEM data. Maybe you have a database of error codes that can be used to look up information. You can also do traditional rag search through the tools that we have on Vertex for doing rag and look up information from the owner's manual. It's not one or the other. The model isn't necessarily making a determination here. It's trying to collect as much information through using these tools as possible to come up with the right answer back to the user. And then we will get a response back to our orchestration system. So this is back to the AAA server. We've used the agent. It's come back with, okay, this is, we know this error code is related to regenerative braking. These are some things the owner's manual says to check for if you have a regenerative braking error. It returns all that as structured information to the server. And then we use the information that we have about the actual agent on orchestration. These might be things like user preferences that they have for wanting shorter or longer responses from their voice assistant. This could be a system prompt from the OEM about the style or personality of how you want the agent to respond in certain cases. And we apply that to the answer that we got from the car FAQ agent. Ultimately, in this case, we're going to show this going now to Google Cloud TTS. We need to actually synthesize the response back to the user so that we can play it out over the speakers in the car. And then finally, we start streaming that audio back to the device so that it can play back the audio and tell the user, you know, this means that you have a problem with your regenerative braking system. Here's what you can do. Here are some things to try. Now, I know that that has taken me a very long time to go through, so I appreciate your patience. But in reality, all of this would be happening from when the user finishes that first query to starting this playback of audio in one to two seconds on the car, right? So this all happens very, very quickly, even though we're taking advantage of multiple models, multiple different kinds of models, multiple different Gemini prompts, all happening together and orchestrated together. So now the user gets some information back. Let's, we'll also store some information about what happened, right? So we have all this data that came up from the car that's on the server side now. We can store that in a database or another data store to say, you know, we know that this error happened. And you'll see why that's useful a little bit later. So continuing with our scenario, perhaps now the user's finished their drive, they've gotten where they're going, or they've gotten back home, but they remember that they had that light in their car and that they needed to do something to check on their brakes or if that regenerative braking system was working correctly. And so they're going to open up their symbol mobile app that they have that shows them information about their car, the battery state, whatever else. And when they open it up, the AAA companion SDK in the app will check in with the server. And it's going to say, you know, is there anything that I should know about? And maybe we can use the information that we stored previously to surface a notification to the user and say, hey, we saw you had this issue before. Is this something that you want to look into again? And in this case, we'll say the user does, you know, they want to get more information. Maybe this is something that they can fix themselves. Maybe they want to get information from the owner's manual about what they could be doing. And so now they're back into a conversational flow. So we're in the app this time. We're not having a voice interaction. This would be a chat or just like a UI modal where the user's clicking next actions in a chat interface type interaction. But we're back using the exact same back end that we were before. And we're also back to our car FAQ agent. So this time we're using different context. You know, the user is on their mobile device. They're asking this question. Maybe it was a query about, okay, how do I fix this regenerative braking problem? But we're using this same agent that was built before that's integrated with those same OEM systems, integrated with the same user's manual. I spared you drawing those boxes on this slide, but imagine it's that same agent that's integrated with those same systems. And ultimately, we can use that information to have a back and forth conversation with the user. So this flow might happen a couple times, right, to find out information. And now let's say perhaps we can show using one more agent. So we find out, okay, maybe this is a real problem that's existing with the regenerative braking system. And you need to schedule service in order to solve this problem. So now we're coming from the same SDK. We have a new query from the user. We go through orchestration again, this same system that's going to determine what agents, what capabilities I have to solve the problem for the user. And in this case, we're going to show using a custom agent. So maybe this is something that, you know, the OEM or the customer has built in Vertex using one of the agent building products. And it's integrated with another back end that they have that helps schedule service appointments. This knows about the dealers around the customer, times that are available, et cetera. And we can use this information to, again, go back and forth and have a round trip conversational booking experience with the user straight from the mobile app. And again, packaging all of that same context and sending it to this API. So when the user arrives at the dealer for their service appointment, they have all the, the dealer already has all the information from the vehicle about what's going on. That could be coming through the orchestration and stored there. It could also maybe be coming directly from this system, right? The car already has data reporting that comes through that system. So what I've shown there is, you know, one example of how one flow could work. And I think hitting on all of these, whoops, hitting on all these different points of taking advantage of the various SDKs that we have, some prebuilt agents, also some custom agents. And of course, everything is based on the Gemini foundations. But I think it's also important to note that this whole setup and the orchestration of, of this whole system on how it works together is completely customizable by the OEM using the design time tools. So how the agent answers, the exact style of the responses, what things can happen on the device or on the server side or where they need to be executed. Or even I showed in this case using online TTS where we use chirp three, like our latest TTS voices from Google Cloud to synthesize the audio. But maybe in some cases you would want to use an on device TTS that you have available to synthesize the response. So all of that is configurable by the OEM using the design time tools. But that's a little harder to show in the block diagram. Okay. Hopefully that gives you some insight into how you can build agents and how we're thinking about multi-surface experiences for automotive companies and AAA. And I'll hand it back to Lillith. Yeah, thank you, Callum. Thanks. Thank you, Benoit. I think the key thing here is that what you've just seen is all of these different components that are essentially customizable by the OEM, but they're prebuilt, right? So you can switch this on right now and customize the way that your users are going to interact with your vehicle, but there's no need to completely build everything from scratch. And if we're looking at what's happening in the market right now, a lot of OEMs are moving very, very quickly with this, right? So building everything new is going to take a long time, which is why we're really excited to be able to give you something that you can just customize. It is also important that it all remains in your ecosystem and in your data ecosystem as well, right? So you own the customer data. You're able to decide what to do with that data and how to customize your experience with that. And you can build further and further on these experiences, right? So you can say, I'm going to start with two or three use cases, but now you have all of these user interactions. You can make the user interactions more personal and then expand your ecosystem with that. That also goes to the next point, which is Google is building a large ecosystem of connectors as well. So that enables you to tap into Google's consumer applications, the calendar, Gmail, etc., but also into other third parties. So we're growing that ecosystem as well. So we're thinking about how do the OEMs in the end grow their ecosystem and enable new user experiences, ordering your coffee or whatever. These are all things that we can think about together on how you can tap into the Google ecosystem here, which I think is very exciting. If you want to learn more, there is a demo in the showcase in the manufacturing booth, which is like in the middle of the, pretty much in the center where the Volkswagen bus is. And you can test that out. And then if you're interested, you can, we're very excited to start a POC with you. You know all of your account teams here, so we work with them really closely. And then we're excited to hear from you what the use cases are that you are trying to enable first. And then we can start testing on those or show you what we, the different use cases that we have in the library. Yeah. Yeah. .