 Thank you for being here and on a fantastic, I hope, session with the same sense of excitement as what we feel with the agentic journey in Google Cloud. If you are in the journey of deciding agents and if you are confused, oh, there is a lot of options, no code, low code, high code, which is the right fit, then this session is tailored for you. Each and every one of you who are developers, data engineers, data scientists, decision makers, we will dive into the practical challenges which we have seen customers face, where we feel the control meets autonomy, and equip you with best practices, agent samples, techniques to confidently move forward. Let's dig into it. A little bit about me. I have my eighth year in Google Next. Two years in this team, applied AI engineering. And every week, I connect with 10 to 15 customers, understanding their evolving needs, especially in the realm of AI agent design. My role involves translating the insights which I get from customers into practical open source solutions and driving our product strategy. Through these conversations, it feels that balance of control and autonomy is very critical for your decision making. Today, first, I'll be wearing two hats. One is me as a solution owner who has gone through the same journey as you probably did while you are deciding AI agents. And I'll walk you through some funny examples. And then, with my customer experiences, what I have heard, what they have shared with me, I will also share with you. So that would be like two kinds of that. I'm followed by Irvan, who is the director of product management, who understands how this product strategy is navigated. And we have two awesome customers who have been with us in the journey. DBS from VML, affiliate of WPP, sharing their self disruption journey for two years in a GenTech platform. And Reno with their awesome use case, and they have a surprise for you as well. Let's start with the challenges. And I can guarantee you that it would resonate with you. If you have been in this journey for a bit, it would resonate with you. And you would see the product strategy of how it closely aligns to the customer requirements which we have seen in the past. And I'll share a little bit about the adoption patterns of how we are seeing. And then we have our customers coming back. Let's jump right into it. This slide is supposed to be confusing. It's supposed to be overwhelming. It's not meant to be easy. I wanted to put this slide because this is the solution I owned. Two years ago, I started the conversations with my customers. And every one of them were like so used to, you know, having, talking to, you know, unstructured data, asking questions against the data. And they all wanted to go to structured data. And there were several prototypes out there. If you have used LinkedIn, you probably saw many of these prototypes. Hey, this works. It does really good. It helps. But there were three major challenges. For enterprises, accuracy is super critical. And those prototypes did not offer that. Hallucination was very common. And I have dealt with that. And the third one is there is multiple data sources. Like, every data source has its own version of SQL. If you have used BigQuery, it's different. Oracle is different. Redshift is different. It's completely different. So this architecture we put together in July of 2023. Remember that time. There was no agents. Agents was announced in November 2023 in React framework. So we put together knowing the things, oh, maybe we need this. So these building blocks were put together. And we released our first version in November of 2023. And if you go and look at the code, you would see in today's agents, it is nowhere close to it. I would not, like, I would still recommend the architecture. But the inside code of it, which I just did share with me, because I want to be transparent with you all, that we need to be aware of it. Products evolve. Tools evolve. Features evolve. Research evolves. Everything evolves. And that is the reality we live in. And particularly if you are in this space, you probably are already aware, like, hey, what does this product do? By the time you dig into it, that product is over and there's a next product which is out there for you. So this is the reality we all live in. So this is my solution manager hat. Let me go to the customer hat. Building agents is hard. Who would say it's easy? I have had multiple customers. Oh, I have a lot of frameworks. They would say, oh, how would I pick between these frameworks? Like, there is LandGraph, there is Autogen, there is AG2, there is Crew AI. And now Google introduces this one, ADK. So how would I navigate it? And there's like, oh, my IT team built an agent. It was very well demoed at my senior executor. But it doesn't live up to the production standard. What do I do? And how do I connect multiple data sources? How do I cost check it? How do I make it interoperable? Like, my one of the business unit has created a LandGraph agent. My other business unit wants to build a Crew AI agent. How do I make it interoperable? These are real problems in building an agent. And we think, does it think we stop here? It makes it even more harder. Productionizing is even more harder. And if you are in this journey, we are still in the starting of the curve. Like, I know there is stats out there, number of agents in production, all of that. But I don't know if, how much of it is real. So I'm going to keep it real for you all here. It is super hard. There are so many words. People ask me, what is AgentOps? What is LLMOps? What is RagOps? What is, like, the terminologies keep coming. The hype keeps evolving. And if you look at, you know, how do I evaluate? How do I visualize with traces? How do I get a session drill down so that I can get insights? These are common problems. Your verticals may differ. You probably are in a different industry domain. But the problems remain the same. So these are the actual user insights we have gathered. What did we do? Irvana is here, the director of product management, to talk about the product strategy. Thank you. Thank you, Kenji. I remember when she came up with open data Q&A, first version, the world of ML to SQL. It was quite a thing. And it's true, it evolved drastically. So let me just tell you a little bit how we're thinking about this from a product strategy point of view. And living in a world of fast-evolving technologies and techniques, the way we're looking into it is that, for us, agents is eventually the next frontier of software. It's about delightful experiences. That's really the opportunity. Today, a lot of them look like chatbots, but many of them will not be chatbots at the end. It's really about thinking the way we work, interact with information. That's how we're trying to think the strategy above and beyond consuming research and innovation. In particular, when we do this in the context of Google Cloud, we want to do this in a way that it will go to production. So a lot of questions are about how do we observe and measure impact of these agents? Are they more than a demo to the boardroom, but truly solving a problem? How do we give you the quality, the reliability, the safety you would expect for a production-grade enterprise service? And then how do we cope with that constant innovation, the associated risk-killing, the need to challenge your architecture? How can we help you abstract some of that by curating the innovation? So we're trying to cover the whole spectrum of building agents and then connecting these agents with other agents, and then having them discovered by users and effectively used. We're trying to think about that whole life cycle because today we'll focus a lot on building, but eventually what matters is also that the agent is discovered and used by somebody. So the spectrum of building surfaces is pretty broad in the industry, from very quick time to result with a no-code interface where I can express in natural language what I want to do, all the way to hand-picking everything in the stack and have a full DIY motion. All the boxes on this slide are available now on Google Cloud. So the out-of-the-box agent, like you don't need to do anything. It's in agent space, for example, a deep research agent. No-code agent development. We put it in agent space as well because that's an experience for any employee. You don't need to be a dev. Now as you move into the space of being a developer building an agent, you go on Vertex, and on the Vertex platform you now have the low-code agent development with ADK, the agent development kit. You have the full-code approach, meaning that you pick another framework, but you use Google Cloud and notably the agent engine to solve your runtime scale and scale the usage of that agent you may have built, say, on LandGraph. And of course you always have the full DIY. So why do we think you need multiple building surfaces? Well, because everybody eventually wants to build an agent. So if I look at the case of agents for internal employees on agent space, we really wanted to give you this out-of-the-box agent by Google. I just use it. My no-code surface where I can build and also the opportunity to surface an agent that my development team would have built, say, on Vertex or elsewhere, or an agent I'd pick from a vendor I trust, an agent from AgentForce as an example. And so we really want to meet the building needs wherever they are, and agent space is the highest level of abstraction. I don't need to be a dev. Now, if I'm a dev, the news that we've announced this week is how we've truly populated agent builder within Vertex. So in Vertex you have model garden, you choose a model, you have model builder, a bunch of tools to optimize your model, and then agent builder. And so in agent builder, the green part is what I'm going to double click on because that's where the news of the week are. For your information, Google uses ADK ourself internally. Agent space relies on ADK. Or if you know our customer engagement suite, agents, they're actually relying on ADK as well. So ADK at the core is an open source Python SDK by Google. Now you may wonder, why did you come up with yours? Well, we think that having both the model and the framework evolving quickly together can save you time, and I'll come back to that. But beside the ADK itself, we have this engine underneath which can run agents you would have built possibly in other frameworks as well, and which gives you monitoring and evaluation capabilities to think of those topics holistically, whatever framework you pick for your different agents. We've also added an agent garden, which is in the Vertex console, and from a developer point of view, that's where I'm going to find samples and suggestions to get going in my builder journey. Why did we develop our own framework and decided to open source it? There are very good frameworks out there. I've named a few already. We did it for a couple of reasons, one of which is our ability to make ADK take advantage of Gemini evolution extremely quickly. ADK works with any model, but it works even better with Gemini. So when the reasoning capabilities or the multimodality of capabilities of the model evolve, you'll be able to take advantage of them very quickly with ADK. That was a big motivation for us. And by the way, the selection of the model matters. The leaderboard to the right is a hugging face leaderboard about performance, vertical, cost, horizontal. The green arrow is Gemini 2.0 Flash. The blue arrow is Gemini 2.5 Pro. Those models are very good to build agents. And that's why we thought we need to offer a combined experience between a framework and a model. The other motivation to come up with our own framework is related to the availability of connection to enterprise systems that we've built over the years with connectors with APIs. ADK fully embraces MCP, by the way. But if you also want to rely on APIs to connect to legacy software applications in the organization, you have all the Apigee and API library. Also worth mentioning is the ability to do deterministic workflows with our IPaaS offering called application integration. Those workflows then can be used in the design of your agent to facilitate sequences where the agent has to follow a predicated sequence with high fidelity. The other piece we have in Vertex AI agent builder is the runtime. So think of that as an abstraction of the infrastructure, adding observability and evaluation services to run agents you would have built on ADK, but possibly also on other orchestration frameworks. We already support LangGraph, LangChain, CrewAI, and AG2. So these are the products we're bringing to the builder to build. And then how about distributing this? As long as you're talking about an agent that's going to help an employee in an organization, you want to think of agent space as your distribution channel. So the agent gallery we've added in agent space allows you to post an agent, whatever it's built by Google, built by a third party vendor, or built by yourself inside your organization, allowing certain employees within your company to find that agent and take advantage of it. We're adding also, we're working on the discoverability of the agent. For example, giving you other ways than the gallery to find an agent that could help you. Ultimately, what we're dreaming of is that you type your prompt in the big search bar of agent space, and agent space is going to connect you or suggest an agent that is very appropriate to your intent. So we're really thinking hard about the distribution and discovery of agents above and beyond building them. So the whole spectrum is now addressed from a product point of view. There's a lot of new things which came today. In building agents, you can do no code in agent space, or you can do low code with ADK in Vertex Agent Builder. In discovering agents, you can use the agent garden as a developer to get ideas. You can use Agent Gallery as a user if you want to look for an agent to help you. And then the one thing I did not mention is that we've added agent-to-agent protocol as well to start building multi-agent systems where two agents collaborate together. And we have 50 large software companies who endorsed it and are supporting this protocol, which we are volunteering to the industry also in open source. So hopefully that gives you a sense of how we're taking the challenges that Kench highlighted and turned them into products. But now let's go back to the reality of doing this today, especially with two customers who tried ADK before it became public. So I'll hand over back to Kench to talk about adoption patterns, and we'll go to the VML and Renault stories. Thank you, Elvan. I hope you saw how the challenges which we have seen with customers drove the product strategy. So let's go to what it means for you because we heard a lot, and we are going to try mapping it and see how you would have the control and autonomy. So first thing we always need to be worried about is the choice of model, right? You want to be flexible. You don't want to be tied to one model. And you want to be testing with multiple models. So yes, Gemini or fine-tuned model, or if you use Light LLM, you can use Light LLM, or a model garden. We have about 220-plus model garden endpoints which are available. Then it comes to choice of framework. Choice of framework, again, ADK was just announced yesterday. I would hope you would want to try it. But if you are used to some other framework, you're welcome to use that as well. Next, choice of samples. I'm sorry. Choice of samples. We have in-agent garden samples currently for tools and agents. However, it's only for ADK currently, but there is plans in including other frameworks as well. So you could go ahead, look at it, download the samples, use it right away, and you would be able to use it. And I'll walk you through some of the things, and you have a surprise from Renault actor as well. So choice of runtime. Of course, you know, deploying the agents keep coming. If you are used to, you know, building agents, you probably used a container-based approach, and you probably have, you know, deployed in Cloud Run. But you have to manage the whole thing by yourself, the agentic platform. And when VML walks through, they'll walk through the agentic platform they built. But you could introduce agent engine for that, which comes with its session, context, memory, all of those available. And then the last thing is, how do you surface those agents? That's where agent space, customer engagement suite, or some other application for you comes. So the choice of products which you heard are mapped to this permit framework here. So I get asked often this question about where do I have more control and autonomy? So when you think about, you know, where you would want to include, some might prefer completely doing it themselves. And I have seen customers like that, and some customers want it to be completely managed service. And do you have an option to pick what you like? What I'm going to do in the next slide is use my same open data Q&A example, which I told you about, right? That complex slide which I had. Let me walk you through that. So the first one, choice of model. At that time, we had Palm. We didn't even have Gemini. So when we were thinking about building it, we didn't have multiple models to test with. So I would like to test it with Gemini and see how it does. And we used a RAC-based vector architecture because that's what was available at that time. But now there is long context window. I would like to see it, how it works. Then it comes to choice of framework. We used Gemini with function calling towards the end. That's what the code is available out there. But I would like to try to see if there is other frameworks out there with differentiated ability. And we did try it. And the choice of runtime, we used Cloud Run in the example. You could deploy it yourself and check it out. But I would like to try it with Agent Engine. Today, all of this we redesigned and we released it as a sample code in ADK samples under Data Scientist Agent. And you would be seeing that live demo pretty shortly. What are the architecture patterns we have observed with customers? This comes pretty frequently. When we are seeing how the orchestration evolves for a multi-agent architecture, we always have, you know, sequential one. There would be one of these kind of architectures. And then there is this parent sub-agent transfer between one agent to another agent, making sure the context gets carried forward, making sure the handoff is done properly. And then there is the loop architecture. In the open data Q&A, we have an example of it. We do debugging and validation for like three times. But there could be other instances where you do. And then there is this parallel agent where I see folks running, you know, like having one orchestrator agent. And it would trigger ten agents. And then there will be a summary agent which summarizes. Again, these are the types of architecture patterns I have seen. But there are other types as well. Like there is always a rag agent. There is always a retriever agent. And there is always an evaluation agent. Some of these patterns you probably see with your customers. So here I talked a little bit about in general. But I want to go into what did we hear from Agent Development Kit. I, we were, you know, privileged to be part of the Allerlisted program. We work with close to 230 customers. My team personally work with 30 customers. And most of them or all of them have talked about the UI and the ability to do local development as a key differentiator. And there is also simplicity with how you're able to build and how it is easy to navigate. This has been quite common. Google Agent Engine. We were also privileged to be part of a core set of a low listed program. And most often they like the flexibility they can with the OSS frameworks. Like you can bring any framework and it's able to adopt. In fact, Landgraf is the first class citizen for that agent engine. And if you haven't seen Agent Starter Pack, I highly recommend you go and look at it. If you like the CLI experience, it's an open source, you know, solution which is out there. But it integrates all of these frameworks together just with the CLI command. So these are the Allerlisted customers and you heard it from me. But we would like you to hear from the customers. And we have first DBS from VML, an affiliate of WPP, sharing their feedback on how they developed the agenting platform. Thank you. Thanks, Kanch. It's a pleasure to be here today. So I'm David Bartram-Shure, DBS, and I head up the agentic lab at VMR Map. Today I've got the privilege of talking you through our agentic platform evolution, which was of course built out on GCP. This actually led us up to the point where we were privileged enough to be one of the testers for the kit that you've heard about today. But I'll talk you through where we started and how we got to that place. So we started with one key question. How can we disrupt ourselves and our clients and our industry with agentic AI? And the second question is how can we live by and continue to adopt this disruption for the long term? So innovation, AI and ML have been a core part of VMR Map success for 20 plus years. But I'm going to take you back to 2023 where this really did start in earnest. But first I'll give you a quick outline context on our business and our problem space. So VMR Map is a world leading center of excellence in marketing, automation, personalization and CRM. Ultimately we use data and technology to humanize the relationships that our global brands have with their customers. And the key thing to note on this slide is in the top right hand corner there, 100 billion communications per year. And to achieve this number we tackle huge global operations across 150 different markets. And we put in place sizable teams of specialists across both VML and our clients. And to supercharge those thousand plus specialists we build technology. And most of you will have heard reference to WPP Open in yesterday's keynote. Well what I'm going to talk you through today actually underpins the Open Experience Studio. And the Open Experience Studio takes you all the way from the strategic briefing to the activation all the way through to optimization. And what feeds this is a huge amount of data signals and yeah. So there's the context. Now onto the journey. So today I'm going to cover both the technical journey but also the business journey. How over the course of two years we managed to gain consistent buy in and investment. And ultimately trust in the systems we built and our strategy behind the Gentic AI. So we started with marketing operations. And our goal was to go beyond the chat bot and look at Gentic operational workflows. So marketing operations for those who don't know are the people and the process and the tech behind how we achieve this global scale. The orchestration of that scale. And they oversee the process all the way from the initial briefing of a campaign. The choices around the audience and the content. The compilation of that content. The rigorous quality assurance processes. And then the activation across our platforms like Adobe, Salesforce and Braze. So we actually started by doing a massive assessment of across those 350 people what are the exhaustive set of tasks that we believe could be tackled with AI. How do they group up? What are the value measures against them? And how can we validate them? And a great example of a task could be within quality assurance. So checking all of the fragments on a communication. The images, the text, the blocks, the links. Do all of the links on a piece of content actually land on a relevant page or an appropriate page? And these would fall under context checks. So we grouped these up and we also applied different levels of autonomy to these tasks. And that's what you see in the big black diagram there in the top right. And this was really a blueprint for us. So the key learning at this stage was to look for compound interest. Across those tasks that had applications across multiple stages of the process. But also across multiple areas of the business. And always apply a value measure to the task that you're trying to tackle. And this really helped with exact buy-in but also with prioritization in the next phase of this journey. So in the next phase we wanted to do a rapid assessment of which of these tasks could actually be solved with AI. And in this case we're talking about LLMs. And which of them actually delivered business value. So technology wise this was lightweight. It was testing a notebook environment or a chat interface. But it was getting the SMEs involved from day one. So they were part of the setup. They were close to the design and started to understand what the inputs were. So we tested the best models first. And we worked with the SMEs, the marketing operations specialists to input on both validation data. But also on data the model could and should learn from. So the task deployment stage focused on making tasks actually work. But also on gaining adoption from the SMEs that fed into those tasks to begin with. And some examples can include segment description to SQL or content QA. So we took these prioritized tasks and we deployed them actually where the users already were. We didn't want to create an extra click or an extra application because there's too much friction there. So we actually deployed these as web hooks directly into JIRA. Because JIRA was the process tool that they used that was directly hooked up into the open experience studio I showed you earlier. And MO started to take ownership over the validation test sets, the few-shot learning, and then ultimately the system prompt optimization against that validation set. And the learning here was we need to make sure that it is ownership and there is validation and we know these tasks are actually working. So next we moved on to the terministic workflows. We took these tasks and multiple tasks were triggered from a single place automatically in the background. Again, we maintained validation rigor. Instead of validating tasks, we validated the change of tasks. The example on the screen here is what we call the end-to-end issue monitoring. It tracks the campaign all the way from that open experience studio into the hands of the specialists into the execution platform. And the key learning on this one is that deterministic workflows work really well once you've got them set up. But from our perspective, agentic AI is there to make us better and to increase our capabilities. So how do we move from a place where we're able to do A, B, C to a place where we can figure out that there's something that should happen between B and C? What should that be? Should it be a fix or an improvement? But we didn't jump straight into undeterministic workflows. We actually focused on tools. And we assessed these tools within that deterministic workflow. So we had that validation set. And this type of visual is really what opened the eyes to saying, well, what can we do more with agentic AI? So we've got tools here. We've got tasks that could be achieved with these tools. But we can then start to say, what are the additional tasks that we can now do based on these tools? How can these tools interact to achieve the tasks that we, you know, we couldn't do before? And finally, once we had those performant tools well-defined and validated, we can then move on to self-directed workflows, which is really what I think of when I think of agents. Goal-driven. They define tool trace usage. And they're based on reasoning. And we also set up triggered goal-based handoffs with the human-based tasks that we could deliver straight back into that operational workflow. And this brings you to the point where you can redesign workflow, human plus AI. So how do we get there technically? So this is a high-level reference architecture that covers both the testing phases, the task-based deployment, and the workflow and agentic workflows. We are publishing a technical article that covers the details of this. Wasim is sat here in the front. He's our principal architect and engineer behind this. So come and grab him for some details. But this is really what got us to the point where we could actually test the ADK in a way that would give us, you know, a comparison set to really assess it against. And I won't talk in too much detail about those learnings. Again, you can grab Wasim or check out the article for that. But I'd say kind of two things. One is that the bi-directional streaming is a game changer. And the second is that we are confident enough now that we're looking at migrating what we already set up over to that. And it was only released a couple of days ago. Or yesterday. So where's this going next? Today I kind of took you through the marketing operations deployment. But this ladders up to a much wider ecosystem that we're looking to build across all of our capabilities in VMLMap. And this is actually something we're building for ourselves and also for some of our biggest clients. And finally, I won't talk about this, but I'll leave this for you as a bit of a takeaway. It's all of our learnings that we captured from those last two years. And if you want to talk about them in more detail, come and grab us afterwards. So I'll now hand over to Erwin to talk you through Renault. Thank you. Thank you, DBS. Thank you, Wasim. This is real. Like, we did six months of Tracer Tester program. I think the team at VML gave amazing feedback. So feel free to grab them. For the second experience with Renault, we thought we'd change the format a bit to keep you awake with us. We'll do a little bit of an interview. Sok, welcome. What a pleasure to have you. Yes, thank you, Erwin. And maybe first a quick introduction on Renault. Yeah, sure. So my team is the AI Center of Excellence of the Renault Group. So our team's mission is to accelerate the delivery of AI solutions from machine learning and deep learning models to, as well, now Gen AI and AI Agent. And if you don't know about us, we are an automotive company. Yeah, one of the largest automotive companies in the world, actually. So let's talk maybe about multi-agents. I know you evaluated a number of frameworks through the journey the last three years. So maybe a quick wrap-up of the journey since 2023. Yeah. Our journey was typical of any company. We started with internal LLNs because our company wanted to use confidential data with them. So next we started to build a platform that was allowing them to create AI assistant doing rags on our knowledge base. And then this year we are starting to study the multi-agent system. And really, because what we want to do is to have something that is complete and have very complex things doing and resolving mini-tasks. So what we did before implementing a really useful scenario, what we chose to do is to benchmark the main framework, AI framework, like Autogen, CREWAI, FIDATA, and MetalGPT. And as data scientists, we felt very comfortable using high-level frameworks because it's easy to use and we want to do fast prototyping, quick demonstration, quick iteration, and get the feedback from the user. And that way we had the chance to be part of the journey with ADK. We received it in January, end of January, and we started looking at the documentation, the code, and we thought that it was really easy to use and we thought all the main features that were in the user framework, they were also in the ADK. So we started to build on ADK and that's where we started. So let's look maybe at a real example with this particular use case. Can you maybe give a little bit of context on the business problem you were trying to solve? Yeah. One of the main pain points for our customer is to find EV charging stations. We are lacking them in France and in Europe. So we have Mobilize, which is a subsidiary of Kono, and what they want to do in the next three years is to install more than 600 EV charging stations. Those stations are very expensive. So we thought it was a good idea to provide the business analysts working on the project with an assistant that would be a multi-agent. And so the multi-agent is helping them decide where to implement? And so if we may be telling a bit more about that agent to help an expert actually. Yeah. That agent, the task of the agent is going to look at different data sources that can be external or internal. And sometimes it's stored in databases, but we also can call many APIs. At Kono we have like 2,000 internal APIs, and we want to maybe later call them, call the predictive models that we are building inside our team within those agents. So if you look at that project and just reflect on it from today, what has been the biggest technical challenge for the team? Yeah, that would be surprising, but from our point of view, there was no big technical challenges using ADK. So everything felt very easy to implement, to code, even if it's not obvious to start with a framework that is private because you don't have the history and the issue that you can find from other developers on Stack Overflow, for example. But we always add the support from the ADK team, the Google team. We could talk to them like every week, ask them questions, and see if there is something in the roadmap that could help us later. So you had started building this with other frameworks in mind, and then end of January you get the ADK as a preview. Do you want to comment on the effort it took to port the work and to adapt to the new framework? That's a good question. It was fairly quick, I think. It took us a few days because the starter kit of ADK came with many examples of data, of agents that were working pretty well. So all we did was finding the right data in Cyber Data Lake, process it, manipulate it, give it the correct format, plug it into the data agent, and started asking questions to the agent. And we saw that it was already answering complicated questions. So we said, okay, maybe we don't need to build a text to SQL agent ourselves. That's something that we were planning to do. And as the one from the ADK team was working very well, we said, okay, we would, yeah. By the way, the data science agent is one of the samples you find in the agent garden within Vertex. So this is the type of things that can help you fast track, and of course, NL to SQL, it's a very common requirement. Okay, so why don't we show this thing, right? So I'm going to move here because we have a little bit of a technical thing. I need to change computer, but the Renault team did actually record a demo that we're going to play for you. Our fast charge agent comes in two forms. First, we have a conversational agent. Here, our expert wants to know if a particular address already has plenty of charging options nearby. This agent is powered by the ADK team's data agent, connected directly to our data lake. Data preparation step was very important for our data agent to perform well. Tables were processed to be clustered, partitioned, and described with description in all columns and tables. We've also added tools to access both internal and external APIs, including Google Maps, intermediate steps visible for increased transparency. Agent is then used to search for and identify nearby points of interest that could be appealing to electric vehicle drivers looking for charging stations. Think of it as an open forum where users can freely ask questions, brainstorm ideas, and request features. Now, let's look at our comparator agent. This agent leverages a pre-validated set of questions gathered from the conversations between our first agent and business experts on EV charging stations topics. We'll provide at least two addresses to our fast charge agent, which will then route these questions to the appropriate sub-agents. Our target is to include and validate most of the recurrent questions asked during the analysis of a location. Those questions are very diverse and require multiple sources of data. As all checks and validation were done with the conversational agent, intermediate steps are hidden in this version. Once complete, the comparator agent compiles the responses and generates a report highlighting the most promising locations for further evaluation by our business experts. Awesome. So that's, by the way, a real demo of the agent with a nice little AI-generated voiceover. Maybe as a final question, if we move to the next slide, I guess, yeah, here we go. Oh, no, I don't want to go to that one just yet. Let's just go back one. But maybe if people in the room wonder if it's worth to be part of the Trusted Tester program, which is an effort, right? The product is new. You don't have the community support and so on. Any advice for people who consider using the Trusted Tester program in the future? Yeah, for us, it was a real accelerator in the sense that we only had to focus on building the specific features of our project. The AI space is moving very fast. There's news every day. It's impossible to follow and to test everything. So we only focus on our business requirements instead of building components that will later be available in ADK or Agent Engine. So that path is really a good thing to do. Yeah. And we know we have a lot of products in Trusted Tester program. You may find it confusing at times. So just talk to the Googlers who support you to help you find the jewel. This particular program was made available to anybody who asked for and will continue that sort of approach in the future. Quick wrap up before I welcome DBS and Kench on stage for a few questions if you have any. So as a recap, if you're building agents on Google Cloud, we did not talk about the left side today. It's the no code experiences for the business user. You don't need to be a dev. You could be a dev. But if you go in agent space, you build agents for employees. You have a no code interface there. If you are into customer type bots, we have a customer engagement suite of products, which was demonstrated. That's the flowers demo during the keynote, if you remember. So that's the left part of the screen. No code business user. What we talked about today is the right part. So you're in Vertex as a developer and you want to build using the framework of Google that works very well with Gemini. You go with ADK that runs on agent engine. Or you use another framework, LandGraph or others, and you rely on agent engine for the runtime, the observability, the evaluation service. And last but not least, you can always go absolutely from scratch, for example, with Gemini and function calling running directly on Cloud Run. These options have pros and cons in terms of time to resolve skills and time you need to invest. Obviously, we're keen on sharing feedback on the latest one, which is ADK. With this, Kench and DBS, if you want to join us, we're happy to take, I think we have time for a few questions. If there are questions in the room.