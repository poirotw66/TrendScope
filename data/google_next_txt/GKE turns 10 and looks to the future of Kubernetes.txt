 Hello, everyone. I would like to welcome you to GK's 10th birthday, where we'll be talking about the future of Kubernetes. I'm Drew Bratsock. I lead product management for Kubernetes and serverless, including Cloud Run. And joining me today is the legendary Gary Singh, as well as Basil Schicken, the CTO of AppLovin. At its heart, Google is a container platform. And every week, we actually launch over 4 billion new containers. Let that sink in. It's 4 billion new containers every single week. And where we've seen the greatest pace of change is actually Google's adoption and usage of GK for core products that you all know and love, ranging from Vertex to DeepMind to Nest and even Waze. I think what makes Kubernetes so fascinating is where it's come from such humble roots. We started many, many moons ago. I think it's actually 12 years at this point coming out of Borg, and that's where the stork is, moving on to that very first code commit in 2014 for Kubernetes. A year later, the very first managed Kubernetes with GK came out. And after that, the pace of innovation continued and continued. We added on-prem. And apologies to a lot of you who deploy Kubernetes on-prem. It's still not the easiest thing, but we have come a long way in the cloud. From there, we've moved on to Cloud Run with a different container platform, added autopilot, and more and more innovations came. One thing I noticed in my slides, and Gary did not add it, but if you look at the pace of innovation on GKE, it really accelerated after he joined the organization. And what we're here today is actually to talk about the milestones we've all reached together. Last year, Gary and I had a blast celebrating Kubernetes' 10th birthday with a huge party at Next, actually in the Skyfall, and a series of events in partnership with the CNCF. Fast forward to this year, and we are very happy and very excited to celebrate GKE's 10th birthday. Now, we did try and organize GKE for all of you, but due to some catering and auto-scaling issues, it didn't work out. So apologies, the GKE is not real. But there were a number of comments slash complaints about Gary singing and my singing for happy birthday. So I think you all dodged a bullet despite the lack of GKE. In the past couple of years, Kubernetes has really evolved itself as well. If you look at its humble beginnings as a stateful, ephemeral platform, the use cases, workloads, and capabilities have evolved, and they've evolved thanks to all of you in this room. You've pushed GKE and you've pushed Kubernetes in new directions, and the community and Google has constantly adapted. At its heart, though, GKE is about innovation and how we can help the community. And this is where Google being a leader in the contributions for Kubernetes is so critically important. Every single year since Kubernetes came out, we have been the largest contributor to the product. And in fact, this past year, and the year before that and the year before that, our contributions in one year were bigger than all the lifetime contributions of almost all the other hyperscalers. There's exceptions to this, but we feel it's critically important you actually invest and put the time into chopping wood and carrying water in the OSS ecosystem and do the hard work to make sure that Kubernetes can address the problems in the future and we're all pulling our own weight. For us, GKE is focusing on the tasks that you don't want to do and shouldn't have to do. And core to this is actually the things that don't get a lot of attention, but for every person in this room are critically important. It's security and reliability. And these aren't the ones you'll see on stage with TKE. There's no AI agent for this. But making sure you've got a platform that you can depend on and don't have to worry about is one of the core investments in almost 50% of the work that goes into GKE. Now, this is something that really stands out on the lack of attention. If you think about it, how many security notifications have you heard about on GKE? Right? Very, very little because we're investing so heavily in this such that you don't have to. The flip side of this is how do we make the platform more scalable for your own platform teams or if you're an individual developer, for you as well, which is critically important for AI where more and more people hit the platforms directly. And we're looking at scalability. We're looking at day two operations and making Kubernetes easier and easier as you use it. Similar to how not all hyperscalers contribute to Kubernetes the same way, not all versions of it are the same. What makes it incredibly powerful and why it is now the ubiquitous platform for cloud computing is the fact that it is OSS. And it's one common language. But there are differences. The joke I always make with Gary and I is that I'm Canadian. I spell color properly with a U. Right? So anyone else that spells about the U is wrong. But you can still understand me. Canadian, polite accent aside. But the difference for the Kubernetes are much more stark. The layer that allows you to have a common platform on top of everything gives you the power to truly be multi-cloud. But the differences underneath are what makes GKE so powerful. One of the most obvious ones is the investment we've had with scale and how far we've been able to push GKE. Actually, in 2020, we surpassed the 15,000 node workload limit on GKE and did this with real production workloads with customers. And actually, at one point, had the seventh largest supercomputer in the world running on GKE. That milestone has not been beat since 2020. And we actually introduced 65,000 nodes for scale for large training workloads and also stateless workloads. We can also support 50,000 TPU chips leveraging multi-slice, which gives you unmatched abilities to run really large training jobs. In fact, there's a great session with Anthropic and GKE talking about how this was achieved with an engineering partnership together. Now, you may say, and I know my marketing folks will say, this is an awesome slide. 65,000 nodes, that's incredible. This is worthless to me. I'm never hitting those limits. So why should I care? Why it really matters is all the time we've spent pushing the limits of Kubernetes from the control plane to the API services, even changing our control plane from etcd over to spanner, we've encountered, run into, and corrected the major performance problems that you scale so all of you don't have to. And that is critically important. You'll actually hear about this from AppLovin later, such that you can scale to immense workloads and know your business can always grow and aren't going to get hamstrung by the limits of what's in OSS. Fundamental to this is understanding the different parts of auto-scaling, but also not. And what I mean by that is you shouldn't have to fine-tune every aspect of this, whether you're scaling up, you're scaling out, or worried about how fast your control plane will scale, but what we've done is make it more and more reliable, such that the more you put your foot down in the pedal, the faster it's going to go no matter what you throw at it. And that is through a combination of OSS to make sure there's commonality. We lift the entire Kubernetes ecosystem up, as well as unique optimizations for Google's compute, network, and storage only for GKE. And the end result of this is actually a little bit of an eye chart. I realize with a large room it may be a little hard to see, but we literally, right now, have thousands of thousand-plus node clusters running on GKE. This is not an anomaly. We have 10,000 node clusters spring up. We didn't even know we're running. And what we'd be able to see is we can burst up from that thousand node, scale up to 8,000, continue on, and scale down because it's a recurrent pattern through all the hard work in the other sections. And this is definitely not all AI. This is a lot of real commerce workloads, and there's a number of sessions you hear about it this week. All this leads to what some of you may have heard about this week. I think AI has been mentioned ad nauseum if it was a drinking game. Well, we'd have other problems than Gary and I singing. But for us, it's helping you once again overcome the AI problems and make it a non-issue. Because if you look at this, AI is creating new challenges. And there's an element of fear where it's like, how am I going to leverage the skills I have and possibly address what my business is asking for, what my lead architect is asking for, what I bet my company on. And the problems do vary. Accelerators are still hard to get, right? We've gotten better and better TPUs, but the best TPUs are in very high demand. And if you look around, the almost hypnotic magic circle, is AI infrastructure can be very different to deal with. And then you've got that classic balance between performance, which is critical to differentiate your business and run well versus efficiency, which really means cost. And then all throughout that, you have very different security threats. It's not just about access. It's about controlling actually your models that have maybe the lifeblood of your business in them. Now, what I want all of you to remember is it's going to be okay. All the skills you've picked up on GKN Kubernetes can be directly applied to Kubernetes now. And this is a lot of the work we're doing with GKE to make sure concepts such as how you slice up GPUs, how you use TPUs efficiently, are just a concept that works in Kubernetes just like your view on CPUs. One of the biggest things we've seen is that AI is forcing a very aggressive evolution within GKE. And what I mean by that is the expectations of all of you went from, I won't do it here because they were already here, and I'm about six feet, so you can imagine, or 185 meters for the Europeans in the audience. But it is really ratcheted up, and that's because the demands of our partners and customers are much more strenuous now. They are literally moving at the speed of AI. And a great example of this is actually work we went from a weekly release to a daily release to a half-daily release to an hourly release with some of our customers and internal partners to giving pre-releases so they're literally testing code with us with not GA yet. And that's because as soon as they find bugs in NVIDIA drivers or enhancements in TPUs, they want to make sure that they can take advantage of it that day to get the performance improvements. And that's forced us, just like it will all of you, to change the pace of everything we do. On top of that, the needs of training is no longer just about cost. It's about good put and how well these large workloads are running and optimally, how do you get it going for your ML teams. And in inference, there's a whole slew of new challenges that we want to hide from you so it's no longer a worry. One of the last things is around obtainability. As I mentioned earlier, a challenge is getting access to the latest and greatest or effectively using older hardware for inference. And that's where, if you look at it all up, what makes Google and GQ really stand out is not us. It's not Gary. It's not me. It's not all of you. It's an end-to-end stack. We're at the foundational layer. We're looking at what we do with AI or hypercomputers such that it's great compute. It's great storage. It's great network. Optimized for the workloads you're running on. Then, if you move up, it really is Google's managed Kubernetes service taking advantage of this hardware and optimizing on day one such that we've already done the heavy lifting and make sure it's running optimally. We also now have, and I believe, I missed TK's keynote, but announced earlier today, Cluster Director for Slurm, which is our managed Slurm offering, which is very similar. It's how do we make sure you even optimize stack on top of that foundational layer for Slurm. On top of that, we actually have different consumption methods that we'll speak about. And all up is our great managed AI offering, Vertex, which brings a wide range of tools you would have heard about earlier today. All of that actually runs on GK. And the fact that this is a layer of fully integrated Google offerings that you can choose individually has forced each layer to be optimal on day one, and that's where the evolutions come into. One thing I'm super excited to announce today is GK Inference Gateway. Traditionally, if you look at routing of your workloads, it was CPU-based or other common metrics based on known behavior of stateless or stateful workloads, which works quite well until it doesn't. I know load balancing is always a dicey topic sometimes, but the problem with that is that inference is not the workload you've had before. It's highly variable. It needs a great variety in terms of latency times or queue issues. And if you take a typical round-robin approach, you're going to end up with some heavily utilized GPUs or TPUs and some underutilized ones with a great amount of variability on latency, and that's a bad user experience. What Inference Gateway does instead is actually route the traffic based on individual models and the behavior metrics you're looking for specific to that behavior. It could be the key value cache utilization or even the pending queue length such that you can ideally route your traffic even where other models are to get the best use of compute and a lot of different fine-tuning all designed around the actual model behavior. And we've seen a huge change in this. It's actually our serving costs have been reduced up to 30%, which is a material amount, as well, the tail latency is 60%. And these days, the responsiveness of the models considering how consumer-facing they are will make and break your entire business. The fundamental layer for all this, as I mentioned earlier, is GKE's full support for the full range of GPUs and TPUs and compute that Google comes out right on day one. And if you can look, some of the technology that is only in pre-announced is coming in later. The GKE team is already working deeply with the hardware folks to make sure it runs great for you immediately. And the results speak for themselves. We covered a lot of this earlier with one of the keynotes, but TPUs have become the de facto platform for large-scale inference for the largest model serving in the world. And we've actually seen three and a half times better throughput, almost 5,000 tokens per second, and yes, these numbers do change all the time, but our benchmarking efforts, not just on our end, but with our customers, is showing a huge difference in the performance we can get, all in all, at a very cost-effective manner. Now, this is all a moot point. If you can't actually get access to those GPUs and those TPUs, right, great performance on one GPU is not going to get your business very, very far. So this is where our work with DWS, or Dynamic Workload Schedule, comes in. We now have two different methods to get access to the inventory. If you don't need to run your batch job immediately, like, oh my goodness, I've got to run this at the end point, I have no choice, you can now actually choose to schedule it up at a future calendar date, which means you can get a guaranteed chunk of inventory at that date at a better price, or you can actually have flexibility to say, run it when there's enough capacity to be able to run these training jobs. Not so much applicable to inference at this point, but in the future, such that you know, you're not going to have stockouts, you're not going to have interruptions, because you do have that flexibility. And this is actually really useful for those non-hero jobs. Let's say you've got that main training run where it's critical within a time period that runs a very long period of time, but there's lots of fine-tuning. There's lots of additional work going on. Both these options open up a great deal of new inventory, too. Now, no offense to Kelsey Hightower and Kubernetes the hard way, but Kubernetes shouldn't be hard. Quick show of hands, I'm always curious, how many people have actually run through Kelsey's Kubernetes the hard way? Okay, so I think this has lasted in years past. That means either Kubernetes is going a lot easier, or people are shying away from the difficulty of it, but we want to make sure those day two efforts are a lot smoother. And this is where release channels and auto upgrades really kick in. I've seen a massive amount of adoption. Gary and I were actually chatting about this last year, so show of hands, how many people use release channels? Okay, so I'll use bad statistical math and say 60%, but we've actually seen it's approaching 80% of new clusters are using this because people are finding it a lot easier to be able to choose how rapidly the channels come out. We actually added last year extended support to allow you to add an additional 10 months of time for the standard supported Kubernetes version such that you don't have to upgrade right away. Now, my call out to all of you is if you really need it, especially in the retail sector, this can be a godsend. But don't use it as a crutch because the longer you extend it out, the more security patches are sitting out there, more vulnerability, the more you're missing out on the magic of what you're going to hear at Cloud Next. But if you really need to be able to have an older version, this is an easy option. It's actually baked into release channels such that there's no surprises of you suddenly being moved over or being charged extra without being aware of it. Beyond that, we've added rollout sequencing. This allows you to look at the fleet of clusters you have and put gateways from each upgrade of a cluster onto the next group. And where this is critically important is you may have that canary test, but before you move on to dev or however you're staging at your rollouts, you can set specific conditions, be it time, be it key application metrics or metrics you provide before that next gate comes. And on to the next one, and on to the next one, and I won't start doing your rap because that will haunt me forever, but it is giving you the ability to have a very complex ecosystem of rollouts without you having to build a combination of custom YAML and batch scripts. So what's new? I think one of the core questions that we're hearing a lot is how do I get better control and also optimize access to compute? So what we actually have now with custom compute classes, and this has been out for a while, so not everyone's using it, but it's really swung up in adoption in say the last couple months, is the ability to actually specify the series of VMs, types, and cost structure in a cascading manner so you can get access to the exact type of inventory you want when you need it and be able to cascade down through it. So let's walk through a little bit of the spec. In this example, we've got it where we want to get access to N4, but in a spot capacity because we're looking at doing this in a cost-effective manner. However, let's say you're a commerce workload or AppLovin, they've got a giant surge because of a huge upswing in traffic with one of their customers. You can then actually cascade down once you're out of N4, automatically in the C4, once again, in spot from a cost reasons. If there's not enough spot, it can cascade down back to N4 standard and then down even to a custom group of resources. What this allows you to do is very rapidly that stockouts surge up and surge down. And even more interesting, we will auto reconcile. So let's say more N4 spot becomes available while you're down at level three or level four or N level. You can then use that inventory automatically. This allows you to design your own custom specs or some of our predefined ones and handle pretty much any workload you can throw at it. Be a good staple performance or burst the state list performance. It's really yours to discover. Now the flip side, what about for accelerators, which is a very different use case, but also the same technology. You've got to commit. You've got a reservation. You want to use that up because you don't want to burn money and not actually use what you've negotiated. You can say, I want this specific reservation. It's the ideal performance, the ideal cost for me. Then I want it to go to a different reservation. And from there, if I can't get it, I want spot. All this allows it to happen automatically without any involvement by you, such you can maximize your costs or specific types of compute. And this doesn't have to be a 100s. This could be actually a combination of TPUs, GPUs, anything you're going to throw at it. With that, we're really excited if Basil should join us because AppLovin has actually been the bleeding edge of pushing price performance and complexity of Kubernetes as part of what they develop. With that, I'd like to welcome to the stage, Basil. Whoa. Hey. Oh, wait. Here we go. Thanks, Basil. Hey. All right. Thank you very much. My name is Basil Shekin. I'm CTO of AppLovin and it's an honor to be here today and speak at Google Next. At AppLovin, we're empowering businesses to bring ideas, products, content to their ideal customers. Our platform processes trillions of events daily, helping businesses reach billions of new users. So our core mission in a sense is connecting creators with their audiences at a massive scale. Now, as you can imagine, a scale like this does not come without its challenges. So today, I'll share how we tackle these challenges all with the help of Google Cloud and I'll try to focus on the lessons that we learned all through the lens of our powerful partnership as well as our cutting-edge ML Ops practices. So first, let's start with who we are. AppLovin is a technology platform that sits at an intersection of mobile apps and advertising. Our machine learning engine called Axon processes trillions of events per day to match the right user with the right advertisement at the right time. We feel growth of thousands of businesses worldwide from indie studios to huge gaming outfits. We now are expanding to DTC brands and online stores. Our significance lies in our ability to turn data into opportunity, driving billions of revenue in revenue for our partners. Now, as our business grew, so did the complexity of managing our infrastructure, both technologically and operationally. A few years back, we were running a mix of collocated data centers and bare metal servers. It worked fine right until the moment where it didn't. Our growth was explosive. Our infrastructure just couldn't keep up. latency spiked up. Compute demands were unpredictable. Basically, managing all that infrastructure became a huge bottleneck. Now, on a metal side, which is essential to our business, our models needed to train on more data. Our team required faster training cycles. We needed to have seamless deployment. Net-net, the manual processes were failing us. Enter Google Cloud. In 2021, we made a bold move to move all of our seven data centers to Google Cloud. As a matter of fact, we moved five data centers in a single day, all without a single hiccup. We leveraged as many tools as we could. The Kubernetes engine, Dataproc, cloud storage. For ML Ops, we revamped our pipelines to make them more automated so we can deploy our models faster, which ultimately led to latency reductions and accuracy improvements. The power of the cloud really came on display when we acquired Twitter from Mopub. Our traffic post-acquisition increased threefold in under 90 days. And the best part is that our infrastructure on Google Cloud was able to handle this increase without any major issues. We were watching the stats, and we were kind of in awe to see the lines go up, the pods go up, and waiting for something to crash, and yet, it didn't. So I think this is something that's only possible with the cloud's ability to scale on demand. Now, moving from the bare metal world, we learned a few lessons. I think one of the lessons that sits very close to my heart is that you really need to always be re-evaluating the old architectural decisions that were made for the physical data centers but are no longer relevant in the cloud, that are no longer relevant in the cloud. Now, here, I would like to propose the following thought experiment, and you can kind of think about your infrastructure, especially if you were in the sort of pre-cloud world, and consider what custom-built data durability mechanisms that you have that sort of were relevant in the bare metal where the servers are not reliable and a hard drive can crash. Do they still apply? Do these architectural decisions, do these redundancy mechanisms still hold in the cloud environment? I strongly believe that the constant reassessment of these decisions, not letting the old design linger, is absolutely essential when you move to the cloud. You need to stay in the cutting edge, and by reassessing these designs, you need to make sure that you're leveraging the potential of the cloud to its fullest. So, to wrap up, what can you take away from our experience? So, first, undeniably, infrastructure matters. You need to have a platform that scales with you and not against you. Google Cloud gave us speed, power, reliability, and allow us to focus on innovation rather than firefighting. Second, automation in ML is a game changer. By embracing continuous training and deployment, we were able to test, refine, and massively improve our model. Finally, third, don't underestimate the power of partnership. Google Cloud isn't just a vendor. They are our collaborator. They work with us to solve real-time challenges. They help us tackle capacity issues and suggest a lot of platform improvements. These three lessons, I believe, are very valuable for us, and I think they will be valuable for anyone here in the audience. You know, with Google Cloud, we were able to turn challenges into opportunities, and we're just getting started. We're constantly creating tools to build smarter, more adaptive, state-of-the-art models. With Google Cloud, we're able to leverage the latest hardware, improve the speed and efficiency of our ML operations. Ultimately, we want to unlock the creative potential of our engineers and research scientists. That is the thing that allows us to stay ahead in our fast, fun, data-driven, hyper-competitive industry. Thank you very much. All right. So for good or bad, I will bring us home. So, Drew talked a little bit about, you know, where we came from and where we are today. Let's talk a little bit about, you know, where we're taking things in the future. So, I'm going to show a little demo of kind of where things were, but maybe a quick poll from the crowd today. How many folks today use cluster auto-scaling in Kubernetes? Awesome. Good thing it means a lot of you use Kubernetes. How many people use horizontal pod auto-scaling? Yep. How many of you try to perform combinatorial mathematics to figure out the optimal level of scale and compute, right, for cost economics here? A lot of people do that, right? So, this is kind of, you know, when we look at this demo here, this is kind of where things were yesterday. Quite a simple demo here. This is kind of dumb. Just a simple hello something app. I just wanted to scale this thing from one replica to ten replicas. Now, the concept here is I'm not over-provisioning, right? So, this was real-time auto-scaling. I was trying to be cost-efficient. I can go back on that one. I might have fast-forward that one a little bit more, but let me go back to that real quick. If you take a look, steer this way here. If you take a look at the end, basically, it took us whatever time here. You know, we get to the end. We fast-forwarded this thing like over, almost like 90, 120 seconds, right? When I used to show this demo, I joke. I used to have to tell you three jokes while the demo was live, right? So, I fast-forwarded it. Now, that's pretty good. That was sort of state-of-the-art. Not bad, but again, you may end up over-provisioning, right? This isn't going to handle bursty workloads. So, what are we coming up with now? And this is available today, GKE 132 on autopilot. Now, on this one, if you don't watch this demo real quick, you might actually miss the demo. So, instead of having to listen to me tell like three jokes, this is the actual demo in real time. I'm going to do the same thing, scale this thing up from one to ten pods. It will literally take you eight seconds, if I remember the recording of the video correctly. So, we're now going from, and remember, in this model, you are only paying for the pod that is actually running, right? So, I was paying for one pod, and then I needed to burst to ten, and we did that within eight seconds, right? So, now this is now available today, GKE 132. So, what have we been working on and kind of, what's the secret sauce? I'm awesome at marketing and big words, I call it the secret sauce here. So, we've been working on a number of things. So, one thing that we worked on was actually, how do we schedule pods super fast, right? So, there's a lot of things that we can do in GKE where we optimize some of this stuff in open source. So, one of the things here was how do we actually handle when pods are coming in? How do we make sure we can schedule them as fast as possible? Many people don't know this, but part of Kubernetes, right, is working in a loop. It sees that you need to schedule a pod, even when this thing tells you, give me ten more, it still goes through the scheduler, right? How do we process that as fast as possible? Right? And we've been increasing this since 129. You can see we're doing this up to, large numbers of nodes. So, number one, we've got to get pods scheduled super fast. Number two, some of you answered you were using the HPA. For those who don't know what the horizontal pod autoscaler is, basically for workloads that are stateless, you know, the idea is run as few replicas as possible and then when you get load, scale up to more of them. The HPA, one of the things we do, we don't actually use the open source HPA. We have our own optimized horizontal pod autoscaler in GKE. And what we're able to do here is make this thing way more efficient. So, we've optimized the control loop. We've optimized the precision of the metrics, right? So, now, we're actually able to react in almost near real time instead of just sort of spinning and pulling and waiting for that. The graph here is basically showing, like, you know, the more steady scaled ramp up as load comes in. This is from a real customer who tried out this feature. This is also available now. Finally, after arguing about for a long time and spending many dollars internally on what we should call this feature, we came up with container-optimized compute. So, container-optimized compute is this new thing that's available in Autopilot clusters. And you can see, now, you can say, that demo's kind of cute, Gary, but what does that really mean? So, if you have bursty workloads, maybe you're over-provisioning, right? So, you're going to have some level of idle capacity sitting around before as sort of a pre-scale. We're thinking that that can be 50% less, right, due to our scheduling. Latency, right? We can now have less, at least 35, we're conservative, by the way, this is being extreme, but 35% less replicas, again, because we can do those things instantaneously in eight seconds. We'll talk a little bit about what's coming up in the future, but we're actually going to do some things where we can in-place resize pods as well. And as I mentioned, this is available out of the box today. So, the nice thing is, are folks here using GKE Autopilot? Awesome. More hands on that. This is actually available just out of the box today, upgraded to 132. You don't actually have to do anything. The default compute in Autopilot is now this compute model. I should also mention that this will be available second half of the year, Drew, on GKE standard clusters as well. So, we're really excited about this. And again, we talk a lot about AI, but a lot of this is also driving things that we need to do at the core to actually scale standard workloads. So, as I said, I'll just show you a quick demo here. I decided, well, it's great, Gary, that you can run kubectl scale replicas. That's cute. What is this actually? And it works fast. But what's like a real-world example? So, in this case, I'm going to put it all together. I did fast-forward it a little bit, but this actually is kind of cool. So, in this demo, we're going to run real load, right? Similar scenario, but we're going to run a base of five replicas. We'll run the HPA, and you'll see that as we increase load, that we're actually going to scale, and you'll actually see how fast we're creating, tearing up, and tearing down new pods if this demo video plays correctly. So, simple case, we had five pods that are in our replicas, so they're working there. We're just going to run this simple load balancer test, and you can kind of see, I did fast-forward a little bit, but you can see the liveness of the pods, and literally, you'll see that we go from, like, one to 40 pods. I think this demo took me, like, it was about three minutes of actual load and traffic, you know, running on this, right? So, again, we're now into near real-time response to actual load coming in, which is a lot different than where we were. Now, why is this important? It's not just about the fact that we could scale these things up. The interesting thing here is that the actual latencies, you know, usually when we measure performance, right, it's going to be, you know, generally you have response times, right, that are in there. So, if you take a look, you know, sort of down below, you know, the lowest of 10% was whatever, 1,200, you know, milliseconds in there, but if you look in the 50 to 95%, pretty good, right? We're pretty scaled up to 90%, not bad, right? So, we're actually meeting demand with actual reasonable latency. If we compare that, now we compared it to ourself because it's easier than doing that, but this could be said for any Kubernetes distribution. If you take a look at what this was like just a few months ago, check out on, like, 129, you went for 10% from 10, 10x at 90%, right? Now we're looking at, like, only 3x, right? So, you really can use, like, fine-grained, per-pod, like, auto-scaling with GKE now. So, this is fantastic. So, again, check this out today. Now, I don't generally do all the AI stuff, but we have to have a little section of AI in the future. You'll hear a lot about this in other sessions. So, what I showed you was just doing it for normal compute running sort of pods. On the GPU side, we're also doing some pretty cool work here as well. So, GPUs are slightly different. GPU nodes can take a while to spin up. There are software drivers that need to load. So, we're also working on a feature that's in private preview, so do contact us if you'd like to use it, where you'll actually see that we're looking at almost 80% reduction in GPU startup time. Right? So, this will start out with L4 GPUs today. We're expecting, again, a little more conservative, but probably need 20 to 30% light-light auto-capacity. And, again, this thing will be available out of the box. So, feel free to let us know about that. So, the TLDR here is we are really changing the game in terms of how fast we can give you the compute you want when you need it to actually use true auto-scaling capabilities of Kubernetes that you always probably thought Kubernetes had when you read the documentation 10 years ago. in my other great marketing terms, what other cool stuff are we working on? This is why nobody should let, they shouldn't let Gary make his own slides, but they do. So, one other thing that's pretty neat, how many people, most people know this, but in 10 years of Kubernetes, you actually cannot change the size of a pod without restarting it. You literally, you want one CPU, and then you want two CPUs, it's going to actually basically create, destroy, and recreate the pod, right? This is why vertical pod auto-scaling is actually problematic for many workloads because it requires a restart. But imagine a world where that's no longer the case. So, we're working on a key feature in upstream Kubernetes. It's in alpha today. We certainly hope that we can get it out in beta by the end of this, what month are we in? April? Yeah, the end of April in 133, which actually allows us to resize pods in place. So, now things like the VPA actually become useful for real-time workloads, right? It's always been useful for ones that could support restarts, but now we can actually bring these things and do them in place. To pick on an old friend known as Java, we know that Java, for example, what does this allow us to do? So, Java, we know, can be, the polite way it would say, resource intensive on startup, right? And we know if we give it more CPUs on startup, it may start faster. But then, under normal usage, its CPU is running a lot lower, right? So, now people might do things like over-provisioning, they may set, you know, different requests and limits, but what if we can now just resize it in place? So, we can do basically, if you're familiar with Cloud Run, this would be CPU boost for Cloud Run, very similar feature, right? We can now annotate this, say, boost up to three, keep it limited to that, then once we've started up, resize it, so that its request and limits are only one. If we also combine that with some of the innovations that we had from the container-optimized compute, which was this kind of dynamic resizing of the compute underneath, you don't even have to worry about providing extra headroom on the nodes. So, we get very close to what I will coin, no, I wasn't making it up, almost autonomic pods, right? You put them in there, we figure out what needs to happen, we can resize the compute, we can scale nodes super fast. We're now taking away a lot of the headaches and stuff. We usually hear from people, what's so hard about moving? I never set limits for my workloads, I don't know how to do this, I'm over-provisioned, this is too complicated for my developers. What if we can just start to throw, you know, those things at the workloads, automatically resize them, and then sort of magic happens, and you can fine-tune. So, that's in-place pod resizing. All right, four minutes left. Another thing, which you'll see a great session on this, has anybody heard of, we made this announcement actually at KubeCon last week in partnership with Microsoft and AWS and a few others, something called the multi-cluster orchestrator. So, Drew talked about how we can have the most massive clusters in the world, up to 65,000 nodes, but some people have resilience requirements, right? I need to have, I want to be multi-region, right? And for a variety of reasons, we don't make multi-region clusters, so how do I, or you may need to find sort of capacity. So, we've had the ability to do things like load balance across multiple clusters for a while with our multi-cluster gateway. We could kind of deploy things with like config sync across them, but what if we can now actually orchestrate the same workload so you're not deploying it three times, right, against three different clusters, and consider that all in our scaling decisions? And that's really what multi-cluster orchestrator is about. In this particular example, and again, I advise you to see a session, I forgot the number, but look up Nick Eberts later on. He'll show you this, but essentially, we're showing, sending load against the gateway API. Then we have rules that say, which clusters should this thing prioritize? And if it needs extra capacity, or if it's just a rule that says for another region, it'll start to dynamically deploy multiple replis across multiple clusters. We integrate with CICD to do this, but a great feature, multi-cluster orchestrator. And then finally, and maybe these are just features I think are cool, but again, we do see more and more people. We were early, I think, in the whole multi-region and multi-cluster routing. We see more people just have the requirement for resilience or defined capacity for GPUs, right? I need to be in more regions. Can we make this work out of the box? So this is great. This is going to be an open source standard, so you'll be compatible with how you do it. Of course, we believe that we'll be the best at it, but that's, but it's key because you won't be sort of locked in standard sort of API. And then finally, one last project. Does anybody today use things like our config connector or the KRM model, right? For those of you who don't know, there's the, if Kubernetes is the center of your world, which it should be, you can use Kubernetes to provision all kinds of cloud resources, right? In Google, we call that KCC. Amazon calls that act, and I forget what Microsoft calls that, I think, ASO, right? That's in there. So the idea here is, what if I want to build these composite resources, right? So your typical app may have a pod and a database, right? So you could have two separate resources, a pod and a database, and those could be two separate Kubernetes resources, or you could use Helm, but I don't want to use Helm. I know I expressed my feeling, sorry. The idea here is that we can now create these dynamically composite resources as CRDs in Kubernetes and now be able to orchestrate these things. Technically, you could actually run multiple of these. You could run KCC, hack, and whatever in the same cluster and actually provision infrastructure across multiple clusters as well, you know, all coming across that. So even though we are competitive, we do all tend to collaborate as well. And with that, I'm at my time, I think. We got 51 seconds. But I guess your feedback is greatly appreciated. I'm sure you've seen this in other sessions. We also, if you want to take some time to meet with specifically Drew, no, I'm just kidding. If you want to meet with anybody from PM, we have a whole bunch of PM and engineering. Feel free to hit this QR code. And with that, oh, I'm supposed to say one more thing. If you do, we'll be here outside the room after the session so the next folks can come in if you'd like to talk to us. With that, I want to thank Drew, thank Basil, and thanks everybody for listening. That is all. Thank you.