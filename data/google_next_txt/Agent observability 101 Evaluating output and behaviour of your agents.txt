 . Hey, y'all. Welcome. Glad you're here. We're going to talk about Agent Observability 101. It's a hot topic, but there's also a developer keynote happening right now, so thank you for joining me. I don't know how that happened, but I appreciate it. Cool. Let's see. I can control things. Yes, excellent. My name is Alan Blunt. I'm a product manager for Google Cloud, working on Vertex AI and building agents. I'm going to be joined with Hussain Chanoi, technical solutions manager and all-around genius that I get to work with a lot. And I also get to talk about Ignacio Garcia, CIO for Vodafone, who isn't going to be here. So I'm just going to do my best reading through his slides after this. So four chapters. Why observability matters. The theory behind it, so you can get kind of a visceral feeling for it. And then I'll invite Hussain come up and give you some demos and a little bit more practical application. And we'll talk through how Vodafone is using this today. There's one of many examples as we sort of go down this chapter. So let's start with unpredictable agents. This is a quick anecdote from Google DeepMind Project Mariner team. We're building computer use agents to help people with everyday tasks, driving Chrome. And after weeks of labs team effort, we're able to finally complete an online order. Yay. Big success moment. But it actually went out and ordered dozens of garlic knots for the team and delivered them to the office, which was a little bit unexpected. Kind of sorry, not sorry. Everybody was very happy. The team actually all has garlic knots stickers on their laptop now. But I like this story because the things that we're doing have real life consequences. Some of those are irreversible. Getting garlic knots delivered to the office is kind of a nice, delightful, irreversible consequence. But not all of them are going to be so. So the stakes are really high in what we're doing today. Also, I think that maybe we've all experienced this. When you're playing with generative AI and building agents, you get to a prototype very quickly. And that prototype is awesome. You're like high fives around the office. We did what we were trying to do. And then days or weeks later, you're going through the review cycle or you're showing your boss, the mood is not quite the same. So what are the types of things that go wrong? Again, the demo to the boss is a good one. You've tested everything. You go to the boss and he immediately asks you to do something that you never thought about. This is going to happen a lot. It's going to come up with an answer that you didn't expect. It's going to use the wrong tool. The room goes silent. You start to sweat. Or maybe you release it to early testers or production users. You start getting feedback and it's not the feedback you want. The real world is a messy, messy place. And they're doing things with your agents that you did not expect them to do. So how do you protect yourself from this? Or maybe before that, let's start with the root cause. I'm a software developer for many, many years in traditional software development. And I think about this on a spectrum between fully deterministic and fully stochastic. I think of deterministic systems as I have a bunch of if-else statements or whatever other implementation that I have and I can control it. On the other hand side, I have very flexible systems. They're probabilistic. They're creative. They're designed to be creative and to solve problems. But that creativity also means they're somewhat difficult to guess what they're going to do. Maybe unpredictable. So what we're trying to do here is provide observability and control so that we know the behavior of the agents we're trying to build. And I'll be honest. I have definitely been in crisis mode trying to get my agents to behave. I have questioned my choices. I have decided that I want to reduce the amount of generative AI and go back to if-else statements and isolate and encapsulate. And that's not a terrible strategy. But you have a lot of options here. We're going to try and reduce the stress and give you more tools. And I probably don't need to explain this. But it's important, right? Like this matters. The stuff that we're building, we're going to give to our users. You can't go flying blind. The stakes are really high. The risks are upset users or brand trust or costs or wasted resources or security risks. It's not just extra snacks in the office. This could influence your project or maybe your business. But you kind of have to do it, right? Like there's a lot of value in these new tools and the new type of software that we're building. So let's talk about some key pillars. Metrics, traces, human feedback, automated evaluation, and kind of a structured approach for putting it all together. So I'm going to start with a mental model. This is just a thought experiment. Imagine you're doing an A-B test and you give half your users this new agent that you're building. And half the users are the control group and they don't get the new agent. How do you measure if the treatment arm is doing better than the control arm? What are the things that you measure? Are you measuring revenue? Are you measuring metrics that then go back to revenue so that you can then establish ROI on your investment in this new agent that you've been developing? What are the critical steps that a user can do in the treatment arm that they can't do in the control arm? Or perhaps it's a replacement for an existing process. And what are the key steps that you're going to measure to ensure that the users in the treatment arm are doing those same steps? This is a great mental model because it puts us down the right path for how we're going to set up evaluations in the future. And you need to be doing this anyway because the world's a messy place. And we've been doing exactly these types of A-B experiments in production for a long time. That's a great mental model for transitioning over into generative AI evaluations. So let's continue and extend that thought experiment. If you have deployed an agent to production and now you're working on a next version of your agent, how do you compare the performance of the old agent to the new agent? You're going to have various key performance indicators, right? What percentage of the time does the agent successfully complete a task? What's the error rate? How long does it take to respond? Maybe what's the cost per interaction or per day or a cumulative total? These measurements, you're going to compare the version between V1 and V2 and look for the delta or the lift. And notice in this, like, V2 has fewer goal completions, few more errors, a bit higher cost, significantly higher revenue. Do you trust those numbers? I have seen this scenario many times in production deterministic systems and in generative AI systems where the numbers that you see are not easily digestible and you really have to understand statistical analysis best practices. Things like confidence intervals and some domain knowledge, right? You have to look around the numbers. But it gives you the armature or the umbrella that you're going to start understanding and debugging your scenario and it gives you the confidence that you can do this at scale. Just be patient with it and don't trust it blindly, right? These are signals to use. They're not, there's no one metric that's going to give you confidence in your system. So those metrics are kind of at a high level, right? The metrics tell you what's going to happen, but traces let you dig down into the details. So when you're trying to understand how something's going wrong or understand the steps of your process, we're going to use traces. That's sort of a normal term these days. I hope you're all familiar with it. Open telemetry is the standard and we'll talk about that more in a second. But basically logging each step of a process or instrumenting various steps within any process, we represent them as traces. And they give you a very detailed view of the actions that your agents are taking or any software is taking. And you can see the tools that are used by the agents, the inputs and the outputs, the reasoning process if that's instrumented. This is really, really valuable for debugging. You could imagine a world where every single trace was a metric on that metrics dashboard, but no one would actually do that because that's too much detail, too much information, and it wouldn't actually be useful. But you can kind of see how they go hand in hand. You're going to instrument the important things and look at those as metrics, and you're going to dig down and debug with traces when you need them. And you do need both because they provide different sets of fidelity for you. Alright. I also think that we've seen the thumbs up and the thumbs down, but I want to highlight this as an extremely important component of your system. Your domain users are the experts in the process that you're trying to automate. Your users are the experts in whatever the system is that you're building. Let them guide your process, right? So provide a thumbs up and thumbs down mechanism. Provide some other form. Honestly, the easiest form is just on a thumbs down, ask for a little feedback, right? That's all it takes. Just a little bit of feedback along the way will help guide your process. Also, this is a funny little self-promotion moment. I thought it was funny. But that's really all it takes. But definitely do that because as you're creating your scenarios, your evaluation questions, the simulations of your environment that you're going to be doing, we're going to talk about that in a second. As you do that, you want it to reflect what your users are actually doing. This is a way for them to tell you. Okay, so automated evaluations, evals is what we usually say. This is simulating production, or at least that's how I usually frame it. You're just trying to automate the process of your agent responding to a scenario. So you automate that testing. This is going to be similar to unit testing. In fact, I think my next slide shows a comparison. If we think about traditional software development, you create unit tests so that you understand the behaviors of your system and can ensure that they don't change on you without being in control. It's really nice to get a green bar and know that your tests pass. And it's scary to work on a system that has behaviors that you don't feel like you are very confident around. And so wrapping them in evaluation tests makes sense, and that's a good mental model for traditional software engineering. The problem is there's not a green bar. There's no pass-fail here. We're dealing with a much fuzzier scenario. So we pass it through some analyses, and we're going to talk about those in more detail shortly, to give us some metrics, and those metrics go back to that dashboard that we were talking about before. So it does all kind of come together, and it's a good mental model to think about, but it is like, if you've been writing unit tests for a long time, you're like, I really wish for that clarity. We don't have it, but we do have things that are going to get us there. So we're going to put this on an umbrella that we're just going to call Agent Ops, and I can defend that turn on the next slide. But it's not just technology. Like, there's no tool that just works for this. It's a combination, or a Venn diagram, of people and process and technology. You've got a team of people working on this, business experts, software engineers. They have to know how to use these tools and implement their actual business objectives. Putting those things together with the technology is how you're going to actually drive value and have control. So, again, Agent Ops sounds like a buzzword. Honestly, I was resistive to that term for a while. But I do think that it's a reasonable term, and I'm going to start with DevOps, right? I've been around longer than DevOps, but it's a good starting point. I think that we're all probably pretty comfortable with that term. DevOps became MLOps when we were training ML models. We had data flywheels, and we were automating processes and people using technology, right? And MLOps is now maybe pretty well understood and acculturated. Gen AI Ops, RAG Ops, Prompt Ops, or maybe Agent Ops, or natural progressions, moving back on the software engineering side of things. And I'd also like to lean into this. It is not a replacement for the other best practices. These are additional layers on top, right? So if you're doing traditional software engineering, you're still going to need DevOps. If you're training models, you're still going to need MLOps. And if you're building agents, you're going to need Agent Ops. These are layers on top of each other. And your agents are probably going to use tools, and those tools are probably going to be APIs. And those APIs still need DevOps to maintain good practices. Okay. I have gone a little bit fast, so you have a little bit of extra time, Hussein. You want to come up and take over? Awesome. All right. Thanks, man. Yeah. Thanks, Alex. Thanks, Alan. All right. Hello, everyone. I'm going to talk a little bit about the tools that you can use, a very specific tool, and then a little bit of a tracing, which will work in a variety of different areas. Okay. Let me see. Yes. All right. So for agents, there's a variety of different things that make it different from just an LLM call. I think we probably know all this, but I think what happens when you're using an agent is that there's different aspects of this agent that you want to make sure it was working correctly, and the combination of all of these tend to have interesting capabilities. So there's some very clear ones like instruction following and intent detection. Is it understanding what I'm asking when I'm saying, let's say we're creating an agent to look up product information, product details, as well as a product price. When I say, tell me how much that shoe is, did it understand that I'm looking for the price? Or tell me what that shoe is like. Did it understand that I'm trying to look for the details of that product? So there's instruction following. There's a few different things in there that you can kind of get from just a single LLM call. There's other things like multi-turn interactions. When I said, oh, how much, what is that shoe like? And how much does it cost? Is there anything cheaper? I have like this multi-turn interaction. So there's a, you want to know if your customers are, you know, interacting with your agents for a long time, a short time. Is that good? Is that bad? Do people like to talk with your agent because it's very chatty? Or do they actually just want that price and they're going to something else? Like what, what is that sort of information? And having a multi-turn interaction doesn't necessarily mean that you're going to do, you know, the classic accuracy response every time. It may change over the time of the conversation and a good outcome might be fine if it's a long sort of conversation, if it's, the person's having a good interaction with that agent. Why not? So multi-turn interactions can get complicated. And then the classic one that we see now with agents and why agents have, you know, a differentiation from just an LLM call is the ability to look outside itself with the tool calling. And being able to call that API, there might be a variety of different things within that API call or within that determination. Did it understand that it has some tools? Did it know which tool to call? Did it formulate the JSON request correctly? Did it get the JSON response correctly? Did it interpret? Like there's all kinds of little things within tool calling. So there's a bunch of things in there. And then memory, slightly different than I guess last year was year of RAG and retrieval. Memory is actually sort of a variation on that theme which is I talked, you talked to the agent yesterday, did it remember what you talked about? Talked to the agent a month ago, did it remember that you, you know, you liked this, you know, Adidas versus Pumas? Memory is actually kind of interesting because it is like an echo of RAG in that you can do now you can do caching plus memory recall. So there's interesting things in there. Reasoning and planning now that we have reasoning models, did it get the reasoning correct? Did it make the plan that you wanted to make? So you have all these different things that are going on within an agent and so you want to measure these to a certain extent to achieve your goal. Operational metrics are sort of the key, the hidden key to all of this, especially when you get to production, you want to know is it doing all kinds of great stuff in a real world scenario and not just your test and simulated scenario? And then, of course, responsible AI and safety. It's not just did this response, did this single response, is it brand safe, but is the conversation brand safe, is the interaction brand safe? There's all kinds of stuff in there as well. So that's a lot to try to figure out where to look at your agent and try to figure that stuff out. Today, I want to talk about two things. The classic one that you'll see with a lot of observability right now is tool calling and then I want to use a little bit of, do two demos, one on observability and one on tool calling so you can kind of see practical applications of how to do this. All right, let's try that. So a little bit of theory on evaluating and also a little bit about what we have on Google Cloud. So Google Cloud has a, in Vertex AI, we have a generative AI eval service that has some primitives for you to use. So let's talk a little bit about tool calling. So with tool calling, when you have an agent, at the bottom here, you'll see, or the top here, you have a user request comes in, an agent, and then a response. Very classic. And actually, I think for all of those things that I had on that previous slide, memory, multi-turn evaluations, it's always good to try to say the, you know, someone said this, the input came in, and then the response was this, and that is correct. These are the bookends of what you're going to do. All the stuff that goes on in an agent is going to get really interesting, but the bookends are, I asked you to do this, you did this, is that the correct outcome, is the hallmark, and the very first thing you should definitely have evaluations around. And then the next thing now that we have tools is if you have something in between those bookends, which is I have to call this API and come out, the term trajectory is just the list of steps that you take that the agent would use to call a tool. So the example, which I will show you, is I have a price and a details lookup for my shoe, for my product. I say, you know, get me the details for this product. You want that to be the details API that's called, and then when you're calling for the price, you get the price API. Those are two tools, and in this case, it doesn't matter. If I say, tell me the price and the details of the shoe, the order doesn't actually matter. It doesn't matter whether the price is called first. There's no dependency between the details and the price in this case. So that two tools are called is the trajectory of the steps that were taken. But maybe there's some dependency in your agent that you need to first call this API, and that data goes into the next API call. That is an ordered trajectory. So like the term trajectory, you'll see in a lot of other sort of agent tools about agent observability tools, and that's the fancy word there. So how do you evaluate this? Alan mentioned that you can vibe check your logs, which I think is fun, and you should. You should look at your data, and you should look at your data. Looking at your data is the best way to understand your data, and so here, what we're doing is we're having some criteria that your team is going to look at when they're looking at the series of logs that are going on with tool calling, and you can evaluate that on a variety of different things. Task completion, obviously, is the one that I think is the first one you should start with. How quickly did it do that? Is the response nice and human-like, adaptability, safety? You can have all the different criterias, but look at your data and make some decisions about what that data is. That's great. That does not scale. So the challenge here is that because it doesn't scale, what you need to do is try to find a way to make it scale. But I'm going to go back to this. Looking at your data is what you need to do to figure out what criteria you're going to try to scale with. So please look at your data. The next thing is, now that we have LLMs, LLMs are great at being able to evaluate a particular type of criteria. There's certain criteria. There's two types of criterias. The certain criterias are calculable. There are calculations that you can do. How long does this take? What's your latency? I asked for the price. Is the price 12? You can just say, is there a 12 in this response statement? You can calculate that. You can do a reg X. You can grab something out there. That's calculable. Is this human-like? Subjective-type behaviors. Is this of a certain length? Does this look like, sorry, not length, but is this of a sort of brand-safe behavior? That's something you want an LLM to judge because you're not going to be able to get a specific calculable number about what friendly is. Okay. I'll take a little aside on that. Some people do have like a quality score for an LLP to determine what friendly is. But if you have that and you can calculate it, calculate it. If you can't, use an LLM as a judge. The nice thing about an LLM as a judge is that once you've looked at your data and gotten out the criteria you want, you can put that into a scoring mechanism, an LLM with descriptions, exemplars, and all kinds of different things so that when it looks at a particular multi-turn evaluation, it can say, yes, this looked like it was good. And it scales. And you can run that in a variety of different ways. The challenges with using an LLM as a judge are, there's a few. I'll probably hit the button so we can move to the next slide. The challenge is, just like when you're doing either agents or any sort of LLM prompting, you can put too much into a prompt and you might get a washed out or not specific sort of response. So you have to actually work on prompt engineering for your LLM as a judge as well. My recommendation here is that you take a policy, especially if you have vague policies that your business team gives you or even image level type of multimodal evals. And if you have a policy, one of my favorite policies that one of our marketing teams gave me is that focus on the product, not the person. That was the full statement of their policy. You're like, okay, well, how am I supposed to write an evaluation around that? You have to work on expanding that statement, going back to your marketing team, getting a paragraph, turning that paragraph into multiple related individual yes-no types of statements that you can then give to an LLM so the LLM can get precise and then aggregate those. So complex scenarios get challenging if you're going to do large prompting. Okay. That's good. All right. How to evaluate? Like I mentioned, there's a variety of different things where you can evaluate. You can have task success rates is the perfect one and then for the RAG example that hopefully a lot of folks are familiar with, retrieval statements like precision and recall that you can calculate are great. Put those two together, you get some really great stuff and that becomes more objective, more scalable. The challenges here on making that real is something that Alan had mentioned as well is that the criteria that you use should be aligned with your business metrics and your business metrics again may have variable ones. So if there's a lot of different types of potential KPI business metrics, you have to work with your teams to come up with things to scope down so you can start with a few and continue to expand. So the process of evaluation is going to be a little bit of a cyclical one. So start with something that you can measure and try not to stuff too many into your evaluations before you can validate that those evaluations are actually giving you good information. All right, that's a little bit broad. Let's talk about some specifics and show you that demo that I can, the two demos that I mentioned. So what we have in Vertex AI is a lot of really great primitives like a computation primitive, autorator primitives, pairwise, pointwise, so you can compare within a model or between two models. And these are great and you're going to use these to build your own custom metrics. And your custom, we don't have a lot of custom metrics out of the box. We have some, a few, in fact, we have a few for agent trajectory ones, which I'll show you. But your teams are going to come up with your own business metrics and you're going to use these primitives to come up with those. Once you do that, you'll be able to have not only these primitives that are tailored towards your things, your particular use cases, but you'll also have some basic ones. Like I mentioned, the trajectory ones about the ordered call API calls. We have a few example trajectory metrics that you can use for agents. There's a whole other series of other pre-built metrics too. We introduced a new one at Next as well. It's called a rubric-based evaluation, which is great for multimodal policies and all kinds of interesting arbitrary stuff trying to turn the subjective into the objective. I'm not talking about that. I'm hitting my mic. I'm going to talk only about the agent eval ones. All right, let's see here. Okay, so I'm going to walk you through the steps that I'm going to show you in a notebook. There's three steps. First, you have to prepare your evaluation data set. Then you create something that's called an eval task in the Python library, and then you collect and create your visualization. So this is what it looks like. The data set, the example data set that you're going to collect, and I did a product name and product details one here. So product price and product details are the two tools that I've defined. And you can see that if I ask for get details for a USB charger, my expectation is that I should get the details and the price. And in this case, it's in that order, but I'll have an eval thing that says don't worry about the order, but at least tools. And that's one. Example data set. I'm going to make up the correct pair of question. And then since I'm doing tool response of whether the agent's calling some tools, I want to know what the tool choice that it is. So I'm going to make up multiple sets of those, and that's going to be my data set. And then once I have a data set, I'm going to take that data set and create something called an eval task. And then as you see these metrics with these enums, trajectory underscore exact match or trajectory in order match, I'm going to say with this eval set, give me the percentage of all the eval data sets that I have that are either in exact match or in the exact order match. So I have these presets that I'll talk about that. And then those will be stored in Vertex AI in an experiment. And so you can use those. All this infrastructure is there because you can run a test, like a unit test, on your machine. But when you get to scale, we have a lot of logs and you're going to analyze them. You want to do this in an infrastructure that will scale. So then you'll have this in your experiments and you can visualize it in a notebook. And I'll show you that in a second. And you can also make nice visualizations like that. Okie dokie. I'm going to switch over to my laptop now and show you what we're going to show you here. All right. Let's see. Where's my laptop? Great. Thank you. So I'm doing this in Vertex AI's CoLab Enterprise. This is a public notebook that we have that you can take a look at. We have this for a few different things. One of the nice things about Google is that we have a nice, wide, open arms policy for a few different things. Actually, let me show you. So this is going to be a very small agent. It's going to be a LangGraph agent. And on Vertex AI, you can run not only the agent development kit that we announced this week, but also Kuro AI, LangGraph, all kinds of different things. We're very happy for you to bring your particular agent framework to our environment and make sure that it runs at scale. So I'm going to show you this. Make a little agent here that does that. And then I'll show you how you actually construct this. So this is available if you want to take a look at it. I think I'm already... Let me double check and see here. So, okay, here, what we have is a set of tools. Like I mentioned, we have these, the dummy data set here and then a dummy data set within these two things. This is get product price. This is a tool and this is a tool. This is how LangGraph defines a tool. You just have an annotation on here. So I have two tools. And then I'm going to take these tool calls to find a router. Say I'm going to use Flash. And then I'm going to put these tool nodes into this agent. And if I test it, my environment's up. Yep, you get this. The get product details for shoes comes out. And so that's just like using an agent. Okay, so we have an agent. So let's take a look at one or two things with the agent's Genet Evaluation Services tool, pre-tool definitions that we have here. So we're going to do a single tool selection and a multiple tool selection or the trajectory. All right. So this is, like I showed you before, this is an agent evaluation data set. It has a series of prompts. And these are just, the other thing about this is just a few. You're going to have a lot, right? You're going to have your team make up a whole bunch or get samples from live data if you've YOLO'd your agent into production without having a data set. And you're going to get a bunch of different things. And then you're going to have the prompts that you expect, the tool prompts that you expect. So this data set, and I'm showing the data set there. Actually, let me put this, make sure that we ran that. And we can display it. And then single tool evaluation. What we're going to do is we're going to have a in the Gen AI, Vertex AI Gen AI library, we have something called trajectory single use. And we're going to say that's the metric we want. We want to make sure that it's going to get the product price. And so we're going to run this experiment. And I have to run this. Then we're going to run this experiment. And what it's going to do is it's going to go through and give us, and this is just five. So five is not exciting, but five is great for a demo. You want to do 5,000, right? You want to do a whole bunch. And so this will tell you the time it took. You can even pop in and look at the particular experiment on this side. If you went to Vertex AI, you'll be able to jump into this to get the metrics. So you can see this within Colab if your team wants to. But you'll also get these summary metrics out as well. So you'll see that this, the latency, the response, all these different things, and you'll get this nice, over here, it says one, because we expected it to do that. This is a sort of dummy data set, but you get to see that the score comes out right away. And then you can also visualize this and see all the request response behaviors if you want to and did it actually match. So we're taking the LLM information and making sure that it aligns with our sample data set and that score is correct. And this is going to be a fun notebook because everything's correct here because it's a little bit of a dummy situation. So here are the different trajectory metrics that we have that are built in. Exact match, in order, any order, precision, and recall for tool calling. So recall is did it get the right tool? Precision is did it get the correct tool? Not just any tool, but the correct one for the purpose, in order, etc. So we have another data sets that we had shown above where we can have trajectory metrics. So if we look back at that data set, this data set has things like get product details for speaker, get details in price for shoes. For this one, we expect to have both. And we're not doing anything in order for this particular example, but we can do that. So let me do that one and then I'll show you another demo where this will be interesting too. So define our metrics. We will create an eval task with the sample data set, the metrics, and we'll just give it an experiment name. It'll run this. Give us all the output and we'll see that we get both the exact match score, the in order match score, the order match precision and trajectory. Everything's one here for our fun demo that we have. So you can kind of see that this will be very helpful. We have an evaluation place. You can do plots and things like that. And point being is you are good to go with doing tool evaluation and trajectory choice. That is a great place to start including request response. And there's a request response example in here as well for that. So I think that's a perfect place to move back to the slides really quickly to talk about another thing. So let's go to the slides real quick and we'll talk about observability. So I have that little two-column list of different things that you could evaluate with an agent. Tool calling is a great place to start. One of the reasons why it's a great place to start is it's well defined in the modern LLMs that we have. You can output JSON really easily. You can define tools whether it's an MCP tool or another type of tool and they've been SFT'd to give you the proper JSON response. So that's a great place to say are you doing it right? For all those other ones memory, multi-turn, intent detection, responsible AI, one of the things we need in order to start to get to the level of precision and interesting things that come out of tool calling is that actual data. And Alan mentioned and Alan mentioned that we have metrics that define the goal you're going after and how you measure it and then traces and then logs. So usually when you just do a print F someplace, that's a log. But what we want to do is when agents are interacting with the internal code, you want to actually say, now it's calling a tool. Now it's stopped calling a tool. Now it's got this. Now it's got that. That's a trace. So there's a, in OpenTelemetry, which is a community standard, people use. There's a set of things that are useful, including specs on how to define spans and traces and also implementations from different vendors. And our tools in cloud observability, Google Cloud's cloud observability, we have something called CloudTrace, which visualizes those open source, open standard definitions. And this is really critical because observability is critical because when an agent breaks, I want to know what happened. When an agent does something great, but unexpected, I want to see why. And when I want to improve the quality of my agent, I want to search through all those traces and logs to see what was interesting and get those insights. And I also want to measure things like latency tokens and other interesting metadata. Our team is part of the OpenTelemetry SIG, the Gen.ai SIG, that has specific labels for Gen.ai. And I'll show you this. There's a whole bunch of things in here. And if you have friends that are on the DevOps side or AppDev side, they probably are very familiar with OpenTelemetry. AI application labeling is new from that Gen.ai SIG. And then let's see. Yeah, okay, good. Let me show you this. So let me show you this demo. I will hop back to this. So if I go to my demo screen, I'll talk a little bit about this. So demo time. If we could switch back to the demo station. Great. So it'd be great to take a look at OpenTelemetry.io. You can see something called Semantic Conventions for Gen.ai Frame Spans. And you'll see these labels. I'll show you these labels that will show up if your agent framework is instrumented well for this. There's also a great article here about the best practices and standards of AI agent observability, which you can see here written by Sujay, who Sujay just left. So, oh yeah. Oh, yeah. So Sujay wrote this. So this is great. So let me show you this example that I have here. I'm going to restart this. So what I have here is, like I said, LangChain's great. Cruai is great. ADK is better. Great. So there's lots of things that are, lots of things that you can use. I'm using LangGraph, hopefully, and I'm also using Streamlit, which is, okay, so I'll show you this example. And what we'll do here is created, this agent here is a SQL agent that instruments a little database that we have here. So I'm going to ask it a question to create some tables. Create a weather, create a table for weather data. and it says, great, I've done this. And you'll expect that it's, you know, made this SQLite database with a table that's called weather underscore data, date, city, temperature, humidity, and wind speed. And I can say, you know, populate that with rows. And that'll populate it with 20 rows over there. And then I can say, add in, add in a weather. Sorry. Some observation notes, I can't spell, which is fantastic for a demo. A column. And we'll add in a column. For the weather observation. And we'll see we've got notes. And then we'll say, populate the notes based upon the weather data. Okay, great. Lots of fun. So, yeah. Warm day in Rome. Mild day in Sydney. It doesn't have Vegas in here. Sorry. Hot. And so, you know, an agent that interacts with natural language of SQL creates a database. Yeah, yeah. Great. Okay, whatever. The cool thing here is that we can pop into any one of these and open in Cloud Trace. This opens Cloud Trace and shows us the spans that are occurring. When I asked for, you know, update this with notes, I can see the time here. This is a very classic thing that you see with observability is latency time and all that other stuff that occurs. If we pop into one of these here, what you get is a new thing called Gen AI, which has a bunch of these open telemetry semantic spans. the project, the model that was used, the input tokens that were used. All these things are fantastic. Super great. Right? This is what we want to see. And then, the Gen AI preview here has other things that we expect from the agent behavior. This one is create a table for weather data. And then you see the assistant message that comes back. You see the time that it took, the tokens. All of this stuff is extremely important when you're doing, you're diving into why this occurs or why didn't this occur. Super useful. And the basis for extracting all those traces and spans to create that data set to do the evaluations. Like I said, if you happen to have no judgment here, but if you happen to have YOLO'd an agent in your production and you're lucky enough to use an agent framework that has traceability in it, you can then extract that data for your definite data set that you want to use. And you'll also want to do this for future things such as memory, recall, multi-turn-based behaviors. So that is, that's that demo. And I think we're on to the Vodafone piece. Are you going to, can you do this in a minute or two? Yeah, yeah. We'll get there. All right. Thank you. We can go back to the slides. Appreciate that. Thank you. Hey, so I'm going to be quick and not do a great job, but I'm going to try and read through these slides. We've had a very strong Vodafone partnership for a long time. CIO couldn't be here, but we've been on an impressive journey with them. Obviously, they're a global company with giant scale. They have big scaling problems in addition to this exploration of what we need to be successful with generative AI agents. We've been kind of on this journey for a long time. I think that it's good to talk about some of the challenges. The challenges are really about standardization when you have many, many, many different engineers and engineering teams and business teams that have to coordinate. Security and robustness, very, very important for a global brand. And then we mentioned scalability. It's a big deal. We've been working with them on several fronts across more traditional ML needs as well, and also now in the new agent building world. Specifically, I would like to say it starts with identifying a good use case or several good use cases. What are the highest leverage? What are going to be the most successful? This is really about your business and business processes and what are those objectives? It goes sort of a tie back to the metrics that you're going to measure for success. And then we work together hand in hand. We've got a technical enablement team. We came up with a roadmap together. We've been kind of building this along the way with them. It has been a really wonderful journey. I'm really sad that it's me telling you about this and not them. But we've been delivering good results. And maybe this is a good quote to end with. They are very, very happy. I really wish I could... I'm just going to stop. This is a good spot. I am going to hang out here. Happy to answer questions after the session. I think we're probably at time. But thank you all very much for attending. Thank you. Thank you. Thank you.