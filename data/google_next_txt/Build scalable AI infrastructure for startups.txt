 . So hello everyone. I'm Will and I'm CTO here at Google Cloud. This is my 10th year in Cloud. And I think this panel is very appropriate because the origins of Google Cloud are really in digital natives and startups. And today I'm joined by three amazing entrepreneurs and technologists who are going to share their hard earned and hard won wisdom with all of you. I asked them ahead of the panel, all right? This is not designed to craft perfect messages, right? This is designed to give like in the trenches, real world, you know, kind of context and candid, transparent feedback on their journey and what they've learned. Some things that have gone well, maybe some things that haven't gone well and lessons learned. So hopefully today you'll get real nuggets that you can take away with you. And if not, you can just grab them right after this and I'm sure they'd be happy to share all about the startups and what they're working on. And so let's get after it. I am going to do a quick intro across the panel here. So first we have Rich here from Osmo, right? All right, Rich, welcome. And we also have Dylan, I see you down there and from Assembly AI and Seanick, I see you down there from Fireworks AI. And each one of these leaders is pushing the boundaries of AI in a kind of unique way. So it's not also just three companies that are in the same space, but actually kind of innovating in different ways, different layers of the stack, using different approaches, architectures. So I'm pretty sure you're going to get an interesting cross section because we're talking about machines that can smell, right? We're talking about speech and we're talking about like open source and optimization and many models and getting value quickly out of LLMs. So should be pretty wild and maybe the machines will even be able to develop a new fragrance, you know, maybe over the next, yeah, talk about that. So before we get started and, you know, really dive deep, I think it's important to set context and tell us a little bit about your startup so people in the audience can understand and kind of put in to a frame what you're going to share with them. So Rich, let's start with you. Tell us a little bit about the origin story. Well, glad to be here. So Osmo originally was a spin out of Google Brain, where the original research was to see if you can predict the smell of a molecule just by its structure. What does that mean is after spinning out, the goal of the company is to digitize smell, which sounds odd, but it really comes down to two main points. One is reading the world and kind of writing the world. Reading the world meaning can you essentially have sensors, chemical sensors in this, build models on top of those and kind of digitize what the sensors are smelling, in this case, volatile chemicals in the environment. On the other side, can you create smells? We do that two ways. One is we develop novel compounds, new chemical molecules. We can sell to the fragrance industry, also insect repellents. And the other way, too, is can you kind of use models to like assist and kind of help perfumery? So create new fragrances and kind of work with the CBG companies to create new shampoos, soaps, laundry detergent, or find perfumes or candles for that matter. It's amazing how it like cascades in ways that might not be directly intuitive, but like solving that problem cascades into so many different like product and consumer and business opportunities. Exactly. That's one of the core problems is like which way to focus and how to kind of go to market on those different strategies. Yeah, yeah, yeah. Absolutely. Embarrassment of riches. So, Dylan, over to you. Yeah, thanks for having us here. So I'm Dylan, founder of Assembly AI. Our goal is to try to build the industry's best speech AI models and developer infrastructure for real-world speech AI-driven applications. So we're working with companies like Zoom, a lot of AI note-takers like Fireflies and others that you maybe have used before, thousands of customers building on our platform. We're handling hundreds of terabytes of speech data flowing through our API platform every month. That's growing pretty quickly. So where this all got started was in 2015 when the Amazon Echo came out, I was just like really into voice interfaces and speech technology. I was working as a research engineer, working on NLP and NLU systems, like classical machine learning approaches. And as I was digging into the speech space, I really saw two things. One, all of the developer tools and API platforms available to developers at the time were just terrible. One mailed me a CD-ROM with a PD... I was like, I don't even have a CD-ROM drive to try this thing. So it was really weird. And then the other was that it was really starting to be clear around 2015 that deep learning systems were going to drive a ton of innovation in all these natural language processing-related systems. So it was around 2017, late 2017, I started working on Assembly, really with that goal of building really great innovative speech AI technology and making it really easily accessible and solving all the developer problems around working with speech through a super simple API platform that enterprises and individual developers could easily access and use. Awesome. Like multimodality before it was cool. All right, Shannik, over to you. Yeah. Thank you. Thank you for having us here. I'm Shannik. I'm the field CTO at Fireworks. The main mission of Fireworks is to democratize the open models that are coming along and provide the fastest inference platform that exists on the market today. So to take a step back, the history of Fireworks starts with the co-founders who were all leaders in AI ML infrastructure at Meta and Google, working on like Vertex AI. And Lin, who is our CEO, was the head of PyTorch. So at these companies, we were optimizing the models, providing like large-scale AI platforms and serving roughly like 50 trillion inferences a day. Now, when ChatGPT came along, we saw a lot of like open source models coming along. And we thought that the amount of work that we have put in into scaling AI ML infrastructure and optimizing these models, we can bring to the open source community as well. And we sort of used that and created this company to provide the fastest inferencing platform such that you could take your models and run it on Fireworks and it would run in like matter of five days and not five years, which is how long it took for us to initially create that platform. So we started with our first CUDA kernel, which we called Fire Retention. It was hyper-optimized for your first layer of models that came along like Lama and Mistrels. Once it started to gain traction, we saw that the actual use cases that customers were sending their traffic through actually differed from what we had initially optimized. So we rewrote everything, came up with the new version, Fire Retention V2, hyper-optimized for long prompts, structured outputs, prompt caching, dis-aggregated serving, all of that that you have seen, but we were doing it probably like a year and a half ago. And then the next wave of our kernels was supposed to target new hardware because GPU scarcity is true. I'm pretty sure a lot of you have already seen it. Working on it. Working on it. Working on it. Yeah. And so, yeah, Fire Retention V3 was hyper-focused on trying to optimize the inference on AMD's stack. And so as new models are coming along, we are releasing new kernels. We are releasing new AI infrastructure. We were one of the first ones to launch DeepSea Car 1, and we are one of the fastest ones out there. Who would have thought that Lama 4 would drop on a weekend at 12 p.m. on Saturday? But it did. And that's the state of the world that exists today. Yeah. And so a lot of your Gen AI-native startups, digital-native startups like Cursor, Notion, Uber, Doordash, GitLab, they all actually use Fireworks today to power their AI applications. And roughly we are serving a trillion tokens a day today as of right now and hope to scale really, really fast. All right. Well, you know, maybe dovetailing off of that because you mentioned, you know, some right and left turns that you had to make on the infrastructure as things were developing. Maybe some of you in the room are startups or at a place in your company where you're like, huh, I've got to make some big decisions about infrastructure and platform, but you don't want to make the wrong decision, or at least you make the best decision you can. You know, I don't know you can always avoid, you know, all the mistakes out there. But I think the three of you may have something to offer in terms of looking ahead and thinking about what are some calculated bets you can take in infrastructure and platform that might set you up for future growth. So let's start with you, Dylan. And how can startups or other companies in the room design their infrastructure today to remain agile and adaptable to future AI advancements? What's the secret, Dylan? So I can tell you one thing that we talk a lot about now at the company, which is that I think there's like benchmark fatigue in the industry, and benchmarks are rapidly declining in their utility to help you to serve as a compass for like which infrastructure and which model or whatever you should use. Because for customers, what we see is their reaction is like, you know, so what, right? Like how does this impact my product or make my product better or make my customer experience better or help, you know, me save costs in my organization? And so this is where, you know, evals are really, really important. And I think the community and the market is talking about this more now. I think, you know, you guys will talk about this too. But evals are critical. And evals, in my mind, are not just like, you know, doing your own benchmarks. It's really trying to understand what are the KPIs that are really important for your business, for your product. Is it a conversion rate? Is it like a churn rate? Is it a customer satisfaction score? Like what is it in your product? Is it some time editing, some file? Really trying to understand that. And then you can write evals around trying out different AI technology and seeing how it moves those evals. And I'll give you an example for one of our customers. We had a really large contact center platform, recently switched over to Assembly, and they're handling tens of millions of customer support phone calls a month. So it's like very large scale customer support operation. They saw a 60% reduction in customer escalations once they switched over to Assembly within like four weeks after they fully rolled out. That's huge for them. That saves them a crazy amount of time just like on customer support because there's a lot less customer support to do. And it is immediately reducing the risk of churn within their existing customers. It's creating better customer satisfaction, which makes expansion better. So like that was their metric, right? Like customer escalations. That's the main thing they wanted to track and care about. And if you look at just like quantitative measurements of our tech versus others on academic benchmarks, benchmarks, it's really hard to know like which one is going to reduce my customer escalations the most, right? So figuring out ways to really understand what are the things that matter to your business, that you want AI technology to drive, and then measuring that and being able to measure that like really in an automatic way is really helpful for you. And if you're a company like Assembly that's building some of this technology, then figuring out like how you instrument this into your R&D pipeline can really help you have a competitive advantage. So I'll give you an example. Like we have built out this whole pipeline now that every time we have a new model checkpoint candidate, it like automatically creates a bunch of outputs, farms it out to human raters against other models, and just ask them to like, you know, vibe check which ones they prefer based on certain dimensions. And that's a huge data point that we look at that actually sometimes is like against the quantitative measurements, but better maps like what customers care about. So it helps us go in that direction. So I think evals are really, really important regardless of like where in the stack you're building to serve as a compass to help you figure out what's the best decision to make. And so thinking deeply about those evals is really important. Yeah, I mean, that's a fantastic point. And actually inside of our organization, the CTO organization at Cloud, evals is one of the top areas that we're doing research in and testing in. And even because what you don't want to have is a bunch of really high quality outputs waiting for evals either. So also like the automation of evals and using AI as an eval construct. So the sooner you can define it and start these pipelines going and get the reinforcement learning going, you have the opportunity then to even automate and accelerate your internal operations even more. So this is a really exciting area. Yeah, totally. Shanik, how about you? I'd love to hear your thoughts on this. Yeah. I think to carry off of what Dylan said, I think I have a few points that you should focus on. I think the first one is to focus on what is critical for your organization. So with new tools coming along, we all are very biased towards trying out those new tools, trying out those new cool features that are coming. And sometimes we bias ourselves towards using those tools or getting those things ready instead of actually trying to figure out, like, what is critical for our product? What is the USP for our product? How does it fit with the market? So I would suggest that, or I would advise that you folks should focus on really the criteria and the processes that you can optimize to save that USP. For example, we talked about evals. What is really critical is the evals that you have and not building an eval infrastructure because a lot of the eval infrastructure already exists and is provided by other customers. The second thing is focusing on the fundamentals. So we all want to ship our products very, very quickly. That's actually the state of the world today, right? And because of that, we take a lot of shortcuts. We have lots of tech debts that pile up. We don't test for unhappy paths and do a lot of unsavory things that I don't think we're very proud of. But it is the reality, right? You have to do it. It's okay. We're in it together. Yes. Yeah. We have to do it. But we need to make sure that, like, you don't get carried away just by trying to ship fast products. You need to come back and also fix your tech debt. I think Dylan already touched on use AI tooling to fix all of this. For example, can you use AI tooling to do documentation? Can you use it to write unit tests? Can you use it to do accessibility fixes? Can you use it to do better documentation? And so on and so forth. And then the last point I would stress on is when you're investing in tooling and partners, invest in those that have shown the ability to scale. So, for example, you want to be on Google Cloud because it has a proven track record of scaling with a lot of, like, other companies. So those are the top three things that I would advise. Focus on fundamentals. Scale with partners. And then third one is build what is critical to your organization. Awesome. Rich. So I'll take a different approach here, too. One thing now about Osmo is our evaluation is very, very expensive because we actually have to physically make the fragrance. So when the model outputs something, usually we either have a perfumer check it or we just send it right off to a compound. But we have to physically mix the chemicals in the fragrance and then have humans smell it. So there might be probably no one here exactly like that. But there is cases where you have very, very expensive eval. And the thing we do there is we try to use different models, either different tune models or different models like Gemini versus OpenAI model. And then try to evaluate kind of more of, like, more similar to a traditional A-B test approach where you're trying to say, okay, here's four results of different models. Have people blindly evaluate them and kind of see which model. And there you can kind of learn. You can even use more specific techniques like multi-armed bandits to kind of, like, more quickly learn on top of this. But we're really trying to be nimble with it because the field is moving so fast. So we're always trying to figure out how to best leverage the different models that are out there and then how to quickly evaluate so we can pick the best one and kind of have the best products to go with our customers. And really, the key here is, like, when eval is very expensive and slow, how do you pick the best one and how do you kind of bring the experts in at the right time? And also, at the end of the day, we also have our models. Our perfumers work directly with our models, too. So making sure the experts aren't surprised by a huge change. And we know with these models, like, you get a new one and suddenly everything is different. And they're suddenly asking you, why is it doing this? And you're like, eh, we don't know. It just did it that way. So, yeah. So let's go back to the other one. So I think what I would say here is, yes, it feels moving very fast. But a lot of traditional practices you've seen for the evaluation. My past, I've actually worked on a social arm and cars. It's kind of a similar way. How do you have deploys? How do you test? How do you compare things? Still holds. It's just in a kind of a different domain now with LLMs. Okay. All right. So I'm going to roll across the panel and give each one of you your own question and kind of in your domain so we can dive a little bit deeper. So, Seanick, open source models. You all know a thing or two about those. I guess, how do you think about open source models versus proprietary models? And then, like, are there nuances in dealing with each? How do you compare, contrast? Like, how do you advise or kind of work on your own technology as well? Yeah. So I think before the coming of DeepSeek R1, I would say that there was a huge quality gap between the open source models and pro-source models. Like, very, very distinct gap. But I think, like, the DeepSeek R1 models and, of course, like, all the new models that are coming along, for example, Lama 4, Quen models, I think Google's own Jema models. That's fine. We're going to get you there. That's fine. Are all great. So what we're seeing is that these models are actually getting you higher quality. Of course, there is still some gap with the state-of-the-art models, but these models are catching up. I think there was a really great talk by Ilya who talked about, like, how there is only so much data in the world, and so almost all the models will eventually converge. While that is theoretically correct, I don't think it's going to be practically correct, but they will be in percentage points of each other, and we will see that some models perform better, and some models will have, like, hyper-optimized for, like, some certain types of use cases. The other thing that we are sort of seeing is that the cost to serving the models is actually much cheaper because people don't need to recoup the cost that they have spent in actually training the models, right? So you can serve the models in different ways. You can serve it for your use cases. You can serve it for the trade-offs that you have to make, whether it is latency, whether it is throughput, whether it is quality. It gives you a lot of flexibility from that perspective. The other thing that we are also seeing is that it provides you with flexibility. So if you want to use the model on one provider, sure, go ahead. You can use the same model on some other provider. You can do it across different types of GPU sets. So you're not logged into any specific vendor or any specific GPU sets or any specific lock provider, for instance. And then the other thing is all about privacy and security and transparency. So these open-weight models give you the flexibility to basically audit exactly what is going on inside the model and how the model is arriving at a decision, which for some people and some customers is actually, like, very important. The other parts are where you can actually run these open models in different types of clouds. You can even run them on-prem. You can run it on your device. So these are, like, typically the trade-offs that you would make when you think about, like, open models versus closed models. I think the beauty of closed models is that they are multimodality. They actually work really, really well. And they have been tuned by some of the best engineers and best ML scientists in the world. So as of right now, like, they are performing pretty well. Okay. Great. You know, speaking of performance, Dylan, this is something you know a thing or two about, and you've been after at Assembly for a while. So strategies to continuously improve models and reduce latency in the real world. Yeah. So I think that it really goes back to, like, deeply understanding the customer value you're trying to create. I think that as technologists, like, we get really excited about the technology. Right? It's exciting. Yeah, exactly. And it's cool and it's new. And I think, like, you know, what you have to shift to, though, is, like, the customer value that you're trying to solve. And I think that small companies and startup companies, you know, I think of your example you gave how you guys noticed the use case for the first flash attention that you were seeing was different. And so you, like, rebuilt your stuff, right? It's because you were so close to your customers and you, like, knew what they were doing with your product and they were probably really innovative teams and you were responding to that feedback. And I think that is institutional knowledge that startups and small teams are able to learn about the markets that they're serving and the customers that they're trying to go after. And so the opportunity I see for Assembly and other startup companies in general is, like, really, really building that institutional knowledge about your market you're serving, your customer you're serving. And then for us, the work is, like, pulling that through to our R&D, right? So it goes back to the evals. Like, what are we optimizing for? Well, you know, that customer who I think is an innovator, they're trying to go here in a year and this is the stuff they need. And that matches with, like, all these other customers. And so, you know, we deeply understand that as a company and we're trying to optimize our technology for that. And we're making very clear tradeoffs for other use cases in order to do that. And so for us, the way we try to really stay ahead is really deeply understanding our customers, what is driving their purchasing decisions and, like, teasing it out of them if they can't, you know, clearly explain it. You know, there's this term I mentioned earlier, like, these vibe evals, right? Well, there's something that is, like, creating preference within that, right? So how do you understand that and then optimize for that? I think that's a great opportunity that startup companies have. And then also understanding, like, it isn't just about the model, right? Like, there's a lot of other stuff you have to do. I mean, it's the whole reason fireworks exists, right? Like, if the model was enough, you would just go use it. But you have to care about reliability and latency and serving and the list goes on and on. So I think that also deeply understanding, like, what is that full surface area of stuff you have to solve for? And also spending time on that because it's really important, even if it might seem small, is what we focus on. So that kind of is why this evals piece is so important for, I think, any AI company, whether you're a consumer of AI or whether you're, you know, building AI, just really understanding, like, what are you solving for will help you drive and create the most value. So I think you may have coined a new term in this panel, the vibe eval. I think I got it from somewhere else. Is that from somewhere? Is anybody else there? I didn't clear that. This is a new one. Vibe coding. Vibe coding is... Vibe coding is... Vibe emails. But vibe evals. It was a byproduct of vibe coding. I think someone on our team said last night, and I was like, that really... We used to call them the, like, stare and compares, right? I mean, it's like the LLM arenas. Like, which one do you prefer? And you pick one. The Rick Rubin. It's just taste. Yeah, exactly. I don't know. It just looks better. And there's something behind that, though, right? So I think... And that ends up being, a lot of times, customer-specific or industry-specific or vertical-specific. And this is why there's, I think, a ton of opportunity within the AI market, because you can pick one of those customers or verticals or industries or whatever and really focus on that and delivering the best experience there in a way that can differentiate your product or your technology. Yeah. That's super cool. So you mentioned earlier, Rich, a little bit of a connection between autonomous driving and computational chemistry. Now, I'm not sure you thought that we were going to head there. We'll get there. Maybe you could help us arrive at the destination of how those two things may be related and maybe a little bit more depth and then what, you know, maybe we can reason about because of that connection. Yeah. So, like, one of the reasons I actually got interested in Osmo when talking to the founder, Alex, was I spent, like, the last four years working self-driving cars. And I was talking to Alex, and one thing I was surprised by was, okay, well, you can put all these sensors in the car. You can do vision. You can do, like, radar, ultrasonic, LIDAR, a whole bunch of sensors, too. But, like, it's like, well, it can't smell. Like, the car is actually not aware of its chemical environment, right? And we actually don't have anything that can do that. There is no sensor that can mimic a dog's nose. And if you imagine everything a dog could do, that's a lot of different things, too. But what I learned over time was we actually do have these sensors. The only problem is that they have to be in a lab. And there was really no models that are looking at the output of the sensor the same way you have for LIDAR and for cameras and other cases, like, all the other modalities that we now have many models for, too. So part of Osmo is, like, okay, how do you take what you do for self-driving cars, like, the really large-scale data collection, labeling, annotation, and model building, and bring that to chemical senses? Kind of build on top of that and kind of, to be honest, like, see how far you can push it. Like, are these sensors good enough yet? Do they have to get better? How far did it get? And challenges there is, like, well, now you're bringing, like, what was traditionally not a really modern ML AI world to the cloud. And a lot of our challenges now, actually, is, like, how do you bring chemistry to LLMs in the cloud and make it work? Like, one thing I found at Osmo since the beginning was that, like, the LLMs really are not good at chemistry. I mean, people talk a lot about, like, drug discovery and things, too. Yeah, protein folding and things. Yeah. We talk about that every once in a while. Yeah, but I think people talk about it in the same vein with LLMs, but don't realize that they're actually very far apart. They're, like, really nothing in common, too. If you go to LMS to do basic chemistry questions, it does not understand anything. And, in fact, as a side note, like, that's why we've had this week we're also highlighting a bunch of science models away from the Gemini models so that, you know, to make that point because there are a family of models that you need to solve some of these problems. Yeah. But I think in the future, like I said, we're getting really, really good at, like, vision and kind of audio processing with these models, too. I mean, on the stage here, right, for that matter? And chemistry should be in there, too, because there's actually a lot of human knowledge about chemistry. We just fundamentally haven't focused it on there, too. So, like, even when you bring all the data and things, we are at Osmo, too. There is a mismatch when you try to actually use kind of the current state transformer LLM architectures with these things, too. And I think I'll change. I think over time you'll see kind of, like, the alpha fold, like, very specific, unique tokenizer models come into, like, general purpose LLMs where, like, chemistry data can actually be useful, but we are not kind of there yet. Yeah. Interesting. Yeah. Multimodality and a whole new meaning this week. So, you mentioned earlier security, practical considerations for adopting AI, and I think this is probably on the minds of a lot of folks, which are just some best practices for how you think about selecting AI tooling, not just models, but, you know, tooling as well to balance data and security needs. So, maybe in the random assignment of first responder, maybe, Rich, we'll send it back this way. So, Rich. I'll go for it. So, we're still a small company. So, we really do want to have people just try as many things as possible. And see, just, like, bring us new things. As the whole company, I'll bring lots of new things to the company in our Slack messages and be like, hey, I found this over the weekend. Is it cool? Like, let's give it a try. As a CTO, though, my role is a little on the security side because one difference from Osmo is a lot of the data we work with is not publicly available. So, these LLMs don't know about the data we're working with. So, for me, security is, like, yes, we want to use these tools, but we don't want our data actually getting into the training set. So, how do you, like, do that security? And it's actually very, very hard now. Like, you kind of just have to hope that, like, the models you're working with kind of don't do that. But, like, so, like, yes, we want to use as many models as possible, use the latest thing and try to see if it can be more productive. But really, it comes down to the core data that we have. How do we protect it while still trying different things like a Gemini or a new ChatGPT model and kind of experiment with it? Yeah, it follows on a theme, you know, in my work, I spend a lot of time with, you know, CTOs, CIOs, boards. And one of the classic things is, like, productivity. It's, like, the balance between the search for productivity but also the realization that when you're pushing the frontiers, it's often a process of experimentation. And so, is it a better measure for productivity at some point or is it a better measure for, like, actual usage because at a certain level of usage or experimentation, you know that you're more likely to have breakthroughs though you can't explicitly predict them. So, that tension is kind of, like, always there. Sounds like both for your business, I see you all nodding, but also, you know, pretty much for every company in the world. So, I like the way that, you know, you frame kind of towards experimentation, rapid experimentation, eval loops, and really getting, you know, like, getting the organization good at this, too, because I think we're also all learning how to do this at scale in real time. So, maybe, Dylan, a little, maybe some insights from you on this particular topic. Yeah. So, maybe my perspective I can share is as a, you know, infrastructure company, like, how what we see from customers, I think that security is really, really important for us, and it's a big part of our product roadmap, making sure that we're constantly hardening our inference environment, we're getting security certifications to be able to meet, like, all the different security certification requirements that our customers in various industries have. And we spend a lot of time on that, away from, like, innovation. And I'm sure you guys have to as well. But we are seeing definitely, like, more and more pull from customers that want to just run our stuff anywhere, right? Like, on device, in their own VPC, across different regions. And a lot of that is from a data security perspective. And I do think that, you know, like, to your point, when there's a lot of experimentation and iteration happening, you don't want to go, like, have to deploy something on premise and then, like, see if it works, right? That's like a... Like, quantize it, send it off, and go do inference, and, like, we're done. Yeah, like, that's, you know, a really slow way to, like, get feedback. So I think in... When there's, like, a lot of innovation happening, you know, people are more comfortable to... To your point, like, try new tools really quickly, prove out ROI, and maybe do that with their own users or customers that are also cool with that. And then once they know what they're... Like, they have conviction around, then it's like, all right, how do we deploy this at scale, do it in a way that's more secure? And that's when they maybe go to, well, I need X, Y, and Z to be able to do that. And those things are, maybe this needs to all run in my own VPC, or this has to be, like, you know, on the edge, or, you know, some other type of certification or security requirement has to be passed. But I do think that, like, privacy and security is a really, really important thing for all these reasons that, like, for various reasons. Like, what you cited, like, you don't want your data to leak for IP perspective. Or maybe there's just, like, sensitive data from end users or, you know, whatever. And I think that it is a really important thing for whether you're building an AI product or using this stuff, like, have to think about. So we spend a lot of time thinking about it, and it is a big part of our product roadmap as an infrastructure company. Sean, I know you must have an opinion or two on this one. Yeah. I mean, like, as a SaaS company, we get to see this all the time with all the customers. Like, but typically our advice to customers and in general is that I think you need to understand what kind of security controls you need for what type of data. So if you're running, like, experimentation, do you actually need to have all SOC 2 compliances, all GDPR requirements, et cetera, sort of hammered out before you actually predictionize? Or can you actually deal with simple things like SaaS endpoint so that you can, like, iterate very quickly? So our typical advice is try to understand your data, try to understand what kind of security requirements there are. Is it, like, does it have PHI? Does it have PII? Does it require PCI compliances? And then based off of that, then you can figure out, like, what kind of data to use for experimentation, what kind of data do you have for predictionization, what kind of vendor to use, whether they have regionality, whether they can run in your VPC, whether they can actually run on device, right? And so, like, these are typically the decisions that I have seen that people ask us and the advice that we give to our customers. Interesting. So you kind of, like, start with the data, and then the decision tree goes out to, like, the implementation. Yep. Okay. So we've talked a lot about technology and a lot about, you know, models, infrastructure, security, machines that can smell. Yeah. So I think we've covered a lot on the technology front, but everybody in this room is also a leader. You're leading some type of project, initiative, company, yourself, like, through the ambiguity and complexity of everyday life. And the three of you have been very successful. So congrats to all of you, but we're not done yet. I want you to pay it forward by giving some advice to folks out there. So as leaders, you know, like, what's a top challenge or two that you've personally had to deal with as you scale a high-growth startup in the AI space? And what advice would you give to someone who's, you know, facing the exponential wave and hoping not to get crushed by it? And let's start. Sean, why don't we start with you? Yeah. Yeah. So growing, like, a high-paced startup is quite hard. Like, honestly, it's very tiring, very mentally exhausting, right? Yeah. You see models dropping at, like, big-neck speed. Who would have thought that a model drops on a weekend? I've already talked about that, but it keeps on recurring in my mind. And you have lots of AI tools, and this gets extremely overwhelming, like, just being very honest and blunt about it. But in general, like, amongst all of this chaos, I think there are, like, two really, really important things that I like to focus on, and it's hard. But the first one is customer feedback. I think customer feedback is basically the north star for your product, whether it is doing correct or not. So you have to listen to your customers. And that feedback might not be good and something that you might want to listen to because you think that you're always right. But customer is almost always right. It's a cliche, but it is true. So try to understand that feedback so that you can align your product and you can see whether you're actually doing the right thing or not. And then the second thing, in the same vein, is once you listen to this customer feedback, the customers typically also realize that you are there throughout their journey. So it's all about showing up, right? You are not going to drop them because you made a sale to them and you didn't follow up on. So customer feedback is very, very important. Just keep showing up. Again, it's a cliche, but it works. The second thing is the people that you hire. So fast-growing startup typically means that you have lots of different things to do at the same time. And you almost always feel like, oh, if I hired, like, one more person here, one more person there, then this problem will be solved, right? But it's a local optimization problem and not a global optimization problem, right? So because the first thing that typically goes away when you make a bad hire is the culture of your company. So when you're hiring the first few people, when you're building out, like, the foundation of your company, focus on hiring the right set of people. Focus on having the people who have the right set of values, who align with your company. Because these are the people who will eventually become the leaders of your company, right? And these are the people who will sort of take the values that you have built and spread it across, like, as your organization grows. Yeah, so take a little bit of time. Yes, there is some pain that you have to go through to basically work a little bit harder. But hiring the right set of people really, really pays off in the end. Awesome. Cool. Yeah, so I would echo the customer feedback point. I think that as a, like, you know, startup company, what's both, like, critical and also creates your competitive advantage is that, like, institutional knowledge you can get from just being so close to your customers and understanding, like, what do they care about? What do they want? And that's just becoming this thing that's, like, embedded within your company. So you go to, like, anyone at the company and they can, like, tell you what customers care about. Like, that's really an opportunity that small companies have. But then I would also say, you know, like, when you're starting a company, I think there's a lot of advice that you get on how to build a company from, like, you know, YC or investors or whatever. But, you know, every single startup company is different. You know, the way I say it is, like, startups aren't franchise businesses, right? Like, you don't just, like, do this and that and this. And then it's, like, okay, you have your McDonald's. They're all different. And so every company's journey is different. And really, like, lean into that. And as long as you're, like, solving for your customers, I think that is, like, and putting, like, the customer first, I think the most important thing. So that would be, like, how I would answer that question. Okay. Rich. Yeah, I definitely agree with all these points, too. Maybe just add, like, one thing we think a lot about Osmo is, like, who is your customer? Like, who is the person you're going after, too? Because I think, especially with, like, me with a tech background, I kind of think about the tech in the platform, what we can build. But we also think about, like, what are the jobs to be done? And, like, what is your go-to-market? And who is your customer? You might have many customers. And, like, at the end of the day, you actually might have to pick one. You also might hear different things from different customers. And kind of following them is always not quite the right strategy. So there's this, like, bias, especially at the stages of startup, where you might see one customer saying something. But really, the bigger market is over there. So how do you figure out, like, deep core, like, what is your position as your company? What are your jobs to be done with the product you're building? And kind of also saying no to certain circumstances, too, because you have lots of opportunities in front of you. So how do you figure out that? And then once you see it, it's like, okay, then deeply listen to your customers. Figure out what they're saying. What are the actual problems you're solving? And really, at the end of the day, like, tech, AI, software really just helps them do some problem, get rid of some problem that they have. You're just trying to help them in some way, too. But really listen to them, but also being at your core, like, this is our product. Here's what it is. And here's how we position it against all of our competitors in the market. Yeah. So maybe hire carefully, build quickly, and listen critically. Yeah. What an amazing set of insights. So, you know, first, Rich, Dylan, Sean, it's been awesome to spend some time with you. And thanks for peeling back, you know, the covers at the end there and giving some real, like, you know, from the trenches views as a leader, too. Because despite all the technology, humans are still, you know, making it happen, at least for the foreseeable future. And I really appreciate you spending the time investing in with all the folks in the room here to share some insights. I would not be doing my job if I didn't tell you that we do have a Google for Startups cloud program. You can get up to $350,000 in credits, right? Yeah. All right. So if you're a startup in the AI space and you're excited about building on cloud, we can get you even more excited economically to partner with us. And I think we also have a future of AI. I think there may be a QR code that is about to show up because you're so awesome. So if you have your phone with you, you can just hit this QR code and you can get this report that we just published on the future of AI perspectives for startups to complement what you heard here live today. Happy building. Thanks, guys. Appreciate it. Yeah, thank you. Thank you for having me. Great being here. All right. Let's do it. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.