 BRAD MIRODTZIERNICKI, BEST PRACTICES FOR BUILDING A LAKEHouse ON GOOGLE CLOUD. BRAD MIRODTZIERNICKI, BEST PRACTICES FOR BUILDING A LAKEHouse ON GOOGLE CLOUD. I'm a developer advocate here at Google Cloud where I've spent the last six or so years focusing on our entire data analytics and data science product portfolio with a heavy emphasis on Apache Spark and data governance. Spark, of course, plays a big part in the LakeHouse story, but I'll try and remove my slight bias towards it as we go through the presentation. Now, just as a reminder, for those of you who are familiar with LakeHouses, it is a very broad topic with a lot of different ways that you can build it and different things that you can do with it. So what we're going to cover here is absolutely not exhaustive. You would definitely miss your lunch break if we did go over everything you could possibly do. So today I'm just going to give you a taste and sort of give you my perspective on how you can go about improving and building your GCP LakeHouses. Okay, so this should not be a surprise to anyone in this room, but AI is transforming everything. Companies, industries, the world. It is incredibly pervasive, and I'm sure you've been seeing a lot of that over the last couple of days here at the conference about how we are including AI in a lot of our products and how other customers are including AI in their workflows. AI has become super mainstream. About 72% of organizations have applied AI to at least one function per a study completed at the end of last year. But at the same time, 74% of organizations struggle to achieve scale and value with AI and are still trying to figure out the best uses to really optimize their data and extract value out of it. Now, utilizing AI, of course, ensures that you have the right data to go with it. If you don't have good data, you're not going to necessarily have great outputs. So this is really important. And this boils down to governance. So effectively governing organizational data continues to be a challenge. A different study published last year concluded that about 88% of data leaders believe that data security will continue to rise as a priority for them. And of course, AI is only going to continue to make this more and more important. But why do you need governance? Why do we have to think about making sure that our data is accessible and of good quality? So you, of course, want the right data. And that involves being able to fund it, being able to understand it, and of course, trusting it. You want to make sure that you're using good, high-quality data. Other benefits of a proper governance infrastructure include simplified data management through automation, faster access to data and insights, navigating security, and of course compliance and risk mitigation. So you're here at a lake house talk. Why am I talking about governance? Well, having a robust lake house architecture is fundamentally a governance problem. So let's start by introducing the concept of what a lake house is and give you what my definition of it is. Most organizations would likely admit that at one point or another, they had or still have data scattered all over the place in what we call and what you maybe have heard of as data silos. We call them this because the data is not always necessarily easy to find. There may also be multiple copies of the data being used in different parts of the organization. And maybe that set of data may have diverged a little bit. The quality on one may be better or worse than the other. And they tend to be frustrating to work with, especially as an organization continues to grow in size. In addition to these data silos, many organizations also have data warehouses in what are typically being used in some sort of analytical database. One you're probably all familiar with here is BigQuery as a perfect example. And then there's also data lakes. Now, data lakes are great. They can house data in all sorts of formats, including structured, unstructured, semi-structured data, as well as multimodal data now. Another term I'm sure you've been hearing more and more of over the last few days, weeks, months, honestly years. Now, they are really convenient, but they can sometimes be difficult to extract value out of the data that's in a lake house. Just given how, excuse me, in a data lake. Just given how they're stored there. So that's where the data lake house comes into play as really a means to tie everything together. You have your silos, your warehouses, your data lakes all in one place so that you can easily utilize and find this data. So really, you know, the goal of the lake house is to unlock the full potential of your data, combining the flexibility of a data lake with the analytical power of a data warehouse. If I may say, it's truly the best of both worlds. So, now that we know what a lake house is, how do we go about actually building one? So, you want to start with your storage layer, optimizing for efficient open table formats such as Apache Iceberg, which I'm sure many of you in this room have heard about over the last couple of days at the very least. And you also want to build an infrastructure that utilizes just a single copy of your data. We were talking about data silos in the previous slide and a result of having those data silos formed because you're making multiple copies of the data that's being used in different parts of the organization. So what we want to do is build an architecture that lets you just work with one copy. There's still going to be use cases where you have to copy it, of course, but ideally, you know, we want to mitigate that as much as possible. Next, you need to think about data processing. A data processing layer to perform both batch and streaming processing using SQL and other open source processing engines such as Apache Spark. Now, moving and processing the data in a lake house often involves complicated and repetitive workflows. So it's important to have the right tool to orchestrate and automate these tasks. Next, you want to gain data insights by incorporating data analytics, AI and machine learning models, and business intelligence tools. So there's a lot that goes into these lake houses. But of course, all of these rely on having a secure set of high quality data to work with that you can locate, trust, and take advantage of. And that's where data governance comes in. And it's really the key to ensuring that all of this up here on top works smoothly and safely. So let's actually start there. Let's talk a bit about data governance and then we'll come back to the other components. Now, when thinking about governance, you want it to be seamlessly integrated into the data to AI lifecycle. Discovery, understanding, and using your data should be as frictionless as possible. To that end, we can actually use AI to our benefit here to make your data easier to use for AI. Automated data discovery, metadata extraction, clarification, and enrichment can all benefit by using AI for those tasks. AI can also power semantic search, data insights, as well as anomaly detection and data skew if the quality of your data degrades over time or increases. Now, of course, we want governance to be open and inclusive. In an ever-evolving data landscape, you want to think about how to future-proof your architecture using an open storage format such as Iceberg, unifying your Metastore, and using open source engines all work towards these goals. To that end, we offer a unified governance experience now through BigQuery. BigQuery is the star of the Lakehouse story. We are continuing to improve it and add various features to it, and we're going to talk about a few of those now. So, you know, we're looking to simplify Lakehouse governance by making BigQuery your one-stop shop. BigQuery supports managing all types of data via structured BigQuery tables, as well as external Big Lake tables, which is another feature I'm sure you've heard a bunch about. You can also stream your Kafka and PubSub data directly into BigQuery as well. BigQuery Universal Catalog makes it easy to centralize, manage, and also discover the data that you have in your organization. And, of course, BigQuery is tightly integrated with many of our open source processing engines, including Hive, Trino, Apache Spark, Apache Beam. Additionally, you can train AI and ML models on your BigQuery-managed data, and you can also build business intelligence dashboards via Looker. And this is all based on data that is being managed through BigQuery. It might not be in BigQuery, but you can manage it through BigQuery, and that's the important distinction. So, really, all of this boils down to BigQuery being equipped to cover all aspects of the governance lifecycle as a focal point for your Google Cloud Lakehouse. It can discover tables of multimodal data across your cloud projects using schema detection on data that's in cloud storage. It can provide detailed views of your data via features such as data profiling and lineage, and we'll actually see an example of that in a demo that we're going to do a little bit later on in the talk. It helps you build trust in the data by controlling user access and performing those automated data quality checks that we talked about. And it also manages how organizations use the data via a centralized metastore, offering AI-powered insights and various data sharing features. Okay. So, that was just a taste of storage, but let's now move on. Excuse me. That was a taste of governance. Now, we're going to move on to storage. So, cloud storage, a tool I'm sure everyone in here is familiar with. It will contain your traditional data lake piece of the lake house. So, you're still going to use it to store that multimodal, that structured, semi-structured, unstructured data. But what can help you with that is using open table formats such as Iceberg or Apache Hoodie. And these give you better performance as well as ACID transactions to your data in cloud storage and just make them more reliable and easier to work with. These open table formats also typically include some sort of automatic partitioning features that go into them. So, partitioning, of course, being very important for query optimization. And, again, a lot of these will help you out with that, but also give you some control to fine-tune it as well. Cloud storage also offers multiple different storage classes that allow you to have greater cost optimization for data accessibility. You may have some data that you don't need as much access to as others or on as much of a regular basis. So, you can move those into different storage classes. Lastly, again, BigQuery and Big Lake serve as a front-end for the data in cloud storage. So, it lets you access this just by using that single copy of data. You don't have to make multiple copies to get access to it. Okay. So, to that end, we have a new feature in preview called BigQuery tables for Apache Iceberg. So, like many of you here in this room, I have thoroughly enjoyed using self-managed Iceberg tables. I think they're awesome in managing those through BigQuery. But there are definitely some limitations. You know, you primarily access them via other external processing engines, and also they are only read-only. And what's great about using BigQuery tables for Apache Iceberg is that they extend the functionality of what you can do with those otherwise external tables. So, it solves this with a fully managed experience, enterprise-grade storage management. And all of this is based on data that is stored in your cloud storage buckets. These fully managed Iceberg tables are price-performant. They have automatic and adaptive optimization to provide for better query performance. They support high-throughput streaming, which lets them scale real-time ingestion to tens of gigabytes per second. And you can query data with zero read stillness. These tables, of course, are also open and interoperable, letting you query from any Iceberg-compatible query engine using supporting Iceberg libraries, including the V2 spec. Of course, with deep cloud storage support, they have built-in failsafe and recovery. They use AutoClass and offer automatic natural language-based insights. So, circling back to governance, these tables also support fine-grained access control with centralized access management and multi-compute enforcement. So, all of that is to say this is a really awesome feature and ties back to those principles of you wanting to use those open table formats. And which just continues to make it even easier to use on the platform. Okay. So, moving on to streaming data, we have our managed service for Apache Kafka, a GA product that is a fully managed Apache Kafka experience. The service supports built-in IAM for authentication, automatic multi-VPC setup, and the data is always encrypted with the option to use customer-managed encryption keys. The service is compatible with all of your Kafka apps and is ready for migrations and integrations. It also includes automatic version upgrades. Of course, it's all fully managed. Resources can gradually be scaled for cost management. You can also configure your service using codes such as Terraform. You can also configure your Kafka data to stream directly to BigQuery using one of our streaming services such as Dataflow, another product we're going to talk about in a little bit. Speaking of Dataflow, let's move on to data processing. Now, there are a lot of different tools you can use for this. We're just going to cover three here, but there are, as I mentioned, there are plenty of others. These are three of my favorite. So we'll start with Dataproc. It is our managed Apache Spark service. Dataproc is available a couple of different ways. So you have it available through GKE. You have it available through Hadoop clusters, through the managed service, and then also serverless Spark. You can spin up a Dataproc runtime actually directly from BigQuery. And we have various open source Spark connectors that let you connect to both BigQuery and cloud storage seamlessly and easily. Next up, we have Dataflow, which is our managed Apache Beam offering. Dataflow is great for both batch and streaming workflows. It's also serverless, so it makes managing compute that much easier. Pretty easy product to use and also open source, so of course we love that. And, you know, if you don't want to use either of those, you of course can use BigQuery. We all love using SQL. It is, you know, pretty easy to use for the most part. And, you know, within BigQuery, you can query your data using standard SQL syntax. You can query both those external tables as well as your native tables or those big lake tables or even those iceberg tables we were talking about. So you can do all of that from directly within BigQuery just using your standard SQL syntax. Now, a question I do get asked pretty often is, which of these should you pick? And to be honest, the answer is not so straightforward. They all have their pros and cons, but they also do have a lot of overlapping functionality. So generally, I just say pick the one that a lot of times is just pick the one that you like the most. Or maybe you already have some workflows. You know, maybe you are migrating Spark jobs or Beam jobs. You know, then by all means use those on the platform. Something that I sometimes tend to see is that if you are building a new infrastructure and you want to use an open source tool, I sometimes find Spark is a little easier to use for batch jobs. And Dataflow is a little easier to use for streaming jobs. But those both come with giant asterisks. So, you know, they both do both jobs pretty well. So let's talk a little bit more about using Spark on Google Cloud since there are a lot of different ways that you can use Spark. Moving from left to right, we start with the flexibility of control to ease of use. So if you are using Spark on GKE, it gives you full control over managing the infrastructure and resource allocation. And it's, you know, it's served as an infrastructure as a service. You can then also use managed Spark via Dataproc. And for this, you're just going to be spinning up managed Spark clusters that do Spark really well. But you do have some control over that resource allocation, as I mentioned. Now, Dataproc is absolutely awesome at running Spark. But it also comes with the option to configure it with over 30 open source tools, including Flink, Trino, and actually even Iceberg. With Dataproc, you only pay for the time that your cluster is running. Now, all the way on the right side, we have serverless Spark. This is Spark jobs as a service. You essentially just ship your Spark code to the service, and it runs it for you and provides the results. You don't have to manage your resources. It takes care of all of that for you. And in this case, you're just paying for the job runtime itself. Now, also announced this week is the new serverless Spark in BigQuery feature now in preview. And this just continues to take that BigQuery story and continuing to add more and more of our products and features into the service itself, making it much more accessible and easy to use. You can now create a notebook inside of BigQuery Studio that is connected to a serverless Spark session. And this profoundly extends what you can do with BigQuery as Python and PySpark just simply unlock more ways that you can process and analyze your data, as well as adding another way to build machine learning models with the data. So it's a really cool feature. We're also going to talk about it in the demo in just a little bit. And I'd be remiss to, of course, not discuss Dataflow here as well. So Dataflow, as we mentioned, is a serverless automated streaming tool that can do batch and streaming processing. It also has an awesome set of tools for streaming machine learning jobs, high throughput model training, GPU support. It also has a rich ecosystem. Dataflow has both built in horizontal and vertical scaling, as well as automatic resource allocation. So again, all great things you would expect from a robust serverless service. Dataflow has it. It's built on top of Apache Beam and, again, supports batch and streaming. It also comes with a set of these low-code templates, which essentially are configured for different data syncs and data destinations. So you can move data in a pretty seamless both batch and streaming way. I've recently started to use these a lot more, and they're actually very easy to use, I've found. And, of course, like most of our GCP, I should say all of our GCP products, they are deeply integrated with each other. So Dataflow has deep integrations with our notebook offerings, with cloud monitoring, and also different alerting tools. Okay. So next, let's talk about orchestration. Now, we have a lot of great tools here, and I'm just going to briefly go over some of the options that are available to you. Starting from the top, I would say is our most complex and involved one, is Cloud Composer. It is a tool that is built on top of the open-source Apache Airflow project. It provides complex code-centric workflows written in Python. Really, really awesome tool for doing any sort of complex task and workflow with as many steps as you'd like configured in them. Next, we'll talk about Google Workflows. So this is a serverless API-centric tool that uses declarative YAML files. Really, really awesome. I've actually been using this a lot more, especially with some of my Terraform builds that I've been working on, since a Terraform doesn't necessarily let you easily make certain API requests. I've found that workflows provide an easy way around that. I can talk more about that after if anyone has questions. Moving on, we have Cloud Scheduler. This is definitely the most lightweight of the bunch. This gives you the option to do more of those traditional cron jobs, if you will. Really great. You know, definitely there if that's what you're looking for. Next up, we have Cloud Data Fusion. This uses a visual code-based or code-free approach, I should say. A really nice tool if you just don't want to bother with any code and you can just do everything more or less in the Data Fusion UI. And then lastly, we have BigQuery Scheduled Queries. A great feature for automating SQL queries, and this is done directly inside of the BigQuery console itself. You're not using any one of these additional tools. You can just do it all right there. Okay. So this is a little bit about orchestration. Next, we'll talk about enabling analytics and AI ML within your Lakehouse. Now, a pretty easy place to start is using these BigQuery data insights powered by Gemini. You can do those directly on the tables inside of the BigQuery console itself. Really nice. It provides a set of curated questions that are based on the metadata from your tables. We're also going to showcase that in the demo, but you can see this running graphic here as well. And again, it utilizes that table metadata with the option to generate a SQL query for you that can get run inside of BigQuery if you're interested in learning more. It's a lot of fun to use, I found, too. So moving on from that, other ways that you can gain insight from your data in BigQuery is via notebooks. So you can load and visualize your data using a notebook, including the new Python and PySpark, Apache Spark, and BigQuery feature that we previously discussed. That being in preview, you can do that directly in BigQuery Studio. If you're interested in more of a traditional Jupyter Notebook, Jupyter lab feel, we have Vertex AI Workbench, which is a tool for managed Jupyter Notebooks. That's going to be the most true to using the Jupyter ecosystem. Now, if you're looking for machine learning, BigQuery ML provides a SQL-based tool for training and deploying machine learning models. Again, just directly using SQL. Do that in the console, command line, what have you. Really awesome tool. We also have Vertex AI, which gives you the ability to train and deploy machine learning models using any framework of your choice. We have a lot of pre-built images for you to use that are, you know, use some of the most popular frameworks, such as PyTorch or Scikit-Learn or XGBoost. But you also can, of course, roll your own and create a custom image using any framework that you like. Now, for business intelligence, of course, you can use Looker dashboards, as we discussed. So you can set those up to run on top of BigQuery or based on BigQuery tables or BigQuery views. Really great tool with a lot of different features that you can configure to make truly customized dashboards. OK. So that was a lot of telling. Maybe let's do some showing. We'll start with this architecture here. So what I've done here is I've taken a lot of these features, not all of them, but, you know, I'd say most of them. I'd say most of them. And I put them into this one architecture. Moving from left to right, we start with our data sources. So in this infrastructure, we have data in cloud storage. And we also have it in Kafka. Starting with the Kafka data, there's, of course, there's events data in there that we can stream out of it into BigQuery. In this case, we're using those big, those data flow templates that I talked about. And so there's just a continuous streaming job in there where any time data comes into Kafka, it'll pull it right out and write it to a BigQuery table. Now for this cloud storage data, we're, of course, managing it through BigQuery. We have, I'm going to show you a BigQuery table for Apache Iceberg that's managing some data in storage as well as some of these Big Lake external tables where these tables were discovered. The schemas were automatically generated and identified via the features that are available inside of BigQuery. We're also going to showcase some column level access control, automated data quality rules, and then data lineage. So data lineage is really cool. You can see where the data came from, especially for these tables that are based in cloud storage. You can see that it started there as accessible via BigQuery. And then also it'll show you where the data has been used. So that's all about what's happening directly in BigQuery. Of course, then from there, we can do data insights and processing. And here we're going to talk about Apache Spark and BigQuery and those Gemini data insights we discussed. You then can also use Vertex AI to create machine learning models and then Looker for a dashboard. So let's jump into the console. Okay. So here we have a, this is an Apache Iceberg table. So this is a set of New York City taxi data. I'm actually based out of New York. So it's a pretty cool data set. And here what we have are, so this is all just things such as like the date and the time that the, that a ride started from using a taxi, as well as how many passengers were in there, how much the, how much the ride cost. And what I've actually gone ahead and done here is added that column level access control using what are called BigQuery policy tags. So, you know, for this table, maybe I don't want certain folks in my organization to actually see the, you know, the, the, the pricing that was used on the taxis. And I just want them to look at maybe how many rides there were and the times and whatnot. So it was very easy to do. And, you know, we can see here, you have restricted access to the data in this column due to the attached policy tags. So if I were to actually go ahead and query this table and I requested one of those columns, it would just error out and say I don't have the right permissions. So working as expected. And what we can also do here is we can look at the details and it'll show us the configuration that we set up for this BigQuery table for Apache Iceberg. So we can see here that there's a connection ID. We can see where this is actually living inside of cloud storage as well as the file format and the table format. And then some other metadata as well. I actually happen to spin this one up using Terraform. So that label is, is there as well. So moving on, we're going to now talk about some of our external tables. So what I have here is, so I have a bunch of different, a bunch of different data sets here. So these three were actually all identified via Big Lake GCS schema auto detection. And this one here has multiple different tables in it. And what the service was actually able to do was identify all of those different tables and publish them as individual tables in BigQuery so I can access them independently. So this one here is an order items table. This is based on a fictional e-commerce website. And this is all, just all different fictional orders that were placed. So here we have the schema. And then looking at the details here, we can also see that this is an external, an external table. It shows where the, you know, the raw data actually lives. We have here this flag that says that the schema was auto detected. The source formats parquet. These aren't compressed. And then also the connection ID as well. And then moving on to seeing more of how, you know, learning about this data, we can look at this data insights feature. So here I generated these before since they take about two to three minutes. I didn't want to deprive you all of your lunch any longer. So here we have just some questions that the, that Gemini identified based on the table metadata. You know, here one question is identify products with unusually high return rates compared to their average sale price. Seems like a pretty reasonable question. And then of course, if we click this dropdown arrow, we can see the SQL code that is generated to run that. And then we can just press copy to query and it'll, it'll show up here. Okay. So moving on from the insights, we can talk about data lineage. So here, this is a really, really neat feature. So I mentioned earlier that these are parquet files that are stored in GCS. So we can actually see that here. We can see that via that metadata discovery that these tables were published to big query, if you will. And then we can also show that they are being used by this view here. It's just the, just a different view that I created based on some of these e-commerce tables to continue to, you know, generate value and insights based on this data. What I can also do is go to this data profile tab. So here, what this is going to do is provide a set of descriptive statistics that give you more information about, about your data. So these, this is really nice. It only takes a few minutes to generate and you can also use these and view these over time to see how your data maybe skews or changes over time. It's a really nice feature. And again, it's right here in the big query console, just right on any table that you're working with. Now, following from data profile, you can also use those results for data quality checks. So here, I've actually set up some data quality rules. In this case, these are just some simple checks to make sure that there are null values in these individual columns. But as you're creating your data profiles, especially over time, you can use that to feed these data quality rules. You know, maybe you only want certain numbers to be within a certain range or you notice the average or the standard range. The average or the standard deviation for something should be, you know, over time should really be something specific. And you don't want that to deviate over time. So you can use these data quality rules to help you view that and make sure that the, you know, profoundly the quality of your data stays up to the standards that you expect for it. Okay. So next, let's dive into a couple things that you can do with your data in, given that it's in a lake house. So here, I wrote this notebook that lets you generate embeddings based on some image data that I have in the lake house here. So I've just gone ahead and ran these before the talk. But basically what we're doing here is we're using this model that's based in Model Garden. It's a multimodal, with a multimodal endpoint. And I was able to create embeddings based on this image data. And I was able to store those embeddings in a table in BigQuery. And I can actually run vector search over those. So what I did earlier was I took my table and I said, show me a water bottle. And sure enough, you know, the first two things that showed up were a water bottle. It's a pretty small data set. So the other stuff that came in here isn't totally relevant. But I guess maybe, you know, you might keep a water bottle in a tote bag. So, you know, it's not the, you know, it's a pretty reasonable response, I would say. And then talking about the Apache Spark in BigQuery feature that we discussed. So here is another notebook. This is, again, happening right in BigQuery Studio. And this is connected to a Dataproc serverless, or a Spark serverless interactive session. What we were able to do here is set up our, any sort of Spark config directly in the code itself. So what I've done here is actually configured the Spark session to connect to our BigQuery Metastore. BigQuery Metastore being a feature that lets you access your metadata, both from external engines, such as Spark or Presto, as well as BigQuery. So it's really awesome here where I was actually able to, let's see if this should run. What I'm doing here is I am connecting to my catalog. And, okay, so this did not run. My apologies for that. But what we're doing here is essentially it'll take the, it can take any one of these data sets and treat it as, you know, the database that it's using. And it can list all of the data sets. So if you modify the data using Spark or if you modify it using BigQuery, you know, you can access it both, you know, in using either engine. And what I can also do here is I can generate Spark code. So I will go ahead and do that. I just don't think I can actually run the code itself. But what it's going to do is generate the code for you that if you wanted to use Spark for anything. So in this case, I went with something simple, which is just how would I query this table using Spark. And it knows that we're using the Google Spark session object here. This is just a wrapper around Spark Connect that Google uses so that we can create, we can provide that customization to the, to the Spark service. And, you know, here we're doing just a Spark read. And it knows that the format is BigQuery because it knows that we're in BigQuery. And it was able to format the table appropriately. So having run this earlier, you know, this was the result that I got. But you can use this to do more complicated stuff as well. So I found this to be, you know, a very reliable feature as maybe there's some new on Spark API thing. That's something that I've just forgotten about that I haven't used in a while. Gemini has been really, really helpful with that to help me just get up and running that much faster. Cool. So that's our demo. So let's actually just switch back over to the slides for a few minutes. Okay. Okay. So we're just going to briefly recap here what we talked about. So we started off the presentation by discussing what goes into a lake house. We talked about a lot of different products and features. And I'm just going to highlight them here and just briefly go over them. So for storage, of course, cloud storage is going to be the main driver for where a lot of your data is stored. Manage Kafka, of course, is another great tool that you can use, especially for that event processing and, you know, managing that event data. And then, of course, Apache Iceberg as a really nice to use open table format that is supported through many of our GCP products. For data processing, we have data proc and its hosted Apache Spark functionality. We have data flow and its hosted Apache Beam functionality as well as BigQuery. For orchestration, we have cloud composer, Google workflows, cloud scheduler, and then also cloud data fusion and BigQuery scheduled queries. For analytics and AI and machine learning, you have Apache Spark and BigQuery, of course, Vertex AI and Looker. And then for governance, BigQuery. We have so many great features you can use there. We talked about access control, metadata management, data lineage, data quality, lots of great stuff. OK. And that's it. So really would love any feedback at all. I can totally handle it. Please share whatever you'd like. And thank you so much for stopping by and enjoy the rest of your conference. Thank you, Andrew. Thank you. So now we're going to talk later. Let's talk about what we know today. Thank you, Andrew. Let's get to the DVR conference. Thank you, Andrew. So welcome. Hey, you are now Crowd4视 Noble.