 Welcome to this session. We're going to talk about productionizing open source agents, best practices for working with agents, frameworks in Vertex AI. So today, my name is Chris Overholt. I work in cloud advocacy or developer relations. What that means is I code a lot. I get frustrated a lot like you all, and I try to make the product better. So today, I'm joined by a number of different people, and we're going to see different perspectives on agents. And so I have myself. I'll open up and talk about Build. Catalina and Mati from Latam Airlines are going to talk about their journey with agents. Polong, also from Google Cloud Drive Rel, is going to walk us through deployment. And Jason, director of data science at Walmart, is going to walk us through their journey with open source agents. So hopefully you learn some things and see these journeys, and maybe you're on the journey yourself. So you can maybe learn some things, maybe teach us some things. So we're going to go through in four steps. We'll start with building agents, one of my favorite topics. So you're building agents on Google Cloud. When you think about reality, and you are looking at agent frameworks, there are new ones coming out almost every day. We launched one this week. Has anybody tried agent development kit? Super cool. We'll talk about that here. So you have a lot of choices. Langchain, Llama Index, Langgraph, AG2, Agno, no-code tools like InItIn, Diffy, it goes on and on and on. So how do we make sense of all of this? And what we really do at Cloud, you know, we've been through all these cycles with TensorFlow and Hadoop and containers. So we look at this one and say, oh, agents, microservices, let's see what we can do. So if you have built an agent, if we zoom in a little bit, what is it that makes it so complicated, right? It's just a library. Like, why is this a hard thing? So there are challenges. Immediately, you already start, I'm building this thing on my computer. It's probably reaching out to different services. How do I need to deploy and serialize and take care of all of the plumbing, right? That's what these frameworks are really about. What we like to do, because we're thinking, we want to think about building and creating, putting the user first, right? All these principles are so important. We want to help you focus on agent logic, not plumbing. If you're like me, you work on an agent, and maybe an hour in, I'll just add this one thing, and then 12 hours later, you're debugging message histories and things. So to deliver on that vision, you might start to see how we're putting these pieces together. Maybe this week you've heard about agent development kit or our integrations with Langchain or our integrations with AG2 or our partnership with Crew AI. So these are all the kinds of things you're trying to think about. Which framework do I want? And remember, that's going to have impact all the way downstream to production. So we really think about things in terms of building. So on the very far right, these are the agents you're building, right? These are frameworks. They might be custom, open source, new framework, older framework. And then when you think about how you deploy this thing, it looks a lot like your standard microservice architecture. You have maybe Python backends, and you have maybe a JavaScript or web frontend. So let's talk about choosing a framework. So when you hear from Latsam Airlines, when you hear from Walmart, you will be able to see what is it they were looking for, right? You go to a framework, what is it you're shopping for? First place I usually go is tools. That's how you're going to be interacting with the world. You think about memory and state management. You think about observability hooks, learning curves, right? These are all... So it's so interesting that all of these things fit into making an agent do things at runtime. And the way we look at it is agents are a system. All of this stuff is a system. How do we make this stable and maintainable and modular over time? Here's how we think about it. I think in my time in data science and tech over the last 12 years, you see frameworks come out, and you see some frameworks are thinking about build and deploy. Others are thinking about just deploy. Others are thinking about just build. We at Google love open source. So a lot of things in general are moving so fast. So our philosophy is write what you want, deploy it on Google Cloud. And that could mean Lang Chain, Lang Graph, Crew AI, or the agent developer kit we talked about this week. This is all how do you build, how do you deploy, how do you maintain this over time? So going through the list as you watch the stories of LATAM next and Walmart later, think about what model, so you have a framework, what model do you use? You need to think about long context, multimodality. What is your agent going to be doing? Is it going to be working with media? Is it going to be pulling large files? When you do this, you need to be thinking about VPC, data government security, tools and functions. So if you have models, we've nailed that down. Gemini 2, Gemini 2.5. Tools and functions. Think about how those tools are defined, right? Are you defining Python functions? Are you defining open API specs? Are you using MCP as part of your architecture? So these are all the little things. It turns into hundreds of little micro decisions you have to make the last one, which is really, really important, is memory and state. And we think about that a lot as storage and persistent storage, but when you hear, again, from LATAM and Walmart, look and think about how they're trying to really give their customers a good experience, and to do that, you need memory, state, and personalization. So as you look through these journeys, just take a look at what model's being used, what framework's being used, what are the tools, what are the memory and state. It gives us a short list to kind of think about these things as we engineer this towards production. So we're going to see some stories now about exactly that. So I'd love to invite Catalina, Matias, to come up, and they are going to walk through their journey. Thank you. Thank you, Catalina. Matias. Hi, everyone. I'm Catalina from LATAM Airlines, and I'm the PM of the project we're going to show. In LATAM Airlines, we are always looking for new ways to elevate customer experience, from inspiration to the next trip. But as many in the industry, we start using AA in the post-purchase journey, helping users with customer support. But we realized something. We were getting late to the party. We were helping users who already know where they want to go. And we were missing the opportunity to help them inspire a new destination. That's why we create an AI concert that helped them dream, plan, and book their next trip with LATAM. So I'm going to ask you a question. How many of you have used an AI chat or tool in the last year to plan their trip? Exactly. AI trip planners are everywhere. They are part of how we imagine travel this day. But the difference is they don't own the journey. They don't know the traveler. And then they don't operate the journey. So this is why we think at LATAM, we are in a unique position. We are the largest network in South America, connecting the region to the world. We serve 80 million passengers per year. And we have the largest loyalty program in the region. That's first-party data of customer behavior and preference. And we have Vamos, our own platform of curated destination content. So we know what our passenger dreams to go, what they want to do there, and how to actually help to get there. So when we built this agent, it was not another trip planner. It was like a context-aware LATAM native experience that helped users to inspire destination, plan activities, what to do there, and actually help them how to get there with real prices and availability. So now I'm going to show you how this works. It's just in our LATAM Airlines app. These have been in production for five weeks now. We start with a friends and family release, and now it's available for 10,000 users in Chile. So where do we want to go? Maybe Asia? There's a few options. Where do you want to go? Let's go to Tokyo. What can you do there? Some recommendations there. Then you can ask for like a day-by-day itinerary. Sounds familiar? And this is my favorite part. You can say dates or date range. And this is where the magic happened. Price quote. And direct to our booking flow. So as you can imagine, building this was not easy. So now I'm going to pass to Mati. He's going to pass you through the challenges and learning we have building this. Mati? Thank you. Okay. Hi, everyone. I'm Mati, and I am one of the developers of Concert. We started working on this project about four to five months ago. So I want to take you to the journey of how we build this concert. Okay. Okay. What you see up here, it's a really high-level diagram of what we ended up with. I know that there's like a lot of different components. There's Gemini. There is tools. There is monitoring, evaluation. But don't worry, because I will walk you through them, and I'll show you the most important one of them and some key decisions we made along the way. So let's start with the tool layers. This is where we wanted to really personalize our users' experience. We wanted to provide with some exclusive contents for them. And we thought about what could our customers get with Concert that they cannot get anywhere else. So to answer that, we thought about three tools. One's for destination, one for activities, and another one for flight prices tool. So for destination, we wanted to use Vamos. Vamos is our own travel magazine, Atlanta Airlines. And for enabling, we just took the website, combined it with Vertex AI Cert, and integrated it with our chatbot. For activities, this was actually the easiest one, because it was already built. We just took Gemini. We used Google Grounding. We wrapped it into a tool and served it to our agent. And the last one is where we really actually wanted to shine. We wanted to allow our users, as Kata said, to do some search flights over date ranges. And that is something new that our users do not have at our app or at our website. So to enable this, we just built a smart catch. And it was actually kind of complex, because we're still working on it. It has many different logics. But we're proud of it. Okay. So now that we have our tools, let's move on how we develop this agent. One of the most important things that we have to think of is which framework we wanted to use. Do we want to use an open source framework? Do we want to use a commercial framework? Do we want to do a custom framework? That probably doesn't make sense. But after doing some research, we found LandGraph. And we thought LandGraph offers to us just the right balance between model freedom and the deterministic flow. And the other good reason is that it's an open source project, and it's always keeping up to date with the latest research and papers. Okay. So let's move on into our versions. And I'm going to tell you a story of three different versions we had to build. For version one, it was actually... For version one, you have to think of it just like a new employee or a new trainee that comes into your company. And we just took all of our tools, give it to his trainee, and tell him, you are a helpful travel agent at LATAM Airlines. And we just let it go and see what happened. So what we found for version one is that it was actually a pretty junior concert. It struggled with managing the different tools on how to call them, when to call them. And it struggled with understanding the real user intent. And on our way to try to fix this, it's pretty natural to start just packing and packing over and over some different prompts onto your agent. And at some point, it got just really overwhelmed with all the business instructions that we started to get in. And it said... It forgot, like, the number one rule. It couldn't forget. That it was just... That I'm an airline worker. And sometimes it suggested to us to find flights with another airline company. Yes, happens a lot. Okay, so after that, we moved to our version two. This time, we created kind of a team. And we split all the responsibilities that we handled to this trainee. We split it into four specialists that we wanted to really master each one of the tools. And above them, we had this supervisor that had to manage the whole conversation between them. So how did it go? It actually worked kind of well. But they faced some challenges that a real new team will change... will face in real life. And agents had a really hard time understanding each other's capabilities and went to delegate a task to the other. On the other hand, they all have to read and write on the same messages list. And so they usually got confused thinking that they could call or use each other's tools. Which doesn't make sense to. So for that, we learned that one of the most important things was to not to keep an eye on the final response of the chat, but on the different history messages that leads up to that final response. So after that, we moved to a new version that was version three. And this time we created some communication channels so the agents could talk to each other without losing focus. And on top of that, we added like some of this extra reasoning step so they can really think about their next action before they execute it. And version three was actually like a real improvement. And at this point, we keep iterating and iterating until we got to a point that probably we're really familiar with, that we fix an issue and we break to others. So we were really stuck at this moment until we met evaluation. And here's a funny story because probably you've all been in different breakout sessions rooms talking about evaluation. And probably they tell you, okay, you have to implement evaluation at the beginning of your journey. And for us, it was just at the third stage, the current one. And honestly, I think it was a good call because I don't think we needed some fancy metrics or statistics to tell us that everything was just failing. Everything was on fire at that point. So I think also we needed time to understand what to measure and how to measure it. So at this point, we implemented like many different metrics that you can see there. We had fluency, toxicity, relevance. But I want to talk to you about the one that was the most important one and it's trajectory. Trajectory answers to the question, is the user, is the agent following the right path to answer a question. And why it was a game changer for us? It's because our topology. We had our topology that it consisted of multiple different agents interacting with each other. And we mostly faced with handoff issues or tool calling issues. So once the metrics were decided, we built a golden set full of examples that our UX team wrote. And that is an important point. A UX team wrote our golden set. And with that, we started this cycle that you see up here. We started to gather feedback from our team. We started fixing the issues that we found. We started evaluating our agent. And if we found that our new version would have a better score than the previous one, we would deploy it back to our team for more feedback, more fixing, evaluating, and we started over and over and over again. And a lesson I want to share with you that was really valuable for us is that you have to build a golden set that you really trust so much that the metric alone has to decide if the new version should be deployed or not. It's a long lesson, so I'll say it again. You have to build a golden set that you have to trust so much that the metric alone decides if the new version should be deployed or not. And once you reach that, you're going to go just way faster than ever before. And yes, so once we mastered all this cycle, we felt pretty confident and we were ready to launch to production. Now I didn't talk about anything of deploying, but for that, I'll hand it to Paul-Long that will delight us with that. All right, awesome. Matthew, I'll just grab this slide. Excellent. Thank you so much. It's fantastic. All righty. Okay, so we've explored the complexities of building agents with OSS frameworks and now comes the next major hurdle, deploying agents. Now getting agents off your laptop and running reliably in production brings a whole new set of challenges. We'll focus on some of the key pain points. So how do you package everything correctly, especially with messy dependencies, and how do you scale these often stateful applications? How do you monitor these complex behavior and evaluate performance, as Maddie had just described? So how do you do it all securely as well? So we've all sort of been there. It works on my machine, but production is different. The reality of deploying agents built with diverse OSS tools, open source tools, it's a gauntlet. First, packaging. You've likely got conflicting dependencies, maybe different Python versions that you've experimented with, and getting a clean reproducible container is rather tough. And then stateful scaling. So agents need memory, as Chris described, but scaling stateful applications horizontally is notoriously difficult, and agents add the complexity of long-running LLM calls and unpredictable tool interactions. And, of course, security is paramount. If your agent executes generated code or uses tools to access databases or maybe some APIs, how do you manage your credentials safely and securely? How do you prevent abuse? And, frankly, generic infrastructure often falls a bit short. You know, basic serverless functions might time out on long agent runs, simple container, orchestrators might not handle the specific, you know, resource patterns or state management runs, and needs as well. So it's a lot of manual, fragile work. And that kind of takes away from building better agents. And that's exactly why we now have Agent Engine on Vertex AI in GA. And its purpose is to streamline this deployment, this deployment process, dramatically. We want you to be deploying to secure, scalable, enterprise-ready endpoints sort of without tearing your hair out sometimes. And crucially, it's framework-agnostic. Bring your agents built with LandGraph, LaneChain, or ADK, or your own custom code. You know, enterprise... or bring your own framework principle applies really strongly here. And it provides a fully managed runtime based on Cloud Run, and it abstracts away many scaling, security, and networking headaches. So beyond generic hosting, it adds agent-specific features like session management, like as Chris described before. And we're building... and... and... we're building towards the managed example stores and memory options as well. And it comes with integrated observability out-of-box, feeding directly into cloud trace, logging, and cloud monitoring as well. So let's look at where Agent Engine fits in the overall Vertex AI ecosystem that Chris had kind of shown in one of the first few slides. So as you can see in this diagram, clients interact with your deployed agent via the Agent Engine endpoint, sort of in that blue sort of box there. Now, Agent Engine itself manages the runtime. So it takes your agent code, and again, it could be from any of these frameworks, Agent Development Kit, LandGraph, LaneChain, LlamaIndex, and it runs in a scalable, secure environment. It orchestrates interactions with the underlying models, like Gemini, and various tools. And importantly, it integrates seamlessly with the broader Google Cloud operational tools, you know, CICD for deployment, observability for monitoring and tracing, and evaluation services. It acts as that sort of crucial layer translating your agent logic into a production-ready service. So just to reiterate, reiterate that core point of flexibility. So Agent Engine is designed to deploy agents built with your preferred tools. Whether you're using Google's Agent Development Kit or ADK for sort of tight GCP integration, a graph-based framework like LandGraph, a multi-agent system like AG2 or Crew AI, or the, you know, widely adopted LandChain, you package your code, provide dependencies, and Agent Engine takes care of deploying and serving it. So it's all about providing a consistent deployment target regardless of your build-time framework choice. So how do you actually use Agent Engine as a developer? The workflow is sort of designed to be quite straightforward, so I'll kind of walk you through this. First, you develop your agent logic locally, maybe using ADK or another framework, and that results in your agent object or class. Then you deploy it using the Agent Engine SDK or API, and you basically point it at your agent code and configuration, like your requirements.txt file, and Agent Engine handles the containerization and Cloud Run deployment all behind the scenes. So that's the agent underscore engines dot create call, and that's the second box there. And once you've deployed, you or other applications can query the agent endpoint using its ID via agentengines.get and dot query. And then you can manage your deployed agents, listing them, updating, deleting through the API using dot list and dot delete. So the key is that Agent Engine provides these simple API calls to handle the complex underlying development and serving tasks. And as we know, deployment isn't sort of this one-time event, right? Running agents reliably requires ongoing operational attention. Agent Engine helps here, too. You need robust session management to handle multiple users concurrently. You need evaluation and feedback loops to continuously monitor agents and their quality. Is it still answering questions correctly? You know, is it regressing? So Vertex AI has evaluation services, and there are, of course, open source frameworks, too. And the key is really sort of closing that feedback loop. Now, effective monitoring and tracing is really crucial here. And not just basic logs, but traces that show the agent's internal steps. Which tools did it call? What was the LLM's reasoning? So this is really vital for debugging and understanding costs. Now, the benefit of the managed runtime is that Google handles the underlying infrastructure, patching, and availability, and autoscaling comes with the Cloud Run Foundation as well. And, of course, robust security and governance can be applied using standard Google Cloud tools like IAM and secret management. So looking at the entire journey from an initial OSS prototype, you prototype locally, comparing, you know, various frameworks and models. You build out the core logic. You integrate tools. Maybe do some user testing and initial evals. Then you hit deploy at some point in time. And this involves packaging correctly, setting up the deployment target like agent engine, configuring access and controls, and so on. And then finally, you want to monitor your deployment, right? So this is ongoing, tracking performance, cost, quality through logging, tracing, and evaluation, and then feeding insights back into your build phase. So agent engine aims to significantly accelerate and de-risk the deploy and monitor stages. So with that, I'd like to bring up Jason Cho to talk a little bit more about some exciting agents at Walmart. Jason, can I go? Thank you. Thank you so much, Polong, for the kind intro. My name is Jason Cho, and I'm a director of data science and machine learning at the personalization team at Walmart. So what I'll be talking about in this session is how we have gone from the prompt engineering aspect about two or three years ago when Google's competitor came up with the generative AI language model all the way to the multi-agentic framework that we are using today. So prior to when Gen AI was introduced, of course, Walmart did have its way of how do we use an external data point and how do we utilize Walmart's proprietary data set? But it was very difficult to provide or generate dynamic content and utilize world knowledge to ensure that we have a very powerful ML-driven solutions. And Gen AI helped us mitigate some of the constraints that we had for item one and item two here. However, there were, as is always the case, there were a lot of difficulties in making sure that we could utilize Gen AI properly. And there's a lot of parallels to what Latin Airlines was doing here as well, where we had a lot of different conflicting constraints. The first constraint was to make sure that we are actually meeting our product goals and not just building and using Gen AI for the sake of using Gen AI. The other one, and we spent a lot of time on this one, is to make sure that any of the content that we generate sound and feels like Walmart. So some of these examples include, is it not condescending or is it not over-promising or does it sound friendly and very down-to-earth? Like all of those. And we have pages and pages of brand guides like related documentation within Walmart as well. The third one, and I'm pretty sure a lot of business folks and product managers here are thinking about this as well, is to make sure that our solution is bringing and improving business KPIs. Walmart also serves hundreds of millions of customers, and we also wanted to make sure our solution is very highly scalable, and because of that, we had to make sure we have a very highly accurate performing models. And when we first started on this track, we were pretty naive and thought, you know, we'll just context window the heck out of it. And how wrong we were, we learned that a lot of LLMs, when you increase the context window, really slows down your runtime and increases the costs significantly. And there are a million token context window language models out there, but the other limitation was you also can't really prevent it from hallucinating if you just utilize that big context window. So we have explored many different approaches before settling into agentic framework. The first one, as I was referring to, was can we really do a good prompt engineering? And it helped us meet our product goals, but it wasn't very good at abiding by our brand guidelines. I'm pretty sure a lot of folks in this audience have heard of chain of thoughts, and that did improve quite a bit from prompt engineering, but it still wasn't quite good enough for us. My favorite one here is brute forcing approach, which is have a lot of bunch of if statements and for loop and all of that. But that had its own issues where we were kind of meeting some of our brand guidelines, but then we had to be very cautious and very hard coding in terms of how do we ensure one constraint is met on the other and how do we sequence them out properly. And because, as the word implies, brute forcing approach meant that it wasn't a very scalable approach. We also looked into fine-tuning, and fine-tuning kind of helped a little bit, but also fine-tuning is used more to understand your proprietary data set and how does those semantic topics relate to each other as opposed to ensuring that your constraints are met. With that, we moved on to Agentic Framework, where it was able to help us meet many of the business constraints and other kind of creative constraints and also have an overarching task that we wanted to achieve. Now the question here was, what kind of agentic design pattern should we be using? There are existing design patterns such as tooling agent, planning agent, human in the loops, and many different kind of agents. So we looked back into what is the problem that we are trying to solve, and ultimately we wanted to make sure that we had an overarching goal that we could achieve such that we could also ensure that all of our subtasks, our subtask constraints are met. And this led us to investigate more into multi-agentic framework because it allowed us to have a more collaborative way of solving a specific singular task. And what we wanted to ensure was that each of those subtask constraints can be orchestrated in an efficient manner. So basically we wanted to make sure that we are orchestrating many conflicting subtasks to achieve a single goal. So these subtasks had to work together to achieve this common goal. And some of the must-haves were that we, and I'm pretty sure product managers in this audience may know, but products do become more complex over time. So we wanted to make sure that we could add more subtask agents over time. We also wanted to make sure that we are not bound to any specific language models, and that those are pretty important aspects for us, along with being able to automatically conclude and converge once all of our subtask constraints are met. And as I have alluded to in the previous slide, we also wanted to make sure that our solution is scalable because we are surfacing hundreds of millions of customers. And because it's, like, this entire area is evolving pretty constantly, we also wanted to be able to prototype new agentic architecture pretty rapidly. So based on that, so multi-agentic framework was set, but we still had one more question that we had to answer, which is how do you ensure and what is the conversation patterns that the subtask specialist should go through? Should you just lay them out sequentially or should you just randomly let them communicate? That was one area that we had to answer. The second part was how much of a context should you be sending from one specialist agent to the other specialist agent? So we looked into, so when we first started this track, this was about a year and a half, about two years ago, when this was kind of still in the infancy. And at that time, Autogen was one of the first available framework that we could use, and that's part of the reason why we ended up using this, but it also allowed us to prototype new agentic architectures pretty quickly and for us to define how much of a context to send from one specialist agent to the other. So these are some of the out-of-the-box conversation pattern options that the framework has had. One is a round-robin approach or sequential ordering of the subtask specialists. Others are kind of a random order where you randomly select which agent should be doing the work. And you could also manually set what order at which these kind of agents should be talking to each other and so forth. What we've also quickly realized is this wasn't quite enough for us to meet all of our needs because we felt that our subtask constraints were still not being met. So with that, we've developed a framework called Evil Town. And the difference between what the out-of-the-box orchestration conversation patterns and what this one has is we have this thing called Planner Agent, which is aware of all of the specialist agent tasks that has to be met. And what the Planner Agent does is it determines what is the right sequences at which the debate conversation should happen so that we ensure that the right amount of context is being sent in a way that we are not overloading what each of those specialists should be doing. And once all of the debate communication steps are set via our Planner Agent, then this is sent to our debate moderator. And the debate moderator has two jobs. The first job is to make sure that it tells which specialist agent should be working on a subtask. The other job, and this is very important for us to ensure we have a high-performing model is to ensure that we limit the amount of context that is sent from one specialist to the other specialist. And there are some parallels to the real-world corporate environment here as well. So, for example, we have our business partners and product partners, developers like software engineers or data scientists, machine learning engineers, and all of those. And it's not important for each of those specialists to know all of the content of what is being discussed from some other professionals. Rather, you want to cipher in some of the main information from one expert to the other. And there's a lot of good parallels to that in the architecture that we have built. Once the task has been completed, we send this over to our arbitrator agent who determines whether the job that has been done is good enough or not. And if it is good enough, then we send it over either to our human reviewers for final review or other times these are pushed directly to production as well. And here are some of the activations that we were able to achieve. Very unlimited, but some of the examples. One example was why a customer may be interested in purchasing a specific product. Another could be dynamic product recommendation titles along with dynamically generating product recommendations based on some of the similar use cases that we may have. We were also able to generate novel marketing materials by utilizing some of the feedback loops and what kind of content has ended up performing really well and then pushing more of those contents outwards. Internally, we call it smart creatives. We were also able to generate both the image and copy or text side of things to push and show it to our customers. Finally, we have a customer understanding which allowed us to be more dynamic about why a customer may be coming to Walmart.com. We also have an offline evaluation result kind of like similar to what Latin American, sorry, Latin Airline was doing. And what we had compared against were four, and here are the higher numbers are better. So you could kind of ignore all the other things. The higher numbers are better. And we compared against three other competing models. Vanilla was a prompt engineering model. Chain of Thought is the COT here. Chet Eval is, at the time of us evaluating our model, was the top performing publicly available model that we compared against. And Eval Town is something that we had built. And we've tested it both on the openly available data set along with the proprietary data set. And one interesting thing that we had found was that Eval Town, while utilizing a less complex language model, actually ended up outperforming a more complex language model by some margin. Of course, if you use a more complex language model along with, you know, agentic framework, that would still outperform everything else. But if the cost is an issue or if the practitioners doesn't have any access to a more complex language model because of the cost, then utilizing agentic framework along with a, you know, less sophisticated language model could be a way forward as well. So what are some of the takeaways from here? And how do we build a high-performing multi-agentic framework and multi-agentic engine? The first thing that you want to do is to make sure that you are chunking your tasks in a way, and basically you want to do a divide and conquer kind of an approach where you precisely define what are some sub-task objectives that you want to meet to meet your overall, more overarching goals. The next one is to make sure that you are not overloading the context that is being sent to different specialists. And this really means that you have to be careful about how much of a information that you send over. Otherwise, what would happen is your sub-task specialist to have go through this information overload and start hallucinating. The third one is tooling and infrastructure support. And actually, when we first started working on this agentic framework, there wasn't a very good tooling and infrastructure in place, but with agent engine, I do know that there would be a lot easier access to these tooling and infrastructure as well. Yeah, and the past three slides were researched and written by our five specialist agents. With that, well, thank you very much, and with that, I'll hand it over to Polong and Chris. Yes. All right. That was fantastic. I hope you all enjoyed the journey as much as we did. And as much as Polong and I, you know, love talking about use cases, thank you, Latam Airlines. Thank you, Kata. Thank you, Mati. Thank you, Walmart. Thank you, Jason, for sharing your story. That was really powerful. Let's thank him again for their wonderful story. Thank you. So to take us out of this amazing session on agents, I hope you learned something, and we want to share some resources with you. I want to say three things, and then Polong will talk about what resources we have for you. So number one, I hope you can see that we really actually want to help you build and deploy agents, and as open source continues to change and evolve, we're going to track that, and we want to track it with you. Number two, hopefully it's clear that Google loves open source. We have ADK, this shirt Polong's wearing. We have all the support for frameworks. Like, we absolutely love open source, and I also just want to take a moment to thank all the open source developers. Maybe some are sitting in this room. Maybe some are watching. Thank you all for making this possible. We love open source. So amazing. Thank you.