 BRAD MIRODTZIERA- Let's briefly go over some best practices for building a lake house on Google Cloud. My name is Brad Miro, and I'm a developer advocate at Google Cloud, focusing on data analytics and data science. To start, let's discuss the critical components of a lake house. Start with a storage layer, optimizing for efficient open table formats, such as Apache Iceberg, and build an infrastructure that utilizes just a single copy of the data. Next, a data processing layer to perform batch and streaming processing using SQL and other open source processing engines, such as Apache Spark. Moving and processing the data in a lake house often involves complicated and repetitive workflows. So it's important to have the right tool to manage and orchestrate these tasks. gain data insights by incorporating data analytics, AI and machine learning models, and business and intelligence tools. Now, all four of these components revolve around having secure, high-quality data to work with. And that's where governance comes in and is really the key to ensuring everything runs smoothly and safely. BigQuery is the one-stop shop for all data management and governance needs. BigQuery is equipped to cover all aspects of the governance lifecycle. Discovering tables of multimodal data across your cloud projects. Providing detailed data views via features such as data profiling and lineage. Building trust in the data by controlling user access and performing automated data quality checks. And managing how organizations use data via a centralized metastore. Offering AI-powered insights and data sharing features. For your traditional data lake, cloud storage is the way to go. You can also manage OpenTable formats and use BigQuery and Big Lake to access your data. Next up, you'll pick your data processing tools. Use Dataproc for a managed Apache Spark experience integrated with BigQuery. Dataflow is Google's managed Apache Beam offering for batch and streaming workloads. And then of course, you can always access your data directly in BigQuery using SQL. Your storage data is available via Big Lake. And you can use the preview feature BigQuery tables for Apache Iceberg for a managed iceberg experience. For orchestration, we have so many awesome options. Cloud Composer, which is built on Apache Airflow, provides complex code-centric workflows. Google Workflows is a serverless, API-centric tool that uses declarative YAML. Cloud Scheduler is the most lightweight tool of the bunch, enabling traditional cron-based jobs. Cloud Data Fusion allows you to build data pipelines using a visual and code-free approach. Lastly, BigQuery Scheduled Queries allows for automating SQL queries directly within BigQuery itself. Now, of course, you'll want to extract value from your data in your lake house. You can load and visualize your data using a notebook, including using Python and PySpark via the new Apache Spark and BigQuery feature, now in preview, directly in BigQuery Studio. You can also use Vertex AI Workbench for a managed Jupyter notebook experience. BigQuery ML provides a SQL-based tool to train and deploy machine learning models. Vertex AI can manage all aspects of the MLOps lifecycle, including training and deploying machine learning models using the framework of your choice. For business intelligence, you can use Looker to build powerful BI dashboards. Putting this all together, we see an example architecture of how our products interact with each other. BigQuery is at the center as a unified governance platform, managing our data where it is and letting us extract value from it using many of the products discussed previously. Thanks for listening, and I hope you enjoyed. Thank you. Thank you. Thank you. Thank you. Thank you. Thanks, Billie. bo UK Deixaã‚’