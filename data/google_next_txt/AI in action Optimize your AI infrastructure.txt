 Hello. Thank you all for joining, especially on the last day of Next. We're really excited. This is our second session of this, so we got an encore because it's been so well received, so really appreciate it. I'm Chelsea Chopp. I'm a senior product manager of AI Infrastructure, and I'm really excited to host this incredible panel lineup. I think they have some incredible stories for you, but first I'm going to start with what AI Hypercomputer is and how it provides some efficiencies, and then we'll hear from them firsthand to be able to see how they implemented it in their companies and how they were able to gain efficiencies and optimize their AI infrastructure. So here at Google, AI is in our DNA. Everything that we do is built by AI for AI. We funnel our industry-leading research into our foundational models, and then we take that learning and we funnel it into our consumer-facing applications. Maybe use them today to be able to get here. So we truly understand what it takes to be able to run and serve an AI workload, and we understand that they're really unique applications and different than your typical enterprise applications. So we take those learnings and we bring them over to Google Cloud so that you're able to take advantage, learn from our learnings, and be able to implement them into your industry and your company. And this is why we created AI Hypercomputer. It underpins our entire Google Cloud unified AI stack. From Gemini to Vertex, we're able to gain efficiencies across training and serving using the AI Hypercomputer architecture. So you may ask, what is it? The AI Hypercomputer architecture has three different layers. It starts at the bottom with AI infrastructure, or your purpose-built hardware, across your compute, your storage, and networking. And then how are you able to take and efficiently use those invaluable resources through the optimized software layer? So with the software, what we're able to do is unlock the value and make it easier for you and your teams to be able to use the resources. And then at the top, we realize that AI isn't a traditional application and requires new consumption models. So that's why we created Dynamic Workload Scheduler, for you to be able to get resources when and where you need them. And you also have your traditional consumption models, such as on-demand or spot or committed use discounts as well. So that is our differentiation for Google Cloud AI infrastructure. We're the only cloud that's focused on delivering an end-to-end integrated system that provides choice and flexibility across the entire stack. So now, let's hear from our panelists as to how they were able to do it. You want to come on up? So we have Cesar, a principal engineer at Moloco. We have Shikradna, principal engineer at Shopify, as well as Casper, who's a PhD and lead researcher at UAE's Technical Innovation Institute. So, Cesar, Moloco was recently named Fortune Time's top 50 fastest-growing company in America and the number one in advertising and marketing. How has Google Cloud helped Moloco scale and support this incredible unprecedented demand? The main way, you know, since with all the success comes a lot of volume, a lot of extra compute that we need to go through, the main thing we've been able to take advantage from the storage side to the compute side has been to be hyper-scaling very quickly and being able to layer on the different consumption models as we go from between with the growth plus continuous migration between one stack and another stack in new models. Yeah, it allows us to layer them in this way very nicely. Okay. And how have you been able to leverage the comprehensive data platform on Google Cloud, and how does that feed into your AI pipeline? So, we have, like, a very common workflow where, you know, we ingest data from real-time systems. We move them through data flow, and we push them down the pipeline into feature generation and then feature aggregation as well through other data flow methods. And we do Keras and Keras training. Yeah, and then we leverage TPUs for training and GPUs for serving. Very nice. And also, how do you use Google Kubernetes Engine across this to help you manage your AI pipelines? Well, it lets us obviously put containers, lets us to deploy and keep a very organized system and just standardize what we're doing for each developer and for each model developer as well. Oh, great. Thank you. And we'll come back with more questions for you as well. So, Shikraddha, do you want to talk a little bit about how your company culture views AI? I doubt you woke up one day and it just magically happened and infused inference across the Shopify platform. Yeah. So, we've used machine learning for a long time at Shopify. It's called AI now. It used to be called ML before, statistical modeling before that. Like, we've used what has been state of the art since it's been available to us. In fact, the first potential, I think the first ML use case at Shopify was almost 10 years ago, 2025, where we were using time series forecasting to predict future sales of merchants, potential of merchants, in order to be able to provide funding and capital to them to accelerate their businesses. So, that today has become what is called Shopify capital and we've grown so many merchants that wouldn't have otherwise made it. And so, we've always used ML across Shopify in different product areas. We used it a lot in recommendation systems, search, marketing acceleration, all of the traditional places where ML was applied before, right? And I think the difference now is pre-a lot of the recent advancements have changed our thinking a little bit. Prior to the last couple of years, ML was almost this, hey, are there places where it can fundamentally make a big difference? Great. Let's go use it there. Otherwise, there's almost a reluctance to add it because it comes with additional complexity. ML always came with additional complexity, right? But in the last few years, the models have gotten so good that that complexity now suddenly seems worth it, right? Because the gains you get from it are worth it. And so, today we're rethinking how we're even thinking about product in general. I can assure, like almost every product that I can remember now in Shopify, it's being built with AI as the foundational piece on which everything else is being built on. And so, that fundamental change has happened at Shopify in the last few years. Oh, that's fantastic. So, you mentioned reducing complexity. Can you walk us through your end-to-end AI pipeline and kind of how you approach it? Yeah. So, there are many ML models and AI pipelines at Shopify. I'm just going to focus on one single one because I happen to work closely on this one. Essentially, and we've been doing this for a long time, the problem statement is essentially Shopify merchants sell many, many products. And we've been doing this for a long time, and we've been doing this for a long time, and we've been doing this for a long time. We have no structured information about those products, right? Like when they sell it, we don't even know what type of product that is. What is the category of the product, let alone more nuanced things like, hey, what's the power capacity of this adapter? And so, there was always this need to extract structured data around products. And we've gone down the route. Like, in fact, I saw in one of your previous slides, BERT. Five years ago, we had BERT-based models doing NLP and ResNet models with computer vision to do these. But then when the vision model showed up a couple of years ago, that was an interesting time for us because we kind of looked at it and went, can this simplify a lot of what we were doing? Because we were building specific models to extract specific things. And then we looked at it and went, can these help us, right? And so, essentially, we tried to use vision language models to unify all these different things we were building and say, can we build one single pipeline that can extract all kinds of structured metadata around products and images that we would want to get? And the answer was yes, right? But it came at a cost, right? These are huge models. BERT seemed big when it came out years ago. And today, it seems minuscule. Like, it's ridiculous, actually, when you think of it, right? But it comes at a cost, right? And so, we had to then, the easy part was just figuring out, can we use NLMs to answer this question? That's easy. The answer is yes. The hard part is then, how do you do it scalably and cost efficiently, right? And that's where we had to build a system. And everything was built on top of Google Cloud, of course. And we used, just like Kasper mentioned, or Cesar mentioned, we use Dataflow quite heavily. It's a streaming pipeline. Kafka messages come in that says, hey, here are tens of millions of products daily that are being either created or edited. Dataflow reads it in. We do some preprocessing. This includes things like downloading images and preprocessing the text. The models themselves are hosted on GKE clusters, specifically with NVIDIA hardware there. And we've optimized them quite heavily. And so, Dataflow makes a request. In fact, it makes multiple requests to these LLMs, gets the response, and writes it back to Kafka. And essentially, that pipeline has been running for over a year now. And we've been able to iterate on it. Models keep, new base models keep coming up in the space every few weeks. And we wanted to build a system that was flexible enough where we could change the models as they come along, fine-tune new models, try to be as close to state-of-the-art as possible. And yeah, we've been lucky enough that we've been able to build that. Oh, fantastic. And how do you decide, or how does your team decide, when to use, say, Dataflow for your streaming or GKE to be able to do training? That was a decision that we had to make fairly early on. Initially, we did think of, like, we started at a pace of simplicity, right, where you kind of say, can I do everything? Like, Dataflow, it's the streaming engine that we have to run on, right? And so the Dataflow choice was almost made for us, right? But then the question is, do I also use GKE on top of it? And we learned the lesson very early on, especially given how fluid the industry was in moving, right? Like, not only are the LLMs changing, but the inference frameworks themselves are changing, right? And so we wanted to use Dynamo. It was called Triton Inference Server back then. Triton Inference Server, but within the Inference Server, you could use TensorRT LLM or SGLang and VLM. LLM deploy, I think we've gone through, like, six or eight of them right now, right? And so we had to figure out which one we wanted to use, and the answer was we can't just pick one and stick with it. Like, these things keep changing, and we want to be flexible enough where we swap these things out. Hardware requirements seemed specific to models and frameworks. So to make this choice to enable us to be able to decompartmentalize these decisions, we kind of said, let's separate this out so that model inference is on GKE and it's flexible enough, whereas the data pipeline itself, which is more predictable, we keep it on data flow, right? And so that way, like, it's just a Docker container, then I push it to GKE, and it runs there, and it doesn't touch the rest of the pipeline. Oh, fantastic. Now we'll have some more questions, but we'll move on to Casper. So UAE's Technology Innovation Institute trained Falcon LLM on AI hypercomputer. Can you talk about how that you were able to achieve some of the performance improvements? Like, you got a 25% increase in faster model time, and then you also had a 20% increase in TCO. What factors were able to contribute to that? I think there are two angles to this question. One was basically diligently working with the Google team, trying to squeeze out every single bit of performance out of storage layer, network layer, the training itself. It sounds like you were able to do that. Yeah, yeah, and the Google folks were very helpful at that. And the second part comes from when having to migrate from another cloud provider. We had to revise and review our workflows because it wasn't a simple one-to-one mapping. It wasn't a drop-in replacement. And we also identified a lot of inefficiencies in how we were doing things. And I think the numbers were even better than those in the slides. Oh, we can update that. In the end, yeah. It was a win, yes. And then you mentioned storage. I think you're using Parallel Store. Can you talk a little bit how Parallel Store has helped you keep your resources utilized as well as help with checkpointing? Sure. So we're talking here pre-training large language models. And we're using the NEMO framework as our training backend. And the way NEMO does things is basically it requires random access to your data sets. And it also, in general, when training LLAMs, you typically work with data mixtures, not with single data sets. So that actually adds a lot of pressure on the storage layer. And we couldn't get good enough numbers just using, for example, Google Storage, right? So if you don't need random access, you can get away with things by prefetching and doing larger chunks and things like that. But for random access, it was impossible. And for us, the Parallel Store was the solution that allowed us basically to stop worrying about being bottlenecked on the storage side. Well, fantastic. And then how has Q in Google Kubernetes Engine helped your team be able to share resources efficiently? Yeah, by integrating a queuing system, it allowed us to have a unified framework for working across multiple teams and to efficiently share resources. So we have like a global quota, and each of the teams typically have the specific quotas. And each team typically uses its own quota with the possibility of borrowing from other teams if the resources are idle. And having a queuing system allowed to allocate the resources dynamically based on the needs and demands. Okay. And also dynamic workload scheduler. Has that helped your team be able to get resources when and when you need them? Or how do you use it in your pipeline? Yeah, basically that's our queuing solution. So each team has its own queue that actually schedules jobs. And depending on the availability, the resources are allocated in an efficient fashion, making sure that we keep the use of our quotas as high as possible. Great. Now let's talk about optimizing your AI infrastructure. Now, Cesar, your team had a very interesting transition. You went from CPUs to TPUs. And you got a 10x increase. How were you able to do that? And how were you able to make that migration? Well, our workload is very special in the sense that we have to do real-time streaming. And we have latency requirements when we place a bid on an ad. So we wanted to improve that while increasing our model size and making it more complex. And we have properties of our model, which are very embedding-heavy, reliant. And we have lots of embedding lookups. And I had been at Google nine years working with TPUs. So you had a little bit of a head start. You have a lot of inside knowledge that is not public that I won't go into that tells me how, like, mechanically these TPUs handle these large amount of embedding lookups. So I started Maloco earlier this year. But people I knew were already at Maloco and also knew these details. So they knew that TPUs for our workload specifically is very beneficial. And the 10x improvement is just what I believe like a starter, as in the sense that we really just wanted to get onto TPUs, which we did, like, over a year and a half ago. Now we went to advance to Trillium. So we're more like kind of going through the migration steps to make sure we're at the latest hardware. But there's a lot more to take advantage once you know the details. Most of these are public, like, you know, quantizing and other things like this. Yeah, so I know that the improvement can be even more. So if you're using a 64-bit float, you can use Int8 in many cases. So you can kind of guesstimate what that improvement would be. And there's many other optimizations there. And, you know, and working with the Google Teams, you know, since I know these optimizations and I know other things that are behind the scene, they've been able to try to help us expose it more and more. So we're like a model customer, and hopefully a lot of these things will also release to more people over time. Oh, fantastic. As we test them out and go through the hard part for you. And what were some of the improvements that you've seen with Trillium? The main part is you've been able to go through a lot of data very fast. Since obviously you can give it massive batches, and it's reduced our time, model iteration time a lot. So at Maloka, like, we're not at this point yet, but I want, you know, researchers to, you know, spin off a ton of models all the time where 50% of their research fails. But it's kind of optimized to that point to balance between being pragmatic and being very, very reaching far. This way we can get, like, you know, get risky projects. You occasionally get a really amazing win. And things that you know you kind of have to be pragmatic are continuously feeding into the system. So what do you think you're going to do in the future to continue this optimization? Hmm. Well, heavily going to quantization and other methods like this, plus taking capacity out of the, we have, like, a critical loop set of models that are in the critical loop where you have to respond in, you know, say, 10, 100 milliseconds. In some cases, a full second, which is a lifetime for us. And, you know, that model has a certain amount of capacity and throughput. And just offloading as much capacity outside of that model and working with caches and much larger data sets offline and then feeding them in real time at different cadences. So you can be effectively, the effective model is all these offline models plus the real time model. And that effective size is much larger than you can place into the real time model itself. Okay. That's very interesting. Casper, now you briefly mentioned, I won't ask you details, but you moved from another cloud provider to Google Cloud to train Falcon. What were some of the initial optimizations you did for Falcon training? The initial optimizations were, well, basically, one, trying to find optimal 3D parallelism configuration. To optimizing the storage layer. So, yeah, testing. Well, we started with Google Storage, then going to Palastore and also tweaking that, making sure we get the max possible throughput. And there were also some tweaks on the libraries and the way things are deployed, which were basically hinted by the Google team. Very nice. And that was phase one. So what do you see as phase two for the future? Yeah. Yeah. In the recent months, there's been plenty of exciting optimizations in the open source community. So, yeah, we've been working on implementing and integrating them. So things like FPA training, so training with this precision, you know, lower memory footprint, better training efficiency, multi-hertz-related attention. So having the KV cache compression, which improves the inference times and latencies. Also things like multi-token prediction, for example, which was a thing. It's got merged into the Megatron code base a couple of weeks ago. So it's available to everyone, which also helps both on the model performance as well as the reduces the inference times. So, yeah. So those are probably the more exciting ones and from the more trivial ones, like asynchronous checkpointing, for example. Yeah, we are using synchronous checkpoints when training for various reasons, but it's also available in the Nemo framework pretty much for free. Are there any optimizations you do on the software layer? Sorry? Any optimizations you do on the software layer? I mean, these are the first three. So FP8, MLA, and the multi-token prediction, these are software optimizations, yeah. So, K, is Shopify already completely optimized, or what are you thinking about for the future? I wish I could say we're completely optimized. Listen, at the end of the day, we make trade-offs. Every single day, we make trade-offs, right? In an ideal world, without any constraints, the answer to our problems are very easy, right? You pick the largest, best model you can, and you run it for every prediction that you need to make. It's a very easy answer to give, right? But hopefully we all live in the real world, and we know, right? And so we have constraints. And in my world, I have to deal with tens and tens of millions of products coming every day, billions of images, multiple millions of merchants. How do I do that in scale? How do I do that in that scale while keeping cost? I guess the value has to be there, right? At the end of the day, it becomes this very big optimization problem of multiple dimensions. The three main dimensions I kind of always look at is how good is the model, the base model itself that you're fine-tuning on. The second dimension is the inference frameworks and how much they can optimize it for you. What features do they have available? How does that model work with this inference framework? And the third dimension is the hardware itself and the cost of the hardware that goes along with it, right? So you essentially have this three-dimensional cube matrix optimization problem. And then the goal of our, like, for us, find that sweet spot in between. Of course, I'm glossing over the other things, like how much training do we do, training data, synthetic data generation. I'm purely focusing on the inference side because that's the largest chunk of our cost. And so there, and so that, like I said, there isn't, like, in that optimization problem, there isn't one right answer, right? The reality is that right answer changes over time, right? You think TensorFlow TLLM is great today, and then SGLAN comes up with a new feature two weeks from now, and suddenly they're better, right? And so we kind of want it to be as flexible as possible. And so we've actually gone through three different base models and, I think, five different inference frameworks in the last year. And we've constantly, the team is constantly looking for, hey, is there, like, there's different sets of people looking at different things. And so we have one set of people looking at, hey, what are the latest open source models coming in? Can we fine-tune them, see what the quality of the predictions look like? And we have another set of people just looking at, like, hey, what are the latest and greatest coming out in inference frameworks? And what can we do? And then we obviously work closely with Google Cloud and folks from NVIDIA to see how we can optimize the hardware and get that going. So, yeah, we're all three dimensions we're actively working on, and it is a point-in-time answer. So if you ask me what's the best answer, I'm like, at what point in time do you get that answer from, right? Because it changes over time. Well, that makes sense. And we're always changing and trying to optimize. I mean, I feel like that's kind of daily life now. So what are some of the trade-offs you've had to make in your architecture? We'll start with you then. You kind of talked a little bit about it. Yeah, we touched upon it. Good intro for it. Yeah. One of the things, again, like I spoke specifically on the model side, right? But we do optimizations and trade-offs even just how frequently does a product require a prediction, right? Because sometimes what will happen is, especially for new products, merchant comes, adds something as a draft, right? And then they go back and they come back tomorrow and they add something else. And so we had to build debounce logic. We had to build caching layers. We had to build all. There's optimizations at every step of that pipeline, including how we store them and how they're retrieved. There are different use cases that consume all of this data. There are optimizations in that layer. And so, yeah, the trade-off really has been a quality versus cost trade-off, right? And so for us, we've discovered that right now, given the hardware available to us, given the frameworks available to us, the sweet spot for this problem is around that 7 to 11 billion parameter models, FP8 quantization. And so we kind of stick to that space. But things can change. And so we've started seeing 3B models also being able to match the performance now. And so things can change, right? And so, yeah, we keep making those trade-offs and we keep learning. Fantastic. Kasper, what are some of the trade-offs you've had to make in training Falcon? I think the three pillars... You didn't have to make any. You just got everything you wanted. Yeah. I think the three pillars we were, like, interested in were basically performance, stability, and automation. And I think in the end we traded some performance for stability because that was where our core focus was, just to have things running smoothly, reliably. And combining that with automation because we wanted to avoid situations when somebody needed to be on call or up during the night because something could potentially fail. So we made sure we have the best failure recovery and detection we can have. And only then we started looking at performance. So I think that was those... We were balancing those pillars with putting the key focus on stability and automation. Okay. And, Cesar, I'll ask you. Yeah. I guess in our world we have mostly custom models, very custom models. And in parts of the pipeline we have off-the-rack models that we fine-tune, et cetera. But the core of our business is these online custom models. So we have to, like everyone else, balance performance and throughput and research cycles. So, again, you can build the largest model, gives you the best quality. You can't serve it. It's too slow. So, you can build the model, or your researchers are waiting forever for your model to train for their test and analysis. So we have to think about a lot in this dimension. And, you know, we're coming to the realization, actually, that we might want the same model but at different capacities and for different use cases. So this thing we've been compromising over time and trading off over time for our specific main use case. We're trying to figure out how to get more granular and think about it more granularly. Again, you know, let's say we have a full second to make a prediction. Let's use that full second. Let's say we have 10 milliseconds. Let's use that same model but maybe, like, a smaller version or maybe potentially the future of something like Pathways, which allows us to select it directly. And, yeah, when developers are developing models, we have, like, a proxy model that is very close to the large model, but they can go really fast and iterate through that. So we're constantly making that balance between performance and quality like everyone else. Okay. So are you using Pathways now, or is this a future optimization? I have a hacky way to emulate what Pathways is doing, and hopefully Pathways does it so I don't need to use the hacky way. And I used that hacky way before my experience at Google before. So roughly speaking, as you can, you know, on TPUs, they have queues and they have different batches. Batch sizes, you can use them in different ways and, you know, route within the TPU. Two easy ways, just have two models. And your surfing stack just route to the one you need with a much smaller decision function or model to decide where to go. So, yes, ideally I want to use Pathways. It makes my life easier, but we haven't tried it yet because it's brand new for us. Yes, it was an exciting announcement we had here at Next. So how has the business had to impact what tradeoffs you've had to make? So for us, you know, being in ads, there's changes in privacy. There's new customers coming online. There's new formats. So we need to keep the business running and honestly iterating and advancing our model to meet those needs. So sometimes we have to balance doing that versus actually improving our infrastructure to make it faster. But the ultimate goal is, like, get the infrastructure to the point where you're investing more in researching and the infrastructure is running itself. And, again, with all this growth that we've had, you know, volume is, like, is something we really have to consider. And so then we even take our machine learning engineers and just put them straight on the data pipeline to help the data teams because we have to. Like, it's just going really fast. Yeah. And then there's some things, you know, like, that, again, Google got a little ahead of us finally on some things. Like, they're integrating Apache Iceberg into BigQuery. And we're already going down the route of, like, kind of doing that ourselves. So it's nice that some things are where we had planned out for the rest of the year. We can kind of hopefully work with Google to do that. Yeah. What are some of the other data and analytics processes or pipelines that you're using to feed into your training jobs? So we have a lot of data scientists going through our data trying to find niches of data where we're performing really well or not performing well. You know, we have to think about Android versus iOS X. So you can imagine that the training data set is also being used by for data analysis. That's one way. Yeah, for training and debugging. Because, you know, in our industry, you know, if you have an LLM and it says large versus huge, it's kind of okay. In our industry, if you get, like, one piece of garbage data, it can pollute a certain customer. So we need a lot of debuggability through that. So we need the stack that we can ‑‑ it's fairly manual, but you need to be able to at least know what places are and do that kind of work. Those are our main work threads on our data. Okay. And are you fine-tuning by specific customer segment or ‑‑ I can't ‑‑ I can't say. That's it. Okay. But we have to make sure we perform well on customers and industries and different formats. All these things we have to make sure we perform well. And exactly, I can't really say. Understandable. Thank you all for coming to Next and safe travels home. Thank you. Man, wait. Yeah. – Madam Staters. Dimensions The Un Testament