 Please welcome Leap by McKinsey partner Vishnu Kamalna and McKinsey principal data engineer Nishant Kumar. Good morning everyone. Thank you for joining us today. We want to start by talking about a little friend that we have, Sarah. Sarah is a business analyst at a large corporation and she was tasked recently with identifying the profitability of a bunch of different products, who are the customer segments, and the digital journey of those customers. She thought it would take a few days and create a nice dashboard for senior management to take product decisions, but what should have taken a few days ended up taking a few weeks. A lot of engineering stuff started happening and what people ultimately found out was you didn't really know where all the data lived. Even if you did, you didn't know which data source to trust and then you have to stitch all of this together to get a cohesive environment to actually start creating this dashboard. This sounds like a familiar problem that we all have on a day-to-day basis. And so today, the talk is all about talking about the advances in generative AI to detect and fix data quality issues and also identify what are the right ways to slice and dice data so that it actually meets business requirements. My name is Vishnu Kamalna. I'm a partner with McKinsey in the New York office, and I do a lot of work in the financial services space where I talk to our clients about how do they leverage their data assets to build new and innovative digital products. I'm joined by my colleague here, Nishant. Hello, everyone. I'm Nishant. I'm a principal engineer at McKinsey based out of New York office. I lead data labs at McKinsey and where I build AI-driven data management products and also applying them at corporates. So, as McKinsey, we talk to our clients on a daily basis about what are the different avenues of growth and revenue generation at the highest levels. What we did was a survey late last year where we spoke to over about 800 executives across different industries to say, where do you actually see the data? Sorry, where do you see revenue generation or growth? As it turns out, consistently over the past couple of years, new products and business building has actually been one of the top drivers and the expectations behind it. Now, if you double-click within that, data, analytics, and AI platforms actually come out to be the top. And we've also shown by industry breakout the level or percentage of executives who feel very bullish about this. But two highlights over here. One, it is actually the largest in 2024 at 48%. But two, it was also the largest step increase between 23 and 24. And the reason is because this was the year where generative AI and advances in AI went from POCs to let's actually get real tangible impact. So, with that, our executives did ask us this question. Hey, listen, over the past couple of years, 10 years plus, I've been hearing data is the new oil. But if it is really the new oil, why am I not getting money out of this? And we actually spent a lot of time thinking about this. We spoke to a lot of clients. We did a lot of engagements. And ultimately came up with a couple of challenges that were present across the industry. Now, before I get into the slide itself, there's a framework called the DIKW framework or the pyramid. It's data, insights, knowledge, and wisdom. What does that actually mean? At the lowest level, you have the core data or the raw data itself that you're trying to monetize. You build analytics models on top of it to be able to generate actionable insights. That's the I. You then provide tools and capabilities to generate simulations, what-if conditions, et cetera. That's the knowledge. That's where you can actually apply it in the real world. And wisdom is the practical experience that goes behind it. There's an idiom that says knowledge is knowing that tomato is a fruit. Wisdom is not putting it in a fruit salad. It's the same thing, right? It's a practical experience about all of these things between the K and the W. But the important aspect here between the entire pyramid is the higher up you go in the pyramid, the more sticky your customer base, the higher profit margins, and more difficult it is to replicate your secret sauce. But that said, all of that is dependent on the base layer of the pyramid, which is the data. Now, if you look at the data itself, data-driven organizations are 23 times more likely to acquire customers. And why? Because they actually make data-driven decisions. It's kind of like my grad school professor always used to tell me when I was writing my thesis. If you have data, let's look at it together. But if you have opinions, let's go with mine. And that's basically the same thing that organizations are doing over here by saying, look, I really want to understand the data, but I also want to make sure that it is not messy data because I'm making product decisions behind it. The second thing is it's about 19 times more likely to be profitable, which is a huge improvement compared to some of the guesswork or intuition-based product development. But here's a few avenues here. One, 30% of lost productivity is actually coming from just data quality wrangling and things like that. Remember, Sarah, the few days becoming a few weeks? And that's a very common example and archetype, by the way. That's where the lost productivity comes in. The second piece is the missed opportunity. In today's day and age where a lot of startups actually show up and have a lot of very fast iterations to market, those few days to a few weeks could actually lead to a first mover advantage that could be very difficult to displace. And the third one is the cost of poor data. About 35% of the current data spent today, and this was just a benchmark that McKinsey did, is actually spent in just trying to improve visibility and standardization of data management systems. So think of the amount of money that an average organization is spending as a direct implication of poor data. Now, if data is indeed the new oil, well, we need to get it from the raw material all the way to the refinement and ultimately to the petroleum products that create a lot of the margin. Now, not to get into a lot of the details behind it, but this is a general conceptual framework that we use to talk about where does the data originally come all the way to getting it ready for the business case. And we'll talk about the two ends of the spectrum here, discovery and fixing the data qualities, and then the right schema for the business use cases. Now, why discover is super important is because, again, when we talk to our executives, we kind of found out that there is a single person in an organization that would know how many chairs and tables and computers that every organization has. But you ask them the exact same thing about data, and they probably do not know. And that's one of the first key problems, which is if data is the new oil and if data is so valuable, why do we not know what data do we have? What quality does it exist in? And what insights can we generate from? That's where the discover part comes in. And then the second part is the detect issues. And we are going to spend a lot of time on the detecting issues and fixing it, which is number five that you see over there. And the second thing is actually going to be around defining the schemas so that you actually can generate data products super quickly and then try it out in the market. So, few big learnings in terms of a four-step process that we've seen that create a lot more impact directly from the data optimization side of things. The first one is business should define tech, not the other way around. We've seen a lot of hammer looking for the nail approaches. And they make for very good POCs and pretty cool demos. But a lot of these POCs do not actually find their way into production. The recent research that we did found that close to about 90% of AI models do not actually make their way into production. So, you know, there's a real piece over here about identifying the real business case. It sounds obvious, but that's actually the single biggest failure point that we have seen across all of engagements. The second thing is the third bullet point over there, which is AI is not a silver bullet. As much as we have seen a lot of reasoning capabilities and things like that come out, the subject matter expertise really makes a huge difference in fixing data quality and also making sure that the downstream data products that are being generated are actually of the highest quality. And that's the demo that we intend to show today with the human in the loop as an augmentation into the AI tools that we have created for you. With that, let me hand it over to Nishant to actually walk you through the demo. Yeah. As we have seen and as Vishnu said across our multiple deployments, which we did over the last three or four years applying AI for data management-related solutions, we saw that there are two biggest pain points in the things. Like one of the biggest is getting the data ready for the context, for the business context. And there are two parts to it. The first is getting the data quality right and then getting the data in the right shape and format via data product so that we can solve the business needs quickly and accelerate them. We'll talk about the data products in a minute, but talking about data quality, like if you think of a situation where we are working with, for example, let's say we are doing an analysis on retail stores and we have customer data. Having emails, a lot of null values there may not be a big problem for that. But having said that, if we are doing an analysis for marketing, their email is a real value. So based on the business context, what is the definition of data quality changes? Similarly, what accuracy of data do we need? Do we need it 80% or 95%? We'll be defined by the use case, which we do. So that's why we have seen across deployments, there are four main things, and we'll talk about those in this here, is that data context changes over time. Like what was the business trying to do three years back has changed and evolved now. That's why our processes for data quality also should evolve. And we should bring in the right algorithms, right tools for doing that. We also, SME or human in the loop is one of the most critical things to do, to have in the whole process. Because AI and Gen AI can give us the solutions, can give us the values of what is good and bad, but SME should provide the feedback back into the system. Like why do we need that? With advent of Gen AI in the last couple of years, enterprises are able to unlock the 90% of the data, which has been unstructured. And that's why, but the problem of garbage in, garbage out is real there as well. And that's why we have started applying data quality on that, on the unstructured documents, videos, etc., to check whether the data is right or not, and to get the right results out of it. And finally, agents are helping to reduce the barrier of usage for our users. Like agents can help us in building the workflow based on the business context. And then we can use agents to build very specific code for a given data quality issue to fix those. Let me make this real by walking through a demo. All right? So this is the dashboard of what we have as a solution, what we have connected across. When we did our deployments for multiple users, multiple enterprises, we saw that there are a combination of tools which we can apply for data quality. And this is the product which we call it as AI for Deque, which stands for AI for Data Quality. This is the dashboard for it. It is a modular tool, too. So every block here is a module in it. What our user asks for, or what we get asked for most of the times, is we don't know what are the unknowns, unknowns in the data. So that's what we go and target for. So in this demo, I'll walk you through some of the things which we do for finding the unknowns, unknowns. Let me start by looking at the data quality rules. So one thing which we have seen in the industry is data quality is driven by a lot of data validation rules. And doing these data validation rules take a lot of time because data owners and business owners have to come together, iterate through the process, and it takes some time to reach there. So what we see here is we use AI for mining out the patents in the data and anti-patterns in the data to inform what kind of data quality rules we can use. For example, if I sort by confidence here and take a very, very simple example just to set the context, what my algorithm is telling me here is that County Los Angeles is in state California 91% of the times in my data. So there is 9% of the times that mapping is either missing or is to a wrong state. And that's something which we have not set any expectation here. Algorithm is telling me out of the box, like, this is what we found out. Similarly, it can go a bit more complex where it can say, hey, when annual spend of customers is between this range and when the number of transactions is between this range, the customer type is standard 95% of the time. So there is 5% of the times when we are misclassifying this customer to a wrong category. And these are the things which we have looked at the data and figured those out for the user. So now a user can go and say, hey, okay, this makes a lot of sense for me. I want to make this as a data quality rule going forward so they can click on this, make it as a validation rule going forward. Similarly, anomaly detection is quite prevalent in the industry. But we have seen that when you do anomaly detection based on distance, it leads to a lot of false positives. That's why we started getting in more context into the data. So what we are doing here is we are using Bayesian networks to find out probabilistic distribution of the data and saying that here, for example, this is the network, this is the context which we have mined out of the data. We are saying zip code defines number of transactions in annual spend and number of transactions plus annual spend defines the customer type of the data. This is something which algorithm is telling us. And a human can go in and say, can add it to, like via configuration file, can provide this input that number of transactions define also some other attributes of the user and feed it back into the system. The system then takes all of that context and the data and here every dot is a row. So it assigns every row a score between zero to one on how anomalous the data is. So if I remove the non-anomalous ones, it's saying that, hey, for this row, customer type was a problem. And the customer type is premium as of now, should have been silver, based on annual spend of $1,520. So this is something which we are learning again, finding out the patterns which we apply for our users to use. Finally, one of the biggest pain points in data quality is entity resolution. And it's one of the hard problems because entities keep on changing. So we have seen that keeping a framework flexible where we can call anything an entity, but apply different attributes. If I zoom out a few bits, what we are saying here is, like there are a lot of information, but let me focus at the bottom. What we are saying is across two different tables, sales and finance, we are saying that the first name of the customer or the user is somewhat different and the phone number is somewhat different. So what the algorithms do is they first do a column-level comparison, find out what are the differences, and then ensemble it together at the row level. And the algorithms will then say that, okay, these two rows have very high similarity scores, so they should be very similar. So the framework here is, like, you first have to start at the column level, apply, because every column is different, so we cannot use one single algorithm and single weightage for every algorithm, then combine it at the row level, and then go from there on. Finally, bringing human in the loop, we apply, we give the human a place for providing the feedback. So here, when we are seeing is, what we are seeing is, the algorithms found out in finance table, there were 12 issues. We can expand on that and say, okay, someone can go and provide. Like, this was actually a valid issue versus this was invalid and so forth, and they can export this as an outcome. So this export then goes and gets applied to the machine learning model or to downstream algorithms that are saying that whether these are issues or not. So this was end-to-end flow of, like, looking at some of the algorithms, trying to find out unknowns and how a human can provide feedback into it. So this was all about the data quality, and now let's talk about how the shaping of data products is also critical in this case. So, again, coming back to the point of where shaping the data for business context is also critical, we are seeing that with Gen AI, we can accelerate this process quite a bit. Because to Gen AI, we can provide the context of the problem, provide it the source tables, either the tables or the metadata around it, or even, like, profile reports or so, into the Gen AI, and with some agentic workflows, we can create schemas which can be used for targets. And for when we have generated these targets, with the source tables, we can use pipeline generators to create the code in different languages to fill in from source to target. Let's make it real by looking at the demo video. We will walk you through very quick steps which our users are seeing. So here, this is the dashboard. Again, the product is called as AI for DP, which means AI for data products. This is the workflow for schema generator. We'll go through each of these steps. So first, what we are doing is setting up the context. So we are providing the system a context that can you create a BI reporting for me which has all of these different tables. And then we add to it, we provide the source tables in format of a DBML. And the DBML looks like this where we are giving in all the different tables like customers, accounts, transactions. These are the tables which we have. Just to explain what DBML in more details, like every block here is a table. Every row then talks about a column in it. And then finally, we can provide more information about what that column is. The more information, the better for the LLMs to do this. Then we can add target, like what kind of target we need. Again, this is coming from the user inputs or from the users that if we want the models to be shaped in a particular way, you can go and do that. The way we have seen that, the first step is to create the tables first. So we create the tables first. And then the user can go and edit those or delete the tables. And then go back and click on next to start creating columns for those tables. These are all the tables which then came back to us from the system. A user can then go and click on the things and then look at what are the different columns which were created by the system, except some of them add more columns, et cetera. This system also helps in a chain of thought way. First we created the tables, then we go on the columns, and the prompts will work in that format. Finally, then we say that, okay, this was the data model I was fine with. I want to now create a system. So we can then download the results in format of DBML, or we can also look at the ERD of the same. So the system gives us, like this is the target model which has been created, which a user can then modify to go and create those. After this target is created, then we feedback that into pipeline generator. We'll go through the same process again of first providing the context, because now I'm creating a pipeline in a language. What I'm saying is, can you create a pipeline with Python 3.1 and PySpark? Now I take the target, take the source which we created, which we started from, and then go from there on. Then we also take the look at the source, define the target from there, which we created in the last step. And we can then also edit the target there, or just keep on going from there on. After that, the system asks us, are these the target? It gives us another opportunity to update those, and then we update them. Also, we can select, from our user inputs, we have seen that these are the three most used languages or extracted languages as of now. In this case, we are going to use PySpark, and then we can download the code from there on. The code will give you, give a user what are the different folders which will be in the code. I can then go and download the file as the code, upload that in my VS code. Here you see there are two folders where we have SRC, which has all the code. We'll go into one of those files in more details. And this basically gives me how that target column was created in the model, and it is in PySpark with all the commands and so forth. And we have test cases for the code as well. So this was the whole flow for creating data products using Gen.AI, which we are seeing how our users are going through it. This is the repeatable pattern which we are seeing in the thing. And now, coming back to our learnings from deployments, back to Vishnu. Thanks, Nishant. So we've deployed these kinds of tools quite a bit across a bunch of clients, and we wanted to share a few of our learnings that can be applied across the board here. The first one is data quality is not just an IT problem, but it is a business imperative. And what we mean by this is not just the fact that there is a lot of value in the data, like the data is the new oil concept, but also we've seen that businesses sort of think about data as just a tech issue and say, oh, hey, let's go fix data. But there is no real definition of what fixing that really means or what is the unlock from it. And there's also not a real business case behind it. Therefore, what we suggest is actually looking at it not just as a business problem, but as a symbiotic relationship between technology and the business. The business actually has a business imperative for fixing the data quality, and technology should be enabling that. On a related point, it's also the piece that data quality may not necessarily be the root cause. It could actually be just a symptom. So as you probably are all familiar with the tale of the five blind men touching an elephant, and all of them touched different parts of the elephant and found different characteristics, and they sort of extrapolated that, that's kind of what we see when data quality, or more broadly, is defined in silos. So every different team thinks of a different issue with the data, as opposed to looking at it more holistically at the company-wide level and saying, what really is the problem with this data, and therefore, how do we go fix this? And so actually having a cross-team and a cross-BU collaboration really is important. That's point one. And then the second point is, why was this data poor in the first place? And this usually has led to a couple of tough conversations around the governance mechanisms behind data itself. So looking at it from both those perspectives actually helps in identifying the root cause behind why this data was poorer. The third piece is around explainability. A lot of advances in generative AI and AI more broadly has led to better technologies and better outcomes, for sure, but at the cost of explainability. We're not really able to articulate exactly why the models were coming up with those decisions or those outputs. And that's fine in certain use cases, but in certain other use cases, it's really not that fine. And it also depends on the industry that you're working in. Some of the more regulated industries, like financial services or in pharmaceuticals, you actually do need to explain why those decisions were made. And in other cases, even in terms of marketing, the actionability of those models actually makes a huge difference. Why those models made those choices is important for you to create the downstream actions and the triggers behind it rather than just being able to predict who is going to do what kind of an action. Yeah, and the final one is like where we have seen applying data quality at one point doesn't help a lot. We have seen that like first thing is doing data quality checks earlier reduces the processing cost, and mainly the human frustration of like, why can't we, why could have we not checked that earlier? That's first. Second is then also, the data quality has to be applied across the journey. We have seen that you can, the data changes its shape and its context, its value over the time of the data pipeline, data lifetime, and hence applying data quality across the journey of the data is critical. So with that, I know we have just a very little amount of time, but we also wanted to share some of our latest insights. These are QR codes of articles that we've published very recently, including how do we scale AI across an organization, and what is the path to really creating an enterprise-wide AI-first culture. There's also the piece around solving data quality with Gen AI applications, the very one at the very end. So that actually talks a little bit more about our learnings, and what we've seen as practical experiences when it comes to the corporate world. All right. Well, thank you all for the time. Please do fill up the survey if you get a minute, but thanks a lot for giving us the opportunity to present here. Thank you. Thank you for joining us. Thank you. Thank you. Thank you. Thank you. Thank you.