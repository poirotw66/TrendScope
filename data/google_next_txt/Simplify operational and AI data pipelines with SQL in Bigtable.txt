 I'm sure you heard a lot about AI in the past couple of days. And one thing AI does is it enables automated decision making at a scale and speed that is not humanly possible. But what that means is you need to process more data. You will create even more data if you want to log every decision made by the AI. And data will come in many different shapes. It's not going to be always tabular. It could be images, JSON files, because it will be generated by many, many different processes. And as you try to make these automated decisions, people typically talk about when you do AI ML, it's garbage in and garbage out. If your data quality isn't good, you're not going to lead to good decisions. So it has to be timely and it has to be high-quality data. If there are any delays in your data pipelines, that immediately goes into the quality territory because you're not working on the freshest data. So today we'll talk about how choosing the right database can help you solve some of these problems and help you on your real-time data journey. My name is Bora. I lead the product team at Bigtable. And joining me is Steve, one of our lead engineers, working on the projects that we're going to talk about. And then Projol, who's the co-founder at Zeotab, will talk about how they implemented a real-time data platform using GCP infrastructure. First, lay of the land. So Google has many databases. Whatever your use case is, we have something for it. It could be an in-memory cache. It could be a relational database. On the other end of the spectrum, we have Spanner, which is a multimodal system that can do many different workloads, including Graph. And this week, you probably heard a lot of announcements. One of them was Firestore with a MongoDB-compatible interface. Bigtable had a lot of announcements as well, like continuous materialized views and the GAO SQL support. And my talk is going to primarily focus on Bigtable here. So many of you probably have come across the Bigtable paper. It's quite famous. From a couple of decades ago, it kind of started the NoSQL movement. It invented the term white-column store. And there were a lot of open-source solutions, like Apache Cassandra on HBase, that took the idea and created similar solutions on their own. And one thing that we are proud of is, even though these products evolved in parallel, since the data model is overlapping, how we store the data, a lot of the capabilities are overlapping, we still have some level of compatibility. We offer compatible interfaces for both of these systems. I think one thing that's interesting is, a lot of people, when they think about key-value stores or wide-column stores, the typical use case that comes to mind is, or maybe I have a product catalog. I want to serve product listings. Or it's a social media company. I have my feed. It could be a DNS service. I'm going to take the URL and convert it into an IP address. It could be a block listing for certain malicious IP addresses, et cetera. A lot of these common key-value workloads are the first thing that comes to mind. And this talk is about real-time analytics. And that is not the thing that, you know, is the first thing that people think about when they say key-value stores. But it is very common, actually, for Bigtable to be used for real-time analytics. And we hear this from customers over and over again. And one thing I'd like to highlight is why it is the case today. Then I'm going to move on to how are we making it even better. And common use cases for Bigtable in analytics is ML feature stores is one thing. If you want to provide a recommendation, you want to bring hundreds of ML features about that entity, and you convert it into a prediction. We've seen use cases where Bigtable is used as a context store for generative AI applications. If you want to have the older user context, older chat history, et cetera, you want to load it up and continue where you left off. And there are a lot of cases where people use it for in-app analytics. Like, you could see a like button somewhere, how many people view this content, et cetera, those kind of use cases. First, you don't think about analytics, but all it does is some aggregation across some dimensionality. So it is analytics in some way. We call it operational analytics. But for you to be able to do this right, you have to have a few capabilities. One, you have to be able to ingest data really fast at large volumes because all these click events, user things that are happening, you have to respond with AI. And because they're happening so fast, the only way you can respond to them is using AI. You have to be able to deal with that kind of volume. The other part is, as we said, data comes in many shapes. You want to be able to transform it. Because if you've got a JSON document where you want a number out of it, if you could do all of this stuff in the database, it saves you a lot of time. And the last thing is, you did all this processing, you want to be able to efficiently read it, and there are multiple access patterns. There's point reads that we often see. Again, ML feature store use case. Maybe you have a user ID. You just want to get all the features for that user. That's a point lookup. And there are cases where you want to scan across multiple users because you're training an ML model. You have thousands and millions of users. Then it's the same data, but the access pattern is batch. So if you do both of them in the same system, it saves you a lot of headache. You don't have bifurcation of data. It costs you less. So there's lots of benefits. And if you have all of these threes, you're on your way to a good real-time analytics solution. So how does Bigtable do this? First, last year at Next, we announced a new feature called Distributor Counters, which is a big part of enabling some of these use cases. And I wanted to show two different examples here because they look very different, but underlying infrastructure to support them are actually quite similar. So one of them is, as you can see, it's a time series reporting. You could see maybe a time series database use case. You're looking at how things are changing over time because every right-to-big table has a timestamp. The second one is the more typical one you see in every application these days. How many people share this content, like this, dislike this content, et cetera. So under the covers, if you have an application that has millions of users, these clicks are coming all the time. You have to have a way to be able to merge these into those numeric values. And historically, people use separate streaming systems, so they will serve the content from one database, get the counts from another database. And there were lots of issues. They can't reset the counters. If they're distributed across the globe, the data doesn't converge. So we came up with a dedicated structure that optimizes for this use case. And what it does is, effectively, we know the math is commutative and associative. So if I do a plus one, another plus one somewhere, plus five, doesn't matter in what order I execute these things. If I know the change, I can construct the current state. That means I can do a plus one in Japan and a minus five in Istanbul. They don't have to take a lock to update that row. They can just take the write immediately and in the background do the merge because the ordering doesn't matter. So this gives us amazing performance benefits. We saw 15, 16, more throughput that we can do by using this approach as opposed to a typical database read-modify write. Latencies are down 93% compared to a typical database read-modify write. So this is a very common use case for a lot of applications, modern applications, and we greatly improve that experience. You can do sums and counts. You can do min and max. You can also do things like approximate count distinct. So if you're looking at my weekly active users, daily active users, et cetera, that's a great way to track those at volume. The other thing is transformation. We talked about three pillars of how to make this work. So we GA'd support for SQL in the last couple of days at this event. So what this does is a Bigtable is still a no SQL database. It can still operate over unstructured data, but we extended some of SQL functions in Google SQL to enable working with wide column tables. And there's a lot of transformation, so we enable a lot of ELT type of use cases as well. Let me give you some examples. For example, let's say you put a JSON file in Bigtable. In Bigtable, we can put anything. We treat everything as bytes. You can put images, whatever you want. But at read time, you want to understand there's this attribute in this JSON. I just want to be able to read it. So in this case, we have some sensor. It has an IP address. I can write a JSON query just to pull that as if it's a structured table. In other things, like we have 100-plus functions that we support with SQL today, if you want to do something like nearest neighbor search because you have some embeddings that you stored in Bigtable, you can do that search between the two embeddings and do a sort based on the results. So a lot of these patterns are not things people are used to seeing in NoSQL databases. And one interesting thing is everything in Bigtable is versioned. So you could do an as-of query. As of this point in time, what was the valid value in the database? And this is quite common, especially in machine learning, because when a customer churns, they made their mind. You want to be able to predict it a week before they'd make that decision. So if I look at write five minutes before they cancel, their subscription, that's not a predictive signal because they already decided. So I have to find out what were they thinking a week ago. So you want to be able to write an as-of query. So it's a very crucial capability, and it is just as simple as saying as of this moment, do a select WeBolt in line. And one other thing we launched as part of this event is logical views. So if you don't want to write all of these complex queries, you can just save them, and the next reader can have this JSON looking to them like a flat table on a relational database. Another thing that's quite relevant is we talked about access patterns. So point reads, you have to be fast, and there's batch reads. For many databases, the moment you say, I'm going to read a terabyte of data, give me five billion rows, your operational latencies are completely destroyed. You can't use a database for anything else. So we GA'd DataBoost a few months ago. What it allows you to do is it connects to the underlying files in the file system. We have this disaggregated architecture. We can scale them independently. So you're not going to the front door of Bigtable impacting your latencies so there's a single digit milliseconds, two, three millisecond reads while you're reading terabytes and terabytes of data under the covers. So what this allows you to do is you don't have to export your data to GCS, BigQuery, et cetera, to train an ML model because it has all sorts of other problems. One is your cost goes up because you have multiple copies of data. You have data governance issues because you have multiple copies of data. Then you have this thing called a training serving skew because now your model is trained on some older version of the data. Your serving traffic is operating on a newer version of the data, and things don't align. So by using one single store, something like Bigtable, and you can have HDD in Bigtable where you can put historical data for long-term, SSD for near-term data for live things, you simplify this architecture quite a bit instead of having online offline stores. The other thing is reverse ETL, which is something we GA'd very recently. And what this does is we talked about all these real-time features. How many times somebody clicked on something? I want to react to that in real-time. If they purchased a product, I don't want to recommend them the same product five minutes later. So those happen in real-time. But there are slowly changing features as well. You may be classifying a user based on their spending patterns, or maybe this is a young gentleman with a kid because they're buying diapers or something. Those things don't change every second. Someone's age doesn't change every second. So you can compute those things still in batch, but at serving time, what you want to do is accessing that data is still as fast as real-time data. So you reverse ETL, that into Bigtable from BigQuery. And with this feature, you don't have to have an ETL pipeline. You write the query. You say continuously run this and export the results into Bigtable. And Bigtable being a wide column store, you can have static columns. You can have a few columns that are counters that are updating in real-time. You can have 50 other columns that are slowly updating and getting this ETL process. But it looks like a single row with hundreds of columns that are updating in different frequencies. Since we can update everything, every cell has a separate key independently. These don't kind of step on top of each other, which is a quite powerful thing to do. Okay. So we talked about why Bigtable is good now. And now it's time to talk about why it's going to be even better. So as part of SQL, we added group buy support recently. So if you're doing all these aggregations, counters, et cetera, and you want to do retime aggregations on top of it, maybe you have daily counters and you want a seven-day total, you can do group buys now. We added something we call structured keys. In Bigtable, everything is a string, the key. You concatenate a bunch of things. Now you can tell me the first key is the user ID, second one is the campaign ID, and you can refer to them as columns in your queries and filter on individual components. It's like a composite primary key. It's supported by Bigtable now. Lastly, we added these pivoting transforms because, as you know, Bigtable has this three-dimensional structure. You have timestamps adding an extra dimension. This will allow you to automatically pivot the data so you can aggregate over different timestamps. So we made it fit into the SQL's two-dimensional structure by adding new functions. Okay. So one last thing before we go into demo is we launched continuous materialized views. That was one of our big announcements this year. And this allows you to create alternative views of the same data. So let's say your write comes in, somebody clicked on this ad. But you want to roll it up and say, okay, which campaign should be incremented because of this? Which advertiser benefited from this click, et cetera? For that, you can write a continuous materialized view. So it's good for large-scale operational reporting and ad tech and media. It's like impressions, engagement statistics. If you have ad campaigns, you can look at the performance reports. Create it this way. If you're in retail and payment platforms, you probably have merchants on your site. They want to understand how their sites are doing. So you can have merchant dashboards. You can have in-store personnel reporting, what's our inventory, et cetera. And we talked about in-app metrics and dashboards. Just think, you know, you go to a site, it says this many people looked at your profile at these dates, et cetera. That is an in-app dashboard. You serve millions of times, and it's a fixed query. And of course, we talked about personalization, fraud detection, feature store use cases. They're quite relevant. And lastly, I guess this is the most obvious one, telemetry dashboards. Now, you can use Bigtable more like a time series database given these functionalities. Okay, so let's see a quick demo on how this all comes together. Steve is joining me for the demo. Okay, so in this case, we have this table called add logs, and that's where all the click events are being written. And it's receiving a high number of writes as we speak. So if you run this query, we could see the table structure in SQL. As I was saying, this is a long concatenated string of keys, but you can click on it, and you know, you could see in a JSON-like manner what the columns were, are. And you could see we have the count, the spend in USD, and the key structure has the advertiser ID as a prefix. Then you have the campaign ID and timestamps, et cetera. So how do you take this and turn it into something that is more consumable for someone like an advertiser? So we will create a materialize view. In this case, we have already created, and this is a definition. It takes the key, splits it into its components, advertiser ID, campaign ID, et cetera, takes a timestamp, and it truncates it because we want to do hourly aggregations. As you know, big table timestamps are in milliseconds. So you want to roll it up to hourly aggregations. And does a sum of the count and the spend. So again, as we speak, more writes are coming into this. So let's go to the materialize view. It's called ads analytics. And see if you refresh it, if the data is updating, actually. So as you can see, our continuous materialize view is updating. And the reason it's updating so fast is, you know, it's continuously running in the background, just operating on deltas. It's a massive table. We're not recomputing the whole table every time. So it's quite efficient. So this is cool. We keep refreshing. But the best way to look at this is, let's put together a dashboard so someone like an advertiser could understand what's happening. So what you're seeing at the top is, we took the materialize view output. And what you're seeing is, for that advertiser, all the campaigns they had, all the numbers at the bottom are the campaigns. How many clicks are different campaigns getting? So as you click, we actually run a separate query, which is at the bottom, for a given ad they're running, what the click-through rate over time is. So it is pretty interactive. And you can imagine doing this, you know, in your application, you have a million users. They're all running these queries. Since we're really good at serving fast reads, this scales infinitely. More nodes you add will be able to serve more. And since it's a materializer, you're processing very little data. So we have hourly. We just look at n hours and do a sum at read time. So again, this is something you don't think of Bigtable about putting a dashboard on and serving your application, but it is quite achievable today. So one thing we could do is go even deeper, because the writes are actually coming. I can track the actual writes. So this is live writes coming into a row using counters. And this dashboard is updating about three times a second. So we're running like two queries, three times a second, like six queries running in parallel. And again, you can run many, many of these things. And you can see how fast, you know, how real-time things are. You can do this with a million writes, and it's still going to perform. You just need to add more nodes. Okay, so I think one cool thing we haven't talked about that much is there's a lot more happening under the covers. So let's go and look at our performance dashboard, because what trick we did was before Steve came on stage, he kicked off a Data Boost job, which is doing a massive scan. So this is the first time I'm seeing the dashboard. Hopefully everything still looks good. So we are reading 25 megabytes, 35 megabytes per second right now. And our latencies are still in, since it's a dual-axis chart, it is difficult to read. And it's Eastern time, so it says 7.15. So right now, around 7.15, you can see it's well below 20 milliseconds. And you are not getting the impact of this large scan that is happening in the background. Okay, so I think this gives you a good idea about how you can mix real-time and batch workloads and how you can have a very heavy write workload in a single system and serve your real-time analytics needs and batch analytics in one database. To tell you more about how this works under the covers and why it's so efficient, I'll give it all to Steve. Thank you. All right. Thank you, Bora. And Demo. All right. All right. So I'm going to dive in and talk a little more about how Materialize Views work under the hood and kind of the ideas we had behind them and the use cases we were trying to solve as we were building them. So I'm Steve Nevins again. And so to start out, we designed Materialize Views really to solve some common problems that we see customers running into a lot of like, I have my data in Bigtable. I want to do some kind of aggregation calculation to it. And I want to put it back in Bigtable so I can serve it because as you saw in the demo, we're good at serving stuff really fast. We're good at storing a lot of data. But the missing piece in the middle is like, how do I get that data out, aggregate it in some way that I can then serve it again? So we thought about, hey, let's go build kind of something like Materialize Views. Because of Bigtable's unique architecture, we were able to provide some really more advanced features that a lot of the Materialize View products out there aren't able to support. So we can support updates and deletes to your underlying data. And they'll basically be instantly, almost instantly, reflected in the Materialize View. Even with more complex aggregations and queries, you can do a maximum. And if you delete the maximum value, it will automatically recompute and give you the new maximum without you having to go in and manually refresh the view or go rebuild it from scratch or anything like that. Additionally, unlike a lot of streaming systems, you don't have to worry about windowing or late data or late arrivals or anything when using Materialize Views. We handle that under the cover. We'll always consider any data that changes, regardless of when it arrives or what time it arrived at and anything like that. We built this natively into Bigtable. There's no external processes or anything that you need to worry about managing or operating. You can kind of benefit from all of the benefits that Bigtable gives you, like load rebalancing and auto-scaling and all of that. All just works directly with Materialize Views and interacts really well with it. There's no need to manage or operate external systems. You don't have to go run a job on the side. You don't need a batch job that you need to monitor. You don't have to have a streaming job that you need to monitor and worry that it's falling behind. You have to go restart it, figure out what's going on, anything like that. Again, we handle that all in Bigtable. It's all managed for you automatically. We built them really from the ground up with performance in mind. One of the big benefits you get with Materialize Views is adding more of them will not slow down your write throughput or anything like that. You can keep adding views. Your write throughput will really stay the same. They're updated asynchronously. In the background, not directly at write time, so you don't have to worry about things like that. Again, like I mentioned, because they interact all with Bigtable's advanced rebalancing and load splitting and everything like that, you don't really have to worry about hotspotting. We've designed this from the ground up to really mitigate hotspots. You could do a select count star only from your table and consolidate. Every write goes into updating a counter of the number of rows in your table, and that would all just work fine. We will handle all of the load issues there transparently in the background. With that, there's what do you use? What's the right tool for the job? Counters are great for a single row. If you have a piece of data that you know you're updating in the background, like a view count of a page, that's a really great use for counters, something like that. And then materialized views are really good for, like, I want to take a bunch of my data across a bunch of rows, aggregate them together in near real time, and get the result somewhere that I can serve it really quickly. So that's kind of the trade-offs there and the decisions of why you want to use what. So from there, we're going to go over to Xenotap, and we'll let you take it from here. Thank you. Thank you. Pleasure. Hi, everyone. I'm going to try and do a few things. I'll give you a little context on Xenotap, the platform, and the kinds of use cases we look to address, which is a little bit about what kinds of problems we're looking to solve. Then we talk about our journey over the last three years and the evolution of our tech stack and our architecture and why we arrived at Bigtable, both individually for Bigtable as well as for its synergies with BigQuery. So Xenotap, customer data platform. If you're not familiar with customer data platforms, they are analogous to master data platforms in a sense, but with a focus just on customer data. We work with some of the largest enterprises across Europe, typically companies with over a billion dollars in revenue that have customer data sitting across multiple data stores internally and externally across different types of systems, different schemas, different types of data, some that's transactable, some that's not, some that's identifiable, some that's not. We basically harness all of this data, consolidate it, and build a single customer view. We do that both for ID data as well as for profile data, and importantly in the context of Europe, but also now globally, we also tend to serve as the consent and marketing preferences master for our clients. So what does that typically look like? Data sitting across multiple silos, as I said, CRM, ERP, web and app, data warehouse, or data lake, and then of course all of that external data from the paid marketing and the social media campaigns. We consolidate those into different types of ID data, getting stitched together, the profile attributes and events, some of them demographics, some of them streaming behavioral data, consent and preferences, and then some derivatives on top, and those that can channel to the last mile systems anywhere where there is some kind of end customer touch point. It typically starts with acquisition and retention on typically owned and operated media, marketing, personalization, next best offer, next best action, and so on. Grow into customer care use cases so that the customer care and the customer support agents have the same view as the marketers do, part of the larger customer lifecycle, and then increasingly that same asset gets used and harnessed by data scientists internally also for analytics. Some creds, just off the top of my head, 8 billion new data points a day across more than 300 data sources, about 1,800 workflows, one third of those in real time, a lot of egress to almost 100 different channels, a lot of the data is consumed in real time, so we have data APIs I'll talk about in a second, and we have very stringent data SLAs, less than 100 milliseconds response there, and then end-to-end real-time workflows on our platform also require less than one second, P95 typically, end-to-end latencies. From left to right, very analogous to a typical data product, so data ingestion, data processing, data storage, and data distribution across different layers of the stack, but what's interesting for us is that we do it both hosted on our platform as well as hosting it client-side within client Google Cloud environments. I'll talk a little bit about that as well. So what's been our evolution? In 2020, we migrated from AWS to Google Cloud. We were a big consumer of Scylla DB. We had one of the largest graphs in the world running on Scylla DB with Janus Graph on top. Most of the processing at the time was batch or micro-batch. We moved to Lambda architecture in 2021, primarily looking to adopt data flow and made initial sort of forays into BigQuery. 2022, we went BigQuery heavy. We dismantled Janus Graph because it just wasn't scaling for us, so it became a pure KV store on top of Scylla DB. 2023 is when we went Kappa. We realized that a lot of our clients were looking for real-time first use cases, so we moved all the batch-based workloads also to streaming real-time, ingestion, and egress. We also had to architect, re-architect our network design from scratch to ensure that we could keep up with the kinds of latencies and SLAs we were promising in market. And then in 2024, we were ready to commit, so we went entirely cloud-native, big on Bigtable, big on BigQuery, and Memstall. No more spark in the stack. The evolution of our technology stack effectively mirrored the evolution of our business use cases. So we went from typical audiences or batch-based use cases looking at, you know, batch-based workloads to primarily streaming first real-time workloads, everything from in-app, in-browser, personalization to next best action, also consent and marketing preferences being consumed in real-time, all low latency, all to ensure that we could meet the kinds of SLAs we promised. So what were our design needs? Streaming first, zero-touch ops, ready to commit to cloud-native, clear workload separation across two dimensions, typically, batch and non-batch, read and write. We wanted an SLA-embedded architecture. What that means for us is that every component of our stack is self-aware of processing times and down times, and to the extent possible, also self-healing, and of course, OLAP and OLTP capable. platform demands. Very high variability in the workloads, primarily because we can't always predict when there are going to be bursts or spikes, but depending on the nature of the client, we have large sports betting clients for whom load can be very spiky. We have large ticketing engines for whom load can be very spiky. For example, when Taylor Swift tickets go on sale. So not all of that is very easy to predict. So we needed the ability to be able to handle very spiky, burst-based workloads. Typically, more than 300,000 writes per second reads almost 3x of that, and of course, TTL flexibility. TTL is interesting because if you consider the average customer data set sitting in our platform, they have different identifiers. Take your data, there'll be an email, phone number, a plethora of cookies, mobile device IDs, also a number of internal identifiers across the different platforms or a CRM ID or an ERP ID and so on. All of these have different preferences and different TTLs. So we needed to be able to reflect those at the granular level. So the question that comes up is why did we pick Bigtable and not just another KV store? We had four major needs. The first, as I said, was around flexibility. We needed the ability to deploy our platform within the client environment. Having third-party tech embedded in our stack made it difficult to deploy. The commercial process or the procurement process was also complicated and of course, post-deployment ops was also equally burdensome. So that was one. Performance was, of course, important for us. We are the only CDP that promises not just uptime but what we call flow time, so end-to-end latencies around data ingestion, processing, and egress. So we needed to have predictable performance even when workloads were unpredictable. And of course, minimal ops associated with keeping up those performance levels. TCO important, especially in the last few years, TCOs come under very heavy scrutiny, not just for us as a platform where we were hosting but also for our clients. When we were deploying our platform client side, they needed to be able to predict and forecast costs as well. There was also some TCO reduction in going cloud native I'll talk about. And efficiency, as Bora and Steve mentioned, we didn't want to deal with having to map hardware to different kinds of workloads comparing the performance and benchmarking the performance of different machine types. We also didn't want to run a script in the background to add nodes when resource alerts came up. All of that overhead we wanted to eliminate. Enter Bigtable. So we primarily had the ingestion layer running via data flow with a combination of MemStore and Bigtable powering inline enrichment, transformation, and ingestion into our database. As I said, we also use BigQuery as a warm store. Bigtable is our hot store. I'll talk about that. In terms of consumption, Bigtable powers the real-time journeys on the platform which are auto-provisioned based on the kinds of underlying use cases. So we have an internal orchestration layer and depending on the complexity and the statefulness of the journey, it picks different ways in which that's orchestrated. And Bigtable also, as I said, powers our data API for the application which consumes this unified data set. It's sort of this point look up, the needle in the haystack kind of use case where we can deliver data in low latency, read from the unified data set to any kind of downstream system we are completely agnostic. So what does Bigtable do for us? A few different things. The core of the platform is ID stitching. ID stitching for us is basically creating a singular view of all identities or identifiers of the customers. Identifiers come in across different systems from different sources. We have a declarative ID strategy on our platform that's configured via the UI. What that basically means is clients can configure a hierarchy and stitching preferences based on their data quality and their needs. So the multi-level hierarchies, what's excluded, what's included, the priorities, all of that can translate to multiple reads and writes happening simultaneously. All of that's powered by Bigtable. The real-time delivery of the Customer 360, the data API, as I said, different types of assets associated with this Customer 360, the identities, the profile, the streaming events, calculated attributes, time series data sitting on top, also all of the consent and marketing preference data delivered in P99 of under 100 MS, again via Bigtable. So, Bigtable as the hot store, BigQuery as the warm store has really been a huge savior for us, both from the perspective of managing the TTLs, as I said, using federated queries, and then having advanced calculations happen on BQ that get, via an ETL job, that get distributed back to Bigtable then for real-time consumption. What's been an added benefit for us is the AI side of things. The AI pipelines have been optimized immensely because our time to market with AI pipelines has shrunk dramatically. BQML output flows back into Bigtable and gets plugged into the consumption pipelines. In the other direction, Bigtable doubles down as our feature store from which BQML can read as well, so it's bidirectional. Through this architecture, we've been able to deploy models for clients in just less than a week, and this used to be a multi-week, sometimes a multi-month process for us. So, in summary, Bigtable has ticked all the boxes we were looking for. From the perspective of flexibility, we have turnkey deployment across different types of hosting, whether it's hosted on our side, managed, hosted client side, in a private context, or some kind of hybrid thereof. Performance, lower latencies, greater reliability, so that we can keep up with our SLA commitments, costs. We've seen a 46% reduction in TCO from moving from what we had previously to Bigtable over the last 18 months. And, of course, efficiency, better resource management, more elasticity, better elasticity with having a cloud-native option. Some collateral benefits. So, we have batch workloads now reading off of our DR cluster. We have seamless integration with BigQuery now, unlocking a number of OLAP and OLTP use cases. And, of course, our overall operational footprint has reduced by about 20%. Looking ahead, we are exploring reactive kappa architecture, and Bigtable empowers us to do so. Of course, the latency improvements are important for us. As I said, the ultra-low latency use cases, such as same-page personalization, overall SLA improvement, continuous materialized views for time series use cases, again, very important for us, large nodes, quite interesting to improve tail latencies that we see from time to time, and, of course, data boost for ML training use cases. Thank you. Thank you. Thank you. Thank you. Thank you.