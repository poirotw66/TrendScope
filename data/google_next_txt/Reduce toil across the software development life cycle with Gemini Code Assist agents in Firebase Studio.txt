 JAY SRIVASTAVA- Hi. I'm Jay, and I'm a product manager on the Google Cloud team working on AI agents. By now, you've probably either used AI agents or heard about them. We think that AI agents should empower developers by taking away toilsome tasks and allowing you to do what you do best, coding. Our approach extends beyond the IDE, recognizing that delivering robust software involves more than just design and coding. By identifying areas throughout the broader software lifecycle where AI can contribute, we aim to improve software engineering by making it less burdensome. In this session, we're going to review two agents, one that allows you to understand and chat with your code base, and another that ensures your generative AI returns safe results. First up, code documentation. When you hear code documentation, you probably think one of two things. First, you might think of documentation for your end users. The other thing you might think of is inline documentation within your code base. But the code documentation agent is neither of those things. Rather, the code documentation agent represents a novel way of representing your code that provides a really insightful overview and, by extension, a more powerful way to chat with your code. The code documentation agent takes your repository and builds from it a hierarchical, wiki-style representation of your code. This value shows up in two places. First, the documentation itself, which you can browse, expand, and collapse all as you would in any wiki-style document. And second, this same material provides excellent context to your AI-based chat. Here's an example. The docs for this repo are all sorted into something that looks really similar to a page on Wikipedia. As you want to learn more about something in the weeds, you can zoom in to the relevant section. By zooming in, you see where that specific item fits in the bigger picture, which is really important for understanding large code bases, especially those with many contributors who may or may not still be on the project. And on the flip side of this, we have an improved chat experience. This links to the relevant parts of the wiki document. You can zoom in and out to see the bigger picture for yourself, in combination with the AI's answer to your question. And of course, it's linked to the source code itself as well, which helps you understand and take action easily. Next, let's talk about the AI testing agent. This allows you to automate red teaming and ensure that what you're launching is safe for the users that you're targeting. Concerns we're hearing from our customers are often around ensuring that the AI systems are acting as intended, based on the audience in question. For example, you might want to ensure that your application is only discussing topics relevant to children. Or you might want to help ensure that your product isn't creating posts with potentially offensive content. AI adversarial testing is our answer to application developers looking to test their generative AI implementation to ensure they're safer users. Here's a bit about how it works. First, the adversarial prompt generator creates adversarial prompts using an unfiltered model. It then feeds those into your LLM. We gather those responses and we take them through an evaluation. We give them a pass or fail grade based off of the content policies in question. And lastly, we aggregate all of that information and create a report that you can share with your team and take action on. One of the ways that you can take action is with Model Armor. Model Armor allows advanced capabilities such as prompt injection protection, gel spray protection, and AI guardrails. We think these products go perfectly hand in hand together. Let's jump into the product. Here you can start by triggering the AI testing agent in Firebase Studio or Gemini Code Assist. Here we're asking to help me make sure my model doesn't have any safety issues. The agent tells us what it's going to do, and it includes all of the content policies that it's going to cover, such as dangerous content, harassment, and hate speech. We can click in to get started. And here we can see that the AI testing evaluation card is in the action needed section of the Kanban board. Here we need to connect our model in order to kick off the evaluation. Once we've connected our model, the card moves into the running section. Here, the agent is being transparent about every step in the process. We've connected the model. It's generating the prompts, evaluating the model responses, and then it starts generating that report. Once the report is ready, we can see a highlight of the results here on the Kanban board. We can see that 97% of response is passed, 3% of response is failed, and we can click in to view more information. Here's the full report. At the top, we have a summary of findings. Beneath that, we have a visual representation of the content policies and how many responses passed or failed. We then have strengths and areas to review for your particular implementation of your model. And then lastly, we have every single response, and whether it's passed or failed, you can also filter by content policy. In the upper right-hand corner, you can download that report to share it with your team, and you can take action by clicking into Model Armor. We want to thank you for being part of the Google Developer community, and we hope that these products help you become more efficient and focus on what you do best. Thanks. Thank you. Adoinette. He is a video. Facebook. Call us as Facebook. De Keith.