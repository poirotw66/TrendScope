 My name is Alex Spiridonov, and I lead the product management team for AI ML software and solutions for cloud TPUs and GPUs. And I'm here today with Nathan and Scott to talk to you about AI hypercomputer and GKE. And we have a great agenda for you today. We are living in the era of AI, and this is driving an unprecedented demand for infrastructure. It seems like just a few years ago, models with a few hundred million of parameters were considered state of the art. Fast forward to today, and we're working with models with trillions of parameters. That's roughly a 10x increase in demand. And that is directly driving the changes that we see here on the right. We're not just talking about pre-training those massive foundation models anymore. And we're not even talking about post-training scaling, things like SFT or reinforcement learning. We're now entering the era of test time scaling or reasoning, where inference becomes really, really critical. So the amount of change and the rate of change are simply breathtaking. And to enable that, we need infrastructure that is not just powerful and efficient, but also adaptable to keep up with the changing demands for AI. I want to emphasize one really critical point, which is when you build your AI solutions on Google Cloud, you're running on the exact same technical infrastructure that is powering Google's planet-scale services. Just think about it. This is the infrastructure that is engineered, battle-tested, and optimized to handle the demands of Google DeepMind, Google Search, YouTube, and Gmail. And on Cloud, we call this infrastructure AI hypercomputer. It draws on Google's decades of experience of running infrastructure at planet-scale. Our goal is to give you choice, flexibility, efficiency, and scale across the entire AI lifecycle, from pre-training models to fine-tuning them to deploying them for serving. And it starts with purpose-built hardware, including our custom-designed TPUs and the latest NVIDIA GPUs. Now, if you're working with TPUs, you're using the same hardware and software stack, JAX, XLA, custom kernels, that is powering Gemini models. That's right. All of Gemini is built and served on TPUs. And that is the exact powerful infrastructure that we're making available to you on Google Cloud. Now, if you're using GPUs, you benefit from our deep partnership and collaboration with NVIDIA. And that means enhanced reliability, optimized communication libraries like Nickel, and frameworks designed for resilient, large-scale training. So whether you're running on GPUs or TPUs or both, our promise to you is simple. We build the best AI infrastructure for our own very demanding AI workloads. And then we make those capabilities easily accessible to all of you through Google Cloud. You get decades of engineering expertise and innovation, allowing you to fully focus on solving your important business problems and not have to worry about those underlying infrastructure complexities. So now, let's look closely at how AI Hypercomputer fulfills this promise for AI training and inference. Now, when it comes to training complex AI models, especially at large scale, efficiency is not just a nice-to-have. It's a must-have. Think about the core challenge. You have a certain amount of infrastructure available to you. TPUs, GPUs, network bandwidth, storage capacity. And, of course, you have a deadline. So within those constraints, the fundamental goal is to train your model as efficiently as possible. But what does efficiency really mean in this context? Well, it boils down to one critical question. How do you maximize the useful work done, the actual progress that your model makes, across the entire duration of your training job? We call this maximizing good put. Now, it's not just about raw processing power. It's about minimizing wasted cycles, recovering quickly from failures, and ensuring that every expensive hour of accelerator time contributes meaningfully to training or fine-tuning your model. So, we want to maximize good put. Sounds simple enough, right? Well, the reality is large-scale AI training is very, very complex. And achieving consistently high good put is actually really challenging. So, this diagram that you see here shows a simplified but still realistic view of a typical large-scale training job. So, this dashed red line shows you the ideal progress. Smooth, consistent execution all the way from start to finish. But this solid blue line shows you the actual practical reality, right? And you can see the difference. So, let's walk through why that is, right? You provision your cluster and you start training your model. And even there, there can be inefficiencies. For example, if the VMs in your cluster are not provisioned and co-located in the right way, that can lead to a suboptimal training rate for your training job right from the beginning, right? But it gets worse. To predict against failures, you're periodically saving your training progress. That's called checkpointing. That's really, really important. But every time you checkpoint, you have to essentially pause your training job. And your accelerators are idle at that time. And that adds additional overhead. And then, of course, as you all know, training jobs, especially long-scale large training jobs and large amount of compute, failures happen all the time, right? They're inevitably interrupted. And that can be because of hardware failures. It can be because of preemptions. It can even be because of planned maintenance. And when interruption occurs, your job stops. And then you have to restart from the last successful checkpoint. And this restart process itself takes time. And it requires compute resources. So look at this gap between the ideal red line and the actual more realistic blue line. It represents an actual increase in your time to market and an increase in your cost. And that cost increase can be very, very significant. Based on our internal estimations, improving your training good put by even a single percentage point can result in over a million dollars saved over the duration of a typical large-scale training job. A million dollars has a significant impact to your bottom line. So hopefully I've convinced you that training reliably and efficiently is a major challenge. But I also have some great news. AI Hypercomputer is here to help. So how does AI Hypercomputer help maximize good put throughout the entire job lifecycle? Well, it begins before your job even starts. We make it easy for you to get started by providing tools like deep learning containers, integrations with frameworks like NVIDIA Nemo, and predefined good put recipes. And that reduces your startup time and minimizes your potential errors. And then remember the overhead of checkpoints that I talked about just a minute ago? Well, we optimize this with something we call multi-tier checkpointing. And this allows for faster, more frequent saves to minimize interruptions and to optimize your training time. And here's where the real magic happens with large-scale training. There is something that we call elastic training, which maximizes good put. So the basic idea of elastic training is that when a node inevitably fails, AI Hypercomputer automatically detect it. And it can attempt an in-place restart. Or it can hot swap a good node in place of a bad one. Or it can even intelligently resize your job so that your training can continue, even though you may have lost some of the nodes. And that significantly reduces this interruption overhead that we talked about earlier. So the key point that I want to take away from all of this is that AI Hypercomputer is not just about providing raw power. It is delivering an intelligent, resilient, and automated system that is designed specifically to maximize the useful work that your AI training jobs are meant to perform. And that, in turn, reduces your wasted time and cost and accelerates your AI development lifecycle. So let's look more closely at just how AI Hypercomputer achieves this resilience through elastic training. So here you have your distributed training job, represented by this cluster of blue nodes over here. And one thing that we all know is that during training, things will go wrong. And that's why we have diagnostic services constantly running, feeding information into the central supervisor. You can think of the supervisor as the brain of your resiliency operation. Now, your supervisor, the supervisor will detect different kinds of issues, and it will take appropriate corrective action. There are some jobs that are, there are some errors that are correctable. And in that case, a simple in-place restart will do. And then your job will just pick up from the last known good state and continue on the same footprint of resources that you were using previously. But there are some more serious errors. For example, hardware failures. And the supervisor will detect those errors as being uncorrectable. And so what happens then? Well, if we have extra capacity available, we will just provision a new node. You can see it marked in green here. And we'll swap out the good node in place of the bad one, and then we will just continue. But what happens if maybe you fully utilize your reservation or if capacity is unavailable for some other reason? Well, in that case, we can intelligently scale down the training job. We can use different parallelism techniques and continue your training job uninterrupted on this reduced footprint. And then once the resources are available again, we'll automatically scale it back up. So to summarize, AI Hypercomputer and GKE provides you incredibly useful capabilities to maximize your good boot and training efficiency. And the other key point that I want to highlight is that we do this at massive, massive scale. GKE can manage the largest clusters, scaling up to 65,000 nodes into 150,000 accelerator chips. And all of those resilience capabilities that I talked about translate into real good boot improvements. For example, checkpointing can improve good put by up to 6%. And the automated hot swaps, replacing bad nodes with good ones, can improve good put by additional 4% to 5%. And again, this is maximizing the useful time that you're actually running your model, accelerating your path to results, and reducing your costs. Now, we've spent a lot of time talking about training, and that's really important. But what's also super critical is inference, once you've actually trained or fine-tuned your model. And so, how does AI Hypercomputer and GKE help you maximize inference efficiency? Well, for AI inference, cost-effectiveness is actually the key metric that we look at. Think about the dynamics. Training is a large job, but it's ultimately a time-bound job. Whereas inference is running 24 by 7. You're always serving your user requests. And the main objective is that you want to serve those predictions as cost-efficiently as possible. But what does cost-effectiveness really mean in this context? Well, it's all about finding the right balance. You want to serve your predictions within acceptable latency bounds. You want to meet your throughput targets. But at the same time, you want to minimize your infrastructure costs. And that requires intelligent scaling. It requires choosing the right hardware for the task. And it requires minimizing idle resources. Because wasting expensive accelerators on deployments that aren't actually serving user traffic is a great way to destroy your bottom line. But achieving cost-effective model inference is actually really hard. And the reason why it's hard is because it's so different from traditional web serving. You need much more compute. You need much more memory. There are complex multi-host and desegregated serving requirements. And the state-of-the-art is changing all the time. And without a powerful, flexible platform like GKE, it's practically impossible to optimize for both performance and cost because the space is evolving so quickly. But once again, AI Hypercomputer is here to help. Now, I'm sure many of you are familiar with VLLM. It's the leading open-source inference engine. The community engagement and growth around VLLM have been simply amazing. And today, I'm really excited to announce a major milestone. We have added high-performance TPU backend to VLLM in collaboration with the VLLM team and the VLLM community. So what does having a high-performance TPU backend in VLLM mean for you? Oh, it's a huge deal. Because it allows you to seamlessly port your models and your serving between TPUs and GPUs. Using the familiar interface and user experience on VLLM on GPU, you can now take advantage of the performance, scale, and cost efficiency of TPUs. And the best part is that it doesn't require any code changes. And thanks to GKE, it doesn't even require any config changes. So now, you can leverage the massive ecosystem of VLLM and also gain access to the cost-effectiveness and performance of TPUs. It simplifies your ML ops, it reduces your vendor lock-in, and it allows you to choose, or rather, it allows GKE to choose, the best accelerator for your specific needs. So having VLLM support for TPUs and GPUs is great, but how does that work in practice? So let me quickly walk you through this example of how you can optimize both cost and performance within a single GPU cluster, GKE cluster, rather. Thanks to VLLM's new TPU support and GKE's custom compute classes, you can now combine both TPUs and GPUs in the same deployment. So in this example, you have your GKE cluster, and as your demand, as your user traffic increases, your first, GKE is first going to scale up your TPU node pool running VLLM to take advantage of the cost-efficiency and performance benefits of TPUs. But what happens if you've fully utilized your TPU reservation, let's say? Well, then GKE will automatically fall back to the cost-efficient GPU spout node pool, and from there, if needed, it will fall back to the GPU on-demand node pool. So again, because VLLM runs without any code changes in both TPUs and GPUs, this deployment can utilize both accelerators and optimize for cost during fluctuating demand. And so the key takeaway is that GKE, together with the flexibility offered by VLLM, allows you to automatically scale and serve the same model across different accelerator types, like TPUs and GPUs, and even across different consumption models, like reservation spot and on-demand, all without making any code or config changes. How cool is that? Now, of course, GKE offers many more capabilities for advanced AI inference. For example, GKE Inference Gateway improves AI serving performance and maximizes the utilization of your GPUs and TPUs. And the impact for your workloads is significant. By leveraging those GKE features, our customers are seeing up to 30% reduction in cost, up to 60% reduction in tail latency, and up to 40% improvement in throughput compared to less optimized solutions. And with that, I would like to invite up Nathan Beach, who's going to talk about additional benefits that GKE offers for accelerator obtainability and for data operations. Thank you. Thank you. Thanks, Alex. So Alex spoke about training and inference. We hear that a very common challenge a number of startups and teams of small IT organizations face is also getting access to GPUs and TPUs. So today, customers consume more than eight times the compute capacity for training and for inference compared to 18 months ago. And the result is that it can be hard to obtain enough GPUs and TPUs because there's so much demand. What we are hearing from startups consistently across the board is that your training workloads are very bursty. You have periods of intense need for training and then periods of a little bit less need. With serving workloads, you have highly variable and unpredictable demand. You don't even know what demand will be like, say, tomorrow or a week from now, and that can make it hard to forecast the required capacity when you're serving in production. And you want to maintain optionality. You don't want to be locked into a long-term commitment when you have this type of variability and unpredictability in your needs. And so we've invested heavily to address this. When you're looking at inference or training workloads, this is a simplification of what your demand might look like. In the inference case, you know, highly unpredictable for end users, for training, you know, your researchers, ML engineers, are kicking off batch jobs also in an unpredictable way. And a naive approach here might be to reserve a certain amount of capacity, but that understandably can lead to wasted spend. We can do better. And I want to talk through a number of innovations that can help you cost-effectively obtain access to capacity in a way that maintains optionality and is very affordable. So first, custom compute classes are an innovation in GKE that are generally available. And what custom compute classes do is they allow you to define a prioritized list of machine types and provisioning methods that you would like your applications to use. So this allows you to say, for example, I would prefer to first use a specific GCE reservation. If that's not available, because all those reserved instances have already been provisioned, then fall back to any GCE reservation. If those are not available, because all those have already been provisioned, then fall back to spot instances, then fall back to on-demand, et cetera. And that way, you can cost-effectively and optimally maximize the obtainability in a way that is compatible with your strategy for getting resources. All the provisioning methods, including reserved capacity, committed capacity, on-demand, spot, dynamic workload scheduler, are all supported. And then GKE will actively reconcile the underlying node capacity with your demand. So as demand falls, then GKE will automatically tear down some of the lower-priority instances, and that way you're only using the high-priority instances. No additional work on your behalf. All the undifferentiated heavy lifting is taken upon GKE, and all you need to do is define your preferences up front. Customers like Shopify today are already benefiting from custom compute classes, playing a really critical role in events that have massive scaling, and you can take advantage of this today as well. Another tool for obtainability that we provide is called Dynamic Workload Scheduler. Dynamic Workload Scheduler is an excellent tool for getting capacity for batch or training workloads or other workloads that are not super latency-sensitive. You place a request saying, here's the capacity that I need, a particular type of node, here's my compute capacity, memory, et cetera. And then GKE will automatically look for capacity from capacity pools that are specially set aside for DWS. You save money. There's no upfront commitment, no lock-in. And then once that VM is provisioned for your workload, there's no preemption, no, like, maintenance windows or anything like that. You get the VM for up to seven days, however long you need that workload to run. There's a talk that you can go to Friday that deep dives into Dynamic Workload Scheduler and a bunch of the exciting innovations that are happening in that space. I'd highly encourage that. If you're looking for a great way to obtain capacity for your batch or training workloads in a very cost-effective, flexible way, customers like 2Sigma have found that Dynamic Workload Scheduler provides substantially better obtainability for their GPUs. Customers like AXA Autonomy have been able to substantially lower their cost, save a lot of money by using Dynamic Workload Scheduler, and that's generally available for you today. In addition, I'm very excited to announce a number of new improvements to Dynamic Workload Scheduler that make it fantastic for inference workloads. So I talked earlier about how Dynamic Workload Scheduler has been great for batch workloads ever since we made it generally available more than a year ago, but we've introduced a number of new innovations starting today that make Dynamic Workload Scheduler great, even for your production-serving workloads. So let me talk briefly about some of those. First, you can, when you request capacity using Dynamic Workload Scheduler, you can specify whether or not you want that capacity to be queued, that request to be queued. If you say, I don't want this capacity to be queued, then we will either provision the node immediately or return a stockout if capacity is not available at this immediate moment. Second, we've added support in Dynamic Workload Scheduler to work well with custom compute classes. So you can now make one of those fallback options that I talked about earlier be Dynamic Workload Scheduler. Third, we're introducing a number of pricing benefits to Dynamic Workload Scheduler so that you can save a lot of money when you are requesting capacity using Dynamic Workload Scheduler with certain machine types and accelerators. This makes DWS FlexStart a fantastic option. One more tool in your tool chest to improve obtainability for production-serving workloads. One other amazing tool for obtainability is Dynamic Workload Scheduler Calendar Mode. So similar to what the name suggests, Calendar Mode allows you to specify a start date and an end date for your capacity. You place a request. Once that request is confirmed, then the capacity is assured, and you will have that capacity for the window of time that you need it. You can request capacity for up to 90 days, and this is a truly fantastic option if you have, for example, upcoming product launches where you know you're going to have a large spike in demand for your workloads, or perhaps where you will be running a large training job over a certain period of time, and you want to have high assurance that the capacity will be available for you. So Dynamic Workload Scheduler Calendar Mode available in preview. Customers like Puma have used Dynamic Workload Scheduler Calendar Mode to have high assurance of capacity for important business-critical events. It makes the capacity available exactly when you need it, and you know it's going to be there. Customers like Databricks find Dynamic Workload Scheduler Calendar Mode is awesome at alleviating the burden of getting GPU capacity, really improving obtainability of GPUs right at the moment when you need them. One final innovation that I want to talk about that helps to improve obtainability is we, within Google Cloud, have provided a lot of flexibility and access to NVIDIA H100 GPUs. So most other clouds will offer NVIDIA H100 GPUs with eight H100s in a single GPU. We do offer that. But we also offer your choice of one, two, or four NVIDIA H100s in a single GPU. This is great for cost savings because perhaps your workload, especially if you have an inference workload, doesn't need all eight of those H100 GPUs. You maybe only need one or two. And so you can just provision what you need rather than needing to provision a much larger VM that has more than what you need and then deal with either overpaying or dealing with bin packing multiple workloads on the same VM. You save money, but it also helps to improve obtainability because it provides yet one more node shape that you can request and get access to for either your training or your serving workloads. So obtainability is a large area that we've invested in. Another that I want to talk about that we are helping teams who have perhaps small platform teams or small IT organizations is through a lot of investment to improve your day two operations experience. Very simply put, you don't want to have to do a lot of undifferentiated heavy lifting. We hear that all the time from startups, from small IT teams. You don't want to do a lot of undifferentiated heavy lifting. Therefore, we are eager to take that work upon ourselves so that you can focus on the things that matter to your business. Another common theme that we hear when talking to many of you is that time to market is critically important. You want to be able to reach market ASAP with new AI innovations, and you don't want to be stuck in a world where each iteration takes a long period of time to make improvements in your product. And many organizations, especially smaller ones, tend not to have giant IT teams. And so we've made a lot of investments based on all the feedback that we've heard from you to improve day two operations. One is managed DCGM metrics. DCGM, common NVIDIA library that provides a rich set of metrics to help you understand GPU utilization and allow you to better optimize how you're using your GPUs. But you don't want to be responsible for managing the collection of your metrics or responsible for building the dashboards that render those metrics or creating the alerting rules with those. And so we've taken that burden off of you. And now with one single configuration, you can enable an exporter that exposes those DCGM metrics, a managed collector, and a pre-built set of dashboards that renders all of those metrics all on your behalf. And GKE takes care of all of this for you. There's a breakout session Thursday at four o'clock that deep dives into this and a lot of other observability innovations that we have. I'd really encourage you to check that out. Another area where we've tried to alleviate undifferentiated heavy lifting from you is in offering managed AI inference metrics. So if you are serving AI workloads, now you can get all the benefits that I talked about earlier for DCGM metrics. Those are going to give you infrastructure metrics, GPU metrics. Now you can get those for your model servers. So for example, if you're using servers like VLLN, NVIDIA Triton, TGI, TorchServe, Jetstream, and others, we're collecting common model server metrics, metrics like, say, throughput. And we've got dashboards that we've built already on your behalf that render those metrics for you. Great way to alleviate a lot of the undifferentiated heavy lifting that comes along with observability, monitoring, debugging. One single flag, we take the burden off of you. But it's not just inference. Training as well. Same thing. If you've got training jobs, batch jobs, that you are using GKE for, we've taken that burden off of you such that you can get metrics like mean time to recover whenever there's a failure or the mean time between interruptions. And if you're using Kubernetes job sets, if you're using GKE, this is provided on your behalf. Automatically, we'll collect the metrics out of the box, render them in dashboards that we've built on your behalf. There's a session that I talked about on a couple slides earlier. Go to that Thursday, 4 o'clock, amazing deep dive on our observability innovations here. But it's not just observability that we're helping with. One of the other burdens that customers commonly face is managing the underlying infrastructure, the nodes. And so several years ago, we made generally available GKE Autopilot. Autopilot provides a mode of operation in GKE where the underlying compute, the nodes, are managed on your behalf by GKE such that your application developers can simply specify the resources that the workload needs. And then GKE takes responsibility for providing the nodes that will satisfy the resource requirements of your workloads. You get Google as your SRE. We provide an SLA at the pod level, and customers tell us what an amazing benefit this is in their overall total cost of ownership by allowing Google to manage a lot of the undifferentiated heavy lifting that comes along with node and node infrastructure. customers like Finnavox, startup based in France, have found GKE Autopilot to be fantastically helpful in abstracting away the infrastructure management for a lot of their GPU accelerated workloads. Autopilot is generally available. All the same infrastructure that's offered in GKE standard is available in Autopilot, and you can take advantage of that today. Many customers also tell us that they're interested in Ray, maybe early stages of exploring Ray. If you're not familiar with Ray, Python native, fantastic library tool set that a lot of data scientists, ML researchers love to use. We offer, within GKE, an array add-on that manages the QBray operator on your behalf, so you don't have to do that. One single flag that you can pass in to the GKE API, we will operate a QBray operator on your behalf. You get all the goodness of integration with dynamic workload scheduler that I spoke about earlier. Logs, metrics are integrated with our cloud observability suite. We've got to talk tomorrow at 8.15 in the morning, get you up bright and early for a deep dive into our support for Ray and integration with Ray on GKE. So a lot that we're doing to help teams who have perhaps small IT, small platform teams to be able to make the most of their AI workloads running on GKE and our AI hypercomputer. Now I'd like to invite up Scott who's going to give us a deep dive into Augment code and how Augment is using GKE and our AI hypercomputer. Thank you, Nathan, and thank you, Alex. I'm honored to be here on behalf of Augment. We've been using the technology that they spoke about, the AI hypercomputer and GKE, to build an application for software engineers. So you probably hear lots of the hype about AI for software development. Most of it comes from what's called vibe coding or zero to one where individuals are starting with a natural language description and ending up with software. They may not even be programmers. Think about us as coming at the problem from the other end of the spectrum. So we look after software engineers that are responsible for tens of millions of line code bases, many of them running in services like GCP, and we help them do their job dramatically more effectively using the platform that you've been hearing about this morning. So I want to share a little bit about Augment, and then I'm also going to talk about how we're using GKE and the rest of the GCP platform underneath. So the tools that we bring to bear to help make this possible are first, we build a real-time semantic index of a large code base. What that allows us to do is to select exactly the right context for whatever AI operation you're trying to undertake at a given time. The challenge, of course, if you've got a big code base is you can't pass it as context to an AI model. And so it really depends on being really smart about that selectivity. And I mention real-time because every time you pull a branch or check something in, you want the AI to know exactly what current code you're working off of you and whoever else shares that particular branch that you're working on. We allow you to keep the IDE of your choice so you can continue to live inside the Microsoft ecosystem with VS Code. You can continue to work with the JetBrains ecosystem. We also support Vim for those that have been coding for a long time and have never transitioned all the way into the Windows-based approaches. And we focus hard on security. I believe we were the first AI to get SOC 2 level 2 compliance more than a year ago. Our AI only sees the code that the developer has access to. So there's never a chance that AI could leak intellectual property into something that somebody should not have access to. And we also manage models for our users. We use a mix of our own customized models that we post-train off of open source as well as frontier models that we access through Vertex. And we make the decisions. There are usually six or seven models underneath any individual query. And GCP has allowed us to build that platform. I'm going to do a really quick demo to just give you a sense for what Augment looks like if we could switch over. So I can say things like to Augment, tell me about my current, if I can type, repo. So this is a chat interface. Now Augment will correspond with a frontier AI model to look through the code base and decide, oh, it's a monorepo. It's got a collection of services that do various things. I can say, give me the primary services. And chat will go off to enumerate the microservices that this service is built on. This is just an awesome way to orient yourself in a code base if you're picking up something new and you don't understand how it works. You can have this great conversation with the AI to dig in. I'm going to switch to agent mode, though, to show something a little more fun. So I happen to have a linear ticket here that asks the agent to make an enhancement to this 10-the-million line order code base. So this is the Augment monorepo. So I can make this invocation and it's going to start working. Now this is an agent workflow, so this is actually going to take a few minutes. We're going to switch back over to the presentation and then we'll come back to this and see how it does a bit later. So if we could move back to the prezo. Perfect. So I mentioned we've been building on GCP for the last three years. We have thousands of GPUs under GKE management. In fact, just last week, we got 500 more because we had this spike in demand and Google was able to turn around that request within 24 hours. We really appreciate that kind of responsiveness to those spikes in demand. We're making extensive use of GCP services underneath. So we use Bigtable extensively. We're pushing order a megabyte per second of data into and out of Bigtable. We use PubSub for that real-time indexing I was talking about, 40,000 files per second, give or take. This is the key part of that real-time semantic index that I talked about. And I don't feel like we're pushing those systems particularly hard. We probably have at least a 5X and 10X in Headroom that we can hopefully use up over the course of the rest of this year as our user base continues to expand. But the fact that we can deploy these systems and they just run interrupted is incredibly valuable to us. I can tell you that our engineering team, like I'm sure all of yours, is a perennially dissatisfied lot. They always have high expectations. When we have problems, and all sites have problems, 99% of the time, roughly those problems are in our own code. So we can solve them and fix them. It's very rare that we have underlying problems from the platform that we're building on top of. And that's really as good as it gets in the cloud. So we're so grateful to have such a robust infrastructure underneath that we get to build on. One other thing that's really helped us, we do use GPUs, but being in GCP for both inference and training has allowed us to mix and match. Like when we have a spike in new user workload, we actually deprovision some of our training works in order to expand inference, and then we can dial it back. And in fact, we can do that in a daily cycle. So we're moving systems into and out of inference and training based on workload, and that lets us get a lot more done with a smaller footprint. So we're going to go back to our agent in just a second here and see how it's doing. But keep in mind, the promise of agents is that every developer can be a tech lead and tee up a collection of agents to go off and do the grunt work that they're less excited about doing. The reality today of those agents is they often disappoint because they don't come with Augment's deep understanding of the code base. If you tee up an agent to do work, but it doesn't understand what it needs to do, and you spend more time guiding and correcting it, you're not saving yourself any pain. These agents often appear magic in the demos, but the challenge then is when you try to apply them in software engineering use cases, there are no simple changes in a 10 million line code base in all of the supporting test infrastructure and so on. So you really want an agent system that can accommodate that. They generally can't take on long-running tasks. If it's work that would take you an hour or two, there's a good chance an agent can do it, but if it's a multi-day kind of cycle, the agent will get lost. And so we're getting better over time. The agent models continue to improve and can take on ever-longer tasks, but this is also where humanity still has huge value to bring. We have the aspirations. We understand how we want the software to evolve, and the best success that we've seen is where humans guide agents to craft the software. Just one proof point for Augment. We currently occupy the high water mark on Sweebench. This result was published last week. We will do our best to hold it, but it's very competitive, as you could imagine. One last piece is agents are only as useful as the tools that they invoke. So the demo I'm about to go back to is using GitHub as well as Linear, but we're integrated with a bunch of the other developer tools, and we support the model context protocol, which lets you plug in the tooling of your choice. These agents, by the way, are functional enough that they built most of the application integrations you see. 90-plus percent of the code was actually written by the agents. I'm actually going to switch back to the demo, if I could. So what happens here is we've kicked in the semantic analysis. It says that session ID exists in a bunch of... Oh, sorry. The linear ticket asked to add a session ID to a particular data structure. And Augment identifies that session IDs exist in a bunch of other data structures, so I can copy that same approach as I apply it here, including updating how the data is persisted and read off of a database. So it goes in and it updates embedded JSON inside of SQL queries, makes a collection of changes through tests, and so on. So it describes all of the work that it's doing over the course of this term. Summarized as it at the end, and then I have the option to keep those changes. I can go in and review those changes myself to see if I'm comfortable with them, or I can update and run the tests and see the result. Agents are generally capable of running tests, seeing results fail, and then making fixes. And so if you give them the room to do some iteration, they can take care of, again, a lot of the more onerous, less fun aspects of software engineering that can slow us all down. I was supposed to leave time for questions. I'm afraid we didn't succeed in doing that. I think the speakers and I will be up front if anyone wants to follow up with anything specific. Thank you so much for joining us. We love building on GCP. I hope you guys do too. Cheers. Cheers. Cheers. Cheers. Cheers. Cheers.