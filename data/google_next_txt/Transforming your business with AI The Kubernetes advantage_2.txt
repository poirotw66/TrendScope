 It's incredible that it's been 10 years of innovation with Google Kubernetes Engine. AI has established itself as the leading workload within Google Kubernetes Engine, thanks to the platform's exceptional scalability with up to 65,000 node clusters, which is 10 times larger scale than the other two largest public cloud providers. At Hubex, we build highly scalable mobile apps for over 200 million users across 160 countries, and we have 15 Gen.AI apps in our portfolio. Agility, speed, and scale are very important to us, which is why we turned to the Google Kubernetes Engine. Using TPUs on GKE for image generation like the newer Trillium for inference has significantly reduced latency by up to 66%. Users get responses in under 10 seconds instead of waiting up to 30 seconds. Today, our customers consume more than eight times the compute capacity for training and inferencing compared to 18 months ago. At LiveX AI, we're building AI agents to simplify everyday life. Google Kubernetes Engine helps us to ramp up fast and go to market much faster. With the help of GKE and NVIDIA name, we're able to achieve 50% lower TCO and 25% faster time to market. Our relationship with Google is love at first sight. I always say that we are where we are because of our partners, and Google is one of the strongest ones. Google is one of the strongest ones. Please welcome Google Cloud Vice President and General Manager Cloud Runtimes, Gabe Munroy. Hello, everyone. Welcome to Google Cloud Next. I'm Gabe Munroy. I'm the Vice President of Cloud Runtimes at Google Cloud. It's a privilege to be here. I'm really excited to share how your existing expertise in Kubernetes is your most valuable asset in the age of AI. Now, as many of you know, starting back in 2014 with the birth of Kubernetes, the evolution of GKE has been nothing short of extraordinary. It's changed how we build and deploy applications. In fact, this year marks the 10th anniversary of GKE. How awesome is that? 10th anniversary. So, from stateless applications to stateful workloads, you know, hybrid cloud, edge computing, and now with AI, GKE's evolution is a testament to the brilliant community behind it, to the contributors and the builders who make Kubernetes happen. Now, platform and infrastructure teams are now, like, starting to wonder, after so many years of building container orchestration expertise, operating production workloads at scale, how do I now enable the next generation of AI workloads? Here's the good news. You don't need to start from scratch. In fact, your Kubernetes knowledge and experience aren't just relevant, they're your AI superpower. So, today, we're going to be announcing some exciting updates to GKE that are going to save you time and resources so you can focus on innovation. We also have some guests joining us on stage and we're going to do some fun demos and show you some of the new capabilities that we're unveiling today. First, we're going to walk you through how to optimize your existing workloads to save compute resources with new GKE autopilot launches. Then, you're going to hear about new innovations in Gemini Cloud Assist that will help you reclaim development time. After, we're going to cover how you build your own AI platform. We're going to dive into new inference capabilities on GKE that will make your application smarter. We'll also share some exciting news about the Ray project from AnyScale before introducing new supercomputing capabilities to scale AI for those of you operating at the bleeding edge. Okay, let's get into it. So, to realize AI's full potential, the first step is really freeing up capacity to support these compute-intensive workloads. In fact, optimizing cloud usage and cost savings, it continues to be a top priority for our customers. 71% actually name it as their top initiative for the year. It's an area that we continue to focus on and prioritize across Google Cloud. So, if you're running web servers, API servers, queue processors, CICD agents, or other common workloads, you, all of you, are most likely over-provisioning some capacity to ensure that your apps are responsive. GKE customers often leave, like, nodes running all the time. And that leads to under-utilization, unnecessary costs. So, that's part of the reason why in 2021 we launched GKE Autopilot, the first service of its kind to combat over-provisioning. Autopilot dramatically simplifies Kubernetes cluster operations and enhances resource efficiency with a fully managed control plane, fully automated management of nodes, and automatic right-sizing of cluster resources. In fact, 30% of GKE clusters that were created in 2024, well, they're using Autopilot mode. Customers like Toyota and Contextual AI, they're all using GKE Autopilot to reduce their operational burden. Now, today, we are announcing that Autopilot supports faster pod scheduling, scaling reaction time, and capacity right-sizing, all made possible by unique hardware capabilities that are only available on Google Cloud. Now, with Autopilot, your cluster capacity is always right-sized. It allows you to serve more traffic with the same resources or existing traffic with fewer resources. Now, currently, Autopilot consists of, like, two things. A best practice cluster configuration and a container-optimized compute platform that automatically right-sizes capacity to match your workloads. Many customers have told us they want to right-size capacity on their existing clusters without having to use a specific cluster configuration. So, we are thrilled to announce that starting in Q3, Autopilot's container-optimized compute platform will also be available to standard GKE clusters without having to use that special cluster configuration. So, now, all GKE customers, all of you, can save on compute resources to create more space for innovation. Now, besides helping you save compute resources, we're also focused on another area, helping you reclaim valuable development time. Nothing slows teams down like having to diagnose and debug a problem in your application. And this is precisely where intelligent assistance becomes so important. Gemini Cloud Assist, now in public preview, provides AI-powered assistance across the application lifecycle on Google Cloud. It helps you manage applications more efficiently, more effectively, from initial infrastructure design, troubleshooting, all the way through to continuous optimization. And we're pleased to announce the private preview of Gemini Cloud Assist investigations. Investigations help you understand the root cause so you can resolve issues faster. It provides structured, AI-powered workflows that begin with a user-provided symptom, analyzes that against a set of comprehensive data, logs, config, things like that, metrics. And the best part, all of this is available to you inside the GKE console. So, you can spend less time troubleshooting and more time innovating. If you sign up for the private preview, you're going to be able to do some special things like diagnose pod and cluster issues from within the GKE console, even across other Google Cloud services. So, you have VMs, IM, load balancers. You can also derive insights from logs and errors across different GKE services. So, controllers, pods, underlying nodes. And because Cloud Assist investigations are done in the context of the GKE UI, you don't need a separate chatbot or a separate observability tool. It's all available right in the console. So, to show you how this works, I'd like to welcome to the stage Bobby. Bobby? Hey, Gabe. How's it going? Good, good. I'm excited. I'm excited. I am too. Now, ideally, you'd build stuff, it would run forever, and it would never break, but that's not the world we live in yet. And so, if we want to take advantage of a lot of the newer things, we have to be more efficient at fixing the old things. Right. So, let's kind of set up what we want to walk the audience through. So, I'm going to play a member of the platform team. I think you're going to be a member of the business. Yep. And if something is broken. Yeah. But we're going to use Gemini Cloud Assist to help us do three things, ideally. Right? So, we want to find the issue, recommend fixes, and then fix the problem. Does that work? If we can do that? That sounds good. We've got to do it fast, though. Okay. So, talk to me, Gabe. How bad is the issue? Is this, like, irritated or... I'm hearing complaints from customers. Sounds pretty bad. Okay. I'm glad you didn't say hair on fire, because that would be a problem for me if you didn't notice. And so, we're going to make sure that we kind of drop into the console now. And so, for those who haven't seen the Gemini or the Google Cloud console, rather, these three lines we call the hamburger menu. This is how you get to all the products. So, we're going to drop in to take a look at our workloads here in GKE. And, Gabe, do you notice anything here that maybe looks a little bit odd? Well, I'm not technical, but as an executive, I can tell you red is bad. Red is bad. And that needs to be green. So, what are we going to do about this? How do we fix this? So, we're going to take a look at pulling up the issue. And when we see this little investigate button, that means that Gemini Cloud Assist may be able to help us triage the problem. And so, we can't pull container images back, but we want to understand exactly what's going on. So, it'll look something like this. Let's try that again. And so, here we go. All right. So, we've got the ability to create an investigation. And so, it copies in the error message. We're going to go ahead and launch this to start our investigation. And we're going to pop this out into another window so we've got a little bit more real estate. So, you copy in the message or you can actually describe your problem in natural language if that's a little bit easier. And while the engine is running, it's going to come back in about 30 seconds or so with some observations based on looking at things like log files and configurations. And then it will come back with one or more hypotheses with some specific recommendations for how to fix the issue. Okay. So, we're seeing this work. I'm imagining how long this might take you. Oh, look at that. What is that? Okay. We actually have some thoughts here. So, watch this, Gabe. So, the issue is that the 1.0.2 version of our issue doesn't exist, but the 1.0.0 version is actually available. So, this might be something we can actually fix. So, about how long do you think this one's going to take to fix? So, I'd love to tell you weeks or months, but we're going to fix this one in minutes. Right? And because the audience is here, let's do this live so they can know this works. Right? So, let's actually go back to our other tab. We're going to close our investigate window. We're going to bring up the workload itself. And we're going to take a look at the YAML. If you didn't know, you can edit the YAML right from the console. So, we're going to go in and we're going to edit using kind of the breadcrumb or the clue that we got from Gemini Cloud Assist. So, that 1.0.2 version of the image that doesn't exist, Gabe, is going to be replaced with the prior version that did exist. And we're going to save this. And Gabe, I think you're going to be happy because ideally, we're not going to have any more red on our dashboard. Wow. Green. Green is good. All right. That's awesome. Now, and this is great, but here's the thing, right? In the real world, issues like this are going to be often a lot more complex. So, what would have happened if you weren't able to resolve it so quickly? Yeah. Great question, Gabe. So, what we do is we go back to our investigation window and you see there's this little create support request here in the upper right-hand corner. This actually takes all the stuff that was automatically generated from Gemini Cloud Assist and drops this into a support ticket so you don't have to replicate all of this stuff. Make it a little bit easier to hand everything off, right? We call this investigate to escalate to make sure there's a smooth handoff. So, you've got your error message. You can change your priority. You can actually decide if I want to add to any of the details that are pre-populated. And so, we want to make it real easy for you to get the help you need and get back on track. Wow. That looks like it's going to be pretty helpful for folks. Appreciate it, Bobby. Thank you so much. Thank you. All right. So, with the time and resources that we're saving here with Gemini Cloud Assist and also with GKE Autopilot, we're now ready to tackle AI workloads. Now, our next step is going to be to figure out what platform to bet on for those AI workloads. Now, if you want a comprehensive machine learning platform, you should really check out Vertex AI. It's a unified AI development platform for building and using generative AI on Google Cloud. If you have access to Vertex AI Studio, Agent Builder, over 200 models in Model Garden, plus the ability to call Vertex from GKE. It's a great solution if you're looking for something easy to use. Now, at the same time, you may want to invest in building your own AI platforms. You might need a platform that can be customized at the software or infrastructure level with the ability to run on any environment in the cloud or on-premises. For this, you're going to need a platform that's based on open technologies. And this is where GKE has your back. Now, over the last decade, the open source Kubernetes project has emerged as the de facto standard for hosting cloud-native apps and microservices. It's got a huge contributor base, multi-vendor ecosystem. It's considered the number two open source project behind Linux. And it has a rich and mature set of capabilities for managing large-scale distributed systems. Today, organizations that need deep control over their infrastructure are once again turning to Kubernetes to build their AI training and inference platforms. In fact, IBM, Meta, NVIDIA, Spotify, all are using Kubernetes for their AI and ML workloads. And Google is proud to work alongside these companies and more in the CNCF to continue to create exciting open source innovations. Things like dynamic resource allocation, which simplifies how Kubernetes schedules pods onto GPUs and TPUs. Q and Jobset, powerful AI training orchestration capabilities like streamlined job management, optimizing accelerator usage. And Leader Worker Set, it's a new Kubernetes API that allows AI models to not just run on a single host, but actually span multiple hosts. Really important innovations. And we are looking forward to more collaboration with the open source ecosystem around Kubernetes and AI infrastructure. So, as the open source ecosystem continues to rally around Kubernetes, most of the innovation is actually centered on a specific part of the AI workflow. Inference. Inference is what happens at the intersection of traditional compute and neural networks. It's where an application calls an AI model. There's some amazing innovation that's happening here. Companies who are operating at the cutting edge of Kubernetes and AI, like LiveX AI and Moloko, they all run inference on GKE. And in discussions with these customers and many like them, we found that platform teams that are deploying inference on Kubernetes face two key challenges. The first one is striking the right balance between performance and cost. Tuning accelerators to meet performance targets without over provisioning them. And it requires extensive knowledge of Kubernetes, AI models, GPUs, TPUs, like specific metrics like time to first token. It's really difficult, right? And as a platform team, you have to navigate that balance. The second challenge is the way that load balancing works in LLM based applications. So, with AI models, the response length is often highly variable from one request to the other. So, the latency also can vary really widely. So, this means that traditional load balancing techniques that we've used for years, like round robin, they can break down and cause high latency, uneven resource use. So, to address these challenges, we're excited to announce two new GKE inference capabilities. First, we are announcing a new GKE inference quick start in public preview. So, you can now pick an AI model with your desired performance characteristics. And GKE will configure the right infrastructure, the right accelerators, and the right Kubernetes resources to match. Second, we're launching the GKE inference gateway in public preview, which reduces serving costs by over 30%, reduces tail latency by 60%, and increases throughput by up to 40%. So, you'll get a model-aware gateway, it's optimized for intelligent routing and load balancing, and it has advanced features for things like routing to different model versions. So, let's bring back Bobby. In this demo, he's going to show you how to make this all real. Bobby? So, I'm back. So, is this my SQL or my upgrade? Which one is it? This is your SQL. This is my SQL. This is Bobby part two. Okay, so we've got to lay it out. So, I'm going to be a member of the platform team again, and I think you're representing our AI application team, right? Yes, AI application team. So, I think there are three things that we want to show to the folks in terms of how our teams will interact. So, one, we've got to spec out what we need. Two, we need to deploy what we spec, and then three, we need to monitor what we actually deploy, right? Sounds good. And since we're gluttons for punishment, let's do another live demo, right? All right. To make sure that this is all real. So, what we're going to do now is we're going to show you kind of the current, somewhat old-school way to kind of decide what we want to do in terms of inference. Any particular models you're interested in, Gabe? Well, there's been a lot in the news around DeepSeek, and the teams are asking about that. Anything we can do with DeepSeek? I got you, Gabe. So, if you look at this new AI ML part under GKE, we've got the ability to deploy models here, right? So, what I'm going to do is we're going to deploy a model. We're going to take a look at some of the most popularly downloaded ones, and I'm guessing DeepSeek is actually going to be there. Let's check it out. So, most downloads, we'll wait for this to load. We scroll down on the page, and we actually were going to do this R1 version of DeepSeek. And so, what happens is we want to deploy DeepSeek to GKE, and at the bottom of the page, Gabe, we actually see our requested resources. Is that what you wanted? Well, I mean, I got to say, and the application team, I don't really know what to do with nine CPUs and 34 gigabytes of memory. Like, what I care about is the tokens, right? What's the customer experience going to look like? Yeah. That's why we need to upgrade from this, which we call the GKE inference quick start. And so, let's show you what that looks like. So, there is a better way. If you go back to the list the models gave, and I'm going to show how we filter this a little bit better, there's actually a list of optimized models where Google has benchmarked particular models against specific hardware. And so, when you see this optimized tag, like we have for this other version of DeepSeek, we're going to give you a little bit more prescriptive guidance. Let me populate this just in case we decide we want to move forward. And so, what we're going to see is, first of all, you've got this cool slider to decide your normalized time per output token. So, do you kind of have an idea about what you're looking for, Gabe, in terms of net time per output token? Yeah. I mean, we want it to be pretty fast. Why don't we move that slider down a little bit, maybe? Okay. And so, what's cool about this is, based on how we adjust our latency, we're actually going to get particular hardware models that actually have prescriptive guidance on the throughput and the latency. Well, there we go. Like, this actually is helpful to me on the application team because tokens per second is actually the thing that we were optimizing for. And I know that we have a lot of ambitions for this new app that we're working on. So, why don't we go with the highest option here? Okay. So, you want some juice. I got you. So, what we're going to do here is we're going to drop in our hugging face token because we want to have some options. So, what we're going to do is we have the ability, Gabe, to deploy this straightaway using the deploy button or to actually view the YAML if we want to copy the full manifest to decide to deploy this at a later time. And this is the full manifest, Gabe. So, everything we need to deploy, everything that we need to do for scaling, so thank horizontal pod auto scaling right here. And then anything we need to do for best in class monitoring is all right here encapsulated in this same manifest. So, I mean, I'm glad you seem to like this. This looks like Greek to me, I'll be honest, Bobby, right, on the application team. So, but like how do we guarantee that this is actually going to result in the tokens per second? That's the thing I really care about. So, this is where you need another dashboard in your life, Gabe. You might not have known that, but you do. And so, it looks something like this. We go back to the hamburger menu. We go to monitoring. And we actually have a customized dashboard that's going to give you, I think, hopefully what your team is looking for. So, I've made this one of my favorites. And we have a VLLM Prometheus dashboard that's going to load here in a second. And then I think we'll hopefully show you something that you're going to want to see. Let me hit a quick reload on this. And so, what's loading, Gabe, are going to be VLLM specific metrics so that you're not just getting generic metrics. Essentially, are you getting what you pay for and what your team expects, right? Wow. Things like time to first token and number of requests that are waiting. So, is this kind of what you had in mind? This is great. I mean, with this, we can actually get a sense on what's happening. But here's the thing. Like, I'm glad we can visualize this, but like we are going to need to actually test this. Because I'm a little worried that this is not going to actually hold up under scale. Yeah. So, what's going to happen, Gabe, is you're going to do inference and you're going to be wildly successful. You're going to build this viral app and everyone's going to want to hit it. That's right. And that's where you're going to need load balancing. But traditional load balancing doesn't handle requests for LLMs very well because it kind of assumes everything is created equal. Right. That's where we have the GKE inference gateway, right? Now, here's what happened, Gabe. So, to make it magical on the stage, we actually ran some load against this before the live demo. And I want you to see, Gabe, if you take a look at this, do you have any hypotheses about where we might have deployed the inference gateway as you look at these graphs? Wow. Any idea about things that might look a little bit different? Wow. Time to first token, for example? Wow. And, you know, time to first token is a really great metric because this is really about the experience of receiving the output from the AI model. So that's really consistent with the end user experience. I mean, look at that. Like, sometime after 11, it looks like we must have installed this new thing. Is that right? That's exactly right. So that's where things went from pretty bad to pretty good, right? We made a lot of things better. You can also see on our end-to-end latency requests, we were trending up towards 60 seconds, then we dropped in the gateway, and then we were down under 40 again. Wow. Impressive. Anything else you want to show me here? I think we're good, Gabe. I think that's all until next time. All right. Awesome. Thank you so much, Bobby. So this is great. An end-to-end approach that really simplifies AI inference deployment on GKE while optimizing for performance and cost. So thanks so much, Bobby. The best solutions for complex problems, they meet you where you are, and they point you to what's next. And this combination of GKE Inference Quick Start and GKE Inference Gateway do just that. So this is what's required for getting started with inference. But there's a lot more to being successful with AI on Kubernetes. And we can learn from the folks who are already doing this at scale on GKE. For that, we have just the person. Let me introduce Christian Lindwall, Director of Engineering at Spotify. Christian? Awesome. Thank you, Christian, for being here. Thanks for having me. And so Spotify and Google Cloud have a long history together. It dates back to 2016 when Spotify migrated from on-premises to Google. Since then, we've come a long way. Maybe you could tell us about, first, just some of the features that folks who might be using Spotify are familiar with that are actually powered by GKE. Absolutely. So one recent AI-powered innovation and maybe my favorite feature at the moment is the AI DJ. So this is something that we have conceptually thought about for quite some time actually, but it was made possible more recently through the power of generative AI and this new paradigm that we're in. And this was, in deploying this to market, taking this to market, GKE played a crucial role. So some of the models here, so what it is, is a highly personalized DJ right in your pocket, automated voice talking to you about your music. And some of the models powering this, such as text to voice or figuring out what to say next, our models also are on GKE. Love it. And I actually have been using the AI DJ feature. You know, I personally really enjoy it. You know, before we jump into the tech though, I think the people side of this is also pretty interesting. Maybe you could tell us about how the platform engineering teams at Spotify are like structured to make the most out of GKE and Kubernetes. Yeah. So we have the core platform team, core infrastructure team, which looks after things like compute, network, networking storage, these things, and sets up GKE clusters for us. Then we have my team, which is the AI platform team that builds and deploys the ML platform, AI platform on top of what they provide for us. And then we have the personalization team, for instance, who do a lot of the research and heavy lifting when it comes to modeling and figuring out the strategy for our AI efforts, building their things using our platform. Yeah. And then they in turn have the experience team who builds actual interfaces using that personalization. It makes a lot of sense. You've got that layer cake there. And it's clear that you've really thought about how to build a strong foundation. Maybe let's move into the platform itself. Maybe tell us a bit more about the platform. Yeah. Yeah. So the ML AI platform is something that we've developed over probably the last seven, eight years. And it supports the full ML workflow, the user journey from prototyping to deploying and doing inference and observability. And highly modular. So you can, depending on your need, you can bring in the pieces that you need. And it's all running on top of GKE. A big priority right now and for probably the next year is the AI gateway, which is coming from this need of at Spotify, we're experimenting a lot on different models and different approaches to distilling models, training models, tuning models. So we need a way for teams to experiment on different models. Yeah. So this gateway is a way to route requests to two different underlying models, make sure we have figure which model works best for the given use case. Here we can also introduce things like security checks and various guardrails and things like that. So that's a big priority. It makes a lot of sense. And it's also why we're so excited about GKE inference gateway, which seems very related to that. But yeah, a lot of change in the last few years as, you know, kind of large language models have just kind of changed the way we work. You know, what do you think like prepared you for this current moment? Yes, it's, if you go back a couple of years, we had a very opinionated ML stack that was quite narrow. It's about taking things to production. But a few years back, we started looking more at how we can support these new deep learning frameworks and have a more open and less and more framework acknowledgement. approach. And so we started shifting towards PyTorch and array based ecosystem, which has been part of that. And the other big part of that is, is that we've been needing this really solid foundational underlying platform for us where we can deploy our, our AI platform using really solid primitives, which is where GKE has been foundational. So by relying on GKE, we've, we've, we've gained this really robust and scalable platform, which has been perfect for our quite opinionated approaches and stack. And has really enabled us to build this platform and lean into this generative AI paradigm. Well, that's, that's, that's great and very helpful. And we're really proud, obviously, to help Spotify capitalize on this moment. I'm curious, any other tools that you're relying on as part of your AI workload pipeline? Yeah, I, I think I mentioned Ray briefly, and I think that's a, that's a big component here for us. Uh, again, if you go back a few years, we had, we started seeing a need for much increased performance and flexibility and switching between different frameworks. And that's when we started looking at options and landed on Ray, which has, uh, it's something that it does distributed processing across multiple GPUs and CPUs really, really well. And enables us to go from local experimentation and scale up to, uh, doing, doing training, uh, across, you know, billions of records, use cases. Uh, and we currently run thousands of Ray clusters, the top of, uh, several GKE clusters. Well, that's, that's, that's great. And, you know, Ray is one that we hear a lot. Um, I think you'll like, uh, hopefully what we have to announce a little bit later. Um, so from the underlying layers all the way to the end user, it sounds like GKE is powering a lot of these new experiences. Um, but you know, kind of stepping back a little bit, I'm sure a lot of the folks in the audience look up to Spotify's platform engineering prowess, right? Um, you know, team that came up with backstage, right? You know, lots of, uh, interesting stuff that's come out of your world. Um, I'm curious, any tips, you know, to folks in the audience worth you working in that platform discipline? Yeah, there's, I mean, there's a lot to say there and you probably heard all before, but I think one important thing is to, you have to treat your internal customers as customers. And so we have a full product team. They're supporting that. And we, we invest a lot in understanding the different needs across the business and figure out also when it's time to platform eyes. And, uh, maybe a key, key learning here is that, uh, to not do it too early. So we, um, if we have a given use case out there in the organization, they have an infrastructural need. We tend to like that, that they actually, uh, fulfill that need themselves and we stay close to understand what it is. But, uh, uh, then we want to see that need cutting across a few different teams in the org. And when we start seeing a pattern emerging, that's when we sort of take it back to platform and turn it into a full platform component, uh, for the org. So I think our key job as a platform organization is to unlock innovation and rapid pace, uh, in the org. So you don't want to bottleneck innovation by taking things back too soon, but rather figuring out when it's time to unleash that power of, of, uh, uh, the full organization and get a ton of innovation going. So it makes a lot of sense. So, so Spotify is using GKE as a foundational layer for the AI platform, enabling these personalized user experiences like the AI DJ. Um, you've clearly been very thoughtful about the overall approach to your platform strategy. I love this emphasis on modularity and, and specifically like knowing how to balance innovation and when the platform as it is really important. Uh, thank you for coming so much, Christian. Really, really appreciate it. Yeah. Thanks. So as you heard from Christian, platform teams have a lot to balance, right? Various customers and user bases, you're defining when and what to platformize. Um, and historically platform teams have relied on Kubernetes and GKE to serve the needs of software engineers, right? Who are delivering microservices and cloud native applications. But with increased AI usage, the same platform teams now, as you heard from Christian, they need to serve a new user base, data scientists and AI ML engineers. Uh, with new users also comes new workload profiles, long running training jobs, burst the inference workloads, uh, many of which need to run alongside your existing microservices and applications. Now, most data scientists and AI ML engineers, they just aren't familiar with Kubernetes. So they need a simpler and more approachable way of interacting with distributed infrastructure. Now to minimize the steep learning curve, many, uh, like Spotify are turning to Ray. It's an open source framework that provides an easy way for AI ML engineers to develop Python code on their laptop and then scale that same code elastically across the Kubernetes cluster. So customers like Spotify, they're already running Ray on GKE benefiting from, you know, Ray support for Python and GKE's enterprise grade reliability, portability and scalability. So let's bring Bobby back out for another demo to show how platform teams can expand GKE with open source Ray and simplify tasks like fine tuning for data scientists and AI ML engineers. Bobby. I'm back. So you just can't get rid of me, can you? No, no. This is like my trilogy, but a good one. Part three, part three should be the best. I had a good one. So, so let's set up what we're going to show folks this time, right? So I'm going to be the platform team again, and you're the leader of our data science team. Okay. And I think we want to show people three things. So one, how do we enable our data scientists with Ray on GKE? Number one. Number two, how do we master resource obtainability with custom compute classes, otherwise known as CCC? And then how do we give them observability during, uh, via QBray dashboard? Does that work? Sounds good. Let's get into it. Let's do another live demo. Now, Gabe, since you're so efficient with your demo resources, for those who don't know, Gabe's like my big boss. Can I get one of these at home? Because like, I need one of these in my life. Like a big touch. I feel like I could save the world one container at a time. Let's see how this demo goes and then we'll talk about it. Okay. Okay. There we go. So let's go back to the hamburger menu again. And so what we're going to do is we're going to, to get our data scientists all set up with what they need, we're going to take a look at installing Ray on an existing cluster. So this demo cluster, as you can see, has other stuff happening on it. But what we want to do is we want to go into the interface back to our cluster portion. And we want to install the Ray operator on our demo cluster. And ideally, this should be something that's not too hard. Right, Gabe? Yeah. I mean, you know, I'd love an easy button for this. We got an easy button. So, Gabe, we don't have an easy button, but is there anything on the page that looks like it might be how you install the Ray operator on this cluster? I see a check box. Yeah, I think there is an easy check box, maybe. Wow. And let's put on log collection and metrics because we might need that later. Let's actually do that. Okay, I'm going to cancel this. So, for the sake of time, we've got an identical cluster to this that we've already spun up that's got re-installed. And so now let's actually pull up our cloud workstation where we want to finish getting our cluster ready to support our data scientists. So there are a couple simple commands we need to run. First, we need to change context and make sure we're talking to the right Kubernetes cluster. Second, we need to make sure that we actually fire up a rate or create a rate cluster. We already did this ahead of time. So this is going to error out because we've already got that running again for the sake of time. And then we want to start a rate session. This is going to be what our Jupyter notebook connects to. And so at this point, let's switch from the terminal to this. Is this kind of what your team is looking for? All right. So I'm a data scientist. None of that other stuff made any sense, I'll be honest. This, though, makes a lot of sense, right? This is a Python notebook. And normally I'd be able to run these cells and sort of get output. Can we confirm that that's actually working? Yeah, let's make sure we're actually connected. So if this works as planned, let me just do a quick refresh here. If this works as planned, we should connect in about 10 seconds or so and see the rate logo pop up. Let's see if we can get that. This will finish loading in a second. And let's see what we get. It's connecting. Oh, looks like that's working. And it looks like we've got Ray running. That's what you want to see, right? That is. Although, I've got to be honest, there was some code below. And, you know, it looked pretty interesting to me. You know, what is this? Okay, this looks like, yeah, it's a training function. All right, so this is perfect because this is the exact kind of thing that me on the data science team, like, the team's going to be writing lots of these jobs. I guess the question I have is how are we going to actually submit and run these jobs on the cluster once we're ready to do that? Ah, got you. So your team is going to kind of tune the dials that you want to do the fine tuning here. Yeah. And then either your team can run it or you can go back to the platform team. And what they're going to do is they're going to do this make submit command. So for those who are recovering computer scientists in the audience, this is that old make file thing from comp sci class. And so what this does is this takes our Ray fine tuning work in the notebook, turns it into a container, and then deploys it to our Kubernetes cluster to produce a fine tune model. Wow, and it looks like it just deployed them. Okay, so this is great, right, because we got the notebook, you know, connected to the GKE cluster. So that's going to allow us to experiment. We got the ability to run the job on the Kubernetes cluster. That's excellent. But we got like 20 people on the data science team. Like what happens when we all start submitting jobs against this thing? Yeah, so that was the custom compute class thing we talked about, right? So this is how you control what's actually happening in terms of the types of compute. Uh-oh, sorry about that. Went away to the wrong window. Here we go. So this is how you control the type of compute that you're exposing for the work that's being done for the fine tuning. So have you ever heard of the concept of reserve compute or reservation? Yeah, I mean, these are the ones that we pay for up front, right? Exactly. You want to use what you pay for, right? So we use that first as our priority number one. Priority number two is to leverage our on-demand resources, and we don't want to use spot because we don't want that to get preempted when we do fine tuning. And then finally, if we want resources that may not all be available, we have something called dynamic workload scheduler or DWS that collects everything so that we have all the resources before we kick off the job. So we can decide the type of compute, the priority of the compute, and if you want to play around with different types of accelerators, this is where you would actually do that. All right. So it sounds like you've got this obtainability problem solved. All 20 of us can get going submitting workloads. That's great. Well, anything else you wanted to show us here? So one thing real quick here, if your team wanted to play with TPUs instead of GPUs, you change a couple lines here, maybe copy this file and then apply that, and then you're working with a different type of hardware accelerator. But I do have one more thing for you, Gabe. All right. What's that? I can't give you more cowbell, but I can give you one more dashboard. All right. Dashboards. All right. One more dashboard. So we go back to the hamburger, we go back to monitoring, and we have a customized dashboard here that's in my favorites. And so we're going to go to our cube ray. And so this is hopefully the type of dashboard that your team might want to take a look at if you were wanting to see how your schedule jobs are performing. Well, this is great. Tasks, you know, actors. These are some of the building blocks inside of Ray. So it sounds like my team can maybe even get access to this and see how their jobs are performing. Absolutely. Wow. That's pretty great. Incredible, Bobby. Thank you so much. Thank you so much. Thank you, sir. Thank you, Gabe. We are committed to making Kubernetes the best platform for using Ray. And we've been working closely with the AnyScale team to do that, the creators of Ray, and making sure Ray's optimized for Kubernetes, particularly the open source version. And we've been consistently impressed with the team at AnyScale. And that's why today I'm pleased to announce a new partnership between Google Cloud and AnyScale. Please welcome to the stage Robert Nishihara, founder of AnyScale and the creator of Ray. Robert. Hey. Thank you, Gabe. Really thrilled to be here. I've had the privilege over the past year to work with over a hundred different engineering teams productionizing AI. And there are common themes, common bottlenecks. There's a common need to build quickly and to build for scale. But this is very hard. Scaling AI workloads is incredibly difficult. We're talking about scaling training, scaling inference, scaling data processing. This is one of the biggest bottlenecks for productionizing AI. And that's exactly where Ray comes in. Ray enables AI teams to develop in Python and scale across any compute, from CPUs to GPUs to TPUs. Yeah. Ray is giving a compute engine that is for AI workloads specifically. It's Python native. It's optimized for accelerators. You've got this fine-grained parallelism, tasks, actors, that kind of thing. And Kubernetes is orchestrating all of the foundational elements underneath. But now we're coming together to make open source Ray and Kubernetes the distributed operating system for AI. Maybe you could tell us a little bit about how customers are using Ray today. Yeah, for sure. And you heard from Spotify. Ray is used across the board for productionizing AI. Uber is a great example. Uber trains all their models with Ray. They run batch inference. They run multimodal data processing. This powers hundreds of production models at a massive scale. Got it. So this is like customers like Spotify and Christian. That's how they're using it. But maybe, Robert, you could tell the crew here about our new partnership and how that's coming together. Yeah. So I'm super, super excited to announce AnyScale's partnership with Google Cloud and the GKE team. So we're working together to bring the absolute best Ray experience to every AI developer. So as part of the partnership, GKE users are getting access to AnyScale's Ray Turbo. This is AnyScale's optimized Ray runtime. Purpose built for AI at scale. This is delivering superior performance for workloads like training, like inference, like data pipelines. This is done through smarter scheduling, through faster task execution, through better resource management. Our users are seeing things like 4.5 times higher throughput for multimodal data processing, or serving large models on half the number of nodes. So this is really trying to enable AI teams to develop in Python, run across any compute, CPUs, GPUs, TPUs, with Ray and Kubernetes scaling everything dynamically under the hood. Love it. And now through this partnership, we're committed to enhancing Ray Turbo to make it even better on GKE, bringing together some of the features of our performance and scalability on the autopilot side, for example, with GKE, and bringing that together with Ray Turbo. GCP will be the best cloud for AI ML engineers to build, run, and scale Ray workloads. Thank you so much, Robert. Really appreciate it. I'm thrilled to have you here and thrilled about the partnership. And it's thrilled to see what everyone here in this audience can build with AnyScale and Google Cloud. But where can people get more information? Yeah. So head over to AnyScale.com, check out AnyScale plus GKE, give it a try today, and stay tuned for new announcements and offerings coming later this year. Amazing. Thanks so much for joining us, Robert. Thank you so much. It was a pleasure. All right. So the last bit we want to share here is about scale, right? Because when workloads gain traction, scaling demands can be really intense, right? Things that we take for granted at smaller scale, they can start to break down. And it's the same with AI. As models continue to grow in size and demand more machines for compute processing, platform teams have to deliver architectures that are going to spread models across multiple hosts and operate massive clusters of GPUs and TPUs, but as a single unit. Because without these capabilities, customers are often going to struggle to complete large training jobs or deliver the inter-machine performance that they need for AI. So to handle these scaling challenges, we are excited to announce that our super computing services platform, Cluster Director for GKE, is now generally available. It delivers exceptionally high performance and resilience for large scale distributed workloads by automatically repairing faulty clusters based on their bill of health. And one of the best things about Cluster Director for GKE is that you can orchestrate all of this just through standard Kubernetes APIs. So that means no new platforms, just new capabilities on the platform that you already know and love. So for example, you can use GKE node labels to do things like schedule pods based on network topology to maximize efficiency and minimize network cops. You can report and replace faulty nodes by gracefully evicting workloads from the node and then automatically replacing them with spare capacity. But not just any spare capacity, spare capacity within your co-located zone. You can manage maintenance windows so that you can manually start host maintenance from within GKE or use maintenance information while scheduling your workloads. Now, to be clear, not every customer is going to operate at the scale where you're going to need these kind of capabilities. But it's important to know that Google Cloud has your back when you do need them. And you can get started with Cluster Director for GKE using configurable blueprints or we have this hands-off approach with something called Accelerated Processing Kit and that requires no previous Kubernetes experience. So, in summary, AI introduces new challenges for platform teams. But we're confident that the technology and products that you're already using, Kubernetes and GKE, with those you can tackle them. Right? Today you've heard about new GKE autopilot capabilities to help you optimize existing workloads. You've heard about how Gemini Cloud Assist and investigations can help you reclaim valuable development time. You've got two new GKE inference capabilities that make your application smarter. You heard from Christian at Spotify about how GKE is being used by Spotify today. Also, from Robert about a new AnyScale partnership for Ray Turbo on GKE. And then lastly, new supercomputing capabilities to really scale AI with Cluster Director for GKE. So, with all these advances, we are confident that with the right foundation, platform teams are ready to supercharge their data scientists, their AI and ML engineers, in addition to their software engineers. And this confidence, by the way, it's grounded in experience. At Google, we use GKE to power our own leading AI services and Vertex AI at scale, relying on the same technologies and best practices that we're sharing with you here today. So, to those of you who've been on the Kubernetes journey with us at Google, we thank you and we look forward to watching you all succeed with the next generation of AI workloads on GKE. Thank you so much. And for those of you who want to learn more, please check out these breakout sessions. I hope you have a lovely next. Thanks so much. Thanks so much.