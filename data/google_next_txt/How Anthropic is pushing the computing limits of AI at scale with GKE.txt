 Welcome to this final session of the day. Great to see you coming here. It's definitely the time for the most determined to come right before the concert. My name is Maciej Kruzacki. I'm a product manager working on GKE. And I'm Arthur. I'm a member of technical staff at Anthropic. And together we're going to give you a presentation about how Google and Anthropic are collaborating together to push the limits of what infrastructure and workload registration technology can do and how you will be able to use it. Arthur will cover how their AI platform powering cloud and AI innovations looks like. And then I will cover how the capabilities that Anthropic is innovating around and working also with us there can be consumed and used by all of you and are more generally available. Thank you, Maciej. Cool. So, yeah, as Maciej was saying, I was going to go over, like, give a brief kind of technical overview about what the kind of AI landscape looks like at Anthropic. And then I'm going to dive deep into this project that we collaborated on together. So the first question that I'm usually asked when I mention that Anthropic uses Kubernetes is, like, how come are you using Kubernetes? Most people would probably be familiar with Slurm, or they will be kind of assuming that kind of big AI labs are using VMs directly or physical kind of hosts directly. Or they might think that we're using kind of some higher-level libraries such as Ray, MLflow, or Kubeflow. But it turns out that, yeah, we are using kind of Kubernetes at Anthropic. And the reasons, if we were to kind of backtrack, is that, like, around 2020, when the company was kind of getting started, there was a big push for containerized, like, workloads, primarily using CUDA. So, like, NVIDIA was making it really, really easy to kind of run kind of CUDA workloads inside containers as opposed to having to build our own AMIs or, like, having to set up everything from scratch. Like, if anyone has had, like, to compile a CUDA or, like, prepare, like, a machine to actually serve AI kind of workloads or do training, they'll probably kind of be familiar with kind of some of the challenges there. And once you assume that you're going to run containers like Kubernetes is an immediate follow-up, and it turns out that, like, the semantics that are available in Kubernetes really lend themselves well into kind of a multi-use kind of, like, use case. So, for example, you have, like, priorities, you have namespaces, you have RBACs. So if you have, like, multiple teams kind of using kind of the platform, like, you soon have, like, all of these APIs that are, like, available to you outside of the box. So it's kind of really, really convenient. And the other thing that's really cool is that, like, if you think about kind of a job scheduling and orchestration, like, you have a lot of APIs and hooks that are available to you to build kind of your own kind of a P&A-ded framework. So if you look at those alternatives that I mentioned, especially kind of the later ones, like Kubeflow, they were more or less being developed at the same time that Anthropic was kind of being born. So we kind of more or less kind of created our own version of those things, which is kind of very opinionated from the inside. So going kind of into a little bit more detail, and this is going to be kind of somewhat technical, but, like, one of the things that's very unique about Anthropic, once people kind of join, even if they're familiar with, like, Kubernetes, is that we're using stateful sets for everything. And I really mean everything in this case. And there are, like, at least three kind of big reasons that I can highlight here for us to use stateful sets. One of them is that you get fixed index pods, and that kind of is really interesting for kind of AI workloads. Immediately you get, like, the pods being named. You have, like, one pod that's pod zero that you can immediately kind of say that this is the leader pod. And if you're kind of using headless services together with the stateful sets, you soon can address each one of the other pods. So in kind of big AI workloads, there's a lot of sharding that is involved, and you can immediately say, okay, if I'm pod 42, I know that I'm responsible for shard 42, and everything becomes very, very predictable if you kind of couple headless services with stateful sets. This is in comparison to if you're using, for example, deployments, that you kind of get a random name for each one of the pods. Obviously, kind of as the landscape matured, you kind of have, like, jobs, and you have indexed jobs these days. These are all kind of alternatives to some of the stuff that we kind of back then had to develop. Another thing that we have that's kind of quite, that we use that's quite unique to stateful sets is the on-delete updates. One of the things that we do is that if we have a stateful set, like, we want to be able to change what's inside each one of those pods, each one of those containers without kind of tearing down the container. So it's almost as if you're kind of using them as mutable kind of containers. And the fact that we have kind of the on-delete update kind of option allows us to make those changes into the containers, reflect that on the spec, but then choose when we actually want to kind of delete each one of those pods and make sure that they come back in the way that they want to, the way we want them to. The other thing that's kind of very unique about kind of an AI lab such as Anthropic is that we actually have an inverted scaling kind of landscape and situation. In most companies, you would probably be dealing with kind of auto-scaling, and you assume that the capacity that you get from the CSP, Google in this case, is practically infinite. In our case, it's the opposite. We kind of negotiate kind of the computing power, the accelerators that we're getting ahead of time. So we have a fixed amount of capacity that we can tap into, and that's constant throughout time. So we get kind of these ranges of TPUs, and we kind of use them through capacity reservations, and that's fixed. Like, there's no kind of auto-scaling that we can use. So one of the things that we have to do is to have a lot of pending pods. So in most companies, if you were to look at a cluster and you were to count the number of pods and you see, like, hundreds or thousands of those, you'd probably be concerned, like, auto-scaling is not working. In our case, that's exactly what we want to have. And the reason for that is because if for any case, like, a hero kind of training run or, like, a high-priority workload suddenly crashed or has finished, you want to have, like, these other lower-priority workloads readily available to be scheduled and use that capacity because we're paying for that capacity no matter what. So we're on the clock. We want to use it. And basically what we want to have is, like, 100% or very close to 100% occupancy or utilization from that perspective. So it's a way for us to practically do opportunistic research. And we actually kind of invite every researcher on Anthropic to put, like, schedule as many kind of research ideas or jobs as possible because they actually might get to run eventually if they're not a high-priority. The other thing that's very unique is that although we have, like, many clusters, we do not have a central place where we keep track of everything that's running, we heavily rely on Kubernetes itself as a state, a place, or a storage for us, especially for metadata. Obviously, kind of, model weights and a lot of the data behind kind of a big AR workloads is going to be on blob storage. But if we're talking about metadata, that's all lived inside, like, Kubernetes. And throughout time, we've built, like, small libraries, like, thin libraries on top of Kubernetes and the API to make it available for kind of other ends, Anthropic, to interact with, like, their jobs and et cetera. And here I'm just providing a very simple example of, like, ant list. And this is, like, for me, listing the jobs that I have kind of available in the cluster that I'm in the context at the moment. And basically all it's doing is just looking at a label, in this case, created by. So it's filtering, like, what are the stateful sets with the label created by Arthur and kind of returning that to me. And we've basically built a bunch of functionality on top of, like, literally labels and annotations in Kubernetes. And that has gotten us, like, a really, really long way. The other thing that's very unique about Anthropic is the way that we do scheduling. So I was talking about the stateful sets. Like, one of the things that you do not get with the default scheduler is gang scheduling. So let's say that you have a stateful set with, like, 1,000 pods and you just want to kind of schedule it. You don't have that guarantee from the kube scheduler that all of them are going to kind of be scheduled and kind of readily running. But for a training job, for example, you actually need all of those to be up because they're sharded and all of them need to be kind of available and running for the job to start stepping. So we actually have to build our own schedulers that will only actually do the pod bindings if there are enough nodes to kind of receive those pod bindings and the priorities are all set and so on and so forth. So we had to kind of build our own schedulers throughout time. There are also topology kind of aware. It's very interesting if you're dealing with kind of big workloads like latency matters so it's very interesting for them to be like physically close together in the data center. If you're talking about TPU pods, for example, and you have a TPU pod with 64 hosts, 256 chips, you want, and you have a job that needs those 64 nodes, like you probably most definitely want those 64 nodes to be running within a TPU pod. So that's one of the things that you get by having kind of your own scheduler. With Google, you also do get that these days if you're using kind of job sets. There's actually a mutating web hook that will actually inject a bunch of variables and ensure that some of those scheduling properties are respected. But in our case, again, having those schedulers just gives us a lot of flexibility in terms of how the jobs get to run in the cluster. So this far, up until kind of what I was talking about was like the previous status quo. And really, if you're looking at kind of GKE, if you're on data plane V1, you're talking to up to 50K, 15K nodes, 15,000 nodes on a specific cluster, if you're on the newer data plane, V2, the limit was 7.5 nodes. And those were more or less the limits that we were kind of keeping ourselves at a little bit below, obviously, for like some headroom and et cetera. But then, late last year, we heard that like mega clusters were starting to be a thing that kind of Google was investing at. And they had the number. The number was 65K, which is massive. Like if you compare that to DPV2, which was the latest version, that's almost a 10K kind of increase. At Anthropic, like we have a very lean kind of infrastructure team. We want to rely as much as possible on Google to do kind of the heavy lifting for us. And one way that we look kind of clusters is an opportunity for us to vertically scale as opposed to horizontally scale. I was talking about how we use at CD, for example, to keep all of the metadata. The bigger the cluster, the more I can delay kind of the decisions to have like a centralized database for all of my jobs, for example. So while there's no kind of a reason for us not to be able to or not kind of heavily invest in horizontally scaling, as long as we can keep vertically scaling our clusters, the better it is for us because then we can invest our time into some more kind of niche and specific problems at Anthropic and rely on Google to do kind of the big heavy lifting from the Kubernetes slash GKE side. So we were very interested. It sounded amazing, 65K. And you'll probably think, okay, maybe you just grab all of your clusters and you just merge them into a same bigger cluster. Not really. Like at this scale, like it's bonkers, like the amount of like data and nodes and kind of information flowing. So we took it in steps and we had like several scaling challenges that I want to kind of share with you folks so that if you do kind of go into that kind of adventure, you can probably kind of think about them as you do so. One of the things that we kind of immediately bumped into was the fact that we had like really large objects. As I was saying before, like we were relying on XCD and putting a lot of information on labels and annotations. Well, as much as going to kind of hint a little bit later, even though we're not using XCD anymore and we're actually using Spanner behind the scenes, it's still like a lot of data. So all of those annotations, all of those labels, they actually translate into a large, large payload. By the time that you're doing, for example, a kubectl, a gap pods, for example, it might be gigabytes of data. And if you're doing that like a lot of times as an operator, as a human, like it really gets into the way. So one of the things that we had to do is like figure out how to do with these really large payloads. The other one is like latency in and out of itself. Like as I said, like gigabytes of data, like if you had a loop that was taking, I don't know, like 30 seconds or a minute to run, like any business logic related to kind of cluster maintenance, it might kind of get into kind of several minutes of kind of that loop to work. So we had to like think outside of the box how to solve those things. One of the things that we did was to move all of our Python libraries to protobuf. Like I don't know if you folks are familiar with like the Kubernetes kind of Python library, but like it's comparatively immature if you look at the Python versus the Golang one, for example. So we had to invest like in using protobuf and actually making HTTP calls directly to the API server as opposed to using that library. And what we immediately saw was a 25% reduction in response sizes just by moving it to protobuf and a 60% reduction in latency when we did that. And this was under the guidance of the GK team, which was really nice. Like they immediately told us like do this because we're probably going to see like immediate results. And not only was the latency kind of on average reduced by 60%, but the variance was also like very, very like reduced a lot. Like the variance like was much smaller when we kind of moved into protobufs. So this was really good. And you might ask like why Python in this case? Like it's an AI lab. Like most of AI, most of ML is doing in Python. And so there was a big, big bias in the beginning to kind of be writing all of this automation orchestration in Python as opposed to something like Go. The other thing that we developed internally was KCP cache, which is actually a very simple kind of application or caching layer in front of the API server itself. It's using kind of informers from the Go SDK library. library. And all of the authentication authorization is basically seamless because we're using the token review API. So there's no changes from the perspective of the client or the server. Like all the authorization and authentication kind of constraints are maintained or guarantees are maintained. And the original implementation was actually very, very small. Like there was nothing kind of fancy here. But it got us really, really far actually. And the reason for that is because one, we can scale those read kind of replicas independently from the control plane. So now we don't have to rely on Google to be scaling the API server. We can have like dozens of those that we are controlling ourselves. The other thing that we can have is specialized endpoints on KCP cache. So when you're dealing with like the API server directly, if you're getting like all of the pods, you're getting kind of literally the full spec of all of the pods. It's almost as if we were using, I joke around like GraphQL with Kubernetes. Like you can create those endpoints on the server side. They're returning exactly the amount of information that I need. So maybe I don't return all of the labels. Maybe I don't return kind of the detaints because everything that I'm interested on is, for example, the resource aspects of the pods, for example. So we kind of created these specialized endpoints that might actually be combining several pieces of information, pods plus stateful sets plus nodes, and that just reduces the amount of calls, the amount of data that's being transmitted. The other thing that we can do on those replicas is basically ignore a bunch of the resources. While the API server has to keep track of everything, all of the pods, maybe we can ignore a kubesystem, maybe we can ignore this namespace that's from GKE specifically because I know that I would never be interested in that type of information. And finally, we can strip the unnecessary fields even on the responses that are, like, fully compliant to the original kind of HTTP paths because, again, we might not be interested in that. And here I just have, like, a very simple example of a workload making a call to the API server server, whereas, like, before we would get, like, 30K of data, we're now getting, like, 0.2 kilobytes of data. And, yeah, we've deployed several of those kind of KCP cache kind of deployments in our clusters to meet, like, different demands that have, like, different scaling properties. So one of the things that we had to do, again, because we're using kind of at CD for everything, in this case, Spanner for everything, like, it was really easy for folks around Anthropic to just use it and abuse it, in this case, to store as much data as they wanted. So at this point, we were, like, as we were scaling and getting kind of our clusters bigger and bigger, we started to have to go, like, and find the worst offenders from that perspective. These might be people, like, writing a lot of data or, like, making a lot of queries. And with GCP, it was actually really easy for us to do that because the observability stack there is, like, really kind of vertically integrated. So we were using Logs Explorer, Logs Analytics, and Metrics Explorer to actually kind of ingest and, like, parse out all of the API server logs. And we could immediately see because we were using user agents for different kind of workloads, which ones are the ones that are abusing kind of the most. And one by one, we would go, okay, this one, maybe we can fine-tune, like, how it's using Kubernetes. Or this one, maybe it doesn't need to be strongly consistent and we can just move that into KCP cache. Or this one, Protobuf is really easy and, like, we're confident that we can just move it to Protobuf earlier. And each one of those, step by step, allowed us to kind of increase and increase the number of nodes that we had in our clusters. The other thing that was really nice to partner with Google on this one is that we saw some scaling issues on the Kubernetes side in and of itself. And it's really nice to have, like, Google engineers that are part of, like, the Kubernetes kind of different special interest groups. So they could drive some of those discussions and some of those changes in, like, newer and newer versions of GKE. But they also, like, took to hand to actually make changes into other CNCF products. And here I'm highlighting Argo CD. And they kind of saw some opportunities for improvement there. They actually submitted some patches upstream. And we immediately made use of those, like, by kind of running a custom version of Argo CD. And also kind of doing kind of simple things like ignoring pods. Like, as soon as you're, like, into thousands, tens of thousands of nodes in a cluster, it's very unlikely that you as an operator are going to look at, like, one stateful set in Argo CD UI. So you can basically ignore that and get away with, like, still using Argo CD at such a big scale. The other thing that we kind of had to give up on is network policies. These are, like, by nature not scalable at kind of fast 15K kind of node size. And we had to be creative. Like, we had to build something internally. Like, security is something that Anthropic takes really seriously. And then, again, it was one of the reasons why we went with Kubernetes in the first place because, again, it has, like, all of these primitives that are, like, lend really well into kind of a separation of concerns and responsibilities and permissions. So our original kind of solution was actually IP tables based. And we were relying on the fact that we were using host networking for most things. And workloads would kind of basically we would have, like, a daemon set in the node basically saying, okay, there's a workload. Like, everything through IP tables, all of the incoming traffic is going to be denied by default. And only the siblings of those workloads would kind of be allowed in. And that was actually working really well in the initial phases of the project. But then we kind of, as we were upgrading the clusters and kind of perhaps some of the IP tables kind of semantics changed, the solution broke. And we kind of rethought a little bit. And we were like, okay, maybe we can actually use the VPC firewall from JCP to actually do those things through node tags. And that actually worked much better. And it's what we're kind of currently using. So something to keep in mind if you're kind of relying on the network policies. The other thing that kind of we had to rethink a little bit was scheduling. Getting workload scheduled fast is actually really important and even more so at this scale. So if a workload finishes, the new one on the line needs to be able to get scheduled like really, really fast so that we don't spend time with idle nodes. So we had to make like some further simplification store scheduling kind of algorithms. And we could do some interesting ones. One of the things that like it's true for us is that you're actually going to have a single workload per node if you're talking about an accelerator node. So you know that like you will never have to bink back kind of a bunch of pods into a node except for obviously the daemon set ones. The other thing that we can do is basically ignore the node resources. Like if you're using a TPU node, I know ahead of time what's the instance type, how much CPU it has, how much RAN it has. So I can actually optimize the workloads themselves to fully saturate those other resources while knowing that I have, for example, four TPU chips per node. So again, the scheduler doesn't need to do any of that bink backing at a resource level. So, and we actually made kind of the scheduler simple because we control and we own kind of the full kind of job orchestration framework inside out. We know, for example, that we will never need anti-affinity rules. So we basically cannot kind of, we don't implement it. Like no jobs are going to ever be submitted with that. So it's like one less kind of complexity from a scheduling perspective. So, yes, that allowed, like doing all of those simplifications allowed us to keep our scheduling kind of throughput more or less constant. The other thing that kind of changed a lot specifically for us, and again, this comes from a deep partnership with Google, is how we manage this TPU lifecycle in and out of itself. So traditionally, when you're consuming TPUs, you would be getting kind of a node pool that's provisioned just in time more or less for you if you submit a kind of a job to the cluster. And that node pool operation, it's fast if you think about it in isolation, like it's in the order of minutes. But for us, because we have like so many different workloads kind of coming up online and offline, we have a lot of throughout time. It's actually better for us to actually have 16 by 16s only in the cluster and for them to be long-lived throughout time. And basically what we do is that we control how Jax uses and kind of subslices or soft slices, as we say, the slice for the workloads themselves together with the scheduling. So here in the picture, what we have is basically an internal visualization of a TPU pod or a TPU slice. And what you can see is that you have some workloads using a 4 by 8 configuration, some others using kind of a 2 by 2. And also representing here some nodes that are potentially tainted and some of those that are even in repair. So we actually have like a 16 by 16 kind of node pool that would be really long-lived, like by that I mean literally months in the cluster. And you're going to have maintenance and repairs ongoing while still being able to kind of run workloads kind of inside a TPU pod at any given time, as opposed to having to be deleting and recreating those node pools a lot. And yeah, so this kind of covers what some of the things that we had to change internally. And when I stop and kind of think about what the future holds for us, again, it's really, really nice to kind of vertically scale. And hopefully I can ask Maciek to move to, I don't know, half a million nodes in the future in a cluster. But literally, we're like, we need to start thinking about internally like how is the multi-cluster strategy. And by multi-cluster I mean more around like federation, like how can we use multiple clusters together while still having kind of maintaining a very lean team inside of Anthropic. The other thing that we kind of keep thinking about is like multi-region. Like right now, clusters are confined to a single region. And as some of you folks probably know, like we have a power problem in getting accelerated. So you can't always choose to have all of your chips in a single location. There are also DR strategies or kind of reliability angles that you might not want to have all of the eggs in the same kind of region. So we're also thinking about like okay, how we can have like a multi-region story for like when we think about Kubernetes clusters. So yeah, that covers my content. I'll hand it over to Maciek. He's going to highlight how you folks can tap into that technology next. Exactly. Thanks, Arthur. So what I wanted to build and extend Arthur's story is with giving you a little bit of insight of how actually the innovations that you've just seen are accessible to a wider community and then how you can tap to those depending also on your use cases, not necessarily even building AI platforms. So first a little bit of just maybe principles of how we work. So GKE has been already around for a decade and we've seen at least two major or like we've seen one and we're seeing a second major IT industry transformation, one with the advent of cloud native companies and now the AI born companies. And as we developed the orchestration product of Kubernetes and the Google host of GKE, we see on one hand we are first we're making sure that we are investing our resources and time into capabilities that actually matter to the customers and solve real problems. Secondly, the top innovators and folks that push through the industry. In some cases like we anticipate and recognize their needs so we actually proactively build already with our opinions and with their maybe advice and then how we observe and analyze what is happening. We build those capabilities into the core product. Sometimes the pattern is what you could also see in Arthur's presentation is that these innovators they leverage the fantastic aspect of Kubernetes where it's easily extensible and you can replace very specific components of the whole stack and tailor them to your needs. And then what we can do as both the open source community and a cloud provider, we can see what patterns and how people actually are modifying Kubernetes to their needs and then start to generalize those solutions and make them more widely available and configurable so that a particular pattern can be leveraged by multiple users in multiple use cases also. So let's start maybe from the scale of the platform that also Arthur spent a bit of time on. We are definitely very proud of the fact that GKE is supporting 65,000 nodes in a single cluster. It is the largest managed Kubernetes service on the market at the moment when it comes to the size of the cluster that we support and also highly scalable when it comes to the sizes of the other visual training workloads and jobs that you can run there. The innovation that is powering it and the architecture behind it, it's a combined investment across many years into both the open source stack of Kubernetes and also very strong integration into Google-specific services that we have. Out of those Google-specific services, probably the most important one to highlight is that we've built integration of Kubernetes control plane into spanner. We effectively replaced that CD and added a layer of translation with a K-span project so that we are able to power our clusters with the two decades right now already of innovation of high-performance, large-scale databases that are built to power Google products. What this enabled us to do is, one, we shifted the expertise and responsibility for maintaining the state to teams that just specialize in that specific problem. And secondly, we've actually enabled very rapid and easy horizontal scalability of the computing power that is available in the control plane. Like the largest clusters that I've seen in our fleet are, I believe, are 28 control plane servers that are stateless and we can easily add them and remove. If you think about the math, it's roughly like 2,500 BCPUs that we are able to just quite easily add to the control plane to be able to manage the cluster. And then on the data plane side, the entire stack is built on the architecture of AI hypercomputer to make sure it's all integrated into our TPUs and GPUs and storage products that I'll talk a little about in a second. At the same time, this all is built with conformance to open source standards. Like, it's a tip from your perspective as a user, it is a normal open source like Kubernetes cluster and a normal GKE cluster that can be upgraded and all of the standard functionalities of GKE are available there. You can, it is part of our CICD testing that we are just like regularly validating it, et cetera. Although, there is one thing I would like to highlight, so like as you extend the scale of Kubernetes in the number of nodes, you also need to think about extending scalability of Kubernetes in other dimensions. It might be the number of pods, it might be the complexity or the number of services, the network concepts that run in the cluster. So, just extending the scalability of all of those dimensions becomes an impossible problem sometimes to solve and some of the dependencies and patterns that we see is that extending a certain scale dimension has actually like quadratic or even exponential impact on the computing power that you need to put into the control plane. So, we need to make sure that whatever we build at large scale is actually focused on a specific use case and is really meaningful to customers. That's why we really like the partnership that we had with Anthropic. We've been thinking about building these very large clusters for a couple of leaders already, but we also wanted to have a partner with whom we actually can make this innovation to come to reality in actual production use case and help us, guide us in terms of what needs to be built. And so, as you could see also in Arthur's presentation, I'm here highlighting a couple of assumptions were taken, meaning which imply also certain constraints or behaviors in terms of how you can leverage these large clusters. So, first of all, like it is a very important simplification for the whole problem to be able to assume that the pods fully fill in the entire machine. That's a typical pattern in AI workloads. And, of course, you also need to add the daemon sets, like on average our customers run approximately 15 of those per node. As Arthur also mentioned, there is no need for node auto-scaling in the cluster. That also simplifies some of the aspects of how you run those clusters. But at the same time, given the size of the cluster and the amount of diversity of workloads that are running there, the pressure on the scheduling domain is very significant and need to be able to really rapidly create and remove pods within the cluster. So the scheduling rates will vary on many factors, but they need to be in the order of multiple hundreds of pods per second. So that you can, for example, quickly recover some workloads from failures, replace inference with training or training with inference and all that, and see this constant flow of tasks that are happening inside the cluster. Node pools in the AI domain, whether it's in GPUs or with TPUs as accelerators, they are also used to model the network constructs, like Arthur was mentioning that a TPU pod is, or a sub, is, the pod slice is really a, is modeled as a node pool, which creates really also defines a network domain. So then these clusters have a significant number of node pools, and we support over a thousand of those per cluster and are pushing actually that limit even further right now. And also network needs to be configured and tailored towards this specific AI use case, which is very, especially, is very heavy on a couple of dimensions, like the amount of data and throughput that you're sending through your network interfaces. Okay, but what if you don't run a platform on that scale? There is not that many actually companies that really need to operate at this massive scale, but still the innovations that are affecting the extreme use cases in the most powerful platforms on the market are actually benefiting everyone. First, like both us at Google and GKE and the open source community, we need to figure out really solid foundations to make sure that the very large scale use cases are actually going to be working. So we need a well sorted out support processes, testing, the whole stack needs to be very well optimized, and monitoring, troubleshooting, all of those tools need to be very heavily hardened, and that benefits everyone. And the effect that we see among our users is that larger and larger scales, not as huge as 65,000 nodes, but still very large, like a 1,000 node cluster is something that five years ago was considered an extreme case. Very, very few users were actually operating clusters of 1,000 nodes or more five years ago. Today, what we see in our user statistics, this is actually quite common and normal, where people just leverage clusters at this scale. It is still, of course, a considerable and large scale that needs to be accounted for, but it's normalized that it can be used by many customers, and we expect that size to keep increasing and moving forward so that you can all use them. Clusters with heavily, very powerful control planes also give more freedom to you and your application teams to really make mistakes even, and some anti-patterns with more horsepower in the control plane. So, yes, like we believe that essentially the whole domain of larger and large scale is going to be changing in the patterns as we see the extreme cases to be pushing the elements of the technology. And we actually get this question very frequently, so what do I need to do to make sure that I can use this super optimized and mega cluster stack? It's actually not much, given that what made sense is actually we contributed to the upstream Kubernetes. What was specific to the integration with our cloud stack is also part of the standard GKE product. So, the most important thing is you just need to make sure you follow your upgrades cadence. Most of the improvements that were done to power these large clusters were done up to the release of 131. They were done across multiple releases, and the 131 is probably the last one that had a big bulk of changes, plus the migration to Spanner as the backend of these clusters is happening transparently to you already first-party services are using it, and more and more clusters of our end users are also being transitioned to Spanner-backed storage. I wanted to only highlight quickly that we also work on scalability for other use cases, not only AI platforms. And as just a couple of examples in here where we're investing into very small and complicated microservices and performance, so like the scalability of horizontal pod autoscaler, where we're investing into solving problems of users who have requirements for very rapid spikes of traffic and usage, so like flash sales in e-commerce or other, where the latency of provisioning regular standard CPU machines also has to be very aggressive and very fast so that costs can be optimized, or when you have these high security and complexity requirements of data plane v2, and the network policies where you need to make sure that you have all of those isolation aspects done and managed by us, then like the innovations in the networking stack that are also done there, and some of them highlighted on the slide. If you believe that for your use case, perhaps, there is something missing where we need to work on extending these scale dimensions, we'd love to get more partners like Arthur's team to work with us, so if you think the product could do better or could be extended, I'd really love to hear back from you, and please reach out to me or your account teams, and we'd love to collaborate with you on pushing the technology forward. Now, I wanted to quickly highlight one thing, that one problem, very difficult to solve, but a little bit easier, is how large is the whole platform, and the second problem, especially for training workloads, is how large the training workload can be. That's specifically related to some of the semantics of training workloads, like gang scheduling, the fact that it's all or nothing, so really even a single VM's failure creates, like disrupts the entire workloads, and usually just mandates a full restart of that workload, so individual workloads are smaller than these platforms, and then the large A platforms, they share multiple workloads, they run multiple workloads and share them, but we are also making multiple capabilities available to boost the sizes of those workloads. Tomorrow, there's actually going to be an interesting session that dive specifically into that topic. Later, we'll show the recommended other sessions that give you deep dive. What I'd like to maybe particularly highlight is that we are making a variety of tools available that help you make more effective checkpointing. We are making some of our internal alpha, but also innovations like pathways on Google Cloud available to all of you so that you can more easily build JAX workloads for training and abstract the complexity of managing hardware and hardware failures and other capabilities that allow you to manage very large workloads. Now, most of the things that I've been speaking about so far are related to really GKE-specific innovations, but we are also a very strong believer in open source and making sure that also the APIs and the workload-level toolkit that you get in the open source Kubernetes needs to enable the new type of workloads that are coming with the new era of AI innovation, and in particular, I wanted to highlight to you four specific investments that we are heavily doubling down on. So one, you could hear from Arthur, like the challenges and the things that the team had to do around job scheduling. So we've been for the last couple of years working on an add-on to Kubernetes that gives you the ability to extend pod scheduling features with full job scheduling orchestration. Then for the workload definition, we've invested into three APIs. One is that I didn't mention we've made lots of changes into the base job Kubernetes, Kubernetes jobs, so that you can actually have those index pods with standard names, addressable through known DNS names. And then JobSet is an extension of that that allows you to run heterogeneous jobs, like, for example, a common pattern in Spark where you have a driver and executor. So sometimes you want to think about the job as a single unit and sometimes as subsets where you go deeper into the complexity. Leader Worker Set is our operator that we've also worked in the community on to enable multi-hose inference where your models cannot fit into a single machine and you actually, you're really deployment that the microservice, the replicas of it are actually multi-VM concepts. So then the whole idea of how you really describe that workload changes. And last but not least, lots of changes are also being done into the base scheduler so that the core Kubernetes scheduler is also enriched with some of the AI-oriented capabilities like dynamic resources allocation and more that are coming and we're working on there together with the open source community. I wanted to quickly just highlight to you the first, more details from the first investment, let's say, from that previous slide, which is related to Q, to the job scheduling extension of Kubernetes that we've built. It is an open source standard that is adopted by already many companies and with a couple of large players also being part of the community that contributes to it. And it has the standard features that you'd expect from a job scheduler like fair sharing, quotas. It does give you also those abilities of understanding our network topology and optimizing your workload placement so that you can optimize performance. It is also, what is particularly unique about this one is also that it understands both batch concepts and serving concepts. So you can then mix training and serving, so batch and serving workloads within the same cluster. It is integrated with our autoscaling stack. So in case you're actually using a cluster autoscaler and opportunistic looking for resources, it is capable of hunting for scarce resources and capacity, especially for the newest, most advanced GPUs if you'd like to use them more opportunistically. We are working on an extension of it with multi-cluster scheduling, so the multi-queue capability. And also, similarly, like in here, we are learning from the journeys of Anthropic and also working with other customers and community members to make sure that it actually fits their needs and solves the key problems. And as a wrap, I would like to just maybe highlight to you the overall philosophy of how we build these things and how we work with customers is that for folks who want to use, who want to run AI platforms directly on our infrastructure as a service stack, we are thinking about it as a unified architecture of an AI hypercomputer where various specific infrastructure products are built and innovated both within their silos and then we're looking at how they all fit together so that they end-to-end give you this high-power, reliable computing backend for your AI use cases. And the orchestrator that is more of a focus in this session of Kubernetes Engine is the tool that helps you automate putting all of those dots together and connecting them so that you can actually achieve your goals through automated orchestration of workloads and infrastructure where, while being portable, it is integrated with our stack and it allows you to achieve high performance, efficiency, while you run diverse workloads and you also leverage the innovations and the rapid changes of new accelerators and new models that we're launching like TPU v7 that were announced yesterday. If you'd like to learn more, we have a couple of sessions that we'd recommend that either you attend or watch them. A couple of how to build AI platforms, also a few sessions if you'd like to learn more about Cloud and the awesome innovations that Anthropic is working on. And I think giving time to take a picture. And also we'd love to hear feedback about the session. We don't have much time for questions, but we're going to stay here also with Varter if you're going to chat with us here in the room. And I would also use a research team. We'd love to get your feedback and schedule some time. So if you have a couple of minutes, if you take a picture of this link to the form, you can set up an appointment with our user research forks and help us also shape our product and innovate more. T