 All right. Hi, everyone. Welcome to the next. I hope you enjoyed the conference so far. Last day, you are almost there, and I hope you had a really good night yesterday. We are going to talk about AlloyDB only on Kubernetes deployed with local AI models. Some, probably some things you may need to pay attention to, how it works together, and what is the AlloyDB in general. Let me introduce myself. My name is Gleb. I work as a cloud advocate at Google for databases. I live in Ottawa, Canada. I run a lot, and in probably one and a half week, I'm running Boston Marathon, third time, so wish me luck. Couple of words about the agenda, what we are going to talk about. First, I will start from AlloyDB, some introduction, because maybe not everybody is aware what it is, how it is different from Postgres, and what we provide for developers. Also, I'm going to touch about some features, and AlloyDB only. Then, I will follow by demo, and I will try to explain what we are doing during the demo, how we deploy different components, and how they work with each other. And hopefully, we will have some time for Q&A, but if we don't have time, I will be available after the talk, and I will happily answer to any of your questions. So, let's start from AlloyDB itself. What it is. And the first what it is. It is 100% compatible Postgres database. It shows really good price performance. What it means, price performance. It means for investment, you are putting inside AlloyDB, you are getting probably the best performance out of the relational database. It is my opinion, but of course, it depends, heavily depends on what kind of type of workload you have, and how you use databases. It is fully featured vector database, and recently, we published a really good blog from our team explaining how AlloyDB scan index can handle billions of vectors, which is quite impressive. It is a real-time analytic database. What it means, real-time analytic database. Analytical queries are usually slightly different than all TP queries when you need some kind of aggregation across verticals, across some columns, and it requires some, usually, scan for full table and everything else. The columnar engine, I'm going to talk a little bit later, helps to improve, boost the queries. It depends, of course, on the query, on your data, but it can really improve your query performance up to 100 or maybe even more time. And it runs everywhere. Not everybody is ready to work with the cloud, and you can download AlloyDB only, which is a downloadable version of our database, and you can actually run it in your environment. So the question is, why Postgres? And the answer is simple. The second year in a row, Stack Overflow put Postgres as the most popular database among the developers. It overshoot MySQL, the previous leader, by 8%. And if we talk about professional developers, which is slightly different community than just developers, it is even more different. It is like 13% difference between MySQL and Postgres. What we do for developers in AlloyDB. We are trying to make the life easier to manage database, to work with the database. First, it is memory management. When you use AlloyDB in the cloud or use AlloyDB only, you are getting automatic memory management. It means memory pools will be optimized themselves without necessity to update parameter file and restart the instance. It is dynamically allocate memory to provide the best performance out of the box. It uses adaptive atavacum, which what is atavacum? It is garbage collection process inside the database, which handles all obsolete records and removes clean your tables, indexes, and everything else. So that adaptive atavacum helps you to keep the performance acceptable even when you need to clean up a lot of information and prevent you from running out of transaction ID. Also, we have index advisor, which it is kind of light process behind the scenes, analyzing your query and provide you advice. If you might need, consider some index for your query and tells you that index will improve your query by so many times. And the last one I would like to mention, we provide Kubernetes operator for AlloyDB only, which helps you to manage database, deployment of database, backups, and some other management operations in Kubernetes cluster for AlloyDB instance. So, what I promised to talk a little bit about Columnar Engine. What is the Columnar Engine? Columnar Engine is a different representation of your data in the memory. So, originally, you have row-level representation of the memory, which is traditional for OTP databases. What we do with Columnar Engine, when it is enabled, it will put to the memory automatically, Columns, you are using your analytical queries, and that Columnar projection of your data helps to improve your query. Of course, it depends on your query and how you use, but in reality, you don't really need to do anything but just enable it. It will handle all the data by itself. It is fully Postgres compatible, no schema or application changes required. Same results, just much faster. It populate automatically and refreshes the data automatically, but when it cannot keep up with your intensive updating or data ingestion, it can use hybrid approach using data in memory, in Columnar, and traditional projection together. It is hybrid access to the data. AlloyDB AI, it is another differentiator from my point of view because we are providing in AlloyDB, Postgres compatible database, out-of-box integration with OpenAI, Dertix AI models for embedding your data. And you can generate embedding using SQL function. So you don't really need application layer to generate embedding or run similarity search. And you can connect to different models hosted in your environment. And we will show today how you can connect with the locally deployed models in the same Kubernetes cluster. Also, as I mentioned already, it is vector database to store billions of vectors, really, and handle it with state-of-the-art Google index. All that can help you to integrate AlloyDB or make it the main backend for your Gen AI application. Effectively, AlloyDB represents some kind of Swiss knife of the databases, trifecta of the databases. It is operational database with very scalable, powerful, with very good price performance characteristics. and on the same time, it is analytical store with colonel engine and vector database, all three in one. It is like in old times when I was a kid we had a coffee three in one. I don't remember seeing it lately, but it was quite popular in our school. So, let's talk about AlloyDB Omni a little bit, what it is, and why would you use it. Not everybody is ready to work with the cloud and you can try AlloyDB without creating cloud account or going into Google Cloud. You can use downloadable version AlloyDB in your laptop, data centers, other cloud on VMs, everything where it supports the container image. It is essentially we provide a container image which you can run either in Docker or any other environment running containers or you can use our Kubernetes operator to deploy it in your Kubernetes cluster. And the cases can be different. You can use it instead of your standard Postgres database as a very powerful scalable performant Postgres SQL with all the features I mentioned already. you can use it in closed secure environment air-gapped environment where you cannot access anything outside of that environment. Like you have special secret data which cannot go out of your data centers. In that case probably it is one of the use cases for AlloyDB Omni. And edge deployment. What is edge deployment? Sometimes what I saw in the field you have for example edge location in your company where application and database has to be close to each other just because of very high requirements for latency between communication between database and application. That's when you deploy database and application in the same Kubernetes cluster and make them work together. Local AI models. Why would you do that if you have tons of models as a service? The reasons are probably the same as for AlloyDB Omni. You use it for security when your data cannot go outside of your environment or you are working with a specially fine-tuned model using proprietary data and you cannot really deploy it in any other environment. Just requirements it has to be in a local data center. Response time, when you deploy it in the same environment as your application, response time is much more predictable because you don't have that potential internet impact on your data. And again, edge deployments when everything has to be contained very close to each other to provide the best performance and response time. So, what we are going to show today is kind of deployment. I have a project at Google Cloud. I deploy a GKE cluster and inside GKE cluster I deploy LODB Omni and two different models. One of them embedded model. I have chosen BGE model and you can have a model card on Hagen Place. It shows really good results for some cases. Maybe not as good as some more advanced model you have as model as a service, but still quite acceptable. And I'm using a GEMA 3 model. We just released it probably less than a month ago. It is four billion parameter GEMA 3 model which provides just Gen AI services inside my application. And compute engine, I'm using a jump box as a compute engine to work with the models. A little bit more about how it is deployed in GKE cluster. You have one pool where you deploy operator and LODB Omni. It is standard purpose nodes like E2 nodes. And I have another pool where I choose G-type nodes with graphic accelerator. I'm using the probably most affordable graphic accelerator. It is NVIDIA L4. But even NVIDIA L4 can handle really good with Gen AI workload. it. And how it works exactly with LODB itself. So what you do when you deploy it in the environment, you deploy LODB database with support of Google ML integration and extension. So when extension is enabled in the LODB instance, in the LODB database, you can use different functions in Google ML schema. And one of the functions is create model. That functions allow you to register endpoint for different models and provide some other parameters we are going to talk about, like a transformation function to convert your input to input expected by model endpoint and everything else. Then when the model is registered in your database, you can use it in different functions. And one of the functions is Google ML embedding functions which can build the embedding itself. So Google ML embedding functions accept two parameters. It is model ID, which is what we just registered, and the second parameter is your text. And that text will be converted to real number array which can be used to fill up your vector dataset. And Google ML predictor is a little bit more universal. It is not only for embedding. You can actually use it for embedding as well, but it is probably not the best idea. But you can use it with any kind of AI model for your workflow. It can be gen AI model, it can be actually a model you train by yourself. So that's how to use it. And what we are going to do now, we are going to do some demo. Some parts of the demo are recorded, and the reason is quite simple. If I would try to do all the pieces, right now it would be not 45-minute session, it would be probably about an hour and half session at least. And the reason is objective deployment takes time. So let me start from deployment itself. So what we are starting from is we are deploying the GKE cluster. And as I mentioned, the first thing we are doing is we are going to create a cluster with default node pool E2 nodes. And what I'm using here, I'm using E2 standard for nodes. It is probably most affordable from E2 instances with sufficient amount of memory for our AlloyDB Omni database. I would not recommend to deploy any kind of significant workload of AlloyDB Omni with less than 8 gigabytes of memory. It is just from my point of view, yes you can run it in 2 gigabytes of memory, but it is not the greatest idea. You cannot use all the features in full when the memory is not sufficient. So when we create the cluster itself, we of course get credentials because most of the stuff we are going to do, we are going to do through kubectl command. And the first requirement is to install certificate manager into our cluster. And that is for mutual TLS communication between AlloyDB operator and all the resources created for AlloyDB cluster. So then we install the operator itself. The operator installation is quite simple and it takes probably one, two minutes to install. And now it is ready and what we do, it creates all the custom resource definitions, CRD, in the cluster. And so we create a manifest to install our, I call it my omni. It is simple. And what I am using as a password for Postgres user is Kubernetes secret. It is not really secret, it is not the recommended way. I would probably recommend to use secret manager for that, but then I specify some parameters like memory, disk, and what also I would like to have is load bancer to communicate with my database inside my VPC. It is internal load bancer, so it is creating a private IP address in my VPC. It is not open to internet, and it is exactly why I have for my database, also VM. We are going to talk a little bit later. So now I am applying that manifest, and that it will take probably two, three minutes to create and initiate a load DB database. it is quite easy, and you can monitor that. First, it will be in progress, and then it will create that load bancer and everything else, and you have endpoint where we actually going to use for connection to our database. So then we create a jump box to work with our data, with database, run queries, and everything else. to run with Postgres, of course, you install Postgres client. That's exactly what I'm doing right now. And then I'm exporting our endpoint. You remember we just checked the endpoint for all the DB Omni, and I'm using it to connect to our database, and providing the password, I encode in base64 as a Kubernetes password, a Kubernetes secret. As of now, we have connected and the database doesn't have anything at all. But we need to create some database because what we are going to do, we are going to load some sample data, register the models, and try to show how it connects to each other. So we're creating a database demo. Simple database, empty, no special parameters, nothing installed there. So that's how you create the database. So let's go forward and stop on a couple of things. The first is name and database version. You have to specify it. Then you have to specify features. It is custom resource definition provided by a load be Omni operator, and we enable Google ML extension by default. for that particular instance. And as I mentioned, we are also creating network load balancer type internal to be able to connect to the database. So when we have database created, the next step, what we are doing, we are deploying the node pool for our AI models. And as you can see, we are using G2 standard 8 machines for that. And we are creating two nodes essentially. One node will handle our embedded model, and another node will handle our Gen AI JAMA 3 model. So what I'm doing for deployment, I'm just using Hacking Face manifest for deployment, and what you need to change in that manifest, you are getting it out of box. You have to provide some limits because you want to use graphic accelerator and you just tell that, all right, I'm using one GPU for that deployment. And you change from Tesla to L4 because our nodes don't have Tesla, essentially, and that's it. And after that you create namespace and service account because they are used in the default manifest, and then you just create the service. It takes some time to deploy, but that model is deployed fairly quickly. So, service is created. So how we can connect and test it? You can see the cluster, it is type of cluster IP. it means you cannot access it even from your private network. You have to access from inside the cluster only. It is fully protected from external access. So I'm just using kubectl port forward command to port forward to that particular IP and service with my local port 8080. And then I'm using curl utility just to test whether the model works or not. and it is simple silly request. I'm checking just what is alloyDB. And I'm getting the vector data in return. So it works really well. So what we need to do right now, we have the model, we know the model works, now we connect to our database. Again, I'm connecting to my jump box and that I'm going to use psql to prepare and register model. For embedding model, first thing we need to do is we need to create a couple of functions. And that is important because each model in point expect request in certain format and provide output in certain format. But our embedding functions has only two parameters. model ID and text. So what we are doing, we are creating transform functions for input and output. And those transform functions just convert our text to JSON, which expect it as an input for the TEI type of environment. So when we created those two functions, we need to get the end point. And we are using the cluster IP here, 34118 225 237, to provide endpoint parameter in our model registration. You can see that we are using ML create model, we are providing a request URL with that particular cluster address, and we are providing the transform functions for input and output. Now we can test it. And we are using the same request, what is LOADB Omni, and if we go down, we can see that response time, it is real-time array. It is exactly what we expect. And you probably noticed it works quite fast. So it is not bad performance. You don't have, and you have kind of predictable time. I tested it multiple times, and the time required to get the embedding back is always the same. So I am loading some sample data here. By the way, everything what I am doing right now, you will be able to repeat using Codelab and my blog. I will point you at the end of the presentation. So I have four tables with sample data set. It is about 1,000 rows for products, probably 80,000 rows for inventory, and some number of stores. So it is symbol inventory, symbol products, and symbol stores. I just loaded the data. It is sample data to create some kind of emulate store inventory for operational database. So now, we can build our embeddings. And let's say how much it takes to build 1,000 embeddings, 941 embeddings, to be precise. So what we are doing here, you can see that we are using Google ML embedding functions to fill up the symbol embedding table. And it took about 10 seconds for 941 rows. I think it is a good result for entry-level GPU deployment for embedding models. Now, we can use those embeddings for semantic search. And we are using Kazin similarity search to get top five records for our tree search inside our query. And we are getting California psychomore, we are getting pepper tree and redwood and some others. So those trees are getting from our inventory based on vector search. search. It took about 30 milliseconds for our vector search. How we can improve it? We can create an index. You notice that it cannot create index because meta scan doesn't exist. We need to create extension a luddb scan to be able to create the index. And index creation is fairly quick. It is faster than HNSW and it is even faster than IVF flat. And then you can see the difference in time. Even from 900 records you have almost twice faster response time. And results factor the same. It is quite accurate in that case. If we try to build query plan and see exactly what exactly we are doing inside and everything else, you can see in the execution plan we have index scan using symbol embedded scan in the axis here. It is working and it is why we are getting that response time quicker. By the way, if we go in down and see exactly where we spend time during the hour request, this planning time 15 milliseconds and execution time 2 milliseconds. So we spent probably let's say seven times more time for planning than execution. That's about embedding models. A couple of things. Of course, in Google Cloud, you can go to your workloads and see logs and everything else related to your deployment of models or alloDB omni cluster. So all observability is built in. Of course, you can use for getting more information using kubectl utility because everything is here, all the IP addresses and all the data about your services. Again, let me put here, it is our function, it is a sample function to transform your input data to format expected by TEE deployment. And how you, again, put the functions here and put the request URL and model type. It is important. It is text embedding. It is how it knows that that model can be used in Google ML embedding function. Speaking about deployment and manifest, again, oh, sorry, about our request. Again, we use Cosine similarity search and we use two parameter. It is our request what kind of tree goes well here and we are using model ID we just registered inside. And then we are returning just top five results based on distance. All right. So it is embedded model, but we also mentioned that we are going to work with Gemma 3 model. And for Gemma 3 model, we can use again, we are using Hagen Face as a registration for the model. And I am using Hagen Face token. It is to deploy Gemma 3 model. And for Gemma 3, it is important. You have to use the image, proper image, which knows about Gemma 3. The latest images published on Google Artifact Registry already know about Gemma 3, so you can easily use it. And of course, you provide some parameters. Some of them default, some of them are slightly customized. I will stop on some particular parameter a little bit later after the video, but it is important to mention. Again, we are changing our type of model. We are changing what kind of graphic accelerator we are using, and then just deploying the service. One thing about that, so you can use HIPCTL wait, command to wait when the service is deployed, but when conditions are met and service looks like ready, it is not radiant. You have to go to logs and wait when you get in the log of your workload, application startup complete. And the reason, it takes quite a bit of time, about five, sometimes seven minutes, to complete initiation of the model inside Kubernetes cluster. So that could be quite misleading. But again, let's go to workload, check your log, application startup complete, you can use it. And we use coroll utility port forward and coroll utility to just get the same request. What is AlloyDB? And when we run it, it takes first time quite a while to get response time, but then everything is getting cached and everything else, it works much faster. I was surprised when I get that response time first time from Gemma 3 because I remember how Gemma 2 was responding to the same question. Some data here are not factually correct, but most of the data are quite interesting and more or less correct, which is for 4 billion parameter model is quite good because it is quite specific question. It requires some kind of knowledge about Google Cloud. So that's the model is deployed and we have cluster IP again as an end point for that model we can use in our database. And we are going through the same process we are going to our database and what we are going to register the model. But we do not need transform functions for that particular model because it is not embedded and Google ML predict troll function which is going to be used to work with gen AI models has one parameter is model ID and the second parameter it is you can see that we are deploying the model here with the end time but we do not specify any transform function here. We just put model type as generic. So when we run the function and you can see here how we run the function request body is a JSON expected by model endpoint already. So the second parameter is quite flexible. It is not just text. It is three JSON file and we can pass to the model. So we registered both of them right now. and essentially we can start working on it. But before going forward I need to mention a couple of things. The deployment manifests for Gemma 3. You can see that I used parameter max model length. The Gemma 3 is quite generous in size of context window. L4 I'm using is relatively small graphic accelerator with limited memory available. And what happens if you do not limit it for 32K? Because it can get up to 128K. It can run out of memory. You have to keep in mind your hardware have to match what you are using as a model. because if it is running out of memory it has just cycled and rebooted itself again and again and again. And when you try to use it you are not getting any result. So that is important for small deployment. But again it depends what you are deploying and what is expectation from that. Model and hardware should match. It is just again I'm repeating here model type generic and then endpoint and nothing really else. It is minimum parameter you have to provide. You can provide more parameter with custom authentication and everything else but here we are not doing that. So how we can use all of them together? In that query I'm using two sub queries. one of them is getting our trees like top five results right? And the second is a prompt request to GMAC model and it is augmented by results of the first query. Essentially what we are doing is kind of emulation of RAG application retrieval augmented generation. When we augment our retrieval request to the Gen AI model with our factual data from our operational database which can help you to get proper results. We have seven minutes. So let me try to skip it here if I can. And what I would like to run live can we switch to my laptop? all right. I think I am here. Yes. It is my laptop. It is the same environment I just showed. Nothing was changed. And here the first query. It is the simple query. It is semantic search. Let me run it. And yeah, it is 199 milliseconds. All right. That's okay. And we have the number of trees from our operational database based on semantic search. Then we run in the request where first sub query, that one, is actually returning the same results. Right? And then we are posting it as JSON aggregation function here, augmenting our response, like you are friendly, advisor helping to find product based on the search and everything else, to request to Gemma 3 model. And you can see that it is my Gemma, which we registered before. And what I'm limited here is max tokens. By default, Gemma 3 in that deployment will have only 128 tokens. In essence, max tokens provide me a little bit more opportunity to get more generous response from the Gemma. And then I'm running the query. Let's see what it will are. What? Let me try. Yeah. So, be patient. It is L4 graphic accelerator. It is not the most powerful accelerator in the world, but it is performing pretty good. and you can see here, and it is providing information from our actual inventory about the database with the price and with the more suitable three white Gemma things out of top five results would be more suitable for you. So, that is a response. Essentially, what you can do, you can use, you can put everything as your backend for your chat application, for example, to the database. Let me show you application. So, here, it is very simple Python application I created. If you want, it is publicly available on DMI DWL for Google Cloud, and you can just try it by yourself. So, what I'm specifying in that application is Gemma endpoint, and you remember that it is exactly the same addresses as you remember, and TEI endpoint for the embedding model. And then, let's put a Gemma 3, confirm, and let's try if it works. Okay, it works. What about a tree for my backyard? I actually don't know how it will respond. Let's see. Oh. It is still thinking. All right. Let me repeat the same question as I asked. All right. What did this growing good here? That's interesting. So, you never know what our Gen AI will answer. essentially what it does behind the scenes, it tries to recognize your intent, and if it is just hello, it is not running similarity search. If it is more than that, it will try to run similarity search, augment the response, and get your results back. Not working very well every time, but in most cases, it works quite accurate. So, it is not returning anything here, right? All right. I'm not going to troubleshoot it, guys. We have only two and a half minutes left, but essentially what I'm trying to say here is we can use LODB Omnia with SQL functions as a backend for your application. The chat, of course, it is just show. But in real life, you can put your trained model inside Kubernetes cluster and do it for actual things like fraud detection or classification or anything else in your actual workflow inside database, which makes it better for management different versions of embedded models and different versions of models itself. So, that's my point. And what I would like to promote here, it is Codelab at Google Cloud. Codelabs, you can use it. You can read my blog, exactly about that deployment and how it works and some details how we deployed the models. And right after that session, we have another session, how to accelerate AI up with LODB Omni. Omni. It is delivered by my friend Bjorn, product manager for LODB Omni with coordination with some vendors. And if you can watch and record it, it was the session today morning, the latest vector search and innovation in LODB, I would strongly advise to do so. And your feedback is greatly appreciated. Give stars, I expect five stars, of course, and complete sessions away in that. And thank you very much. I think we are done exactly in time. Nine seconds left.