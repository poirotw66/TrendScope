 . Hello, everyone. If you're interested in the intersection of stream processing and general AI, you've come to the right place. My name is Mehran Nazir, and I'm a product manager with Google Cloud. We'll first start with a little bit on what's new on Dataflow. I'll hand it off to my friends at Shopify, who will be talking about their multimodal streaming AI use case. And we'll conclude with Paul Alta Networks and their story of how they process millions of events per second. We'll conclude with a brief Q&A. In 2017, Google published the Transformer paper. In the eight years after that, we've seen that Transformer architecture completely reinvent our understanding of generative AI. But there's been another revolution happening, a little bit with less fanfare, the real-time data revolution. We've seen our customers double, triple, and quadruple the amount of real-time data that they've been ingesting because they know that the faster that they can act on an event, the better chance they have of capturing that business opportunity. And we've been seeing this across many different industries. We have healthcare companies that are collecting data from in-home patient care devices. We have e-commerce companies that are delivering personalized results. We have financial service companies that are detecting fraud at the edge. Streaming is hard. AI is hard. So you can only imagine how hard it gets when you combine the two. Firstly, these are complex pipelines. Multiple stages, multiple systems, and obviously an incredible amount of failure modes. Data is also messy. Data can come in many unstructured ways, whether it be images, audio, and text. It can also come in a variety of structured formats. And if you can imagine, you need a system built to be able to handle all of them. You also have varying load. And you need to be able to have infrastructure that can adapt to the needs that you have in the current moment. Enter Dataflow. Dataflow is our fully managed streaming platform. We've been running streaming pipelines for our customers for 10 years. And not only that, we've been running them at an immense scale. Our largest pipelines are ranging in the tens of gigabytes per second. And on the batch side, we have customers that are shuffling multiple petabytes in a single shuffle. Dataflow is powered by the Apache Beam SDK, an open source SDK that allows you to do all kinds of expression. It includes a number of different IOs that help you integrate with operational systems. If there's something that we've learned in the past few years with generative AI and streaming, it's that that connection between analytical and operative systems is essential. But why specifically Dataflow for ML? Well, first, I would start with our powerful and expressive SDK. You have primitives like the state and timer APIs that allow you to manipulate data as it comes in. But also, you have turnkey transformations that allow you to basically accomplish certain use cases with just a single line of code. We have accelerators available that allow you to accelerate data processing and attach GPUs to your Dataflow jobs. And not only that, we understand that all of our customers have different requirements, different libraries, different tools. And we allow for our customers to bring those libraries into the Dataflow job. This isn't just a division. This is actually reality. One of our customers processes text and audio files to produce podcast summaries and podcast snippets. And what they've seen is new customer behavior and potentially a new business line. And to mention, they are all doing this at scale, generating 8,000 previews per hour and processing in their back catalog of 14 million episodes. So, let's get to the specifics of what we've been building. Every ML model will have a stage called prediction. Well, you need to call a prediction for that model. We call that inference. We call that inference. But if you think about that problem, it's pretty complex. It's not like making a call to a web service. You're thinking about making sure your systems stay up, making sure you have exponential back off, and you're doing this while you're trying to find the right balance between latency and cost. Well, we've solved this with a transformation called run inference. Run inference is a turnkey transformation that does all that heavy lifting for you and makes it as easy as just basically calling a function. Because you're able to use run inference, our service can do smarter things. For example, we've recently introduced a model manager based on the OSS VLLM that allows you to basically maximize the efficiency of the models that you're using on your pipeline and ultimately save you money. A lot of our customers also know that LLMs are better when they can incorporate their own data. That architecture is called retrieval augmented generation, otherwise known as RAG. RAG can generally be split up between two specific paths, knowledge ingestion and the actual prediction part. And with Dataflow and Beam, we have a turnkey transformation for each of those stages. We have ML transform to take those incoming records and do the chunking and do the embedding generation to write that to your vector database. And as events and messages come in, you take that same ML transform, generate those embeddings, use enrichment to find the corresponding value in your vector database, and then you have run inference for prediction. There, I built a RAG application in just four lines of code. We also think that infrastructure matters. We do full stack development. This is our approach to making our infrastructure ML aware. As I've already mentioned, we support GPUs with TPUs coming very soon. We also offer our customers the ability to use right fitting to save money, ultimately, to basically be able to allocate certain resources like GPUs to specifically resource hungry stages of your pipeline. As many of our old time Dataflow users know, you might have to wait two to four minutes for your data to start processing. Well, we are now introducing a new integration with an internal stack that will bring that down to tens of seconds. A sigh of relief to a lot of Dataflow users. As you probably already know, open data lake formats are shaping the world nowadays. And we are proud to announce that Apache Beam now has support for Iceberg I.O. Two things I want to emphasize. Iceberg I.O. is a managed I.O., which means that we can update your I.O. So that you have the latest and greatest features. Instead of having to manage a particular component and upgrade the entire pipeline, which might require downtime, a lot of fine tuning, a lot of optimizing. With managed I.O., we take that all on for you. Our Iceberg I.O. also makes streaming CDC available. So if you have an Iceberg table that is storing features, you can make sure that it's synchronized with your vector database like Bigtable so that your predictions are based on the most up-to-date information. Dataflow is now also supporting data lineage. So you can now inspect the end-to-end movement of your data in Dataplex. And this helps prevent outages from happening. Again, now that you can see the various dependencies of your pipeline and all the resources that it touches, you can now prevent yourself from making errors that might modify some of those subjects. A few other things that I also want to call out. Firstly, Dataflow has historically been for Java or Python or Go developers. Well, now you can build pipelines with our no-code pipeline builder tool called JobBuilder. It allows you to basically define any pipeline with arbitrary complexity using just a few forms. And if you want to expand on the complexity of that pipeline, we also allow the ability to export that or write a pipeline in YAML. So your pipeline is a config and not code. We've also given more control over the streaming autoscaler. Our at least one streaming mode helps customers save costs and also speed up their processing. We've now also introduced anomaly detection turnkey transformation. Once again, this is very complicated logic that could take thousands of lines of code. We've now taken that and exposed that as a simple function. Passing in a few parameters and you have an anomaly detection pipeline. A lot of our customers also use MongoDB. We've had a template that helps take data from Google Cloud and export it to MongoDB, both in batch and streaming modes. That is now generally available. And with that, I'll hand it off to Shopify to share the story about multimodal streaming AI. Hello, everyone. My name is Franklin. I'm going to be talking about streaming ingestion at Shopify. And later on, my colleague, Shatranga, is going to talk to you about multimodal AI. All right. So a bit about Shopify. We are a global e-commerce platform. We host about like millions of merchants. And last year, about 800 million of you use Shopify to shop online. And during Black Friday weekend, you guys spent around $11 billion on a Shopify store. And to put it into perspective like of our Kafka clusters, our Kafka infrastructure, this is about 30 million messages per second. It's also 10 gigabytes per second or about roughly a petabyte of data a day. So what we'd like to do is take that incoming data in Kafka and write it to our warehouse in BigQuery. Our Kafka topics are distributed over multiple regions. And within each Kafka topic, we have multiple schemas for that data. And these schemas can also kind of conflict with each other. So they're not always compatible. So what we'd like to do is build a streaming ingestion platform. At the core of this platform is Google Cloud Dataflow. We've also developed a kind of constellation of services around Dataflow that help us automate and manage our Dataflow jobs. I'm going to walk you through our Dataflow pipeline. And then we'll talk about how these services help us manage things. So this is our kind of typical Dataflow job. It extracts data from multiple Kafka topics, transforms that data into BigQuery table rows, and then loads that data into BigQuery. Something to note here is that there are multiple topics in a job. And the reason we do that is because the distribution of topics is kind of skewed. So we have a large amount of small, low-traffic topics. And so we pack in multiple topics into a single job. So we get good resource utilization. And the thing that helps us do all this packing is our control plane. So our control plane analyzes Kafka traffic across our clusters. And it decides which topics to bin pack into which jobs. And it also manages the kind of lifecycle of those jobs. All right. So what we like to do is scoop data from a topic and write it to a dedicated BigQuery topic table. But like I mentioned before, we have various schemas in these topics and they conflict with each other. So we don't have a kind of dedicated schema that we can create in BigQuery as the single table. And we don't want to handle complex schema evolution inside our pipeline and make our pipeline complex. So what we decided to do is write each individual schema into its own dedicated table. So instead of having one destination table, we have multiple. And we use BigQuery I.O.'s dynamic destinations in order to do this. You tell BigQuery I.O. the schema information of the row, and it routes you to the right table at the right time. If you do this naively, you're going to get really bad performance because serializing schema information for across like billions of rows is really, really costly. So what we ended up doing is instead of tagging schema information, we tag schema identifiers on each row. And then inside our pipeline, whenever we need to understand what the schema for row is, we resolve that from the schema registry. And we also aggressively cache that within the pipeline so we don't make too many external requests. And this ends up being like a huge kind of performance boost for our pipeline. So it's a pretty significant change. All right. So that's how we performantly extract data and write it into BigQuery. But we have this problem of our data is now kind of strewn across multiple tables, across multiple schemas. What we'd like to do is take all of that data and give our data scientists a unified view over the data. So we developed a view manager service. It takes all the schema tables that are in BigQuery. It computes a unified schema and view over that data. And then it registers a view in BigQuery so that our end users can use that view instead. And this view manager understands our kind of complex schema evolution rules. So it's able to compute the right schema across all the data. A bit about resiliency in this pipeline. We use a common kind of like dead letter Q pattern in the pipeline. So if there's any bad records that end up going through the pipeline, we write them off to a dedicated storage area. So if we have bad data in our transform layer, we write it to a dedicated BigQuery table. Any bad data that goes through the load step gets written to a dedicated GCS storage bucket. And then we have playbooks and tools that help us examine this bad data and also potentially even recover the data. So this kind of dead letter pattern covers our known unknowns. But what about our unknown unknowns? How do we make sure that we are reliably delivering data into BigQuery? So we also have a kind of quality assurance system. And the thing that it does, it's basically a BigQuery batch job. And it incrementally processes our data in BigQuery, looks at the offsets that are being written into BigQuery, and determines if there's any gaps in the data. Because we should really be getting all of our offsets from Kafka consecutively written into BigQuery. And so when these quality assurance jobs find gaps or missing records in BigQuery, they kick off gap recovery processes that go back to Kafka, extract the data, and write them back to BigQuery. So we have an automated way of validating our data and also fixing any issues that come up. So yeah, that was our streaming ingestion pipeline. And I'll pass it off to my colleague. Hey, guys. So my name is Kshet Ragnar. It's a difficult name. Most people call me K. So I want to talk to you about an actual ML pipeline that we built. And I'm actually very proud of the work we did here because everybody talks about LLMs and Generative AI. And there are the obvious use cases, right? There are obvious use cases of chatbots and agents and all of this. And then we kind of like, what were the non-obvious use cases, right? And one of the places we thought we could make an impact with the non-obvious use case was extracting structured data, essentially using multimodal LLMs as a classifier to extract tons of metadata about tens and tens of millions of products on a daily basis, right? And how do we do that? So the first thing, before I even get into the actual LLM and getting into the streaming pipeline that we had to build, this is the Shopify taxonomy, right? So every product that any of our merchants sell, we would like to project on to this taxonomy, right? And so you see in this case, I'm talking about a shoe, right? And I'm saying, hey, I have athletic shoe as one of the categories I have. And you can see that's a hierarchical tree structure. And then on the, you're right, you see all of those attributes and each of those attributes internally also has specific values, right? So for example, color could be red, green, blue, white, black, whatever, right? And so given any product, I want to be able to project it onto this. And I say, this is the category of this product. And these are all the various attributes. The attributes are dependent on the category, which means the attributes I would extract for a shoe is very different from the attribute I would extract for a television. It makes sense, right? And so this is the problem statement. The scale at which we need to do this is on any given day, we get tens of millions of new products. And there are also products that are constantly being updated, right? So anytime a product is either updated or created, I want to be able to do this for that product. Before I get into the actual pipeline, this is how the model itself works, right? So we essentially take two passes at it. Because remember, I told you that the attribute depends on the category. I need to know what the category is before I decide what attributes I want to extract. For reference, we have over 12,000 different categories. There are about 1,200 different attributes with over 40,000 different values. That's the potential solution space that I have to work with. And so we take a product, we take its images, we take the textual details of it, the title, the description, the vendor, whatever, right? All of those details. And then we pass it on to a vision LLM. I think our latest model in production right now is a QEN 2.5 VL 7 billion vision model. And it's a fine-tuned model. We have LLM annotators and human annotators creating training and testing data sets for us. And so we've created a model that given a very specific prompt, it is able to extract the category of that particular product. We then make a second call to it saying, hey, great, thank you for telling me what the category is based on what you told me previously. I know these are the attributes I need. Can you extract them? It does it. And so in this case, I gave it a hoodie and it says, hey, that's an outerwear and its color is black and on and on and on and on, right? This is what I essentially want to do for tens of millions of products on a daily basis. And this is how I do it. And so essentially Dataflow forms the backbone of this entire processing stream, Kafka in, Kafka out. Our core application sends us a message into our Kafka topic that says, hey, this product ID requires a new prediction. Here are the features that like it'll give me a URL. It'll give me the textual features. And then it says, make a prediction for me. We made a deliberate decision to separate out most of the preprocessing and all of the IO. We have a data flow which happens in Dataflow from the actual LLM inference itself. The actual LLM inference itself is handled on a Kubernetes cluster. We're using NVIDIA Dynamo with internal other LLM frameworks inside. Specifically, we tried out various different, like I know Maran spoke about VLLM. We've tested VLLM, SGLang, LM Deploy, Tensirati, LLM. We've tested it all. In fact, we've had different versions of these in production for different models. Because what we figured out is that there is no one universal answer saying this is the best framework. Because it depends. Different models are supported differently by different frameworks. And so we kind of constantly have to play around and change which frameworks, what GPUs are we running the specific models on. We optimize these differently. And so we kind of decoupled that and said, okay, we're going to have these Kubernetes clusters optimized differently for different models. And then all of the IO operation, the image download, all of that happens inside of Dataflow. It makes a call. In fact, it makes two calls. So it will make the category call first. And then it will make the attribute call. And then it lands everything back to Kafka. And so this way we kind of have the separation of those two things. And yeah, internally within the Kubernetes cluster, you have an ensemble. So we do specific pre-processing there as well. And that has your KV cache in there. And it does in-flight batching. And it does all of those LLM specific optimizations there. And then we optimize the rest of the pipeline inside of Dataflow. And so, yep, this is essentially the pipeline that we've built. And this system has been running now for over a year. And we've categorized billions of products through the system. So, yep, thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Hi, I'm Sumita from Palo Alto Networks. So we are here to walk through from a different perspective the same story that this presentation is talking about, which is you have Dataflow, which has all these great applications. How do you scale that up to, you know, process millions of events per second? So, as you know, Palo Alto is the number one cybersecurity provider for many, many customers. And there is a, we have a data lake. So, okay, just an intro slide. I'm Sumita. I have my colleague James Chang here. And some context on, you know, this data lake that we're talking about and how it fits into Palo Alto's mission. So, Palo Alto, sorry. Okay. Okay. Let me start from here. So, Palo Alto sells a lot of devices. I think we are all aware of that. There's a lot of hardware devices, software devices like hardware firewalls, software firewalls, VM firewalls, firewalls that run on cloud service providers, all of those. And each of these can actually generate data activity logs. There are also cloud security services such as, you know, malware detectors, detectors for, you know, malware DNS, all of those things. And we also have other products like Prisma Access Browser, which is part of the SASE framework, like Global Protect VPN. Many of you may have that on your laptops. So, all of these devices are actually, can generate activity logs. So, what are these activity logs? If you're a user, if I'm a user in, you know, my headquarters, I'm doing my laptop, you know, connecting to Google.com, the firewall detects that there is a DCP SIN that is set and a session that's starting, which connects to the external world. Right? And that generates an activity log. So, that's an activity log. And every, you know, the progression of the session, we get activity logs for that. And it's not just the, you know, session itself. Say I'm downloading a file. I should know better, but, you know, I'm downloading a malware file. So, the firewall detects that and drops that. So, there's an event generated for that, too. That's an activity event that comes to us. Suppose, you know, it's a benign file that I downloaded. Again, the same thing. We get an event for that. So, as you can imagine, the scale of these events can be pretty huge. Right? If you think about a corporate office with thousands or tens of thousands of users, every session generating these logs, pretty huge. Right? So, that's where these numbers come in. Right? This is kind of the scale that we're talking about. We get about, across 25 cloud regions, we get six petabytes a day. There's about, just in the US region alone, we have three petabytes a day. Right? And, you know, the sources, individual firewalls, devices, cloud services, all of these come to, like, about 500,000. And many of these, as you saw, are actually connecting directly to us through HTTPS, TCP, SSL, gRPC, and so on. Right? So, this is sort of the scale we're talking about. Now, the structure of the actual, the internals of the data lake itself is very, very close to what we're talking about. It's very, very close to what actually Shopify described. Right? We use Kafka the same way. We use Dataflow the same way. And store in BigQuery. But we don't just store in BigQuery. We also allow our customers to take these logs and feed it into their CMs. The, you know, if you use third-party CMs, it requires specific setup by the customers. But if you use something like XIM, that, you know, PowerAlto itself, the hookup is done internally. So, you can buy that and get all of these activity logs into it. So, you can actually do a lot of security operations and operations. Right? So, now, on top of that, as you can imagine, this data is gold for a lot of AI-ML-powered applications that PowerAlto runs to predict what's happening in the network, to detect what's happening in the network. And, more importantly, when you detect a threat, can be mitigated live. Right? So, these are, as you can imagine, critical applications that customers rely on to not just, you know, surface things on a dashboard or provide insights, but do actual live mitigation of threats. Right? So, that's kind of the context of this. So, the topics in Kafka, just, you know, similar to what Shopify mentioned, right? It's a per customer, per log type type of topics. Just. So, here I'm talking about the very first generation of what we have. So, as we know, Dataflow, incredibly easy to boot up a system with. Right? Bootstrap, there's nothing like it. You know, you can build jobs easily, run jobs easily. Google takes care of the scaling, all of that stuff. Right? Great. But, as you grow, you know, go up in scale. So, this is kind of the scale that we had at one point. Right? 85,000 jobs. You can see the jobs per region. This is a huge number of jobs in terms of managing scale and also cost. Right? So, it has great reliability. It's wonderful. You can operate at great. So, two sort of considerations we had at one point. Okay. Should it be cloud agnostic? And, of course, the cost. Right? So, this is what we sort of did next. So, this is stage two of our journey where we try to move towards a more self-managed. We still had, you know, some jobs on data flow important ones. But, we actually tried to create a more self-managed solution and scale that. So, since we are focusing more on operations here, I will not go into the technical details of exactly how this was done. But, as you can see with this, if you look at the lower corner here, right, I'm actually saying, hey, you know, all our raw logs, that log processing, which is not very compute intensive, does not have a lot of complicated transforms, easy. Right? They were done easy. But, where we had problems was when it came to these complex transforms, aggregations, like at five, ten, fifteen minutes. And, these are exactly the jobs that are used by our AIML models. Right? So, this is where, so, cost obviously came down heavily. But, reliability operations was a nightmare. Right? So, when you write your own custom code to actually manage your own network that runs into 85,000 jobs, it becomes very, very hard to operate such a network. Right? You need a lot of operation support. You know, even when you write custom code, that means you need people who can optimize the code, run the code, maintain it, all of this. Right? So, there was a rethinking that happened. And, I'm trying to summarize everything that sort of came out of all of the rethinking that we did, which is, which brings us to, okay, these are the considerations when you are trying to design a streaming system that can stream at very high scale. Right? First one, what are the application needs? What type of job do you have? Is it stateful? Do you have aggregations? What is the throughput? What about the availability? Those things are important. Now, control for independency. So, this is a mistake where we made. We were, we mixed, mixed the, you know, having the per tenant, per log type job means that when a new tenant is added, you're actually updating your infrastructure, right? Creating a new topic in Kafka. Bad idea. Don't do that. Right? So, control plane, and you should put mechanisms in place where any changes in the control plane does not actually impact the data plane, right? For example, your jobs don't need to restart because your customer configured something new. Right? So, that's very important, especially at this scale, like the 85,000 job scale, you know, restarts are too much. The other one, is the design like predicted versus potential growth, right? We had, when the product originally existed, there was a predicted growth, right? Okay, this is how the product, these are the dimensions in which the product is going to grow. But, when you design, you should also consider that, okay, maybe, you know, there is predicted growth and there's other dimensions that the product can grow on, right? This is something that definitely happened with us, right? We had more growth in the tenant and the log types than we expected, which is why we had the, you know, Kafka problem. So, always, you know, think about whether your design is sustainable if other dimensions in the product grow, right? SLA requirements for end customers, and how do you put in, what metrics do you put in to make sure that you are meeting the SLAs, right? I mean, this is pretty commonly known, but I think the one addition that we have to make there is, have SLAs to know so that, or have metrics so that you can measure the drift, right? This is your expected behavior, this is the expected growth. Your metrics should be able to show you if there is any drift from whatever you were expecting, so that you can cast that drift early and, you know, make accommodations accordingly, right? That's important. And, of course, what kind of AML requirements you have, right? I think that's important. The other part, perhaps, may not always be thought about is, you know, people. You know, there are custom code always requires coders, you know, people who develop, maintain systems, and optimize obsessively, right? That is very important. And with people who build things, it's easier, you can assemble things and actually build a pipeline. It's not so complicated. But there are different considerations when you're building a system, right? Again, optimization, all of that. Now, balancing all of these actually requires pre-planning. So if you plan to do custom code, I mean, you should have hired, you know, enough coders to do all that, right? And what about operations, right? How much investment are you willing to do in your operations? You have, on one side, you have, you know, the Google Cloud and their entire power behind supporting data flow versus how much are you willing to support by yourself, right? And, you know, there is, for support, do you do, again, same thing, right? DYI or considerable expertise that Google has. Do you leverage that, right? Of course, you know, for a certain cost, right? That's always part of the consideration. The other part, yeah, stability, SLA, you know, who are the consumers of this data? Do they require, you know, high stability, lower, you know, is it more of your AML models that are consumers? How do you design for that, right? And productization timelines, right? It is very, very fast to iterate a design in data flow, right? When you're considering something initially, and I think we heard about, you know, like, codeless connectors and things like that, right? There's nothing to beat that, but you can actually iterate pretty fast, get to a stage where you're really confident that, okay, the model, the flow, everything works, right? Post that, you should also look at sort of cost tolerance, right? So, you know, common problems. It's a, I mean, generally, as a team that provides a platform and infrastructure that's used by a lot of other teams, there is always a cost tolerance concern, right? There's a value versus cost. And it is important where, you know, if you can put in some, again, some metrics and design in a way to quantify the value that you provide, right? So if you are, you know, doing all of these flows data, providing data to different teams, it's actually good to think about, okay, how do I, you know, project the value that this infrastructure or data platform provides, right? Because, you know, how much cost, the cost tolerance that the company has for this infrastructure always depends on that, right? So, considering all of this, I think this is just the final kind of picture that we have. So we designed, redesigned in a way that, you know, the number of jobs considerably reduced. And in this model, we have data flow running along with some self-managed jobs. And the summary of that is, again, you know, raw, simple enrichment transforms, great. You can use your own custom jobs, right? They can be simple, easily manageable, easily optimizable. But anything really complicated, you want sort of the power of Google behind it at data flow so that you can actually, you know, all of those things that I mentioned, right? I read fast. So, you know, you have to sort of consider cost against how much, you know, predictability, reliability, scalability, all of those you want and how much support you want to do. I think that's about it for my talk. Thank you. Thank you. Thank you. Thank you. Thank you. That was an excellent presentation. So for those that are interested in learning more about data flow, I'll call out a few more sessions. Firstly, we have our session regarding, sorry, not just data flow, but I'm assuming that most people here are into streaming. Data flow is not the only product that does stream processing. We have our Kafka service, which will have some Kafka announcements tomorrow morning. We also have continuous queries that uses BigQuery in PubSub, and that'll be covered tomorrow afternoon as well. For folks that are interested in getting hands on with Beam and data flow, we have an online training event called Beam College that is scheduled for about next month. We also have an annual event called Beam Summit that allows for customers to kind of share their stories and their best practices. We'll also attach a free data flow workshop for those that have been selected.