 Hi everyone. Thanks so much for coming to this session at the bitter end of Cloud Next. Throughout the conference, you've been learning about some really interesting enterprise applications of AI. Back home at your own organizations, many of you have been experimenting with AI models and testing them with your market. At a certain point, it's time to scale. We are here to show you best practices for ensuring that users of your AI-enabled apps experience fast performance and successful workflows. Let's get started. I'm Kate Brea, and I'm a product manager on Cloud Observability. I'm YQ. I'm a software engineer in Google Cloud Observability together with Kate. And over the past six months, we've been collaborating with the Vertex AI team to build beautiful observability experiences that we're excited to show you today. So today we'll start by defining our terms, including monitoring and Vertex AI model. We'll run through a scenario and a coding demo, and then we'll walk you through a dashboard we made for monitoring Vertex AI calls and show you how to customize and enable alerts about them. As promised, I wanted to start off with some definitions. When I say monitoring, I mean making it possible to understand rate, error, and duration metrics, to understand speed, uptime, and cost of running a model. Vertex AI also offers a suite of tools for evaluation, which can help you figure out how well a particular model works for a specific business use case, but our focus in this talk is on speed, reliability, and cost. Next, I'll define Vertex AI model. In this talk, I'm focusing on managed models found in the model garden, including Gemini and Anthropic Clod. Vertex is also home to self-deployed models, including open source models like Gemma and traditional ML models, models, but today we'll be focusing exclusively on managed models as a service. So picture this. You're a flower farmer. You're looking to sell flower seeds online, and you're putting together your website to sell them. You need a set of on-brand product descriptions that outline color, size, shape, and optimal growing conditions of these flowers, but it's time consuming to write them by hand. So you decide to assign the task to Gemini. I'm going to switch over to a coding demo. You can find my code on GitHub by scanning this QR code or on your laptop following this short link. I'm going to switch us over to my Colab notebook, and we'll run through it. Okay, this is the GitHub repository that you'll find this on. And that same code is here in Google Colab, here in the Vertex AI space within the Cloud Console. I'm going to run through this and walk through what I'm doing here. The data set that I'm accessing is from Kaggle, so I'm importing Kaggle and making it possible to use data frames, downloading the data set, loading it into a data frame, making it possible to look at some images. I'm loading the first image from the data set, and you can see it's a nice pink flower. Now I'm going to query Vertex AI, and you can see here that I'm using the CCO Test Lab project in US Central 1. This will tell us which is required for any query, and it also will tell you where the metrics will go. We're loading in the image in byte format, and we're instructing the model. Please write a description of the flower shown in the image and give us some information about its optimal growing conditions. We submit this to Gemini 2.0 Flash. We could choose any model here, and give it our image and text as parameters, and submit it to the API, generate content. And you can see here that we get back an on-brand polished description of the flower shown in the image and its optimal growing conditions. So what did we just do? We prompted the Vertex Gemini API with an image, and we received back an on-brand product description. In the background, Vertex AI produced a set of metrics about our traffic that we can access and manipulate through Vertex AI and cloud monitoring. Let's see how the traffic we sent to Vertex AI performed. You can go to the Vertex AI dashboard page to find our dashboard if you've submitted a query to a Vertex AI model. This is Vertex's homepage, so as soon as you search Vertex into this search bar, you'll find it. Scroll down and find model observability, and click into this and see show all metrics. I'm going to hand it off to YQ to walk you through the dashboard. Yep. Thank you, Kate. So over in the dashboard, let's ground ourselves and see what we're looking at here. Cool. So this is a dashboard that, as Kate mentioned, is out of the box. You can get to it from Vertex, or if you're in monitoring, you can go from monitoring to dashboards, and it's in the list of dashboards. If we detect that your project has any Vertex AI usage at all. So top level kind of summary, you see a table here for every model and location. You can see its invocation rate, latency, token, character throughput. But now let's dig into what every section is and see the charts, which kind of show you more historical data. Number one, throughput model requests per second. You can also say QPS queries per second. You can see if I pull this out to say the past 30 minutes, then what you're going to see is that there is some kind of like just in this project, a baseline like background, like traffic or activity for these models. But the thing that Kate just did by invoking a Gemini 2.0 flash double one from the Colab notebook shows up here as this tiny little blip. And you can see that request per second, but also, hey, these are streaming endpoints. You can see token throughput for each of these models. Broken down by model. But if you want to split it up by a different dimension, you also have split up by like input and output. I personally don't think that like looking at, okay, how many tokens that I send as input, how much do I get back as output, is super meaningful when I like aggregate over all of these different models in my project. And so I can zoom in by drilling down from the top level of filter, like say to one of these models. And now I think this is kind of like a more accurate view for that model. What is the input? What is the output? Next, we also have a latency section kind of governing if you're using this to back like a chatbot or an app, like what is the responsiveness that you expect the user to see. Once again, these are streaming endpoints. And so we care about two things. We care about the latency to get back your first token, as well as in some cases, for example, if this is gonna, because that's when the user like kind of perceives there to be like some pause or some spinner, or if this is part of some like more complicated agentic workflow, then maybe you do need the entire like kind of output back before you can do anything with it. And so you also care about the total like model invocation latency. And you can kind of see the different like kind of like magnitudes here, where this is like 400 milliseconds, and this is like 13 seconds. So this is, I think, also more meaningful when you zoom into a single model. But if you want to compare across different models, then let's undo this filter. And you can kind of see that breakdown. This is also useful when you have, for example, before you go to production, you're just like benchmarking, you want to like test out and like evaluate the performance of different models for latency and other kind of like performance benchmarks. Next, we have errors. So this pretty tame graph, because these are all 200s, nothing going on. But let me switch, I think I have a different project here, which is what I would call a bit more spicy. You have 200s, you have 400s, 403s, 499s, which I had to look up, I didn't know what 499 was, 500s, 500s internal server error. And for each of these, then you can kind of like see like, like either the raw rate of these requests, responses coming back, and it as a percentage of the total like rate, like response rate. Going back to this, we have same similar looking breakdowns for tokens and characters. You might be asking, why do you care about both? And that's a great question. Because, and the answer is that the newer models 2.0 and above look at, are being built, built by tokens, while the 1.5 models in below care about characters. So depending on the ones that you care about, that's what you are drawn to, to kind of like get a handle on your, the cost implications here. Final section, provision throughput to understand this better. I'm going to switch back to this graph here. So if you're a developer who has worked with like kind of like made different network calls before, you should be familiar with like a 400, which is bad request, 403, which is forbidden, so you didn't provide the right credentials. But one thing that you might not have seen very often is this 429. So what's a 429? If you look it up in the HTTP spec, that stands for too many requests. And in the context of Vertex AI, where you have all of these managed models with Google-operated capacity per region, it means that there's just like, we're unable to kind of like, respond to that request for that model in that region for that moment in time. Under the default scheme, which is pay as you go. So there is a different kind of like request structure, which is instead of pay as you go, what brings us to what this is, which is provision throughput. So it's a fixed cost monthly and weekly subscription, you can guarantee a reserve set aside a bunch of throughput, let Google know ahead of time, you're going to be calling it this much. And that's for certain models in Vertex AI. And what you're going to see here is that if you did set this up, then this chart is going to show some metrics. This, if I open inspect this, these metrics you can see is a ratio of your usage divided by the quota that you've purchased. And so you can see a here is a ratio. If you look at the y-axis, these are all percentages. And that is like a quick like bird's eye view into what that is. If you want to know more about provision throughput, there is also kind of a more like, let's pull this up within Vertex AI, like a single purpose UI to like kind of like drill down into this to higher fidelity. So that's it for the dashboard. But before I jump back to the slides, I want to preempt a question, which I know is kind of like forming in a lot of people's minds here, I can see the visible thought bubbles above your head. And that is, hey, YQ, you just showed us a bunch of metrics. But what about trace? And if you were thinking that, great job. That's a great question. If you weren't thinking that, let me explain to you why that's important. Two reasons. Number one, if you're looking at metrics, you're necessarily aggregating over like hundreds, thousands of requests. But you can't drill into a single like misbehaving request. The way to do that is by inspecting a trace ban. Number two, the demo that Kate showed off is with a single colab notebook making a single call to an LLM. But nowadays, it's also fairly typical to see like a more sophisticated, like agentic workload where you have like an agent like orchestrating different tasks and making multiple models, multiple invocations possibly to different models. You want to trace through all of that. And that's not something you're going to see with this type of metrics. And that is not the focus of this talk. But I'm just going to super quickly acknowledge that yes, within Google Cloud, you can see Gen.AI traces within a Cloud Trace product. So here, there is you can see a trace from a single agent invoking Gemini, making calls to I think, some database, making some SQL call. And there is also this preview Gen.AI tab where you can see the system prompt, the user prompt, and like walk through that for a single trace. There was, I wish I could have said like, hey, this is happening soon, so you can go and watch it. But there was a talk about this, maybe some of you were there yesterday, called Agent Observability 101, that went into a lot more detail about this. So look that up if you're interested in that side of observability. Let me go back to the slides. All right. Cool. All right. So now, I kind of went through this a little bit already. But like, if you see all of these metrics, you want to like think, look at each chart and think about what it's telling you. But also, the next step is how you might want to act upon it. So like a chart, my philosophy is that a chart is not meaningful unless you see it and there's an action that you can take. So the first two, you have a lot of requests, you have high token throughput. That's not like a problem by itself. If all of your other metrics are doing, are looking great. Because if you think about it, that just means you have a lot of traffic. And maybe that just means your app is super successful. The one thing that you want to be careful about is just think about the implications to cost and whether that is kind of like in the ballpark in the range of like what you were prepared to spend on LLMs in the first place. Latency, if that is high, if that's increasing, look at the aforementioned like 429 errors. Maybe you want to tweak your prompt size, your response size. And you can, as we mentioned before, benchmark and compare different models. 429 errors, you're running out of capacity. Switch to off-peak hours. Switch to batch processing. Look into provision throughput as a different mode than pay-as-you-go. 500 errors, those are errors which legitimately happen on the server side, but you could still look into what is happening in the client and inspect like the requests and the parameters that you're passing into it. So I want to pause here because and contextualize this in terms of like other things that we've presented at Nix. Our team, other than building these dashboards with Vertex, we also built very similar dashboards. If you were running your AI workloads on GKE instead, and those I think were shown off and demoed yesterday at a different talk by our teammates, Jan and James. And if you listened to that yesterday afternoon. So we're pretty proud that on both sides with Vertex or on GKE, you have a pretty similar and consistent like observability experience with one key difference, which is that on the GKE side, you manage your infrastructure. On the Vertex side, you don't. So let's rewind. How many of you have heard of like the kind of that common SRE term like the four golden signals? Raise your hands. Yeah. So four golden signals that an SRE would look at to see the health of a system, request rate, error rate, latency, and saturation, which in the GKE context would be talking about like your CPU utilization, your GPU utilization, your memory utilization. But in the Vertex side, that's not really something that we have because all of this is kind of abstracted away from you. And that's why at the start of the talk, Kate used a different acronym, RED, R-E-D, for rate, error, and duration without saturation. This is kind of like both a pro and a con when you compare the two systems. Do I manage my own infrastructure or do I use Vertex where like Google takes care of everything for me? Because number one, you don't have to think about like, oh, like I have to configure all of these auto scalers or like have to think about what level to set my requests or my limits. But as you see here, they're not, when you want to optimize, there's not that many like levers that you can like play with. That is, and so what I want to point out from here is that this is not necessarily, but it also doesn't mean that everything is like totally unactionable. And one super valuable thing, as with any other dependency that your app has, is that when something goes wrong with all of these metrics, you can see really quickly, hey, is there something wrong with my app? Or maybe there's like a region-wide like Google issue. So next slide. We're really proud of this, which is that no matter how you query the model, the monitoring all looks exactly the same. Kate just showed off like, oh, let me hit it with the GenAI SDK for Python within a Colab notebook. But you could call it from an agent, you could call it from different languages, you could even, and I did this to set up some of the traffic that you saw in the dashboard. Just send like curl commands from the terminal and hit the rest endpoint. And no matter what you do, we're tracking usage the same way, we're using the same metrics that all free system metrics out of the box, no setup required. I have one more demo, and then I'm going to pass it back to Kate. And this is about, we just saw a dashboard. This is our best effort dashboard to show off like everything that we know about the system. But obviously, we can't predict your use case. So let's talk about what it might mean to like customize this, how you would customize this, and what, why you, why you would do so. So here you have copy. And if you did that, you would generate this like kind of like copy of the dashboard. And now this looks like any other custom dashboard that you can create within cloud monitoring. So you can resize things, you can delete things. Some of our recommendations is that if you have multiple models that you want to look at, this table is going to be the best kind of visualization. But if you know ahead of time that you only care about a single model, then you might want to replace this with a bunch of scorecards, bigger numbers, easier to glance at all the important numbers all at once. Other things you might want to do. As I mentioned earlier, you care about tokens or you care about characters, depending on the models that you're looking at, you might just want to delete some of these if it's not actually that relevant to you. Also, let's see, yes, down here, one thing that you can do, and I'm a huge proponent of this, is just like embed your team's like kind of documentation, know-how, like troubleshooting advice within the dashboard in situ, kind of like co-locate them with the charts and the metrics. So this is huge because, and the title of the talk is like best practices, because the person who typically sets up all of this stuff, who like designs dashboards, etc., is not necessarily going to be the same person who is waking up in the middle of the night to like respond to a page. And so if there's anything that you can do, so for example, to get more coda, let me just, to get more coda, follow our team's standard operating procedure here, and then you can have like a link to your own wiki, you can make this, I don't know, yellow, you can have like a pointer, and right, left, middle, and then that kind of like highlights some of this a little bit. So these are all things that you can do using our existing dashboard as like a starting point, but there are totally legitimate use cases that you might, you know, just like create a blank, a dashboard from scratch. And why would you do that? Because as we mentioned, you're not really like invoking these like LLMs or like Vertex AI models is not something that you do in a vacuum. You might have some setup with like a load balancer and a server and like a database. You might have like an agent that's invoking both an LLM and some like external service, and you want to see those charts side by side. And so we could, that's why you might want to just like take your own step at like creating your own dashboard. If I try and create a line chart here, notice that when I select a metric, I don't see a list of metrics, I see a list of resources. And that's because that's how Google Cloud is associating metrics with a consistent list of labels. The ones that I care about are all going to be under this category called publisher model. And so these are the seven metrics that we saw earlier, but you can mix and match, join them, filter them however you like. The thing that I would call out here is that all of these have launched stage beta. And in fact, two of them for provision throughput, we just launched like one or two months ago. So there is all of like the rest of the AI landscape, things are changing here very quickly. And if you were, if you were playing around with this, if you're a Vertex AI customer, there are some metrics that you would like to see. And we don't have that yet. Come talk to us. We'll learn, we'll love to learn about your use case. So let's, let me pull that together. I can group this by response code. And yeah, so then I can also put side by side with it with other metrics from other resources that I have within Google Cloud, my Kubernetes containers, my VMs, whatever it is, and like situate those side by side. One thing that I did want to call out here is that, oh no, actually, let me just do it here, is that all of these dashboards are representable in JSON. And so if you've customized them and like edit them in one kind of like, like project, it's fairly easy to like take that and like instantiate that in any other projects. And also one more thing is that while we've demoed all of this within Cloud Monitoring, within the console, all of these metrics are being written to Google's like global scale, like time series database, which can be, so you could, if you already had your own visualization solution, whether that's Grafana or something else, you can query our existing monitoring APIs and recreate the same charts, the same types of views. So before I finish my demo, hey, it's a, it's a Friday afternoon, it's the end of Nix, I'm feeling a bit rebellious. So I'm going to show off like a little demo that doesn't really have anything to do with Vertex strictly, but has to do with like all of these dashboards. And I know that a lot of people who rely on these monitoring dashboards have been asking for for a long time, which is, hey, these are super blinding, they're very, and I don't, yeah, okay, so how many, this I think was like announced as being part of like public preview, I want to say like last week. And so like how many people didn't know about this? Cool. So, so this is, this is a useful thing to show off. So if I go back to where I was in monitoring, let me just pull, let me just refresh this. Yeah, all your dashboards are now a dark mode. So that's pretty cool. That's me. Back to you, Kate. Great. Thanks, YQ. So I want to pick up on something that YQ mentioned, which is that it's really important when looking at these dashboards to understand how to take action on them. And I'm guessing that when something goes wrong in a different region in the middle of the night, you're not going to catch it by continually refreshing your dashboard or just continuing to babysit it. Instead, you're going to need to find a way to find out that something went wrong. So to this end, we've developed a few out of the box alerts. And I also want to show you how to create your own custom alerts. So here in the monitoring integrations page, you can search for Google Vertex AI, and you'll find that under this alert section, you have a number of recommended alerts. And this makes it super easy for you to simply click one, click one, select a notification channel, and create it. That's all you need to do to ensure that, for example, if you're getting 429 requests for more than 1% of your requests, you'll find out about it and be able to take action. Second, I wanted to show you how to create your own custom alerts, because you might be interested in something totally different. You can go to alerting, create policy, select a metric, select one, configure a trigger to show like what's anomalous. And I think what's really helpful here is that in the documentation body here of the alert, you can give instructions so that the person who's waking up in the middle of the night knows exactly what to do, saving time and improving time to resolution. So what did we do during this talk? We explained monitoring AI workloads at scale, we ran through a coding demo with a scenario, and we showed off some and out of the box dashboard, how to customize your dashboard, how to make your own dashboard, and how to configure alerts. But yeah, please find us if you have any further questions.