 . Great. Hello, everyone. Welcome to our talk. Thank you so much for joining us. My name is Charles Baer, and I'm a Product Manager at Google. I'm joined today by Ram Kojigini, who's SRE Director at Charles Schwab, and Benji Soto, Cloud Product Manager. We're excited to talk to you about four main things today. First, we'll talk about cloud complexity and its impact on the troubleshooting process. Then, Ram will share his process of working through investigations and using Gemini to be able to expedite the troubleshooting process. Then, we'll talk about our new product, Cloud Assist Investigations, and how we're building an out-of-the-box way to streamline the troubleshooting process. Then, we'll show a demo and take care of your questions. Without further ado, why don't we get started? Benji, tell us about Cloud Complexity. Thank you. Outages can happen at any time. I think everybody here has been personally impacted. It's why we're all here. We all work on enterprise software. Let's be honest, they're complicated. They're distributed. Lots of service dependencies, lots of different teams, and they break at any time. But what really hurts is when they break on your time. I'm not talking about Monday through Friday when you're working. I'm talking about like 9 p.m., Saturday morning, Sunday evening. When there's an outage, you look at your phone, your team's responding to it. You have to open up the console, start bringing up some tools, looking at telemetry, doing the RCA, all that good stuff. And then the pressure's on. Teams might even start blaming each other in a Slack group or whatever communication tool you use. And then you fix it. You look at the clock, and it's 6 p.m., and you lost your Sunday, right? Those are the outages that hurt the most because they take time away from you. And all of us have been impacted, and I think all of us can relate to some type of incident that has done something like that to all of us. Sure, your customers may have received a degraded experience because of that outage. Revenue might have been impacted, and there might have been lost revenue because of it. Some product APIs may have been impacted as well, but what people don't really talk about is the human impact. These outages, these first responders are people. They have families. They have kids. They have lives. And when these outages occur, time is not only being taken away from the first responders, but from their own personal lives and their own personal families. And, Bram, tell us, like, have you been personally impacted by outages at Charles Schwab's? Oh, of course. See, still I carry my phone here, and I never know when I get my page. No, I am in a light sense, right? Definitely, I have two boys. Even oftentimes, they come and play with me during the weekend. But I say, oh, go away, go away, I'm in a call, you know, I'm trying to fix something, right? This is very personal, you know, it can be in life of everybody. It's not just me. We have a lot of teams here. They're all experiencing the same. Yeah, that's the human element. That's the human impact that really comes from these types of outages. And really, like, what led us here? Our product team, we have decades of observability experience in the market. We've seen the tools. We've seen the capabilities that have been offered. IRM has its feature-rich. There's lots of tools out there. But we've compared it to other spaces like AI-generated code, dev tools, all that good stuff. And the reality is that innovation within incident response has lagged. It's not automating as fast as other sectors. You know, and that's really due to the problem space. All the problems that we have are abstract. They're unique to our own organizations. There's an infinite possibilities of workflows, tools, data sources, signals that you need to make meaning of. And the workflows tend to be a bit more manual, right? They're very reactive because of that. And you've all have probably trained up a workforce. And that training of that workforce didn't come overnight. It takes time, right? You probably have some SREs or first responders, DevOps or developers that are so good at incident response, but they've been there for five, seven years. And they just know where to look. But the ones who are adversely affected are the new hires, right? And that basically means that there is a correlation between expertise, time, and cost. It's expensive. And to further complicate that, applications are deploying faster than ever before. I don't know if you noticed or not, but at Google, 25% of our code is now AI generated. So that means more products, more features, more applications are being deployed to production. But that also means that the ask for your operations teams is also going to grow. So how do you scale up a workforce and train a workforce that can maintain 5X the amount of workloads in production than you do today? Because it's going to continue to accelerate. So a few options exist. And I'm going to try to oversimplify it as much as possible for the sake of this demo. We could build perfect apps that never break, but that requires AGI and let's just scratch that off, right? Depends on where you sit in believer and non-believer with AGI. You could hire more people, but that requires resources, money, budget, but also time, right? We don't have a lot of time because we need to move fast. The third option is you could boost productivity. That same workforce you have today, how can they maintain 5X more applications tomorrow? And that requires new innovation. So for the sake of this conversation, I want to focus on option number three and tell you a bit about the mission that our team has set out over the last few years to try to solve this problem. Which is how might we reduce mean time to detection by boosting workforce productivity and incident response? And this is the mission that we've set out to solve for. So I'd like to welcome every single one of you to our journey. It's a journey with Google Cloud and Charles Schwab where we've partnered together and we've worked with each other over the last few years. We've built this new product that you're going to hear about with Cloud Assist investigations. And really, it didn't happen overnight. It's not a linear path. We failed. We experimented. We got things wrong. We pivoted a few times. And it truly happened over a few years. So in 2022 when LLMs were just blowing up, everybody was throwing AI everywhere. Some people call it the AI slop era because, you know, AI was going everywhere. At the time, Charles Schwab was just migrating to GCP. 2023, a bunch of co-pilots became available. GitHub co-pilots, the top clouds have co-pilots. You know, they worked, but they kind of didn't work. There's still hallucination problems, missing context, things like that from your data. At the time, Charles Schwab was just adopting the observability platform on GCP to make meaning from all of their metrics, their logs, their data, and all that good stuff. And then in 2024, agentic reasoning became a thing and everyone started talking about agents, right? And we had to kind of like rebuild our entire system with like an agentic architecture and try to do it again. At the time, Charles Schwab was investigating into like, or investing into proof of concepts of how to make meaning from their data, how to analyze their data better using Gemini. Which leads us to today, 2025, where we now are offering, it's part of this Google Cloud Next conference, an AI investigation tool and it's a new product launch that's going to investigate issues for you. And we're currently onboarding Charles Schwab and you're going to hear a little bit about that story. So with that said, I'd like to pass it over to Ram Gojujini, SRE director at Charles Schwab, to tell you about how Charles Schwab experienced this journey. All right, thanks Benji. All right, good afternoon, everyone. I'm thrilled to be back here at Google Next stage and trying to talk about a subject that is near and dear to everyone that is incidents and more about how AI can help managing the incidents from chaos to clear clarity. That's why we are here today. Let's talk a little bit about Schwab. Charles Schwab is one of the leading financial services in the US, which manages over 10.28 trillion assets with moreover about 37 million active brokerage accounts. Ram, is that T as in 10.28 trillion? You correctly heard Charles, it is T, 10.28 trillion assets. Let me put that into context, right? If Schwab were a country, if you rank the Schwab assets with the GDP, Schwab stands after US and China. And we do manage assets bigger than countries, so GDP, Germany, Japan, and even in India. So that is the scale we are talking about. Massive. Yes. Yes. I mean, by a quick show of hands, can you tell me how many of you have the accounts with Schwab? Wow, almost room full of hands. So thank you. Thank you for being, trusting us. We truly honor to serve you all. And our mission is simple. We empower people to own their tomorrow. What that means, you know, the systems we build should have to scale more available and reliable all the time. I mean, for us, the reliability availability is not something nice to have. So those are really key for us. These are the built-in foundation models for us. So on a daily basis, we process millions of logins and trades across the US and other, across the US. Now, let me take you to an application which really is doing this, the login for secure logins. The application we are looking at is called client authentication in cloud. We sweetly call it CAKE. This single application powers secure logins across Schwab digital platforms. That is, whether you are logging to either Schwab.com website or Schwab mobile, or even the award-winning Thinkorswim platform. This is where all authentications get in. And this application is running in the Google Cloud platform. So in case if CAKE is down, that means none of the participants were able to log into the application. That is really impacting their ability to check their balances, trades, and all that stuff. So, I mean, especially you guys, I mean, we're all learning from the past two months and even the past two weeks, the entire markets are chaos. There is a lot of volatility. You know, the good and bad part is, even though when the market is really good, that means, you know, when everything is positive, still we get higher volumes. I mean, that is an example for yesterday. A lot of volumes came in, even 10 to 14% market increase. And even there are bad days, market is really down. We see higher spike volumes. In any of these situations, right, we want to make sure our platform is really up and running. We are serving the people to able to log in and take control of their financials. It must have been a crazy two weeks considering the volatilities recently, right? Absolutely. Yeah. Too crazy. Yeah. Even most of us trying to work here as well, you know, to make sure systems are up and running. Yeah. Yeah. Thanks. Yeah. So we talked about, you know, how critical this application is and see how we are achieving the availability and the reliability of this platform. So to keep our cake humming, we built a scaled observability platform. What that means is apart from the cloud native application telemetry, we are bringing the logs from on-prem as well into a single pipeline. This gave us a single pane of a glass for monitoring and our diagnostics. By centralizing this data, we are able to create dashboards, trigger alerts, and visualize real-time metrics in a way that is scalable and actionable. We leveraged the Google Cloud scale using the BigQuery, cloud logging, log analytics, you know, some of these, all the products which are running the complex queries and we generate the billions of logs in a day. We are able to quickly find what is really happening in the logs and all that stuff. This sets us the good foundation and we are able to monitor and observe the application very well. However, but we knew there is a room. This is not everything, you know, working, especially current monitoring tools and also everything around here is more of a traditional observability, right? Let's go look at it. What are the some of the challenges with the traditional observability and how we are planning to observe those in a complex environment? Today's microservices world, there are several services ecosystem and these continuously sending the logs across the all the services. They make a lot of stunts of logs, explosive growth of logs really creating the noise in the logs. So then you lose the context of the real noise versus signal. And once you lost state, there will be alert fatigue and team team lost what exactly they're trying to do. And there is a chance, you know, you miss a real alert and all that stuff. And the fragmented insights as you have the number of logs and there won't be a not necessarily a real correlation or auto correlation is happening. It's more of a manual correlation process, which makes it harder to correlate events. What is really happening? We already talked about signal to noise imbalance and the reactive posture, right? Most of the tools today, you know, we are getting alerts when system really start degrading. Sometimes it's fast, faster degradation really put into an outage. So more into the reactive posture, lack of context. Most of the time we built several dashboards, everything. However, when there is an issue occurred, it is very hard to identify what is really happening from the dashboard. That's where you are really missing the context. Scalability bottlenecks. Sometimes the tool itself is unable to scale with the newer architecture and all that stuff. So what is the net effect of all of this? When you're dealing with some of the challenges in today's world, the net effect is your MTTD and MTTR is really high. You are unable to find the issue in the real time or in a quicker manner that is adding your MTTR. So, and why it matters, right? In incident response, every second matters. If I go back to the analogy, what I'm saying, we process like millions of logins every day. That means every second you are processing like thousands of logins. Something impacted in the application means in every second you are impacting thousands of people to take control of their financials. And also when the incident starts, the first few minutes really set the context of how the incident is progressing. When the humans are really analyzing, a person says that I found an issue with the service A. Somebody's, you know, call out. No, no, it is not an issue with A. It is B. A is the victim of B, right? There is a lot of thinking is happening around that. You may or may not travel into the right direction from the initial state. Once you go into the, we don't know, you, if you go into the right direction, you may go. Otherwise, if you went completely south, your incident response will take longer. This is where we pause and see, take a step back and talk to the Google team. What else we could do with this scale observability platform we already have? How can we enable us to for a quicker MTTD and MTTR? We said, okay, there is a lot of for buzz is happening with A and all. Why can't we take A as well? So with that said, I just want to let you know, you're going to see two demos. This next demo is what Ram's talking about. We worked with Ram and Charles Schwab to come up with a proof of concept to test Gemini capabilities. The second demo is a demo of the actual product which Charles is going to share in just a second. So with that said, Ram, let's tell us a bit about this RCA tool, the proof of concept. Sure. Yep. Okay. Here is the one. Then, okay, first we partnered with Google and said, okay, let's build an RCA agent, root cause analysis agent. And the goal of it is whenever there is an incident or alert fight, by the time our SRE team assembles, usually take five to 10 minutes. We want to get a root cause analysis done and provide a detailed report for SRE team to take an action. What that means is by the time it is already MTTD, right? The detection is already done by the agent and it is telling us what to take the next steps. And that is the real goal. And we want to start. We have designed this RCA agent using the Gemini tools and the cloud native tools. So just, you know, think of it's an AI powered Sherlock Holmes for your infrastructure. It can go detect and everything. What is really happening by the time you come to take an action. What is really happening by the time you come to take an action. Let me quickly review the architecture, right? Once the alert fights, we built a cloud run app. And the alert will go and fetch the logs around that in the context window. Like, you know, say in the past five minutes or past 10 minutes, it go get the cloud logs. And also provide the symptom like, you know, what is happening now? Maybe somebody is unable to log in. Our application is performance degraded. Those are the symptoms it provides. By combining the log and the symptom, we are sending that context to the Gemini, the prompt plus the logs. It goes and comes back with the clear detail instructions like what is the really happening? And what is the issue summary? And also detailed steps for how to remediate it. Once the results come, honestly, you know, the results are Gemini written. Some of them are really remarkable. It is ranking the order, you know, the potential root causes and how, what, with the evidence. Say, basically you are in the logs, right? It will provide the log as well. This particular log is showing unable to connect to the database or, you know, there is a query is taking longer time. It provides the details as well. So, how did we test? We haven't really tested this in the vacuum and say that is working, but we really identified the common, the real world scenarios like potential thing, right? Anyone can correlate or any app can happen. A potential database failure and an application performance degradation. Also, I mean, we are happy to be running in the GKE cluster, you know, some of the cluster performance issues. Oftentimes, whenever there is an outage, first thing we need to know, what is the blast radius? Right? You know, who is impacted? Is this my regional impact versus zonal? Or, you know, those things we want to find out. Also, how do we empower the SRE teams? As Benji pointed out, really it is taking a long time to try in a new resources or even to existing resources to go further into this one. So, we went and validated all the scenarios. Now, in the interest of time, I wish I would do few more demos here, but I have one quick demo I would like to showcase here. Okay, this is our RCA agent. The use case is database is unavailable. Sorry. RCA is database is unavailable. If you look into here, the input it is taking is just the log query, right? You know, the basic log query and the symptom is very basic, unable to log in. It went and sent this, the results. It will go under analyze, comes back. If you look into the title, it is saying root cause analysis for user login issues. The time. Yeah, here, let me pause here for a quick. Yeah, here it is notice like two things. And one is the noise from the launch darkly. Another one is application is unable to connect it to the database. So, Gemini is smart enough to differentiate the noise versus the real impact. It clearly called out the repeated connection failures are likely the root cause of our customers enable. And also called out launch darkly is more of a noise than it is not part of the critical problem. So, that's how it called out and also it gave the immediate actions. The first one is, right? Can you verify it? Make sure your database is in maintenance mode or something. Who manages the database? DBA is asking to contact. And there are, you know, several use cases, several scenarios to try out that one. This is how it provide the clear details of, you know, what is really happening and all. And also, I tried, this is all really based on the prompt you provide. So, you know, for a fun, even I tried to create several runbooks also document the workflow as well. It clearly documented how our login application is really working, you know, starting from the browser all the way to the back-end databases and all. So, let me take one quick minute and summarize the POC. We evaluated this, right? We found out that AI can successfully identify the issues within log stream as long as you provide the log has rich context of the data. And it is able to understand the workflow, you know, what is really happening and all based on the prior knowledge. One key thing is the prompt, right? Based on the prompt, we all knew AI works based on the prompt. It needs some advanced prompt engineering to make sure it is written the right results. To quickly summarize, AI observability is not something for tomorrow. It is already here. It's like a today's thing. We want AI to really do the grunt work of finding what is really happening within the system and let our engineers to take care about the restoration process. It's not more about eliminating the humans. It's more empowering our SRE teams. With this, I am passing over to Charles to talk about the real product they build investigations. Great. Thanks so much, Ram. First, I want to say thank you for sharing your challenges with troubleshooting, as well as your experience with using Gemini to help assist that troubleshooting experience. Our goal in Google Cloud has been to build an out of the box troubleshooting experience that will improve and accelerate the process for Ram and for all of our users. And to do that, our team has worked with Ram and many other customers to really do a deep amount of research and thoroughly understand the troubleshooting process. And with that, we've been able to abstract the troubleshooting process into a couple of steps and then apply Gemini to help those steps to resolve issues faster. So let me walk through them. The first is collecting observations. You have to ask the right questions in order to be able to troubleshoot. When did it start? What changed? And has this ever happened before? For any of you who actively troubleshoot or have troubleshoot before, these should feel very familiar questions. The thing is, you need to answer these questions. And in order to answer these questions, you need to be able to query the data, formulate the observations, and then generate hypotheses, which is the next step. Once you query the data and have a set of data, you need to be able to develop the insights from the data to consider as observations. And with those observations, it's quite difficult because often there's a large amount of data, there's a lot of noise. Ram talked about several of these challenges earlier. So the hard part is actually assembling this amount of data in a way that makes it easy to develop some insights. So for example, developing an event timeline so that you can understand all the series of events that happened. Or developing a graph of understanding the dependencies. For each of these, you need to be able to consider both all of the observations as well as any of the dependencies between them and their relevance to the given issue to develop a hypothesis. And this hypothesis is about what potentially caused the root cause. And so what is it about this whole process that is challenging, that led to the challenge that Ram called out, like the lack of specific context, data overload, or low signal-to-noise ratio? Well, we did a lot of research and understand that it's really hard to know where to start troubleshooting. For example, you have questions about where do you find the relevant data? What tools can you use? What queries should you use? And then how do you write those queries and what do you do to analyze that data? Then there's a lot of complexity. Benji talked about this in the beginning of the presentation. Digging into the right place requires knowledge of the application and infrastructure. How do these components fit together? What are the dependencies? What code are you running? And what configuration? And lastly, it just takes time. Troubleshooting is an iterative process. You often uncover new information as a part of your analyses that you then causes you to redo your analysis. Running through many of your steps, the troubleshooting steps, it all takes time. And this is often done at a time when you're trying to resolve an issue as quickly as possible, when seconds or minutes or stretching into hours really makes a material difference on not only your KPIs and your business and your revenue, but also your personal life, as we talked about earlier. So, to help address these challenges that Ram and our other users face with troubleshooting, we built a new product called Gemini Cloud Assist Investigations. We announced it yesterday. We're excited to talk about it. Investigation uses the context in your own project, so the configuration, any of the logs and other data that you have in your project, the resources, and then does an analysis on all of this data and generates hypotheses about how to solve and resolve these issues. The real power of this is that users with less experience can do more and they can effectively troubleshoot. And your power users, they can do even more as well. They have a superpower that they didn't have before and can resolve issues even faster. So, let's take a minute to see how Cloud Investigations actually works. There's kind of a busy slide here, but I'll kind of talk through what Investigations does. So, the first step in the investigation is taking the input to understand what happened, right? So, think back to the questions, what changed? What happened? What is it? They're all about capturing some of the data you need to start an investigation. So, for example, the time range. When did it start? The error message or description. What is it that's causing you to start this investigation? How did you know to start the investigation? And the resources or applications. What's involved in the investigation? With that amount of data, we can start the first step, which is to discover the app and the topology of all the resources. So, for any of the Cloud resources that you supply, go and look at it and see if there are any related resources. For example, if you are starting an investigation from a Kubernetes pod that's running in a node that's running on a cluster. So, being able to connect the dots between those two things, your pod could have been impacted by something that happened in the node or in the cluster. So, being able to understand those relationships is really important. If you have defined an App Hub application, which we had announced yesterday as a part of several of our sessions, we will also look at any of the infrastructure related to that application and then include that in the investigation. So, once we have widened the number of resources in that graph, we will then move on to the next step, which is to develop all the observations related to the analysis. So, we will look at things like relevant events for all of the resources. We will look at configuration, configuration changes, deployment events, logs, error rate spikes over time, log themes and how they may have changed over time. We will look for well-known errors that may have well-known resolutions. And then we will even look for any service health events that might be impacting your environment. Now, with all of this data, we're able to generate observations. Now, we shouldn't just take all the observations because some of these may not be relevant. And so, the second step here is to actually rank those observations based on relevancy to the specific issue at hand. Once we rank and filter these observations to the most important ones, then we feed it to the AI agent, the AI troubleshooting agent, which will then synthesize all of this data and come up with hypotheses for the root cause. We'll also recommend, come up with recommended fixes for each of these root causes as well. So, if there's a, you can start this, if there's any other resources, by the way, when you go through this investigation, if there are any other resources or any other insights that you gain when you start an investigation, you can go back and add that to the investigation and rerun it. If you think back to the troubleshooting steps, iteration is one of those important steps. And so, investigations support iteration by adding additional resources and rerunning the investigation. You can start this investigation from logs. So, anywhere you can find logs in the Cloud Console, whether that's in the Logs Explorer, if you use that, or in other places around the Cloud Console, like GKE pages, et cetera, you can start an investigation there or you can start it directly from our new troubleshooting page. So, with that, does anybody want to see a demo? So, we're excited about showing you. See the demo. Awesome. Great. Benji, over to you. Talk us through Cloud Assist investigations. Some like to say it's too good to be true, but here we are. So, with that said, I'm going to show you a pre-recorded demo. We're not going to do live. I don't like live demos anymore. With that said, so I'm just going to click play. So, we're going to start off in the Google Cloud Console. As you can see here, there's been an alert that's been triggered. What we're running is this movie recommendation app. It provides an LLM, provides recommendations. That's what the app does. An alert has been triggered on a custom metric that I've created. Now, what you can see here is there's a bunch of labels around this alert, but you can also see that this application was onboarded. I'm not going to get too much into the application details, but there is another session on the application platform that's being launched on GCP. You can actually onboard applications and configure them across different GCP resources. But in this case, we've already onboarded this application. I'm going to go ahead and click this application icon over here. Applications can span across projects. And what you can see here is a list of services and workloads. I just know that the alert was triggered on that server. So, let's go look at that server page. GCP has a bunch of out-of-the-box dashboards. You can see who owns this component from a business or even an engineering perspective. You can also see some golden signals. You see a bunch of charts. All the charts in the console have these event annotations. This is something that we released in the last couple of years. These are basically log-based events. But what you can see here is there's a bunch of pod crash events. But you can also see that network traffic and memory and CP utilization are having some type of regression at some point in time. So, I'm just going to quickly zoom in and say, hey, what's going on around here? Now, I'm going to scroll down. We have a bunch of logs. But for the sake of this demo, let's go over to the logs explorer. So, the logs explorer gives us a view of all the logs. We filtered down to the warning logs. And there's a bunch of back-off restarting failed container logs. This basically represents that pods are crashing in GKE. If you're running GKE workloads, it's a typical error. But what was interesting of what just happened here is these logs all have this investigate button. Right? And this is not just in logs explorer. This is on every dashboard in the console. For now, we're just talking about this specific entry point. But you could start an investigation from the investigation UI starting from scratch. You could start it while talking to Gemini directly in the console. And then in the next month or so, we're going to be able to trigger them on GCP alerts and provide an API so you could bring them into your own systems directly as well. But for the sake of this demo, we're just going to trigger it from this specific entry point. And what happened was all the context of that log was pushed into this investigation form. As Charles mentioned, we have inputs. The inputs are time, right? The error message, and a resource. I don't have to do anything because the form already took it from the log context itself. All I have to do is just review it. Maybe I could add some context, add some instructions, and hit create. In this case, we hit create, and we have an investigation. So the investigation opens up in a new tab. And when we look at it, we can see what comes back. What this basically is, is, you know, the reason Charles was talking about the troubleshooting process is, we've tried to build something like this, but we really needed to understand, like, what is an investigation in real world models? And convert that into a product model. So all these symptoms came in, and we returned a bunch of observations. This investigation typically takes about two minutes, is our like 98th percentile latency. So it's, you know, it's pretty quick considering what a human operator might take. But if we look at these observations, we can see the first one is a deployment failure. We see some auto-skilling problems. We see some container restart failures. And there's a few others as well. But what you can also see is that these observations are coming from various data sources. Some came from logs, and this last one actually came from a configuration diff analysis, meaning a config has changed over time, and Gemini was able to pick that up. That's because all the GCP resources exist on GCP, Cloud Asset Inventory, and we're able to analyze that data source. That's the Cloud Asset API. And there's also the Cloud Logging API. And there's many other APIs that are being onboarded as well. Each of those observations provides a summary of how Gemini is finding it relevant to the investigation. But what's interesting is actually this hypothesis that Gemini was able to come up with. Gemini was able to take in all that context, analyze those observations, rank them, push them out, and then provide a root cause analysis. So the hypothesis that was provided to me is that some deployment is causing a failure in this application. Some of the recommended fixes that are provided here are based off of what Gemini knows, right? We've seen these issues before at Google. We have world knowledge. We have public docs. There's Google search as well. But what it's telling me is I can add more memory to this application, which might fix it pretty quickly. But I can also go and review the deployment itself. Now, what I can do next is, you know, I want to verify this hypothesis because no one's going to trust a root cause immediately unless you see the evidence. So I'm going to go ahead and take a look at these observations a bit more. And what's interesting is that each observation is associated to your data. So I can click from each observation, go directly to the raw data. So I'm going to click that deployment because it looks suspicious. And I know we made a deployment recently as well. So that observation was able to find three deployment events that occurred in the past. So these deployment events are available in the logs because logs just store events really well. And what I notice is this deployment actually has a commit URL. Now, because we've onboarded this application to the application platform that I mentioned, we also can collect the deployment events from our logs if your logs have deployment events with commit IDs. So I'm going to copy and paste that get URL. And just like that, I'm able to basically correlate the code diff pretty quickly and figure out what bug was actually causing some type of memory consumption event, memory leak, boom kills, pod crashes, and then eventually being able to get to the code diff. And that's basically the RCA of what this product can do. Many of you are probably going to say it's too good to be true, right? So, you know, it's, I'd like for you all to figure that out for yourselves, right? Because every company has different data and you really have to test it as well and give us that feedback. But what I do want to mention is what happened at Waze two weeks ago. Waze is a Google Maps app. It has a bunch of social features. And this SRE pings me on Sunday. They had an outage at 11 a.m. This Waze SRE just onboarded onto this tool. We give it to all of our, we've given it to about 300 of our support engineers, all of the Waze SREs, to basically use this product internally at Google. And this Waze SRE was able to use the product. He was able to get the root cause analysis within three minutes. Within one hour, they fixed the issue. 45 minutes of that one hour was just procedures and process. And they fixed it. He sent me a message and he said, hey man, like, thank you. You saved my weekend. Otherwise, he would have lost a Sunday. What did he say? You saved their weekend? He saved his weekend. You're going to save hours too? I'll save all of your weekend if I can. All of hours? Yeah, I'd love to save everybody's weekend. Let's give our time back. And that's what I opened up with. It's like, there is a human element here, which is your time's important. And it's truly my belief that this product can save people's time, boost your workforce productivity, and really get us to the point where, you know, when we're responding to issues, it's no longer eight hours. It's just an hour, right? And that's not what we wish for. So with that said, I want to hand it over to Charles, who's going to talk a little bit about the product launch before we wrap this up. I just wanted to say a huge thank you to our teams that worked tirelessly to tame the complexity and bring you the investigations. Very excited about it. So a couple of ways that you can engage with our products. It's now in private preview. Take a picture of the QR code. They'll be available. Also stop by our booth in GCS 23. You can interact and see a demo of it as well. We'd love to get your feedback and understand and work with you as you try out our investigations product. So with one moment, if you'd like, if you want to join the private preview, I recommend taking a picture of this QR code and then we will be in touch with you all. So Ram, if you want to. I would like to take one quick minute. So thank you for everyone from the Schwab, you know, helping us to build these tools, including several teams, you know, many of us here. Thank you for joining us here. And we are hoping to do more better, you know, to bring our brand better and, you know, improve availability. Thank you to all of the Schwab team and for you for presenting. Thank you. Thank you team.