 Hi. Good morning. Good morning. I assume that you were all up bright and early at the 8 AM, and so now you're like at least a cup of coffee in if you decide to spend, you know, 45 minutes of your morning in line for coffee. Did you already have your two cookies this morning, Steph? My what? Do you have cookies? I know. I ate lots of cookies yesterday. Anyway, no, I didn't. Not yet. It's after this. Anyway, hi, everybody. I'm Steph. This is Nav. Hey, I'm Nav. I've been at Google for 17 years working in security fraud and abuse, safe browsing team, building systems to detect malware and phishing, and then reCAPTCHA for bots. But, you know, I've been having a big career on fighting attacks as they're happening. But now I work in cloud security and security command center where I get to spend my days trying to prevent attacks from happening in the first place. Aren't you a developer who does things that are introducing security risks? I don't know if you folks saw me in the keynote yesterday. Introducing security risks? Yeah, yeah. That's me. But that was my foray into cloud security. Steph, don't you have an interesting story on how you got here? Oh, I also, I was working for a large fintech that experienced a data breach, pretty high-profile data breach. And that was my foray into security. I come from a background in user experience design and research. And when we were doing the retro of how that perpetrator got into the system, it turned out the signal was a thousand pager duty notifications went to one human. And I thought, that shouldn't be the signal that somebody who's working really hard to secure their teams has to rely on to know that something's really wrong. So over the last five years, I've been leading the cloud security UX design and research team. So part of the, you know, what we're going to be talking about today is some of the changes that we can expect in, I think, this new exciting era of agentic AI. Some of the changes we've already been seeing, bringing generative AI into our tooling. And so I'm pretty excited to get into that with everybody today. And in fact, if you've been here for the last couple years, you maybe have been coming to these, like I know Alex knows this, you've maybe been coming to these sessions. You may have seen this slide where we talk about shifting from the manual toilsome workflows that a lot of our security practitioners have to deal with today into sort of level two assisted. And so we want Gemini to be alongside all of the existing security workflows to do the things that LLMs are really good at, you know, summarizing metadata on cases, translating natural language queries so that you can look over all of your event data without having to type in the specialized query. These are the things that are going to enable us to sort of uplevel a talent. But since last year, we've really been focused on trying to get to this state, semi-autonomy, L3, which is, this is the agentic AI era, where we want Gemini to be meaningfully precise, so that the security teams can confidently offload some of their tasks to AI fully. But because it's security, we definitely want to make sure that it is right, it is precise. And that's why we're stopping at semi-autonomy. We want the human in the loop. And you're going to see a little bit about that today, too. We're not worried about getting to autonomous yet. I will give a hat tip to Bashar Abbasuedo at the CISO of Charles Schwab, who, when we were on stage last year, he was saying, no, no, I want you to get to autonomy. I would like to have more of my things just happening autonomously. So maybe, maybe we're a little conservative in that regard, but still, semi-autonomous is what we're, what we're trying to, to get at. And this is the time to do it. We are at an inflection point. Now it's going to get into some, some detail about what our agents, we were thinking about agents, and then show some of the agentic work that we are announcing across the board today. But from a user experience perspective, I'm particularly nerdy about this because it means that our UIs are going to change. In fact, they already are changing. Because unlike today, where I might have to go into a tool in order to monitor what's going on, tomorrow, an agent will have already done investigations for me. And now I'm coming in to check what's happened while I was sleeping. In addition to that, an agent might start making different decisions as it learns and grows. And the UI is going to have to accept different kind of material that's being generated by that agent that is getting smarter about how to get work done. And this presents some pretty exciting challenges from user experience perspective that I get a little nerdy about. And so I'm going to, I'm going to invite you to get nerdy with me a little later when I get into a little bit more detail about that. But all of this we welcome because we have been over the last 20 years struggling as security practitioners with what we've been calling lovingly the three Ts, threats, toil, and talent. We really want to be able to use AI to, you know, tackle the exponential explosion of threats, to reduce the toil of all of the different tools that somebody has to use in order to do their job. Because, for example, with Gemini 2.5, just being able to process massive amounts of disparate data and normalize that and synthesize that and describe that, all of that is what is at our fingertips now. And so we can reduce the toil required to do work. And then really to up-level the talent, we want to free the practitioners. We know we're winning when we are freeing those specialized practitioners to focus on the things that they probably get out of bed to do every day, the mission-based stuff, to catch the bad guys and not do the 100 alert triage again that they are required to do every single day. But I want Nav, if you would be so willing, to talk a little bit about what agents are. Sure. Can we just go back to the last thought? I just want to share a story before we move on to take it from where maybe it's a little abstract for people and make it more real. We can all empathize with some of what we're talking about here. So I worked at a startup for a little bit in that 17-year span of here. And we had Sneak that was connected to our GitHub repo and scanning for software vulnerabilities, right? When I got there, there was a long list of volumes that people had added to the allow list. I'm seeing people head nodding already. They're like, yeah, we've been there, right? That's the left-hand slide, the massive backlog of things you have to triage, right? And I was like, look, I came from Google security, right? I know how to do security, right? I'm going to fix this. I'm going to take one of these things and I'm going to resolve it. I'm going to set this culture up, right, so that we don't have vulnerabilities and everything, right? So I had the right mindset. And I thought it was going to be, let's bump the version to the version that doesn't have that vulnerability, right? Sounds simple. I did that. But what happened? The build broke, right? Because there was something else that depended on that. So I had to, you know, play this spiderweb game of trying to find the right... People are nodding, right? They're empathizing. This feels good. I feel cathartic, like, sharing this story with everybody. And then I had to start changing functions because signatures changed in these things, right? And the unit test broke. And I swear, I just got to a place where I felt like this was impossible to do. There was no combination to make this work. So what did I do? I added the CVE to the allow list, just like everybody else. And it's kind of squinted and said, yeah, I don't think this impacts our application, you know? And then I just moved on, right? So, like, that kind of toil, I think people can empathize with the room. And so I'm really hopeful where ages can take on that work. Because I think at the end of the day, it was possible to refactor the application. I just didn't have the time to do it. And there was just one of many threats that we wanted to do, right? So let's see if we can make progress towards helping people like me and you fix those things. So first and foremost, we want to, like, offload those tasks. The exact thing I just talked about, bumping the version, having agents fix that spider web of vulnerabilities, refactor the app, get to a place where the build is working, the tests pass, the application not breaking, and we have less phones to focus on, right? And they can do that because they're trained to do work like security practitioners do. And you'll see examples of that in just a few minutes. But as we have agents starting to do the work, I think there's a big question that, you know, Steph, you and I talk a lot about is, like, how are we supposed to interact with these agents, right? Something I say a lot is that, and we're going to see this theme, I think, through today's talk, is, like, I feel security products today, for the most part, are, like, work-to-be-done dashboards, right? You go to this tool, and it gives you a list in various ways, tables, forms, findings, things like that, or threads. And if you don't do anything, nothing is happening. That list is just getting bigger and longer, right? So it's work for you to be done. But what happens when these agents start picking up that work and doing it for you? What does the experience, you know, how does the experience change, right? Maybe they become work that has been done, dashboards, right? But in that world, you know, is it still tables and lists? Do you still go of the tool every day? Do you get emails? Do you get summaries? Things like that. So I think we're going to talk a little bit more about how the experience might change, right? And the reason we're doing all of this is to get to these exponential outcomes, right? And I think we're already starting to see some of these really interesting outcomes with just the assistive kind of stuff that stuff I was talking about earlier. But at the end of the day, if we can do those things, we're going to get to a world where you're never drifting from your ideal state, right? What that means in practice is that, like, if there's a misconfiguration or vulnerability, an agent picks it up, kind of remediates it, makes sure your application and your infrastructure continues working as expected, sends you a PR, and you move along, move on with your life, right? Creating threat detection rules, right? You have to stay on top of threat intelligence, understand if this threat actor is focusing on your vertical, right? Are you in finance? Are you a bank? Does this, do I have the logs necessary to create the rule rule for this? Test the rule, make sure these threat rules are, like, maintained and working over time. There's a lot of toil in just these first two things, right? But if we can, you're going to see agents in just a few minutes that start chipping away at these kind of things, right? Investigating threat alerts, right? So you first, you're creating the detections in the first place. But when they fire, you know, do you have, like, a team of people that are reviewing and sifting through that noise to figure out, is this something that warrants investigation? Do we have a breach here? Do we need to call somebody? Or is this some noise? Like, somebody downloaded a file that actually is not malicious and we can move on, right? And then writing playbooks and things like that, right? So I'm super excited about this future. And I think, you know, a year ago, we started, like, asking ourselves what the future of AI would look like. And we would say things like this, and it sounded like that's 10 years away or something like that, right? But the progress that we've seen in agentic workflows and stuff that we're going to show you today makes me feel like we're actually, like, getting... This is very realistic goals for us. And I think there's also just, you know, hidden in there... You don't have to go back. I don't know how to go back on the clicker. We know how to build products, but not use clickers. The UX on this stuff. There's hidden parts of that, too, which teams just don't even have time to pay attention to, like hygiene of a playbook, hygiene of whether... You know, what are the governance controls on detections, which things are obsolete, which aren't... All of that stuff is, like, on top of other work that teams are already having to do that is taking higher priority. Sometimes you have those things that are obsolete, out of date. They're not... They're actually opening you up to risk. These are the things that if they're always on monitoring, the agent knows what the playbook or what the detection is trying to accomplish and is able to reason about whether or not that thing is achieving the goal, that governance, actually. Hygiene becomes part of all of this, which is a pretty exciting... Which is a pretty exciting outcome. Oh, that's right. Yeah. So... As Nav said, we're going to keep working on assistive capabilities. In fact, this is one that we're announcing today in Security Command Center. Now you can ask Gemini to assist you in creating a hardening policy. And this is the kind of thing that is similar as we're talking to sort of code generation. You might use a code generation AI to move the ball down the field, so to speak. Apologize for the sports metaphor. And you have the same sort of thing here with policies. To be able to create the kinds of organizational policies without having to spend hours or knowing as an architect, as an admin, knowing all of the different parts of your environment. This is the kind of stuff we're going to keep investing in. Because it's really critical to assist the practitioner. This is the kind of stuff we have been investing in. This is Gemini on the right. This is our Gemini and SecOps multi-turn chat. Multi-turn chat, of course, is a pretty ubiquitous part of the generative AI UIs these days. And what's happening here is the user is putting in a natural language input. And Gemini is translating that to the detection rule, to the syntax of the detection rule. And there are some pretty cool unintended positive outcomes that we're learning from this. This is all still a very high experimentation zone that we're in and will continue to be in. But I just want to play this short video for a second. This is Etsy. They had migrated all their detection rules in just a couple weeks' time. But I'll let Etsy speak for themselves here. Wait, is this the play or is this the... No, this is one of these. AI, how that has helped you both in the migration and the post-migration. Yeah, it's funny because you even said that. But when we first heard about AI, we're like, oh, everyone's talking about AI. Of course, we're going to see this in the marketing. But we did actually find it to be very, very useful. And I think the thing we specifically like it for is you can use the AI to build detections. Like, I can give it a plain text English of what I want the detection to be. And it'll come up with the structure of the detection. What's nice about this is obviously you're going to have to make some changes. There's certain fields that you may need to extract, certain complex logic you may need to add in. But, you know, 70, 80% of the work is already done for you. So it saves you time. But the other great thing is it adds a lot of uniformity to how everyone on the team writes detections. So one thing that I found in the past, and I'm sure a lot of teams find, is just like with code, a lot of people have their own personal style and how they write detections, right? Maybe one person likes to extract all the columns first and then do the logic. Maybe I like to do the logic first and then kind of do the specific extraction of columns. But what can happen from that is when I'm reviewing someone else's code in our detectionist code pipeline, I might be a little confused at first. Oh, how are they approaching this detection? And then it might lead to me not catching bugs that I otherwise would have caught, right? And then it might be a little bit more of a problem. But I think that's the kind of thing that I'm trying to do with the detection of the detection of the same way. So when I see how someone else has written their detector... Yeah. That's the kind of thing that as a researcher, I was like, oh, well, of course, that makes so much sense. But it wasn't an intention to actually standardize all the detection rules. But that was going to be an up level for talent. And that was a pretty exciting moment. And I think we're going to have more of those even on the assistive level. But Agentec, as we've talked about, I think really has the power to transform security, to completely offload some of the work that folks are having to do today so they can focus on the really complex stuff. You know, Steph, the other day I was talking to another company that used that same assistive UI. And they shared even additional outcomes that we didn't anticipate either, right? So, like, Etsy talked about the detections all became kind of uniform, which made it a lot easier for the team to review, get to speed, and things like that. This other company talked about the fact that they had people all over the world speaking different languages. And now they're able to use the same tool in their own native language to get there, right? So it was like we're already starting to see these exponential outcomes, right, in these kind of assistive UIs. But let's move on from assistive and, like, let's talk and let's jump into what everybody kind of got here for, right, is agents, right? And we're going to talk about three agents today. We're going to show you demos of each one of those. And then we'll talk about the implications of those going forward. So we have an agent for investigating threat alerts. It's the Gemini investigations agent in Google SecOps. So as a threat alert comes in or multiple threat alerts and a group didn't get into a case that, you know, we talked about investigating the noise, this is what the agent does. The agent will take a case, do some investigation, and come to a conclusion at the end of whether or not this is likely a false positive or needs to get escalated to, like, a tier two or tier three analyst. And we'll show you a demo of that in just a moment. We have cloud security. You know that we said imagine a world in which you don't have to manage your posture, where it's kind of being you're not drifting from your ideal state. We're starting to make progress with that with an agent that will be able to remediate certain types of cloud posture violations, if you will, like identity, for example. Identity-related issues. And propose changes in your own Terraform code and validate that the code is going to work before it sends you a PR in a way that's not going to break your application, right? And then finally we'll have a threat investigation or threat intelligence, where we show an agent which will investigate a potentially malicious binary, which is a really, really cool demo. I'm very excited to show you about it. But again, comes to a conclusion at the end, is this likely malicious or is this a false positive we can just close it? Again, removing noise and allowing the team to get to the exponential outcomes, right? Getting to focus on the most interesting part of the job. Okay, can we switch to the... Look at that! It's starting to play already. Oh, yeah. Great, great. That was a very smooth transition to the back. Okay, so what is happening in this is there's a piece of software that was found on an endpoint that is a common tool used in like post-exploitation, right? But it itself doesn't mean that it was an exploit happening. So what this agent is doing is like performing additional queries in the sim to figure out what happened on that same host leading up to like the download event, right? And using that additional data and additional context is able to make a conclusion that this actually is a potential breach and it needs to be investigated. So it's really cool. Do you want to add anything to that? Yeah, I was just going to add too. This is all baked into the existing SecOps case workflow. And this is part of the principles that we want to infuse Gemini in existing workflows. Even though we're going to have to change some of these UI patterns, invent new ones, and it's all very experimental. Right now you're operating on a case. The investigation was done for you on the alert. It's rendered here in your case view and you're able to interact and look through the investigation timeline and see all of the decisions that that agent made. This agent is being trained by Mandiant analysts on how the analyst would triage this particular alert. So that's the sort of specialization that we want to both bring to the agentic experience and also infuse in the existing tooling so the practitioner doesn't have to go learn something else about now that an agent is part of their team, so to speak. You see a pattern in all the agents is that they leave this audit trail of what they did and why they did it. Ideally, just like a human analyst would have, but not always it does, right? But here it leaves this trail, as you can see. And what's cool is we're not actually watching the agent do the work, although you could if you wanted to kind of sit here, but the agent moves to the case so quickly. It's like Steph said earlier, this agent is operating while you're sleeping, right? While you're having dinner at home with your family. It's doing this work. You don't sit in the console and like vibe, you know, vibe this thing. You're just doing in the background, right? Vibe agent? Yeah, vibe, I don't know, vibe investigation, right? It does, yeah. Okay, let's move on to the next slide. The next agent, it says you don't have permission to play this. One more, click. One more? Yeah. And... There we go. This is the cloud security agent. And this is the one I talked about earlier where we're remediating cloud posture kind of violations. And this one is about identity. So there's a service account that has permissions it doesn't need, right? It's overprivileged. And it's leading to risk. So what the agent is able to do is identify the right Terraform code to change. And it does this. It's not as simple as it sounds, right? Because people write Terraform in different styles and they have different templating and hierarchies and things like that. So if you just throw your Terraform into like a ChatGPT or Gemini and ask it, it's going to take a kind of a guess, right? It may not work, right? What we do is we are able to use the state of your cloud, the state, the TF state. The agent is able to use the state and pinpoint exactly the code that it needs to modify and then test it, right? So that's the agentic workflow here. It doesn't just propose the change. It actually validates it. And if it doesn't work, continues trying to make the change and find it without bothering you, without you being there until it gets to a point where it can validate that the plan that's being created by Terraform matches like the goal. And then it will send a PR to the right person, the right developer, in the tools that they're already using, right, in these PR review tools. They don't need to hop in the security console to understand what a security finding looks like. They just get a PR that says, like, this is a test we ran. It's passing in your CI CD pipeline. Security is better now. Just click on this thing. And that's, you know, Steph, earlier you said, like, autonomous versus semi-autonomous, right? We call this semi-autonomous because there's a human at the end that has to approve the PR. I'd argue this is actually autonomous because this fits exactly how we do work today, right? We don't just, like, hop in the console and, like, click a button. We send PRs and have a review, right? So I guess the make this fully autonomous is that there's an agent on the other side that would review this PR and say, like, let's go for it, which, you know, is a future we could get to, right? But that experience, too, similarly, I mean, this is in Security Command Center. And we were just looking at all the findings of overly permissive findings that were being generated. And Gemini is in that interfacing. We have identified these things while you're sleeping. Here is what you need to do, having done all the work. All of the context of that finding and the code fix, as Nava was saying, is then provided to the developer. And this became, this also came from research where a security team is saying, like, the hardest, a lot of times the hardest part of my job is to find the right person to do the fix and then get them the information they need to do the fix. And they got to trust the fix or they got to look at their email or they got to look at Slack or whatever that is. And so we also think that there's, you know, a lot of power in agents really bringing to bear what we hear over and over again, but have a hard time doing, which is make security a team sport, really. That there are people who need to do security who, it's not their day job, it's not their primary, but they're really required to fortify the organization alongside those security teams. And we think that agents in being able to bridge some of those things and even do some of that work, maybe we just start with some of the lower risk, you know, high volume things. And we get better and better in building trust to be able to get to this point where we're just offloading to the agent things that might take two or three different people across an organization to do and days in some cases that leave us open for attack. And all of this agent is doing a change in like Terraform or IC, right? It's the same pattern we would use to change applications, right? So the example I shared at the beginning about bumping a version to remove a vulnerability from an application, it's the same agent that would do that work. All right. Now the third agent around threat intelligence. This is an agent that is investigating a file that was found on an endpoint that triggered some sort of signal. The question the agent is trying to answer is like, should we be worried about this thing that this user just downloaded? Is it malicious or not? So what is happening here is that the user is uploading a piece of file to analyze it. And you're seeing the agent go through these steps, right? And has access to various tools that a reverse engineer would have access to. And like Steph said earlier, it has those tools because we trained it with the knowledge of like these Mandiant front line researchers who are doing this work, right? What this agent is doing is a combination of static and dynamic analysis, right? So it will like spin up a VM and run the binary inside of it to collect signals if it needs to, if it decides it needs to. And it may do that if it doesn't get enough signal from static analysis, right? In this particular case, what's happening behind the screen is that the binary is obfuscated. So the agent recognizes that it's obfuscated, recognizes the obfuscation technique, generates Python code, which gets run in a sandbox to de-obfuscate that binary, that content. Inside the de-obfuscation is a PowerShell command that shows like downloading a file that creates a reverse shell. And at the end it comes to a conclusion like, yes, this is indeed malicious. Here's the URL, the command control center URL and everything, right? And does it like within a very short period of time. Really cool is that this is the productization, if that's a word, right, of the research that we did last year. And you can read about it if you like do Google search for like a WannaCry Google or WannaCry Threat Intel, where the WannaCry binary was analyzed in the same flow, but manually. We turned that manual process into like an agentic flow. And it was without being trained ahead of time on this binary and the specific, you know, this threat actor, it was able to get to the conclusion that I think the reverse engineering team took like, what, seven hours or something? Seven hours, yeah. Seven hours to get to. Which is super fast. Which is still pretty fast for a human team, but it did in 34 seconds. And it was able to pinpoint the, you know, the command and control URL and conclusion that it was malicious. And this is also pretty exciting too, because this was a research, this was research we were doing last year about whether or not we could get it to work on Gemini 1.5 with like a 2 million token context window. It might only have been 1 million token context window at that point. So now we're up to 2.5 already and this is in product and it's doing more than we were doing a year ago. So just the pace of innovation too and what's at our disposal is really exciting. A little nerve-wracking sometimes that we're like, we're not moving quick enough to be able to stay on top of what's really, you know, going to be vital to get in the hands of the defenders. So I'm like very excited about these and I get excited sharing them with the audience, right? But they're different than what we've been seeing on the coding side, right? Where you hop in a console and you watch this agent and the text is streaming. You can see this stuff happening, but they're working in the background while we're sleeping, coming to conclusions. You know, maybe there's many PRs that are sent through the evening, many binaries and cases that are reviewed. And I'm wondering, Steph, like as we move from this world of like work to be done to work that has been done, how's the UI and our interactions with these apps going to change? Like do I need to go and log in and look at these things every day or how do we think about that? Yeah, I would not. But yeah, this is a little of what Nav was saying earlier today. In fact, that investigation that we were just looking at from the malware analyst yesterday, we were in a CISO event where we were talking about it there too. And one of the CISOs afterwards said, you know, if the verdict is the most important thing, why is that not at the top of the view? I was like, why is that not at the top of the view? Because they're weird. All we have to do is just change it to be like sort descending instead of ascending from how the investigation goes, right? But these are the kinds of things that are going to change our UIs now. That investigation history, that lineage there is really critical for two reasons. One, to build visibility or to build trust by having visibility into the decisions that the agent was making. And number two, which is not part of what you saw in the demos yet, but is, you know, something we've got to design for. We have to enable the controls to be able to allow the future security practitioner to be doing tuning of these agents. How, where did you, where did that agent make a decision that you would have made a different decision? That kind of level of fine-grained access or fine-grained control, I think, is going to be the future. A lot of what security practitioners may be doing instead of actually doing the investigation themselves. And that's, that can be really nerve-wracking or really exciting depending upon who you talk to. But it's going to move us away from these dashboards of widgets that are really displaying all of the event data, all of the information that's being sort of correlated that you've, you've got streaming, that you've got different relationships that you might not know about. These widgets are really sort of there to surface them in different visualizations. And now we're going to get into much more dynamic UIs where the agent is going to have to, let's say today, maybe generate the information in a table. And tomorrow might create a podcast for you instead, because that's the best way to convey the complex information. And that's what generative AI, like that's the potential of what we have to, what we have at our disposal that we're going to have to design for in these new containers. Steph, are you saying that like in the future, instead of me hopping in the console every day, I'll get a podcast so I can listen to you? Do you want one? Yeah, I would love to listen to a podcast in the morning. Let's go ahead and start that, yes. And I want the podcast to say, everything's all good. Like, vulnerabilities are down. You don't have to worry about that. Very short episode. Zero day that everybody else is worrying about. Just enjoy your run, you know? Yeah. So these are the sorts of things that we are in the user experience side of the house, really focused on right now, creating UIs and containers of UIs like multimodal chat that are really going to take generative content that needs to change over time. You can build out the next few. The content itself is going to be relevant to the individual, like Nav might want podcasts. But all of this, I think, under the hood, all of the agents, today we might have three, tomorrow might have 30, 300, 3000. The worst thing I think we could do is require any human, but especially security practitioners, to have to know and interact with the agents individually. Because now you're having to do all this orchestration, which is kind of a similar situation that we're trying to reduce right now, the toil across all these surfaces. Which is why having an abstraction layer like a Gemini, like a multimodal chat, having the UIs, having the AI infused in the existing tooling is really critical. So that way, all of the agents can be, as you maybe heard yesterday in the keynote too, the agent-to-agent protocol, you may have heard of MCP as well. So this kind of under the covers orchestration is going to allow agents to communicate with each other, is going to, I think, going to really require the level of abstraction like a Gemini, like a multimodal chat, who's sort of the brain, the voice through which all of that can be filtered. And as I said before, an agent, because the agent is going to get smarter over time, today they, the agent may make a decision of how to render some information, that tomorrow there's some new way of rendering that information that is going to be easier to digest for the human. And the containers, the UIs, are going to have to be able to accept that. That's very different than the UIs today, where I say, like, you know, here is the slot, here is the widget where you have to put this exact kind of data. That is, that is not going to be our future with agents in our interfaces. In fact, you were actually testing some concepts before. That's right. So, that going back to the idea of like work to be done to the future, that work has been done, still kind of playing in the same like space of like there's a web browser and a web page and widgets and things like that, but the widgets change. This is a mock-up of what, you know, Google Unified Security could look like in that future, right? So, it's actually like Gemini is saying, look, I've done these things. These are the cases that we've been, that I've been working on. This is what the mean time to Clojure is. And maybe there's a place where it's like, oh, these ones we need to escalate, right? Or this thing is not configured correctly, right? So, like, it's basically exception-based for the human to get into the console and try to do work. Otherwise, it's a report card of like how the world is progressing, right? Those rules, the state, the investigations, everything we talked about, and those agents, they're working in the background. You don't need to go look at them. But if you want to go deep down, you can go see all the audit trails of what they did. But the high-level view is like progress over time and movement, right? And then you can imagine this thing being turned into a PDF that you get on Monday morning or really a podcast that you could get like a 30-second recap or what is the things you need to work on this week, that kind of stuff, right? But... And this is a bit of a step change concept for what we have today. It's still a dashboard. These are still widgets. This is Gemini exporting information in a more static way. But this is the beginning of using what we have right now and starting to get to this place where agents are really generating more of a dynamic UI. Yeah, this is just one of those widgets kind of zoomed in a little bit more. But, you know, we... You know, Steph, you and I think agree that right now we need to like continue building trust, right? To show that these agents are doing what you want them to do. Give that transparency, right? And maybe over time, you are trusting that the work is being done so you can kind of stay at the high level. But for now, we're including this concept of like audit trails, if you will, right? So the... Investigation timeline. Exactly what the investigation did, the timeline. Just like, you know, a really good analyst or cloud security person would do on your team to kind of show the work that they've done, right? And this is what this is showing. Here's all the findings we've found. Here's the ones we send PRs for. Here's what's left. Here's the PRs that are out there. And even an estimation of how many hours we've saved for the team. Yeah. Super exciting stuff. And this is... You know, when we actually show this to customers, Steph, it was before we had those agents. So they were kind of like, this sounds amazing, but it's like super futuristic, right? And now they're like, yeah, so this is what I want to see now. Yeah, this is actually what we're designing now. Yeah, yeah, exactly. But it's still very early days, right, Nav? Oh, yeah. I mean, like, it is so early that, you know, we just really don't know, right? And that's what we're doing. That's why you see a lot of experimentation, these kind of like agents on the side of a product where you go in and you watch it do its thing, but it's not incorporated. But the agents we showed you, like Steph said, are incorporated into our product, right? But we've been jamming on this idea of podcasts and PDS, but we just really don't know yet. So we need a way to think about that, Steph. It's like, I'm not a UX person. I'm an engineer, right? So I can figure out how to build the agent that does the thing. Yeah. But I have no idea how we're going to, like, what the future interaction of these things is really going to look like. Yeah. So like, can you inspire us with your expertise? I don't know. That's a tall order. But I'll say, like, some of what I've talked about already, where Gemini are an abstraction layer that's sort of doing the orchestration of the agents under the hood. Containers, I think multimodal chat is a perfect example of that that can accept generative content. All of that stuff is sort of, we're in that moment right now. But fortunately, we have done this before. I know we can do it again as an industry. This, 20 years ago, Jesse James Garrett wrote a paper, coined the term Ajax. That moved us from having PDFs that were, you know, published on online to being an interactive web. Ten years ago, a major designer, Dave DeSandro, came up with a whole bunch of mobile patterns because it turns out what a human wants to do on the go is very different than what they want to do when they're sitting at their laptop. And also, what a human can do with their mouse is very different than what a human can do with their thumb. So pagination, something that existed on the desktop, couldn't exist. You couldn't get your thumb or your finger to hit the thing on a mobile device. So things like endless scroll started showing up. And these are the kinds of patterns that we're going to be in right now, again, with agents, again, sort of just being infused throughout these UIs. But our teams are going to have to look different too. And a big part of that is data scientists who might be over there normally sort of, you know, on some special project are now part of the UX team. They're now part of engineering teams. They're the ones who are coming up with the precision and the benchmarking and making sure that the agents are hitting the quality expectations that are the primary experience now. Like the quality and precision and the output, a lot of this is straight content, is a lot of the experience now. Just pretty exciting stuff. I already mentioned some of the new UI patterns that we've got to come up with to get to this place where a lot of what you might be in the end of the end of the day. And so maybe more of the controls is, are built into seeing what the agents are doing and tuning the agents rather than interacting directly with the kinds of workflows that we have today. And finally, having telemetry across the board through all of these workflows is going to enable us and the agents to understand how NAV might triage an alert. If we are actually paying attention to how NAV triage is alert and that is directly accessible to the agent, then the agent starts to get better at knowing how to be NAV. So NAV can then start focusing on other things. But you don't want to train on me because I couldn't fix the bug in the beginning. No, maybe not NAV, but probably. But in the meantime, we've come up with a framework. This is an autonomy framework really been focused on, again, assistive and semi-autonomous here. But we will know when we are getting to a point where more of security has been offloaded to AI when there are decisions being made by the agent that they, that without having, without having the manual intervention of the human. And so this is sort of part of the telemetry piece of it as well. We've been studying across a particular journey how much of that malware analysis does the end user, you know, trust to be able to then move on to the next step in their process. So there's a lot of research. There's a lot of rich territory for us to continue experimenting with. So it all comes down to the same outcomes that many security teams are already focused on doing today, but just doing them with less toil from the human and doing it better to be out front a little further on our toes. Things like reducing our exposure, things like reducing our time to detect and remediate. So that's what we're pretty excited about being able to do with more of these security agents over the next several years. But ultimately these were some that we put up there, but ultimately we're hoping to spend the next five minutes or so doing some Q and A and hearing what kinds of agents maybe your team wants to or other questions that you might have. But in any case, thanks so much, Nav, for being up here. Thank you, Stav. And also thanks everybody for joining us today. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.