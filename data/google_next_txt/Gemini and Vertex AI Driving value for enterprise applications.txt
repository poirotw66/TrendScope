 All right. Hello, everyone, and welcome. We're going to get started right on time because we have an absolutely jam-packed session for you today. So what we're going to cover today is how Gemini and Vertex AI are truly powering transformational enterprise use cases that are driving immense amounts of enterprise value. My name is Lily McNeilis. I'm the director of outbound product management for Cloud AI. I'll be joined today by my colleague Fabian, who is our lead product manager for Gemini on Vertex AI. We're also thrilled to have S. Young from LG, Zach from our DevRel team, and Kashav from Labelbox join us today to really bring to life a lot of these amazing use cases you're going to hear about. So we'll do a quick tour or a highlight reel, if you will, of all the new Gemini advancements that we announced this week on Vertex AI. Then we'll dive into a tour of some of the use cases we're hearing and seeing, and we'll do a quick wrap-up at the end. So diving in, Gemini and Vertex AI. I like to start with this bird's-eye view of our open and unified stack that Google Cloud is using to bring the ability for our enterprise customers to build and adopt agents across their organizations at scale. Today we're going to focus on Vertex AI, our end-to-end model and agent development, deployment, and monitoring platform. We're also going to talk about models. So within Vertex AI, we have our model garden, which is where you can find over 200 enterprise-ready curated models from Google, our partners like Anthropic, and the open ecosystem such as Meta's Llama 4. All those models are optimized for deployment on Vertex AI and deeply integrated with our model builder tool chain. This means if you want to evaluate Anthropics' cloud model, you can do that with our Vertex AI evaluation service. If you want to do supervised fine-tuning for one of the new Llama models, you can also do that with one of our managed services. So while I love talking about our partner and open models, today what we're really going to focus on is Gemini. So as you're probably aware, Google launched Gemini 2.5, our first family of Gemini models with native thinking capabilities built in. This truly unlocks an immense amount of value for enterprises with advanced reasoning and new capabilities with coding, understanding, and generation. Thinking models also have the ability to show their thoughts, which helps us better understand the logic they're using to get to their solutions, which is a huge unlock for agentic workflows as well. We're very excited that we've brought Gemini 2.5 Pro to Vertex AI in preview. This model is topping many, many benchmarks across advanced reasoning as well as mathematics. Here are just a couple. There are many more. And also Gemini 2.5 Pro premiered in the number one spot on the LM Arena leaderboard and also saw the largest score increase of any model on that leaderboard to date. Additionally, we're excited that coming soon we will have Gemini 2.5 Flash on Vertex AI. 2.5 Flash you can think of as our workhorse model. It's optimized for speed and efficiency. It will also, as a Gemini 2.5 model, have native thinking capabilities built in. But with this model, we'll also offer what you can think of as a thinking budget. This budget is something that you can use to tailor how much thought is actually applied to any given prompt, because we know not all prompts require thinking capabilities. Also, we've now launched Live API in preview on Vertex AI. I'm not going to get into this too much right now, because you'll hear a lot about it later in our presentation. But now, with the Live API, developers can build truly multimodal, real-time, immersive experiences powered by Gemini. So I do want to take a quick tour of some of the new components of Vertex AI Model Builder, which is our platform for discovery, customization, evaluation, and monitoring deployment of models. So first of all, Vertex AI Model Optimizer. We know one of the biggest challenges organizations face today is model selection. So with Vertex AI Model Optimizer, what we're doing is we're allowing users to provide an input of cost and performance. And then under the hood, we actually help select the best Gemini model, as well as the best tools and optimized deployment to meet those user preferences from cost and performance perspective with the absolute best solution. We also offer a range of customization techniques and services on Vertex AI. Everything from in-context learning to full fine-tuning we have available today. A few new announcements this week. We've launched supervised fine-tuning for Gemini 2.0, coming soon to 2.5. And we also launched new capabilities for open model full fine-tuning, including our Vertex AI Model Development Service, as well as a managed service for full fine-tuning of LAMA models. Evaluation is also one of the most challenging components of the end-to-end workflow these days. With the Vertex AI Evaluation Service, we offer eval for all types of different models, whether those are ones that you've built yourself, models from Google, or our partners. This week, we expanded that to be also inclusive of agents, regardless of the underlying framework, or if you've built those agents yourself. And we're also excited to be bringing multimodal eval, batch evaluation, and rubrics-based evaluation to Vertex AI Evaluation Service this week. And lastly, I want to share that we've also announced the global endpoint in preview. With global endpoint, you have capacity-aware global routing. What this means is that if you don't have regulations that require you to host a model in a specific region, you can significantly benefit from the global endpoint's ability to have intelligent health checks built in and automatic failover. So if there is an unprecedented event in a region where your model is deployed, those queries will automatically be routed to the next available endpoint in a different region, providing you additional uptime and reliability of service. I always like to land everything in Vertex AI and make sure everyone knows that enterprise readiness continues to be a cornerstone of everything we do in AI. We want to make sure our customers have peace of mind and trust in the AI that they're deploying to power their customer experiences. And that's why with everything from data and privacy and security to compliance to safety and responsible AI, these all remain paramount in the way that we build our services today. And with that, I'm excited to turn it over to my colleague Fabian, who is going to kick us off into our use case tour while we'll walk through these different categories of use cases. Hello, everyone. I am Fabian Blancpac. I'm product lead for Gemini on Vertex. And so in this first section of the use case tour, we're going to explore how Gemini's strong content understanding capabilities enable large-scale insight from multimodal data. Okay, first. So Gemini is natively multimodal, right? So Gemini has been trained across text, audio, image, video, all at once, providing insightful analysis across all these modalities. And you can mix and match, right? Second, Gemini features a massive one million token context window. So the context window defines the amount of information, whether words, image, audio, snippets, video, that are represented as tokens that the model can process and understand. And roughly, to give a sense of scale, one million token equals to about one hour of video, about 3,000 image, 11 hours of audio, or 10 books, or, you know, full code base. And this is one million now, and we are, you know, two million is coming soon. So we're expanding here. And crucially, Gemini, again, can analyze all these modalities at the same time altogether. And third, as Lily mentioned, so with the 2.5 families of models, so we are integrating thinking capabilities. So that allows the model to reason internally before responding, which is basically boosting performance and accuracy and really excelling at complex tasks, such as problem solving, math, or coding. And we'll see a bit more on the coding side. So, okay, all these are, you know, great capabilities, but let's look at what are the real-world use cases that you can build with Gemini. Okay, first one. So let's have a look at the first use case, which showcases multimodal content understanding that is applied here for a scenario for automatic product catalog enrichment. So imagine you're a retailer. You have a warehouse full of product, millions of SKUs, right? And maybe one like this bed frame, right? And your website really needs more accurate, detailed, you know, product description to drive sales and improve your customer experience, you know, better search, better recommendation, and so on. And with Gemini, you can close the gap in seconds because the issue here is that, you know, if your vendors provide you the information, you know, the image of the product, maybe the description is incomplete, maybe there are some errors and so on, you need to, how can you close that gap with Gemini? All you need to do is very easy. And you just need to provide Gemini all the product image, right? And the text prompt, such as this one, right? So the text prompt is asking Gemini to provide specific attributes of your product, right? So for example, here, the material, the shape, the pattern, the finish, and so on. And ask Gemini to return a response. And you can format the response, ask Gemini to structure the output. Here we're asking Gemini to respond in JSON format. So let's get Gemini gets to work, and this is the response. So Gemini analyzes accurately the image, and accurately picks out all these attributes, right? And I think the material, the finish, you know, it's varnished, it's the shape, it's rectangular, and so on and so on. And outputs in a JSON format, which is really taking unstructured information, a matrix of pixels, and converting that to a structured output that can be fed directly into your downstream tasks. And so there's, you know, less manual entry, right? More higher, you know, quality in terms of data you're getting. And there's also that notion of scale, because as I mentioned, with one million tokens, you can import, you can feed Gemini with up to 3,000 images. So basically, with a single prompt, you can extract all the attributes for those products. So it scales really, really well. Okay, that's a second one, second use case. Still, you know, that's an application that showcases, you know, how powerful the multimodal content understanding capability are with Gemini. And we're going to here extract e-commerce video to extract marketing insight at scale. So let's say you are an e-commerce company or a brand, and you know that those, you know, product review, especially the video product reviews about your customer, about your product from your customers, are just marketing gold, right? But it's taking a lot of time to dig through all those videos. You need eyeballs to look at them and take notes. So it's really, really hard to do. So this is where Gemini can help and really change the game here. So we'll have a look. So here is a, it's a 10-minute product review for a couch. So we're not going to play 10 minutes, just a few seconds to give you a sense on what we have here. This is the back of it. And you can see, like, it has that silver bar on the back as well. And then all of the... All right. I spare you the whole thing. Right? You get it? So now, instead of having someone watching that entire 10-minute video, let's just ask Gemini with a simple prompt. Okay. Describe what this person thinks about the product, please, the pros, the cons, and the delivery experience. Let's check Gemini's answer. Here we go. So the answer summarizes really well, you know, the full video, you know. And so it's comfortable, it's cozy, it's the pros, the cons, it's about the delivery experience, and so on. Great. All good. So now, imagine, and again, you can, you know, if you ask in JSON format, you'll get the structured output just like the image one, right? So imagine, you know, you're doing this not just for one video, but across thousands, hundreds of thousands of hours of video reviews, and then you can spot trends, understand customer feedback, and truly, truly understand what your customers are telling you about your product. So this is the power of Gemini for video understanding content at scale. Okay. A third one, I'm very excited about that use case. This is coding, right? So as we mentioned earlier, 2.5 model, our thinking model, capable of reasoning, you know, through the thoughts before responding, resulting in, you know, enhanced, you know, quality, right? And with thinking, thinking model really revolutionized how we approach software development, right? And you may have seen some claims about, you know, how, you know, AI, it gets super important in the, you know, companies for software development in the world now. So there's a few use cases that you can do with thinking. Here we're going to deep dive a bit. What's it exactly about, right? So first, imagine you have your coding assistant, right? So you feed Gemini with your full code base, right? No matter how large or complex it is. And Gemini is going to be able to quickly grasp that intricate logic and navigate across the vast project at unprecedented speed, right? Seconds. So it's really, think about, you know, it's a lot of time saved when you onboard a new team member, right? Or you want to understand legacy system. So this is, you know, to get up to on a code base, this is awesome. Second, the great Gemini is good at coding itself, but Gemini is really good at understanding, you know, existing codes. So Gemini can assist with architectural design. So leveraging its vast knowledge and, you know, and the reasoning capabilities, it can help you craft robust and scalable software architecture. You know, finding out, you know, issues, bottleneck and suggesting improvements, which, you know, just basically, you can have more, you know, robust software, planning for robust software. And finally, writing the code itself, right? So by, you know, Gemini obviously can achieve, help you achieve quality implementation in terms of coding by understanding, understanding, sorry, best practice, for example. It can generate code with less errors. It can automatically document the code, right? How awesome is that one? And, or document previous code, writer that you haven't wrote yet, and just leading to more efficient, you know, software cycle and higher quality software. All right. So now I'm going to hand off to Keshav. Keshav is a CTO at Labelbox, and Keshav is going to tell us about how they're using Gemini with their product and workflows. All right. Thank you, Fabian. I'll take a few minutes to talk about how Labelbox is enabling training data and evolve solutions for AI teams. So we are a one-stop shop for data and evolve. We have a pretty comprehensive platform and set of tools that is our SaaS offering, and we have a global workforce, expert labelers that we call aligners that uses that platform to produce training data and does conduct evaluations. These aligners work on Labelbox platform, but they can also work on customer platforms. We call this Labelbox Data Factory. And we have built this factory on Google Cloud. We have been with Google Cloud since day one. Over the course of the last couple of years, we have introduced a number of AI capabilities into Labelbox platform built on top of Gemini. Synthetic data generation, AI critiques, agentic workflows, and really large number of labeling and evaluation tools and Jara AI interview that I'll talk about a little later. So aligners produce training data in several spoken languages across the globe. STEM reasoning, coding, multimodal evaluation, and number of industry vertical use cases. To take the example of audio and video real-time evaluations, we're doing a number of these projects right now, and it's really exciting. So aligners conduct these conversations with the model real-time using, as an example, LiveAPI. And these conversations are happening using our desktop and mobile apps. so the data set that we're generating for training purposes is really diverse and very authentic because the conversations are happening in real-world context, not just on the desktop. And we provide a number of evaluations of the training data that we capture, such as, you know, is the model helpful? How is the tone? Is the help that model provided accurate, et cetera? And all this fits into making the model better. In addition to the conversations, we also do evaluations of generated data, such as images and videos. And like Fabian was talking about, for coding, we have a really comprehensive environment to do code evaluation, including a browser-based IDE, where you can, like normal programmers work, that's how we do coding output for the model improvement. end. If you're building agents, Labelbox provides solutions for editing trajectories and annotating the trajectories to improve reasoning and planning of the agent, as well as optimize the tool calling, including custom tools, such as you might write some code, that's a tool that integrates with the RAC system, as an example. So all these things I described, and many more, are done by our global network of expert AI trainers we call aligners. We have built a platform for hiring, training, managing, providing tools, paying them. the entire platform is built so that we can have a really large global workforce of AI trainers that do the training data creation and model evaluation. And we built this using Live API. I won't get into the architecture details, but it works really well. You'll see an example of how well the AI interview works a little later. In addition to the interview, we also use AI to evaluate the interview and conduct scoring and various attributes to determine whether to hire somebody or not. After the AI interview, they also go through labeling tests and a few other things. But we notice we use Flash for this, and Flash really works very well for the evaluation of the interviews. and interviewees really love Zara. It's truly a great experience. And let's take a quick look at a couple of short clips of how Zara does interviews. Hello. I'm Zara, an AI developed by Aligner. I'll be conducting your interview today. You mentioned a probabilistic analysis of the developed algorithm. Could you elaborate on the specific probabilistic methods you used and why they were appropriate for this problem? I would love to, and it's my favorite subject. It all goes back to a famous theorem from 1959 called the Beardwood-Halton-Hammersley theorem. Thank you. Thank you. With that, I'll hand over to Fabian. Thank you, Keshav, for sharing all those great use cases. Okay, I'm back. Last part of the use case tour. I'm very excited about that one, and, you know, thanks for the segue with the live API, Keshav. So, multi-model customer engagement. So now, let's look at how Gemini can help bring better customer interaction to life. Okay, so yesterday, we just launched the live API on Vertex, making real-time audio and video engagement a reality for your application and your users. Picture your users speaking to and hearing from Gemini in real-time, with Gemini understanding and reacting to their world, or on a global scale, transcending language barriers, just like what we've seen. And so, let's uncover how it works, explore some exciting use case, and I think we're going to see it live in action. Okay, so first, real-time audio. So, Gemini live API provides real-time, low-latency audio interaction, right, that are coupled with high-quality audio voices. So, users can literally speak to and hear from Gemini in real-time. So, this is perfect for building advanced voice-to-voice applications, while providing very, very snappy, natural-sounding, human-like experience to your users. Like, you chat with the AI here, right? So, imagine this for application like a voice-based shopping assistant to create interactive retail assistant experience, or an app for voice-driven inventory for fulfillment in warehouse where your hands are busy, right? Next. So, real-time video. This is the exciting part. We're coming on top of the audio. So, with live API, we're going just beyond just audio, right? So, you can show things to users, and you can add a visual dimension to it. And Gemini can understand what it's seeing through the glass of the lens of the device. So, either, you know, often it's mobile device or a computer, but also can analyze what's displayed on your screen, like think screen sharing. So, that opens up, you know, possibilities for use cases like guiding users through very complex software and boarding with screen sharing. And yesterday, Shopify, here on stage at Next, showed a demo, which is basically customer support with remote customer support with screen sharing. So, if you were not here, just, you know, watch the, I think there's, there's, there's, everything's going to be available on YouTube. Check it out. It's really mind-blowing. So, yeah, just audio and video experience for your users. And the last part is, so, internationalization. So, Gemini understands 109 languages. So, if you, you know, interact with Gemini and starting with 2.0, right? 109 language and with live API, so, obviously, Gemini understands all these languages and can respond in 31 languages. So, language and local, like French, France, French, Canada, for example. And really, you know, that really, you know, breaks the Burying language. You can, you know, build really, you know, multi-language application for if you want to build application for field support across the continents or, you know, also learning language application, you know. Just try it out. Try to interact with Gemini. Ask Gemini to, you know, a question in a language and ask Gemini to translate in a different language. It works super well. So, language learning is also opening with LiveAPI. Okay, now, so that's great, but now we're going to get into a live demo. I've got a favor to ask all of you. Please take your phone. Show me your phone. All right. Everybody here? Now you're going to go to settings. All right. Airplane mode? All right. Who's not in airplane mode? Who is not? Okay. Okay. I'm asking for that because you know how the connection can be spotted today and this is a live demo, right? So, we want to be sure we give you the best experience. We've set the bar pretty high. So, just heads up, Zach Akiel from DevRel team, so developer relation, has prepared a bunch of demos for us. So, thank you so much. Thank you. All right. You heard it. It's going to be all live. I also want to add because now there's Wi-Fi on planes. So, airplane mode and Wi-Fi off. Hopefully, it's all going to be good. But, if things don't go as planned, we actually, I'm doing a session later on today, four o'clock, developer jumpstart where we've got a lot more time to show a lot more live demos. So, fingers crossed, you see it all now. If not, you can see it later. So, we're going to switch. We've got our screen up. We're going to show the first, like, classic use case of live API, which is in, sort of, audio to audio. And, you're used to seeing this a lot. You know, you've got, like, automated customer calls, that sort of thing. But, the thing with those is if you've got a very predictable conversation flow, like, maybe you're onboarding a new customer, it's like, what's your name, this, what's your date of birth, this, live API is a bit overpowered for that. That's not really where it shines. Where it shines is those more unpredictable conversations. So, think technical support. You don't know what question's going to be asked, but you know the context that you're going to need to answer it. So, you probably already have documents of, here's trying to troubleshoot these different things. You can take that document and throw it right into the context window of the live API, and now you've got your technical support agent ready to go. So, let's test that out right now. Breathe, everyone. Hello. Hello, can you hear me, Gemini? Hi. Okay. I hear you asking if Junior can hear you. Are you trying to set up communication with a Symbol device? If so, which product are you using? I'm having some trouble connecting my router to the network. It doesn't seem to be connecting to the network. It is fast. When actually I'm speaking this on a strong Wi-Fi, it will respond within one second, and that is our sort of expected response rate. So, it's really fast. That's why everyone turns their Wi-Fi off. Okay. I'm going to do one more test, and I want to show another demo. This one is showing the use case of the complex software navigation. Who here has ever used or tried to use the software Blender? Anybody who's familiar with it? Or if you've ever tried to use complex video editing or photo editing, very steep learning curve, very intimidating. Fortunately, we can use the Live API after that. Okay. Is there anything specific about it that you wanted to know? Now you're talkative. Okay. So, I'm going to, this is a very complicated piece of software here. What can you see? Okay. So, it looks like you're showing me a wireframe of a three... And the audio feedback will probably not be very kind of it. Let's try that again. What can you see? Again, very fast. Later on, we will be able to show this one more time. Okay. We're going to quickly jump on. We have a very special demo. Fingers crossed. We're actually using a hotspot for this one. We have been working with our amazing partners at Snap. They have an amazing piece of hardware called the Snap Spectacles. So, what I was able to do... If you've got space to set this up. There we go. You're going to do a hard switch, switch, as you can hear. And just last week, I was able to get access to one of these pieces of hardware. And then, working with the team, talented engineers, we're able to get the Live API in the Spectacles. And hopefully, it is a very... It's a development kit. We're hopefully going to be able to see what the glasses see. And we're going to be able to show one of the most exciting use cases, I think, of the Live API, which is combined with AR. Because anything in front of a desk, it's usually pretty predictable. Everything not in front of a desk, things in construction, in engineering, I think, is a really exciting use case for this sort of AI that is able to handle very messy environments, very unpredictable environments. So, we're going to switch to the demo laptop. And hopefully, we can see... Yes. Fantastic. Thank you so much. No problem. All right. Fingers crossed. Hotspot's on, yeah? Hotspot's on. Okay. So, you can see... Hello, everyone. We're doing this live. Hello, Gemini. Can you hear me? We don't have audio. Hello, can you say that again? Yes, I can hear you. How can I help? What can you see? I see a large crowd of people sitting in an auditorium, and there are multiple screens in front of you showing some content. Can you give me a sticky note that reminds me to look at the audience? Yes. Yes. Yes. Yes. Yes. Yes. Yes. Yes. Yes. Yes. Yes. Yes. Yes. Yes. Yes. Yes. Yes. Yes. Yes. Okay. Okay. Okay. Okay. All right, we're going to turn it one more time. Okay. Yes, yes, yes. Okay, we're going to hand over to Mitchell. Very talented from Snap. Let's see. You do it. Oh. Hi, everyone. Okay. So I'm going to talk to Gemini right here. Hey, Gemini. How's it going? Hey there. Everything is going well. How can I help you today? Yeah. Can you put a note? Can you on a sticky note, please put, say hi to the audience? And put this right here. Yeah, that's correct. I also, Gemini, I need to like fix my computer. Can you pull up the schematics really quick? Sure, I can do that. I'll pull up the schematics for you now. I can make this a little bit bigger so I can debug this over my computer. And what's going on at Google Cloud Next today, Gemini? Google Cloud Next is happening in Las Vegas this week from April 9th to 11th. Today is the second day of the conference. There have been several announcements. Thank you so much. Give a massive round of applause. There's a lot of amazing use cases you can imagine in construction, education. We've got a session again at 4 o'clock where we'll do live demos. You could potentially have a go with these spectacles. Thank you so much. We're going to hand it over now to Young Lee from LG who's going to show us more about how you can use Gemini in their businesses. So thank you all so much. Hello, everyone. I'm Young Lee from LG U+. And now I'm reading the Mobile AI agent team. It's a great honor to speak here at Google next day. So today I'll introduce Ixio, our AI core agent service. It combines our own AI engine, Ixigen, with Google's powerful Gemini model. This gives users strong data security from on-device AI, along with Gemini's impressive AI capabilities. So why did we choose to create an AI core agent? The first phone telephone was invented about 150 years ago. And since then, the way we transmit voice itself has remained largely unchanged. Also, even smartphone era today have not greatly changed basic calling features like contacts or dialing screen, etc. So despite this kind of technological advancements, customers still face common core-related pain points. So we want to aim to solve these kind of problems and satisfy customers' needs. For example, we want to provide recording calls and providing summaries, especially for iPhone users. Answering calls with AI for those who are nervous, which is called call-phobia. Protecting against dangerous calls like voice-bishing in real-time. And quickly searching for information during or after call without interruption. And showing live transcripts and helping find key information during calls. So what is exactly is XCO? Let's talk about it from now on. We launched this service, launching first on iOS on November 7th last year. And then on Android this past March, XCO quickly become a hit. It's already a must-have application in South Korea. And mostly on impressive engagement rate around 67%. Now let's get back to calls. We found out that every call has three stages, obviously. Before calls, during call, after call. So how does XCO improve this call journey? First, before the call. XCO shows call history on incoming screen. Helping users prepare or decide whether to answer or not. And this especially helps Gen.G users overcome call anxiety. So called call-phobia. And during calls, XCO shows conversation in real-time, on screen. This is extremely useful in noise environments or when hearing is difficult. Proudly, one of our service development team members with a hearing impairment used this feature to have his first-ever phone call to his mother right after this launch. And then after the call, users get an AI-generated summary, key points, and suggest next task. Think about it. No more forgetting details after the call with your boss. All call summaries and tasks leverage Gemini to ensure high quality. And next one is AI search. We are using Gemini grounding search. Also helps user quickly explore topics discussed in the call. And even suggests relevant YouTube shows for edit time. This is actual call I had with a team member about this business trip here to Las Vegas. And safety is another key focus. XCO protects user before by spam-related number warning. And during the call, we are putting voice-phishing alerts. And after call, we are providing spam reporting function. So users can now confidently answer calls without worries. So what is the next step for XCO? This year's enhancement focuses on call, convenience, and safety. In call area, we are expanding AI call answering for seamless user call management. And in convenience area, we are enhancing in-call search with Google Gemini and Vertex AI. And safety area, introducing AI voice detection for filtering suspicious calls and anti-phishing URL blocking. Now we are partnering with Google to develop AI search and AI autocolling. And this is AI search. We'll soon enable real-time in-call searching. So customer, no need to leave your call to look up information. So currently, we offer search call after call. But real-time search is our next goal. AI autocolling, powered by Gemini, will soon handle calls naturally and intuitively on behalf of users. So our promising test results. And we expect to soon make AI-driven calling a reality. I believe that by combining Azure Plus high-quality customer's data and commercially proven AI core agent technology, XEO. And also Google's collaboration capabilities through Vertex AI and Gemini on GCP. And partnership with Global Telecom Company. We can create a telecom-driven AI core agent ecosystem. So we are always open and ready to collaborate with you, telecom providers. Thank you for your time, and thank you for listening. Thank you. Thank you. Thank you so much, S. Young. So with that, we're going to take us to the last stop on our use case tour, which we're calling hyper-personalization. And so when you think of hyper-personalization, there are a few new products and enhancements that we're going to touch on. One is actually native image generation as part of Gemini. Of course, we have our robust set of tuning capabilities, as well as RAG and grounding. And lastly, we have something really cool to share with you today, which we're going to touch on. It's called our podcast generation API. So as you probably know, Gemini can understand a lot of multimedia inputs or multimodal inputs. It can also now generate a lot of multimodal outputs. So we recently actually opened up the ability for everyone to use Gemini to generate images. And so, of course, we continue to have Imagine, which is our high-quality text-to-image generation model. The really amazing thing about multimodal outputs with an advanced reasoning model is that you get a massive amount of cohesion in those outputs when they're generated together. So the key use case we've seen is text interleaved with images. So when you think about how could I personalize a newsletter for every one of my users? How could I personalize any type of recipe so that the images and the text in that content is so cohesive that it really makes a lot of sense and is personalized to those end users? So we think it's a really powerful unlock of use cases here for text and images interleaved. So as I mentioned, podcast generation API is something new. How many of you have used Notebook LM? How cool is it, right? So what if you could generate that same type of podcast audio overview for every one of your end users and have it customized through their own preferences and background? That's exactly what we envision with podcast generation API, which is going to be a scaled approach for you to generate these types of podcast overviews for every user. So thinking about the types of use cases this unlocks, what if for every one of your subscribers, if you're a news publisher, you could actually generate an audio overview personalized to their specific interests that they could listen to while they're dropping their kids off at school? What if you're in the financial services industry and you want to have a monthly, a weekly, a daily audio overview for your investors that's customized to their own investment portfolios, as well as the market context that they could listen to on their commute to work? Or what if you're a real estate agent and you're going to meet a home buyer or a home seller and you want to quickly listen to an audio overview of the market context for that specific location, as well as tighten that together with the preferences of your buyer or seller? It's really amazing to think about the scalability of something like this, and we're really excited to hear what our customers build with it. So with that, we're going to wrap it up. We hope today what you heard from us is a huge update to our Gemini and Vertex AI product portfolio. But more importantly, you heard how our customers are actually using these tools and capabilities in production to drive real business value and that they're not doing it alone. We're working closely with tons of customers and partners all over the world to bring these experiences to real life. We have a ton of additional exciting sessions. Here are just a few that I would ask you to bookmark. I think many sessions are going to be great. Sorry, Zach, because I know one of your sessions is on here. But if I had to pick one, I would say Agent Observability 101. I would definitely try to get to that one. It's a huge hot topic. We have a lot of new functionality launching in that space. If I could go to one, I would go to that one. And of course, your feedback is greatly appreciated. So if you could take a few minutes, complete the survey in the app, we would be very grateful. And with that, I want to thank all of my co-presenters, even the ones we didn't know were going to show up on stage. So thank you, everyone, for sticking with us through the demo. And we'll be around for a few minutes for questions afterwards. So thank you all. Thank you. They'll be around for some hoes. Thank you.