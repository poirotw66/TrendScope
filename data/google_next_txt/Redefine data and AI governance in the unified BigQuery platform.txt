 Welcome. Hi, everyone. Thank you so much for joining us today. And welcome to our session on redefining data and AI governance in a unified BigQuery platform. My name is Lu Yan. I'm the product management lead for BigQuery governance and Dataplex here at GCP. And joining me today are two distinguished customer speakers. We have Eric Risher, who is the head of governance and product analytics at Walmart Data Ventures. We have Asmita Kulkarni, who is the product management lead of data platform at Box. Super excited to have you here with us today. So here's our agenda for today. We'll start with an overview of our vision and strategy for data and AI governance in a unified BigQuery platform. And then we'll hear from Walmart and Box on their impressive use cases. Last but not least, we'll be sharing with you our latest product innovations in the space of data and AI governance. Now let's get started. As we all know, AI adoption is rapidly accelerating across different industries and different geographies. And it becomes extremely critical to ensure that you have a robust data and AI governance strategy in place. On the one hand, this is dictated by strong regulations across the world. On the other hand, this is the only way to ensure that any generated insight remains trustworthy. Actually, I'm curious to do a quick poll here. I wanted to see how many of you have encountered data governance challenges like data quality issues on your AI journey. Wow, I see a lot of hands raised. Thank you. And you're not alone. In fact, according to a Google research survey, 70% of organizations have experienced difficulties with data, especially data governance, when it comes to AI use cases. And it is the key bottleneck for scaling AI from pilots to production. So in order to meet those critical needs, governance has to be universal, intelligent, and open. Universal as it needs to be seamlessly integrated into the end-to-end data to AI life cycle, from the moment when data is ingested, all the way to when AI models are deployed. It needs to be intelligent, as in governance should be AI driven, especially given the strong need to automate key governance tasks as data becomes more complex. Last but not least, governance must be open and inclusive to be fully compatible with various storage and compute options for a future-proof data data architecture. As you're aware, BigQuery is evolving into a truly unified AI-ready data platform, supporting multimodal data of various formats, storage options, and supporting multitude of compute options, including SQL, open source, and AI processing. Now, in order to meet the governance need at the heart of the new BigQuery, spanning across all the different compute and storage options, is the single metadata management and governance layer. And this is the new BigQuery Universal Catalog. So today, we're super excited to be bringing to you the BigQuery Universal Catalog, which is your one catalog for all of your metadata needs. It includes all the capabilities of Dataplex, which is our native offering for GCP-wide data to AI governance. It also includes BigQuery security, BigQuery sharing, as well as the runtime Metastore capabilities. To truly deliver data to AI governance, it provides the following key values to you. Number one, it provides a single pane of glass for discovering data and AI assets across the organization using natural language. And it also helps you to share your discovered data assets within and across the organization. Second, it leverages AI-powered intelligence to enhance trust in data from automatically generating metadata descriptions to monitoring data for anomaly, all the way to recommending insights based on data. Last but not least, it is open. It provides runtime and governance support for open storage formats, such as Apache Iceberg, and it works with a rich ecosystem of partners. So think of the Universal Catalog as the brain of your data and AI ecosystem. It starts from automatically harvesting metadata from all the different data and AI sources, ranging from technical, operational, to business metadata. We then combine this powerful metadata foundation with Google's state-of-the-art large language models to help you make governance much more proactive and efficient. And it would enable all of the agentic capabilities you have been hearing about here at the conference. As you know, Gen.AI and agents are really all about context. And context is really all about metadata. So a key focus of this new Universal Catalog is that it is fully integrated into the BigQuery environment. As you may have noticed, many key governance features are already available in the BigQuery experience, including lineage, data profiling, data quality, data insights, and GCS metadata discovery. And looking ahead, we have more exciting new features lighting up right within BigQuery, such as business glossary, universal semantic search, and all of which you'll learn more about at the later section of the presentation today. To further simplify and streamline governance, we're also super excited to introduce the governance center experience of the data management center. It's a dedicated management console experience for data stewards, data admins, and security professionals to really help you manage, secure, and govern across your entire data and AI landscape. Already, we have been seeing very strong customer adoption momentum, thanks to your partnership for the Universal Catalog. We have been providing data asset discovery for every BigQuery customer powered by the Catalog search. We have been seeing governance capabilities like Dataplex's data quality and data insight rapidly gaining traction. Our customers are already sharing hundreds of petabytes of data with BigQuery sharing. And there are millions of policies set in place to safeguard your data with BigQuery security. Below is an example list of customers we serve across different industries and different geographies. And again, truly appreciate your partnership here. We have been working on our customers. And now we'd love to hand it over to our two customer speakers, Walmart and Box, to share with you their experiences and strategy for implementing governance. So first, let's welcome Eric from Walmart to the stage. Thank you, Luke. So before I go on our journey with Data Ventures and how we leveraged BigQuery Universal Catalog to curate our data and ensure that we have trust in it, I think we need to go back and really go where it first starts. And that's at Walmart. So first, how do we describe ourselves? We are a people-led, tech-powered, omnichannel retailer dedicated to helping people save money and live better. So let's unpack that for a second. So it all starts with our amazing 2.1 million associates that span across 19 countries that serve over 10,000 of our stores. And by the way, we do actually have a pretty cool e-commerce platform if you haven't seen it. And what we do is our customers that have 255 million customers and members that shop with us globally each week, they want four things. They want a great value. They want a broad assortment. They want a great shopping experience. And they want to do it with someone that they trust. And so the way that we do that, we have a mindset that we've had for over 50 years. And you're going to hear these acronyms, EDLC and EDLP. And that's called everyday low cost and everyday low price. And you're going to hear those throughout my message here. But those two are really fundamental for us to help save money and live better. So as I just mentioned from our first, you might see some similarities and some key words in regards to Walmart State of Ventures mission. We are creating value from Walmart's first party data. And we do that through our customer journey from our digital and physical shelf on path to purchase and other various data sources. And we bundle all that data to help create a platform that we call Centilla. What Centilla does is it helps drive collaboration between our merchants, our operators, and our suppliers. And what we're able to do there is we're able to take that value from a data perspective, turn that into insights, and insights into action. Now, we all know here that data and trust have to go hand in hand. And a lot of them would think that it's very simple. But it's a really complex dilemma. And so one of the things that we have at Data Ventures is we have a massive opportunity from our side. And we have a lot of momentum. But one of the things that we wanted to be able to do is actually keep that momentum. And so what we did is we did some self-reflection. And we said, what do we need to focus on? And we focused on the three P's, people, process, and platform. And as we deconstructed that, we really came out with five key things. We needed to be able to have dedicated resources to be focused on the right things. We needed to be able to deliver a flexible delivery model and be nimble. We also needed to be able to scale from a mass amount of data and be able to deliver that data at speed as what our customers or our users expect. The other thing is we needed to be able to understand our data. We need to be able to put the pulse of our data like a doctor does to your heart. And more than anything, of course, we have to have AI if we're talking about data at some time. I imagine you guys might have heard AI maybe once or twice during this conference. But what we wanted to be able to do is we wanted to be able to autoimmune capabilities through artificial intelligence. We wanted to be able to be more proactive and be able to have early warning detection systems. So now that you know where we needed to go, how did we need to build that framework? And so who all has ever watched a 3D movie? Please raise your hand. Okay, thank goodness I'm not that old. What was really cool about a 3D movie, right, is you could literally turn and see it in three dimensions. It wasn't like just looking at a TV. And so because data is in the heart of our product and our agents, we had to adopt a 3D approach to data quality and get a 360-degree view of that data. And so what we started with is on the other side, on the technical rules. So we have our technical partners that will go out and deliver those rules online within our pipelines. And then from my side, we're able to really focus with our team on the business and logical rules. And so by pulling that all together, we're able to stitch it and to have a 3D view. And just a quick FYI or fun fact, 70% of our rules are business and logical rules. So we've created the methodology. Now how do we bring it all together? BigQuery Universal Catalog was able to do that for us. We're able to take a look at the inputs that come from our product managers, our data analysts, our data stewards, our engineers. And we're able to build out rules that are very flexible and give us and be more menimble. The other component to that is we were able to do solid implementation with free from SQL. We could scale, drive EDLC from that. A quick fun fact as well, we've been able to reduce 30% in our compute cost. And we're continuing to do that throughout. Because you all know rules are like a fine wine. You've got to always refine them and filter them. The other thing that we're really proud of is the scheduling made easy. We're able to scale and schedule data quality rules when we need to and at the right time. And the other thing, we want to be able to look. Sorry, pun for Looker. But we're able to leverage Looker to really be able to visualize and remediate against our data. And more importantly, be able to take action against that so it doesn't impact our users. All right. All right. So we've taken it all together. And now we're able to implement. And we're able to basically power our products with data at scale. And so as we've been able to do that with BigQuery Universal Catalog, we've been able to keep the pulse on it. We've been able to do that across three countries. We've been able to curate over 2,000 data quality rules. We're able to monitor across 131 tables that span over 1,500 attributes. And we perform about 185 scans daily and weekly that span roughly around 25.5 trillion records. All right. So you heard about our journey, where we've been. Where do we need to go and where do we want to focus our efforts on moving forward? We want to expand more into features. We want to be able to unlock more from Data Governance Center. We also want to unlock more from a column-level lineage to be able to stitch that together and take a look at our assets. We also want to be able to drive more insights from our data stewards and be able to lock sources faster and be more nimble from an EDLC perspective. The other thing is we want to automate more in data quality. So how do we unlock and deliver rules faster for new assets and new features? And lastly, we want to be able to focus more on the business. And what I mean by that is we want to be able to focus more from business acumen versus focus on the technical side. Technology is changing at the speed of light. And so the two technologies that we want to look at a little bit more is Data Canvas. We see great values in how we can unlock more insights, save hours, and deliver faster. And then Vertex AI. Of course, I had to say AI again, right? So we wanted to do that around proacting on alerting, focus more on data drifting, and anomaly detection. So by focusing on those items, we hope to create more value for our users while continuing to instill trust every day with our users. All right. Now I would love to have Amisha come up and deliver on Box. Thank you, Eric. That's a super interesting use case about delivering the value at scale. Hello, everyone. So I'm Asmita Kulkarni. I'm product lead for Data Platform at Box. And I'm super excited to be here today. So before we get started, let me ask you something. How many of you have ever struggled to find a crucial piece of data that you desperately needed? So I see a lot of hands there. So now imagine the world where you know exactly where to find the data that you need and then access it securely and trust it completely without jumping through the hoops. That sounds ideal, right? So that's exactly what we are solving in Box. Before we get into details of how we did it, I just want to acknowledge the incredible work of my team at Box because they are the ones who turned this vision into reality. So at Box, we are revolutionizing the future of intelligent content management. Our ICM platform enables over 115,000 customers around the world to manage and secure content, provide advanced collaboration, and power AI-driven workflows. So our platform is the trusted home for their valuable content. But most of you agree with me that every great platform comes with a massive data challenge. So let's take a quick look into the steer scale that we are working with. So while supporting 115,000 customers globally, our platform handles 200 billion objects, including files, folders, and links, and processes around 600,000 events every second. So we manage around 20 petabytes of data spread across 12,000 tables supported by BigQuery. And our team runs around 200,000 queries every month. So it's clear that we are operating in a highly complex ecosystem. So you can imagine finding the right data set can feel like searching for a needle in a haystack. So how do we manage this data all without slowing us down? So the answer was to build a scalable, multi-tenant data platform based on data mesh architecture. So this allows us to empower our teams with decentralized ownership so that the teams with the best context can manage their own data assets. With our self-service platform enables the teams to build and operate data services independently with federated governance. So this maintains the consistent governance across all our decentralized domains and at the same time promote agility and efficient data use. But with this agility, scale, and decentralization comes a significant challenge. Because instantly finding the right data set with the right security control seems far from reality. So to give you a better idea about these challenges, let me give you a real-time example. So early last year, our enterprise reporting team was developing a product insight solution that should have been a six-week initiative but stretched into months. Because our engineering team spent three weeks finding the right data sets, sifting through thousands of our tables. When one of our critical pipelines failed, it took them 10 days to pinpoint a data issue. And as a direct consequence, the issue resolution time increased and data downtime went up. So that delaying the key insights. Most importantly, our security team struggled with tracking who had access to sensitive data and that created potential security risks. So they were the daily frustrations our team faced. And they actually threatened to slow down our innovation. And that's where we decided to leverage BigQuery Universal Catalog powered by Databricks. So let me go over the key components of implementation and how they address these critical pain points. So the first component of our implementation was addressing the data discovery. So we introduced Databricks aspects just as metadata tags or labels so that we can create standardized frameworks for defining operational and business metadata. So this made it easier for our teams to find and understand the data. So beyond the standard aspects, we also let our teams to define their own custom aspects so that they can create the domain-specific tags. So they can do the faster discovery. So for example, our analytics team created a report type aspect. So now they can instantly search for all the tables powering the specific report. So on the screen, you can see some standard aspect types that we created, like table ingestion and resource owner details to accelerate the data discovery. And as a result, a team can now quickly find the right data that they need, understand its context, and immediately start using it, significantly accelerating their development life cycles. So the next key piece of implementation was ability to have the end-to-end traceability across our data pipelines, spanning BigQuery, Dataproc, and Dataflow. So here is an example of one of our core data pipelines. And as you can see, the complexity can be very significant. But after we enable the table-level data lineage, our teams can now visualize the complex data pipelines and track the data from its source to destination. So this means the faster impact analysis and proactively detecting potential data issues. So while data discovery and lineage are crucial, security is paramount for our intelligent content management platform. And that's why we built a robust data classification framework, leveraging Dataplex and Google's data loss prevention to take our data protection to the next level. So here is how our data classification framework works. First, our data owners identify the sensitive data across our data ecosystem. Then they classify it into appropriate box data classification categories using custom workloads. And then we run the DLP scanning on the sample data for the automated classification. And then processes automatically update the aspect values in the most restrictive classification between the manual and the automated classification. So this improves accuracy. It reduces false positives. And then it prevents the unintended exposure of sensitive information. After the classification, we then protect the data through the fine-grained access controls so that only the authorized individuals can access specific data sets. And finally, we continuously monitor the access patterns to detect and respond to any anomalies. So this is a key piece of this framework, as you can see on the screen, is the column-level classification powered by aspects and the DLP, data loss prevention. So this allows us to effectively manage data classification and sensitivity data labeling at the most granular level. And at the same time, it gives us a unique way of setting the same amount of validation. Now, this Communication was just kind of noticed. So one of the last charactersRC-SFPs, the læ-DaMiz comes from that is the absolutist team. And also we 안 passed around by a completelyierte dataasında. So one of the unusual stuff that's planned on the left. We only have other tools that we can help Falkers that target, for instance, is the better capture that time for everybody else. So, what does that mean for our team? finding the data that they need and spend more time in driving insights. Next, our data downtime, a major pain point, now plummeted. So detecting and resolving data issues that used to take weeks now just take days. So this has also boosted our developer productivity by 30% and saving hundreds of engineering hours. And now they can spend that time on driving the innovation rather than troubleshooting the data pipelines. And most importantly, we now have 100% coverage of the sensitive data and the ability for the fine-grained access control. So as a company managing the critical content for enterprises globally, we have strengthened the data governance and security posture. And for our customers, this means the greater trust and confidence in how we manage their most valuable content. So while we have achieved significant milestones, we are just getting started. So our journey continues to leverage Dataplex with the exciting new initiatives on the roadmap. We are expanding into column-level data lineage. So metadata and classifications automatically flow across systems for seamless data discovery and governance policies. We are also extending our data classification framework beyond BigQuery to include GCS, Cloud SQL, and Bigtable so that we can ensure unified security and compliance across all our data stores. Then we are extending our data lineage tracking to the third-party solutions through the power of open lineage, bringing our end-to-end observability and traceability beyond just BigQuery. And finally, we are super excited to explore the JNAI-powered metadata curation for table and column descriptions so that we can make the metadata management and data discovery effortless. So data is truly the foundation of everything we do at Box, from powering insights to driving data-driven decisions. With this implementation of Dataplex, we have transformed how we govern, secure, and discover data at scale. Our teams can now move faster. They can focus on what truly matters that is driving the innovation at scale and powering amazing customer experiences. If you are facing the similar challenges and drowning in the data and struggling to extract meaningful insights, I highly recommend you to explore what Dataplex can do for you. It might spark the innovation and unlock new opportunities for you. Thank you. And with that, I welcome Lou back to share what's new with BigQuery in unified governance. Thank you so much. Thank you so much. Thank you, Asmida. Thank you, Eric. What an amazing set of use cases and really appreciate all the effort you have put into implementing Dataplex, the universal catalog for your enterprises. So next, let's take a look at what our latest new product feature announcements that we're bringing to you in the governance space this year. If you think about a lifecycle of data through the lens of the universal catalog, then you would see four distinct stages. You want to start by discovering the data first, and then you want to be able to better understand it and gain a better understanding as well as more trust in it, and then start using it as soon as possible. So for each one of the four stages of the lifecycle, as you can see, we have new feature to be sharing with you both in the anchors category, which are foundational capabilities, as well as the accelerators to further drive innovation. With the interest of time, I'll be focusing on some of the key highlights. So let's start from the first stage, which is data discovery. As you know, metadata is really the foundation, the bread and butter of governance. And a key value proposition of Dataplex is that we can automatically harvest that valuable metadata for you across all the GCP native data to AI sources. And this requires no effort on your end. This metadata foundation then enables org-wide search, which powers all of the other governance capabilities. So this year, in addition to the existing integrations, we're super excited to have further extended our first party coverage to include the GA of cataloging of Vertex AI feature stores, in addition to Vertex AI models and data sets, as well as the GA for Cloud SQL support, as well as the preview for data form automated metadata cataloging to cover your notebook assets. So that we can ensure that both data and AI assets are automatically cataloged for global visibility. We're also excited to bring the data. We're also excited to bring the Dataplex's GCS discovery functionality directly into BigQuery. This functionality automatically scans your GCS buckets to detect and create Big Lake and object tables for multimodal data. It would also keep the metadata up to date while data evolves. So this way, our data is going to be a good idea. So this way, our data practitioners can directly work within BigQuery, no matter where the data is stored. Next, we truly understand that most of you have environments that are multi-cloud or hyper-cloud and have self-managed data sources within and outside of GCP. So we're super excited to announce to you support for managed metadata cataloging for third-party data sources within Dataplex. This is where we provide a bulk metadata import API supporting at-scale ingestion of metadata entries, as well as support for running your own metadata connectors in a serverless way. Now, once all the metadata is cataloged, we want to help you easily discover it. So what better way there is to discover your data than to just ask a natural language question? So we're super excited to announce the upcoming preview of universal semantic search directly within BigQuery. So this will allow you to perform org-wide search and discovery of your data and AI assets right within a familiar BigQuery environment. It also supports natural language interactions to make search much more accessible and intuitive for different personas in your organization, making it easier for your whole organization to adopt. Yay. So now we have covered discover. Let's take a look at what new capabilities we have to help you better understand your data after discovering it. There's a lot of content packed into this one single page, and this is lineage. Lineage is the critical underpinning of governance, and it is becoming even more important in the age of AI. It allows you to quickly understand where your data is coming from, where it is going to, and it is often and increasingly mandated by regulations. So Dataplex already automatically captures lineage for a whole set of GCP sources, such as BigQuery, Dataproc, Composer, and Data Fusion. It is also extensible to third-party data sources given multi-cloud, hybrid cloud deployments via open lineage and API integrations. And this year, we're super excited to announce a set of new integrations lineage support for Vertex AI pipelines, for Vertex AI feature stores, for Dataflow, for Dataproc serverless, and Hive. We also offer improved usability with better visualization capabilities. Now, in addition to table-level lineage, column-level lineage is also top of mind for us and for you, as it provides the ability for you to track the impact and root cause at a column level. So this is where we're making continued progress as well. We have launched column-level lineage in BigQuery in preview, and it is now enhanced with graph support for better visualization as well as time filtering for better ease of use. Now, another key element of truly understanding your data is this feature called Business Glossary. It provides a standardized taxonomy of your business terms across the organization as well as the ability for you to map them to individual data elements in the catalog. So this is especially important for Gen.ai use cases to really help bridge this critical gap between business semantics as well as technical jargons. So we're super excited to announce to you the GA of Business Glossary from Dataplex built into Dataplex, built into BigQuery as well, coming up this quarter. Next, as you know, Dataplex already provides the ability for you to capture business metadata annotations. At the same time, as data volume continues to grow, it is very, very difficult to rely on manual effort to update every single column and every single table. And that's exactly what this new feature, the automated metadata generation, is intended to help with. So this is built on top of the existing metadata foundation, data profiling results, and combined with the latest Gemini models from Google to create meaningful descriptions for tables and for columns to help you really automate the data documentation process. Now moving on to the next stage of gaining better trust in your data. So as you know, Dataplex already provides data quality checks to help you detect deviations against data quality rules, such as completeness, validity, and accuracy. Now here we're very excited to announce the next generation enhancement, the data anomaly detection. This is where we help you monitor your data for any outliers and inconsistencies using machine learning algorithms without you needing to configure rules. And any anomaly that is detected can then be surfaced back to you to reduce the time it takes to identify and fix any data issues. Finally, you have found the data, you have gained a better understanding of the data, and we then want to help you get started analyzing and generating insights from this data immediately. So this is where Gen.ai powered data insights really come in, which is now available in GA. This is the feature that automatically recommends a list of natural language questions with validated SQL queries that are ready to run in BigQuery. And you're also able to ask follow up questions to further interact with the data in Data Canvas. Now all the Gen.ai capabilities you have seen here are powered by our knowledge engine, which turbocharges this rich metadata foundation we have aggregated for you within Dataflex within our catalog with large language models. When this knowledge engine is applied at a data set level, what it does is that it is able to automatically recommend data descriptions to infer relationships between data elements and to be able to make intelligent recommendations on cross table joins to really help you get over the cold start problem. And this is only the beginning. And this is only the beginning. We see great potential and lots of possibilities for innovation with a knowledge engine to truly automate data governance and management to enable agentic capabilities and to truly make governance intelligent, proactive and ambient for you. Last but not least is our upcoming launch of data products. So having a product centric mindset is becoming increasingly critical in this age of AI to ensure proper governance. We're super excited to launch GCP's native support for curating, packaging and sharing data assets as products in order to help you drive true business value. This will also enable the data product marketplace to help you achieve better discoverability, easier collaboration and faster time to insights. Overall, this is the future that we see for data and AI governance and intelligent AI powered platform of automatically curated, well governed and easy to discover data products off the shelf. And to truly help you empower your organization to unlock the full potential of data and AI innovation. So with that, thank you so much, everyone. This is the fullness of our presentation. Thank you so much for joining us. Thank you, Eric. Thank you, I mean, you are.