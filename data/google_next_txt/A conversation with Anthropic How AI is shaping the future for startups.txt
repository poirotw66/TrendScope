 FRANCIS DE SOUZA, Good morning, everyone, and welcome to day two of Google Next. My name is Francis De Sousa. I am the chief operating officer at the Google Cloud. And I am super excited for the session this morning. I have here with me Matt Bell, who is VP of product research at Anthropic. Welcome Matt. Thank you. It's great to be here and I'm excited to chat with you all. Super. Maybe we'll start with, tell us a little bit about Anthropic, the mission of the company and sort of what you focus on there. Sounds good. So Anthropic was founded with the mission of creating powerful yet safe large language models. We believe that society is going to transition to the AI age over the next few years. And this has tremendous potential to amplify human flourishing, accelerate research, accelerate human health span, eliminate toil. So there's tremendous potential upside from creating powerful AI systems. But the downside is risk is significant. And so we want to make sure that that transition happens in a way that is safe and beneficial for all of humanity. And to that end, we are designing our large language models with steerability, interpretability and safety as core principles from day one. Great. Well, I don't think it's an exaggeration to say that, you know, most people believe we're at the beginning of a pretty fundamentally new era in how AI is going to change the way we live, the way we work, the way we play. And a lot of it is going to be driven by startups. And so, you know, at Google Cloud, we focus a lot on the startup community. And we are seeing a lot of AI startups and we're seeing a lot of traction with Google Cloud, you know, powering these startups. In fact, I saw a stat somewhere that, you know, 90% of the Gen AI unicorns are on Google Cloud. 60% of all Gen AI startups are on Google Cloud. And one of the reasons, you know, they tell us that they come to us in addition to the platform and the models we create is that they love the fact that we're open, that we have a model garden, and that we allow startups access to a variety of different models. And I know a lot of startups are very interested in the work that you guys do around your model, Cloud. And so maybe you can talk to us a little bit about how some really exciting startups like Replete, for example, are using your models on Google Cloud to power what they offer to their 20 million developers. Happily. So Replit is a coding platform that started as a mobile-friendly introduction to coding. So allowing people who were relatively new to software development to quickly get up and running. And over the last few years, they have leaned very heavily into AI acceleration of development. So they now have a system that is powered by Cloud and on Vertex AI with Google Cloud that lets people who are brand new to software development create small apps in a manner of minutes. They also have features that accelerate the productivity of senior developers. So basically, no matter where you are in your journey as a software developer, you're able to have your efforts accelerated via Replit. They now have 20 million users, a tremendous amount of traffic. So they've been quite successful. And the Replit agent feature is quite impressive in terms of what it delivers. To give another example, there's a company called Augment Code that uses Cloud on Vertex AI. Augment Code is aimed at professional developers with large code bases, where essentially they're getting an AI agent that can crawl and understand the entire code base and then make complex multi-file edits at the developer's request. And Anthropic did a study with Augment Code and basically found that in one instance, a project that a company expected to take several months was able to be done in just a couple of weeks using the AI acceleration of software development. And separately, employee onboarding to the code base went from weeks to days. And so you can imagine as a brand new developer coming into a large code base, you know, it's often kind of the sink or swim feeling of like hundreds or thousands of files you have to come up to speed on. And Cloud is able to read all those files in a manner of a couple of minutes and is then able to guide the developer around the code base and act as a basically a spin-up buddy to the developer. To give a third example, there's a company BrainLogic that has created a WhatsApp-based shopping assistant called Zapia. And it's mainly in Latin America. They have around 2.5 million users in the first year. And this is all powered by Cloud on Vertex AI, where Cloud assists users with their shopping requests, so helping them find the products they need, and then also finding the best deals for those products. And so this is a complex agentic system that interacts with the web, again, all powered by Cloud on Vertex AI. Super exciting. One of the challenges that startups have to navigate in their own use of AI is the whole question of security. And on the one hand, you know, the reason you're doing an AI startup, presumably, is you have this bold vision of where the world could go. And you're pushing the envelope in terms of capability. But at the same time, you have to make sure that the way you deploy your AI models and your application are secure. And security itself is being changed pretty profoundly by AI, both on the offense and on the defense. And so, you know, companies have to think about how they build security into their applications, how they protect the models that they deploy, and how they maintain a future-forward mindset around the possible future threats to their models. So how does Anthropic navigate this balance? You guys have among the boldest visions out there. And you were, from the beginning, started, you know, with a very strong ethos around alignment and security. And how do you balance the two? Yeah, happy to talk through that. So I think there is a quick distinction I want to draw between data security and model security. So for data security, you know, we've chosen to partner with GCP via Vertex AI so that you get all of the great security guarantees that you get via GCP. So essentially, we never see your data. And, you know, you can operate entirely within GCP infrastructure. We, on the model security side, have done a lot of things to ensure that Claude can handle the tasks you give it responsibly. To draw a metaphor, you can think of bringing Claude into your business as, like, hiring a whole set of employees to do key work at your company. And in the same way that if you're bringing in a bunch of employees and trusting those employees with your customers' data, with customer service, interactions with your users, you want to ensure that those employees represent your company well, that they handle information responsibly, that they don't leak data, and that they're not vulnerable to being tricked or misled by malicious users. And so, you know, you want all of those capabilities from the model as well. And, you know, to that end, we think of model capabilities and model safety as going hand in hand. That in order to create a model that is useful in the task you give it, it needs to be able to follow instructions. It needs to be able to stay within its lane, do the right thing, be responsible around your data. And the fine-tuning that we do for Claude builds in those capabilities from day one so that you can trust that the model will have a low hallucination rate, that it's relatively immune to jailbreaks, and that it's great at following directions and essentially representing your brand well. Well, let's pull on that thread a little bit. From what you have seen both at Anthropic and with companies you work with, what are the technical and organizational best practices that companies should follow as they build their applications, all the way from proof of concept through production? Yeah, I've seen this arc many times, and I've seen it go quite well in many cases. And so I want to walk you through what series of steps tends to lead to a really good outcome. So I'd say the first thing to do is to pick the right problem to apply AI to. So you'll want to pick something that is a genuine user problem or opportunity for new functionality. So in some cases, it's an internal pain point. So you may have some expensive, toilsome processing of data internally that's currently being done by humans. You could have a significant customer support burden where it's like similar problems over and over again. Anything that is quite toilsome and just like not overly difficult but needs to be done right and is currently being done by employees inside the company is often a good target for automation with Claude. Separately, on the customer-facing side, there's a lot that you can do to simplify user flows and make for a friendlier user experience via an LLM interface. So first step, identify a genuine problem, something that's either a significant cost savings or a significant opportunity to transform the user experience. Secondly, you'll want to make sure that this is high impact from a cost to revenue perspective to ensure that it's worth the investment of incorporating this AI system. Next, you'll want to make sure that it matches the current capabilities of the models. Model capabilities have been this rapidly moving target where the model of two years, a year, even six months ago is significantly different from the model of today. And so assumptions that were true a year ago are often no longer true. And to that end, it makes sense to very rapidly prototype and just try the feature out that you're looking to do. If you get decently good performance within a few days, chances are you can do prompt engineering or agent orchestration in order to boost the performance to the level that you need. Next up, it is very important to have evals. We've sometimes seen customers have vibes-based evals where they play with it for a bit and then declare it to be good. And that's often good for the first round where the jump is obvious and you're going from nothing to something. But as new models come out or you want to tweak the prompt or add new capabilities, it's very important to have a quantitative metric that allows you to see how Claude is performing on the task. And these evals can be human-graded. They can be automatically graded. There are a lot of different approaches you can use, but it's important to have a metric that everyone agrees on that you can iterate against. Basically, there's this process of prompt engineering where you're trying different approaches, you're trying different models, you're trying workflows, different types of agents, and you need a way of grading yourself. And essentially, if you follow that process and then you have good operational techniques for monitoring how the model is performing in the field, how it's interacting with users, what user happiness looks like around it if it's a user-facing model, that will really set you up for success. That's great, Matt. That's great, Matt. All right. So definitely no vibes-based evals. That's not a thing. They're okay for round one. Like, basically, you can do crappy science if your effect size is large, but you should move past that phase as quickly as possible. Absolutely. So maybe let's zoom out for a bit and try and contextualize this moment in time. You know, what's fascinating to me is of all the big trends we've seen emerge in technology and all the big shifts that happen, it feels like this one has happened faster than anything we've seen before. It really isn't that long ago that Gen.AI sort of emerged in our consciousness. A year ago, if you were at Google Next, you'd hear lots of companies that were doing pilots, AI pilots, but very few in production. And now a year later, we have some very large companies. You know, Verizon is using our CC.AI product as part of their call center to actually handle live calls as a first line for customers. We have Toyota that's using Gen.AI models on their production floor and are saving, you know, 10,000-plus hours a year in terms of savings. And so we've gone very quickly from, you know, the pilots that we're seeing last year to lots and lots of companies in production. I mean, Seattle Children's Hospital uses Gen.AI to provide guideline help to oncologists. And so, you know, we're starting to think about, well, where are we, you know, where are we in this moment with Gen.AI? And I'll do a little plug. You know, we've actually just put out from Google the future of AI perspectives from startups that, you know, actually interviews a bunch of key opinion leaders and thought leaders in this space to try and sort of put into context where we are. So with that, maybe I'll turn it over to you to say, you know, where do you think we are in this big AI wave? And what do you think the next big thing is going to be in AI? Yeah. Yeah. So I think in the same way that Moore's law has predicted the power of microprocessors for, I think, like 45 years now, the scaling laws, which I believe the paper on it came out in 2020 or 2021, has done a great job of predicting continued improvement in model capabilities. I think the shift that we'll see over the next year or so is agents operating in much more open-ended domains. So people mean, like, fairly different things when they talk about agents. And a lot of things that people refer to as agents are actually, like, these tightly constructed workflows where, like, one prompt feeds to another prompt, which feeds to another prompt. And that's great. Like, there are a lot of advantages to that approach. But as models get more powerful, we think we'll see fewer workflows and more open-ended agents that are able to follow instructions so well that you don't need to have those workflows. You can simply give it the problem description and let it go. And I think where this is most impactful is that you can go from the more, like, limited domain agents that we've seen in the past, where, say, it has access to, like, zero to one information sources, to something where the agents are given API keys to, say, 10 different enterprise SaaS services and then asked to do a task that involves reading data from several of them. And writing data to several of them. So this starts to look like medium-sized tasks that might take a human hours to do and reflect the kind of work that employees do, you know, at their jobs on a daily basis. And so we think we'll see a lot of the more toilsome aspects of work start to get outsourced to AI systems. And this is pretty exciting. Like, I think pretty much every job, no matter how glamorous, has this, like, significant component of toil. And we think that a lot of that toil will get to be outsourced to AI systems. Well, it certainly seems like, you know, agents are the big theme. Certainly we hear a lot about it in the conference. You've seen, you know, the announcements we've made around our agent developer toolkit and agent space. We've also heard a lot of talk about the importance of interoperability between agents, right, and the different protocols that are emerging to make sure that we have that interoperability. As you look at your customers, what are they telling you about why agents are important to them, what they're excited about in terms of developing agents for? Mm-hmm. So agents are great in terms of task flexibility. So essentially agents are able to take on tasks where the information needed to do the task is not necessarily known in the prompt. And so the model needs to go out and get that information. Or separately, the system may need to make changes or take actions that are too big to be done in one shot. And so you see this a lot with coding agents where essentially it'll make some changes to a few files. It'll look at those changes. It'll decide to make more changes. You know, it may write some unit tests and run them. And so this is the sort of thing where, like, it was not possible a year ago to do this sort of thing with a large language model, but it is now. And so that's opened up a whole category of tasks that didn't exist before. Another thing we've noticed is that search agents have become incredibly powerful. In general, I think of these AI systems as not being standalone, but instead being, like, a partnership between the human and the AI. So almost this centaur, a human-AI hybrid. And how do you make that centaur as capable as possible? And so this draws me to think about the unique strengths of humans versus LLMs like Claude. And one thing that really stands out is that Claude is able to read and recall and synthesize information way better than I can. So Claude can read 100 times faster than I can and then draw the relevant facts out and present them to me in a structured way that's optimal for whatever task I'm doing. And so humans, on the other hand, are better at long-term planning, better at strategy, and, like, still have a bit more common sense. Like, we can often figure out when something is irrelevant or out of scope. And so when we think about making agents as effective as possible, we don't just think about the agent skill, but also the interface between the agent and the user. Well, Matt, there are a lot of entrepreneurs in the room. And even for large enterprises, on the one hand, everyone can see how much is possible in the open space out there, the white space. But on the other hand, they also see the rapid progress, you know, LLMs like Claude are making in terms of improving their functionality with each release. And so if you were giving advice to an entrepreneur or to an enterprise team that's looking to develop AI apps, where would you advise them to look so that they're actually creating, you know, value but not necessarily in the path of where the models are going to go? Yeah, it's interesting. Yeah, there is this, like, skate to where the puck is going aspect of this, where, like, if you are 6 to 12 months away from launching your product, you'll want to aim toward what the models will be capable of in 6 to 12 months. And that requires being particularly nimble and doing a lot of experimentation to see what is and isn't possible and what is on the edge of possibility. Because in general, things will be on the edge of possibility about a year before they are, like, robust enough for large-scale enterprise adoption. So in terms of startup advice, I would give different advice to AI-native startups versus startups that are focused in a different area and looking to incorporate AI. For AI-native startups, you know, a lot of this comes down to identifying a way that, like, AI can be at the core of driving a tremendous amount of value. Because essentially, these startups are, like, betting their entire existence on the capability of the model as opposed to just using the model to accelerate some aspect of their business or improve the UX. And probably the most successful AI-native startups we've seen are either democratizing some area of expertise where, for example, most people can't afford a lawyer. Most people can't afford regular sessions with the therapist. But via an AI-native startup that is delivering those services at much lower cost than a human can, they're able to provide a market that's not currently served by the human experts in this area. And, you know, it won't be as good as a human expert, but it will be available 24-7. It will give very rapid answers, and it will provide a significant bump in value over not having access to this expert at all. And I'd say on the flip side, we've seen a lot of successful startups that are accelerating the work of those experts. So if someone is, say, a lawyer or software engineer or, say, working in network security, Claude can do a tremendous amount to gather and synthesize information to help that human do their task much faster. And so those two categories of, like, democratizing access to expertise and then also helping the experts do their job better and faster by creating this human-AI hybrid, those two patterns have both been very successful. So that's AI-native startups. For startups where they're looking to incorporate AI into, like, an existing product, I think it comes down to what I was talking about earlier, where it's either, you know, reduction of internal costs or internal friction points in the business, so automating workflows, partially automating customer service, or something where the user experience of the product is being transformed by an AI front end. Great. If I were to put you on the spot and say, what are the areas that you think are, that you're most excited about to see startups where you think they'll be the most disruption or the most value created? Yeah. So I think one of the biggest areas is going to be fully leveraging the knowledge that could be available to the model. So the models have great world knowledge. You know, you can talk to them about the Peloponnesian War or quantum physics, and they have, like, you know, a very detailed encyclopedic knowledge of that. But models are only as good as the knowledge they have access to. And so if the model is doing something on a task that involves a large private knowledge base, so, for example, if it's doing, say, legal discovery for litigators or due diligence for venture capitalists, anything where there's a large private knowledge base that contains information that the users need, the model can do a great job of organizing that knowledge and using it to perform a task. And so we're starting to see this emergence of very good search agents, and in some cases, multi-agent systems. I actually think search is the first big, like, product market fit achieving application for multi-agent systems. And, you know, so we see systems out there in the market that will be able to consume a large knowledge base on the order of a handful of minutes and produce a high-quality report. And I think that is a very high-leverage capability that only a few startups are taking advantage of right now. All right. So you found an area. You were excited about the potential for AI to disrupt this category. Another big question is, okay, the foundation models are getting more and more powerful, and they're accessible to everyone. Agents are getting easier and easier to build through, you know, tools like ours and others. So then the question becomes, well, how do you create a competitive moat as a startup? Because the models are getting stronger. It's easier to build agents. You know, what are the dimensions you think that startups should think about in terms of creating their competitive moat? Yeah, so I think a lot of the areas that have historically have been pointed to as great competitive moats are still great competitive moats. So anything around network effects, you know, if you have some kind of two-sided marketplace, something else of that flavor, where having a large group of users continues to reinforce your competitive advantage, that still works really well. Knowing your users very well is still a fantastic competitive moat. You know, we've seen startups that really know how to apply AI toward satisfying the needs of their user base continue to do well. Data is still a great moat. You know, given that there are still these huge pools of private data that enable you to customize products toward your users' needs, you know, that's still a fantastic moat. Yeah, I think one of the things that I talk to a lot of startups, and, you know, a couple of themes emerge. One is if a startup has access to proprietary data sets, and it could be, you know, in partnerships with companies that, you know, were sort of older school companies but have this private data set, that is a, you know, strong competitive advantage. Another thing that I've heard a lot about is, you know, if you have proprietary distribution, if you have access to a customer segment in a proprietary way, as you said, that has always been a good competitive advantage, and that will continue to be a powerful competitive advantage going forward. So if you are building on proprietary data then, and that is part of your competitive advantage, how should startups think about protecting that sensitive data and making sure that while they're using that sensitive data, they aren't accidentally leaking either their own proprietary sensitive data or their customers' data into a model inadvertently? Yeah, that's a great question. It's actually a very relevant one as people are moving toward a world of agents. So there are standard data security best practices that continue to make sense, you know, whether you're using, you know, a traditional software system or an LLM. There are a few additional things you can do around LLMs. So, for example, doing data anonymization before feeding it to the model. For Clarity, Anthropic and Google are never training on your data. Your data is your data, and there are, you know, options for zero data retention and things like that that make a lot of sense. But when you start to get into situations where the model is an agent with access to lots of different systems, that data anonymization can become quite important. This is especially true if you think about an agent that is talking to users, browsing the web, potentially entering data into websites, and then also has access to proprietary systems. It is particularly important that you manage those information flows. And so, for example, in a customer support interaction with the user, the model should only have API access to that user's data. You know, you would not want to give the model carte blanche to search anything when talking to that user. You know, that is standard data security applied to LLMs. Separately, you'll want to make sure that data that is proprietary is not accidentally exfiltrated if the model also has access to the web. And that's something where data anonymization makes sense or having a guardrail that inspects any kind of data going out into the web to ensure that, say, proprietary code or PII user information is not leaked to the web. And, you know, we've done a lot at the model layer to ensure that the model does the right thing around this if instructed. But it is important to have extra security at the application layer. And there are a lot of architectures for this that can help ensure an extra layer of security. Great. So we've covered a lot of ground in terms of, you know, advice to startups. But maybe I'll give you an open-ended question around that. We've talked about security. We've talked about areas that may be interesting for startups to look at. But any other advice you'd give to an entrepreneur or an entrepreneurial team who is looking to do an AI startup? Yeah. So I would just say this is a very exciting time. Like, I've built startups for most of my career. Previously, I built and ran this company called Matterport that does 3D reconstruction in physical spaces. So I'm very familiar with the startup journey. And, you know, Matterport is ultimately successful, but it was an eight-year slog to get this tech to market and get it adopted. I would say compared to the world of, like, the 2010s, you can move much faster. You know, with AI-accelerated coding, you can build complete applications in, you know, a manner of months or weeks. And this is exciting in terms of being able to be nimble, being able to run a lot of experiments, and, like, very quickly iterate on product market fit. It also enables you to build big things with fewer resources by heavily leveraging AI as part of your development. Additionally, we've seen that you can accelerate other types of employees. So, you know, whether they're operations or marketing or business development, using AI. So everyone can do their jobs a lot more quickly. And I think startups that understand how to accelerate their orgs with AI will do a lot better than ones that don't. And so there are, like, many layers of advantage that you can pull on now, both internally in terms of your company's operations and then also in terms of your products. And this is also a thing where you have a chance to outmaneuver incumbents by understanding your users better and understanding how to use AI in your products better. So, honestly, this is an incredibly exciting time to do a startup. Super. We've talked a lot about what's happening right now, what are the opportunities, you know, that exist today for startups. But now let's look maybe a little bit further out, you know, in the three- to five-year time frame. How do you see, and it's hard to, you know, given how fast things are moving, it's hard to imagine, but where do you think we'll be in three- to five years? What are the big themes that will emerge that will define sort of the next three- to five years of AI? Yeah, it's five years is a long time. I remember getting a demo of GPT-2 in, like, January 2020, 2020, and, you know, that is so far from what we have now. So it is a bit hard to predict that far out. I think we will have much more powerful AI systems that can be assigned much more general tasks. And so I go back to this whole, how do you maximize the performance of the human-AI hybrid? And I think we'll start to see human employees become much more in a CEO role, where each individual employee has a whole team of specialized AI agents working for them. I also think we'll see organizations be set up fundamentally differently. And so to draw an analogy, like, I imagine that the workplace of 1975 was, like, radically different in terms of organization and sharing of information. Like, you couldn't just write a Google Doc and share it with your coworkers. You needed to, like, you know, type it out on a typewriter and bring it to a mimeograph and make a bunch of copies and hand-deliver them. And I think the transformation from, like, 1975 to, say, 2020 is about the same size as the transformation from, say, 2020 to 2030, where, given the already superhuman ability of AI systems to digest enormous amounts of information, you know, imagine what an organization looks like where an AI system has read literally every public document the company has, knows what every employee does, knows the whole code base, knows all the marketing activities, and that system can optimize the organization. And so I think this is a thing where, you know, organizational theorists talk about things like Dunbar's number, where it's like, oh, once your org gets above 150 people, you get a loss of trust, you get siloing, you get, like, coordination failures. I think a lot of that will go away with powerful AI systems that can help keep much larger groups organized. So I think those are a couple of large trends that I expect to play out over the next three to five years. Got it. Well, if that's the direction we're heading, if you look at what people are talking about today, what do you think is most overhyped and what do you think is most underhyped in terms of the conversation today? Interesting. I'm actually going to think about this one. I will say on the underhyped side, I think advanced use of knowledge bases is underhyped because, again, you know, access to, you know, access to more information unlocks additional capabilities. You know, in terms of things we've developed internally to accelerate our employees, I feel like 70% of its success was just hooking Claude up to each new source of information that employees use to do their jobs. I think there's an aspect of MCP that is underhyped. So to give some background on MCP, it's model context protocol. You can think of it as a low-level standard to standardize the way that LLMs talk to any kind of external API or any kind of tool. And what's great about MCP is that, you know, actually writing a tool description for a complex API, so, you know, suppose you're hooking up to, like, the Google Workspace API. It is a lot of work to get that right, and right now, developers are having to figure it out for themselves every time. MCP creates a standard whereby one company, whether it's, you know, Google or Anthropic, writes the MCP server once, and then everyone can use that high-quality API in order to effectively interact with Google Drive. And so I think the full downstream implication of MCP is that every standard enterprise SaaS app is going to be easily accessible by a large language model. And so I think we're going to see people load up an agent with, like, access to 10 or 20 different MCP servers, and that will enable the model to do much more complex tasks and the sort of tasks that employees do every day. And on the overhyped side, I will have to think about things. I'm generally very optimistic about a lot of areas. Yeah, you're right. I mean, it's hard to think of what's overhyped because there is going to be such a fundamental transformation in front of us and that almost every area, certainly on the enterprise side, almost every area of enterprise IT is going to be redone in an AI-first way. And so you're right, it is hard to think about, given the big wave in front of us, is there anything that's really overhyped or does it all play out eventually? I think we will see the era of, you know, the one-person companies that become large very quickly. I think that, you know, every enterprise is going to be AI-first, and I think that's just the way it's going to play out. So, well, with that, I want to thank you. This was a fascinating conversation. I really appreciate you making the time to be with us. I want to thank all of you for spending time with us this morning. And if you want to hear other perspectives on the future of AI and what that means for startups, I'll make another pitch for this publication that just came out. So thank you. Enjoy the rest of your day. Thank you, Matt. Thank you.