 . Hello, I'm Nathan Beach. I lead the product team for AI on Google Kubernetes Engine. I'm here today with Liyong from Snap, and we'll have a chance to speak to you about scaling your ML platform from zero to hero. At a very high level, there are three main paths that you can take to incorporate AI into your application, into your business. You can integrate with a model-as-a-service provider like Gemini, OpenAI, or Cloud. You can rely upon a platform-as-a-service like Vertex AI, or build on top of infrastructure as a service like Cloud Run, Google Kubernetes Engine, and Google Compute Engine. All of those options have very different pros and cons. There are great reasons to do each of those three. They make a lot of sense in different business scenarios, different teams, different applications. But today, we're going to focus on building on top of infrastructure as a service. And importantly, when you build on top of infrastructure as a service, you need to provide an ML platform so that that way your data scientists, ML researchers, application teams, do not need to interface directly with the underlying compute infrastructure, but can instead interface with your ML platform. And typically, when building that ML platform, you have a handful of key requirements that you need to satisfy, including being able to train models quickly, easily, effectively for your data scientists or ML researchers, being able to handle batch processing, data processing, other batch jobs like batch inference jobs. You need to be able to serve workloads reliably, in production, at scale, cost efficiently. All of this requires obtaining access to underlying accelerators like GPUs and TPUs. And you need to be able to observe the workloads that are running in production, debug them, monitor them, be able to take corrective action on them. And of course, all of this is done in a way that is secure, compliant, cost efficient, helps you get to time to value quickly, and done in a way typically that is vendor neutral and portable, flexible. And building out that ML platform takes effort. But in this talk, we will spend time explaining how Google Kubernetes Engine makes it easy and fast to build your ML platform, providing the capabilities and components that allow you to scale up and increase your time to value, scale up cost efficiently. And we're going to do that in two parts. First, talking about the components that make it easy to build a training platform. And then second, talking about the components that make it easy to build a serving platform. And then we're going to apply those, and Leon is going to come up and share about Snap's ML platform. So first, training. How does Google Kubernetes Engine help you build a cost efficient, scalable training platform? If you think about the journey for building AI training, typically you'll see that in three parts. First, customers evaluate the particular setup, hardware, architecture. Is this going to allow me to get the performance that I need for my training workloads? Once you do that, onboard an initial set of workloads and be able to demonstrate fast time to value in scaling up those workloads. And then as that's effective, then scale up to production, onboarding the rest of your organization, many workloads, many teams. And so in that journey, I'm going to talk through how GKE provides the tools and capabilities at each step of that journey. So let's start with evaluation. How does GKE help you build out an ML platform that allows you to quickly evaluate whether you have the right set of accelerators that deliver the performance that you need, the right storage, networking, compute? Google Kubernetes Engine makes available a diverse set of accelerators. These are accelerators all the way from GPUs to TPUs. We have the latest best of breed NVIDIA GPUs, as well as our newly announced seventh generation TPU called Ironwood, which is coming later this year. All of these accelerators are available through Google Kubernetes Engine. And you can find the right accelerator, whether that's a small model inference, large model serving, frontier model training. You can mix and match accelerators to exactly the use cases that you have. Now, once you settle on an accelerator, set of accelerators, you need to obtain capacity for that. When you're maybe doing the initial evaluation or prototyping proof of concept, you probably aren't at that point ready to make any sort of a long-term commitment. Dynamic workload scheduler is a means of accessing capacity that is fantastic for batch and training workloads. So with dynamic workload scheduler, you can say, I want to run this particular job. Here are the requirements that it has in terms of compute, storage, networking, memory requirements. And then we will place that job into a global queue and schedule that job as soon as the capacity becomes available, often immediately. Dynamic workload scheduler has access to capacity pools that are exclusive to dynamic workload scheduler. So you can get access to capacity much faster, more easily than, say, on demand in many cases. And one of the beautiful parts of DWS is that once that node is provisioned for you, then it will not get preempted, unlike, say, spot nodes, which can be preempted. And so that makes it easy to run your batch job, data processing training job continuously up to a seven-day period of time. Many accelerators are also available at a pricing discount when accessed through dynamic workload scheduler. If you want to learn more about dynamic workload scheduler, I'd encourage you to attend this session tomorrow at 1230. Fantastic deep dive into DWS. Customers like Two Sigma have found that DWS is great for improving the obtainability of their GPUs. About an 80% better obtainability is what they have found compared to on demand. Customers like AXA, Autonomy, have found that by using DWS, they can dramatically lower the cost of compute by ensuring that they don't have idle resources sitting around, but instead access compute exactly when they need it. Now, if you have a period of time where you know that you're going to compute capacity at some point in the future, dynamic workload scheduler calendar mode is perfect for you. You can say from this start date to this end date, up to 90 days, here's the capacity that I need. Once we confirm that that capacity is yours, then you have assurance of capacity for that period of time. So this is really great if you're doing maybe a proof of concept evaluation or you have an upcoming product launch and you want to make sure that you guarantee availability for a spike in traffic during that product launch. Customers like Databricks find that DWS calendar mode is great at alleviating the burden of planning for GPU capacity. A fantastically helpful tool here in your onboarding prototype evaluation stage. One other tool that I want to highlight that's very helpful for evaluation is a tool called XPK, Accelerated Processing Kit. It makes it super easy to kick off a job. This is a command line tool, fully open source, that provides just a thin layer on top of GKE. It allows your researchers, data scientists, with simply two commands and zero Kubernetes knowledge, to create a GKE cluster, schedule a job on that cluster, and therefore confirm, validate that you're able to get the performance that you need with a particular set of accelerators. Totally open source tool. Check that out. Okay, so once you've evaluated, you've confirmed you've got a good configuration that's going to help you get the performance that you need, then you're onboarding maybe your first set of workloads for training, for data processing, and Google Kubernetes Engine helps you there as well. Cluster Director for GKE, which is now generally available, is fantastic for this. So what Cluster Director does is, as you scale your workload, you need a lot of nodes, highly distributed environment. In a traditional computing environment, the burden is upon you to coordinate all of those separate VMs, separate nodes. Cluster Director allows you to do that. Cluster Director allows you to treat those hundreds, thousands, even tens of thousands of nodes as a single computing unit. So Cluster Director makes it very easy to manage things like maintenance, to handle faulty nodes and repairing those nodes, to be able to have those reports, to be able to have those reported to you and have Google automatically replace them on your behalf, to be able to do things like topology-aware scheduling, where you are training a large workload and you want to make sure that you're making the maximum use of the network connectivity between those nodes. Cluster Director for GKE makes all this easy. It's all available via Kubernetes native APIs, such as node labels. So you don't have to learn any new APIs. It's Kubernetes that you know and you love. But it takes the complexity of distributed computing and simplifies it so that you can view the underlying VMs as a single compute instance. Many of our customers love Ray. Ray is fantastic, especially if you are a data scientist, you love Python. Ray makes it very easy to have a Pythonic native, Pythonic friendly API to then scale your compute, everything from your laptop to data centers global scale. However, if you're a platform team, Ray becomes one more thing that you need to manage. We, as Google, want to take that burden off of you and alleviate you of the undifferentiated heavy lifting that comes along with Ray. And so we offer, generally available, a managed Ray operator in GKE. A single flag that you can pass into either a new cluster that you create or an existing cluster. We also collect together all the logs and metrics coming from all of your Ray clusters, Ray jobs, your workloads that are running through Ray. Make it easy for you to get good visibility into those and apply security best practices through the managed Ray operator. Customers like Geotab have found that using the Ray operator on GKE has dramatically simplified their workflow, helping them to spin up new Ray clusters in just 30 minutes. And so that's a real game changer for them. Now, if you want to interact directly with Kubernetes native APIs and you're running jobs at large scale, we've been working extensively in the open source community and have made available the Job Set API. Job Set is a Kubernetes native API for managing a group of Kubernetes jobs as a single unit. So it alleviates you from needing to do all the coordination of multiple jobs together spread across multiple hosts. You get a single Kubernetes API. And this is ideal, especially for cases like multi-slice training on TPUs, where you're training across many large TPU slices, one single workload that's spread across all of them. The Job Set API is fantastic for that. Okay, so you've onboarded maybe your first set of workloads. And then it's time to scale up into production, where you want not just one team within your organization to be running training jobs, but you want many teams across your organization to be running all of their workloads, training, data processing jobs. And here, GKE helps as well. Typically, when you have many different teams, that introduces a new set of complexity for you, where those teams perhaps have different relative priorities or relative levels of importance within your organization. You want to make sure that none of those teams are starved of underlying compute resources. But you also want to make sure that compute resources don't sit idle, maybe because one team doesn't need all the compute resources that you would have otherwise allocated to them. And therefore, other teams can borrow those resources that would have otherwise sat idle. Q, with a K, is absolutely perfect for this use case. We, as Google, have partnered with the broader Kubernetes open source community to develop Q. And Q provides a job scheduling system that works perfectly for multi-tenant environments, allowing you to maximize the utilization of your underlying compute resources, share those resources across many teams, give each team a guaranteed minimum amount of access to resources, while still allowing other teams to borrow resources. And within GKE, we have integrated Q with all the best of Google, including dynamic workload scheduler and topology aware scheduling that I spoke about earlier with cluster director for GKE. One of the other key aspects of productionizing your environment is monitoring, observability, and debugging. Here, too, we've got you taken care of. So GKE provides a managed DCGM exporter and collector. If you're not familiar with DCGM, it's a common NVIDIA tool to collect and expose a whole bunch of rich metrics on GPUs. But we don't want you to have to do all the burden of collecting those metrics and building out dashboards for them. And so if you would like, you can enable within your GKE cluster our managed DCGM component. That collects DCGM metrics from your GPU nodes, stores them in Google Cloud managed service for Prometheus, and then renders those metrics in dashboards that we have already built on your behalf conveniently available in the cloud console for you. So you can get metrics like what you're seeing here showing GPU utilization across workloads, across node pools, across your fleet. You can better understand things like memory utilization and IO utilization, power consumption, lots of other rich metrics. There's a talk later today at 4 o'clock, a breakout session that gives a great deep dive into all the observability tooling that we have provided to you in GKE. I'd encourage you to check that out. It's not just DCGM metrics that we provide. We also provide TPU metrics. So whether you're using GPUs, TPUs, or both, we've got you taken care of. Within GKE, we export metrics coming from TPU nodes. Again, collect those and then have dashboards that render those. Those are default on. They're totally free to you. You can get similar metrics to what you get on GPUs. You can also get those on TPUs. Things like utilization of the underlying compute, memory, IO, and more. If you are running jobsets, which I talked about earlier, we also provide observability for jobsets. So because we know jobset is commonly used for training, and one of the common challenges that customers face when training is that you get preempted for whatever set of reasons. Maybe underlying hardware failure, node failure, a job gets preempted maybe because it's a lower priority job. You have some other interruption. And we don't want you to have to do all the work to evaluate the health of those workloads. So built in to GKE is the ability to get metrics like the mean time to recover when there is an interruption, as well as the mean time between interruptions. And we render all those in a dashboard that we've built on your behalf in the Cloud Console, as well. The breakout session that I referenced earlier, later today at 4 o'clock, we'll deep dive into all of this and more. We also know that it's not just observability that you need into production health, but you also, in a multi-tenant environment, want to be able to attribute cost. You want to be able to say, I understand this team is spending this amount of money. These workloads are costing me this amount of money. And so we've built in support to GKE for cost attribution. So we'll look at the Kubernetes labels that are on your workloads, on your nodes, and you can break down cost by Kubernetes labels, whatever arbitrary labels you choose to apply within your organization, and get great visibility into how much workloads or teams are costing you. All this can be exported to BigQuery, as well, for rich analysis within your organization. So in all these ways, GKE provides you the tooling you need to build out a rich training platform. But it's not just training. We also provide you a rich set of capabilities and tooling to build out an ML serving platform. And like training, you tend to follow a similar customer journey. Evaluating, then onboarding, then scaling up into production. At each of those steps, GKE provides you the components, the tooling, the resources you need to build out a robust, cost efficient, and scalable ML serving platform. So first, how does GKE help you build out an evaluation platform? We are thrilled to announce GKE Inference Quick Start, which is available as a preview today. GKE Inference Quick Start, which is a great tool for the software. One of the common problems that we hear from customers is that it can be challenging to decide what is the right set of accelerators, compute, memory, underlying infrastructure for a particular model that I would like to serve. How do I configure Kubernetes? How do I configure my load balancing in a way that gives me optimal performance, given the latency goals that I have, the throughput goals that I have? And so to help with this, we've done the heavy lifting of doing extensive benchmarking, automated benchmarking for a large combination of open models, open model servers, accelerators, and various target latency and throughput goals. Then you can come to GKE Inference Quick Start and say, here are the goals I have. Here's perhaps the accelerator that I'm looking at. And we will provide to you a Kubernetes manifest that will help you achieve your target latency or throughput objectives with the given accelerator model, model server that you're looking for. This makes it really fast and easy to get started in your evaluation of GKE for inference workloads. We also know that obtaining capacity is a very important part of evaluating any platform for AI inference. And we've got you covered here as well. I talked earlier about dynamic workload scheduler being great for batch workloads. We are thrilled to announce a new set of improvements here at Cloud Next to dynamic workload scheduler that make DWS work really well for production inference workloads as well. So you can have DWS function in a queued mode that makes it great for training. But newly introduced today is a non-queued mode where just like, say, a spot VM or an on-demand VM, you can request that VM. And either you'll get it provisioned immediately or you'll get a stock out. That's now the same way that dynamic workload scheduler behaves too. So you can go to DWS and say, I need this particular capacity. You know, these accelerators, node, compute, memory, et cetera. And DWS will access its capacity pools to grant you that capacity immediately or we'll return a stock out. That makes it great for production serving where you need the capacity immediately. But we've done more than that. So dynamic workload scheduler will grant you access to nodes up to seven days long with no preemption during that period of time. But we know that then you need to go look for new nodes to replace those nodes at the end of that seven day time. We don't want you to have to do that. So we've taken the burden upon ourselves to do that for you. You can configure it. And we call this node recycling. So the way that it works is you can say, when this particular node has less than a certain amount of time left to live. Say less than 48 hours to live. I want you, GKE, to do the work of finding a new replacement node via DWS. We'll go looking for that capacity for you. Once we find it, we'll provision that new DWS node on your behalf for, say, seven days or however long you configure it. We can shift your workload over to the new DWS node that we provisioned, tear down the old one, and it's all seamless on your behalf. We're taking that undifferentiated heavy lifting off your shoulders. You, as a platform team, can do more of what matters to your users and will take the burden of getting capacity on your behalf. All of this also has the pricing benefits of DWS. Custom Compute Classes is another amazing innovation that helps you find capacity for your workload. So Custom Compute Classes allows you to specify, here's my top priority, machine shape and provisioning method. If that's not available, then please fall back to a second priority, machine shape provisioning method. So for example, you can say, I've got this reservation, this specific reservation, and I would like workloads to use that reservation whenever it's available. But if that's not available, then please fall back to any reservation that I have for this particular machine shape. If none of my reservations for this machine shape are available, then fall back to Spot, or fall back to Dynamic Workload Scheduler, which I just spoke about earlier. Customers like Shopify have found that custom compute classes play a critical role in helping them find the capacity they need during their most critical and demanding events. One other tool that we have that's amazing for inference workloads is that with other clouds, most other clouds provide you a single VM with exactly eight H100 GPUs. We do offer that. But we know that many of your workloads don't actually need a full eight H100 GPUs, especially for inference. And so Google Cloud provides VMs with one, two, four, or eight H100 GPUs. The only large hyperscale cloud to offer you this level of flexibility when accessing H100 GPUs. Customers like Tab9 have found this level of flexibility in accessing GPUs to be very helpful for their business because they can right size and get exactly the number of GPUs they need, which saves a lot of money for the latency goals that they want to achieve. We're also thrilled to announce VLM support on TPUs and GPUs that allow you to flexibly move between GPUs or TPUs. We've partnered with the VLM community and the open source to add support for TPU as a back-end to VLM. There's a breakout session later today that I would encourage you to go to if you're interested in using VLM and TPUs. Once you've evaluated, then it's ready to onboard your workloads. For that, we've done a lot of things to help you onboard your serving workloads, including partnering in the open source community to build out leader worker set. Just like job set makes it really easy to coordinate a set of jobs, leader worker set makes it very easy to coordinate together a set of Kubernetes pods as a single atomic unit. And this is really important for multi-host inference. For example, spread across multiple TPUs, or if you have a large model, spread across multiple GPU nodes. We also know that you want to scale up quickly. That means cold start latencies need to be low. For that, we've introduced secondary boot disk, which allows you to store container image on attached disk and dramatically accelerate your time, spinning up new nodes and getting a container running on those nodes. Vertex.ai's prediction endpoint, which is built on top of GKE, found that in their tests, this offered up to 29x faster container image polls in startups. Google Cloud Storage Fuse also provides fantastic benefits for startup time. So Cloud Storage Fuse gives you portability across clouds, but it also allows you to read data from Cloud Storage much faster, which is particularly important for workloads that require data loading before those workloads can be ready to handle user traffic. Customers like OpenX find that GCS Fuse reduces pod startup latency by up to 40%. Now, once you get those workloads onboarded perhaps from one or two initial teams, you want to scale up and have your production traffic served using GKE, have all of your ML serving workloads go through GKE. And for that, we've got you covered as well, helping to do a lot of the work that you need to build out your ML serving platform. GKE Inference Gateway is one of the new innovations that I'm most excited about that we've announced here at Cloud Next. GKE Inference Gateway transforms the way that you serve models delivering substantially better throughput and lower latency. And it does this because unlike traditional web serving where most of your requests are homogeneous, your responses are relatively homogeneous in terms of compute required, in terms of the latency profile of them. What we've observed with generative AI serving is that you have widely varying amounts of compute required and latency for your responses. Inference Gateway intelligently routes responses to one of the replicas of your model servers in such a way that you can maximize throughput and reduce latency for each individual request coming in. Inference Gateway can understand the intent of the request and route that request to a replica of your model server that's going to deliver optimal performance, maximizing cache hit rates. GKE Inference Gateway We also know that in production, you need to be observing all of your workloads. And for that, we've built out a number of serving dashboards that make it very easy to understand application serving metrics. So we've built out support for VLLM, Triton, TGI, plenty of the common model servers that are out there, scrape application metrics that are exposed by them, and then render those metrics and dashboards that we've built on your behalf. We also know that one of the additional burdens that comes along with building out an ML serving platform is finding the right amount of compute resources for you. And for that, we've made GKE Autopilot generally available for the past couple of years. Autopilot is loved by our customers. Autopilot alleviates from you all the burden of managing compute and making sure that you have the right set of compute available for your workloads to run on. We provide SLA at the pod level for workloads running on Autopilot, and they're backed by Google SREs. This leads to lower total cost of ownership. And customers like Finnavox have found that GKE Autopilot helps to abstract away a lot of the infrastructure management and the burden that comes along with managing that infrastructure, helping them focus on what matters to their business. So in all these ways, I've shared a whole bunch of ways that GKE helps you build a training and serving ML platform. And now, Lian from Snap is going to come up and share about Snap's ML platform and how they are using the tooling that I've talked about here. Thank you. I'm glad to introduce how Snap will integrate with GKE to scale up our machine learning platform. So for us, basically, we use GKE to get the results, including all kinds of CPU and GPU. So basically, we are mostly using QB flow to implement our work job management system. Based on the trainer operator provided by QB flow, we can easily ask GKE resources and then define how it's being scaled up from other technology provided by Google. So this is very convenient for us to scale up our machine learning platform. We are asking different kind of resources from Google. So based on different GPU, we actually have different strategies to manage our job. So for example, our team is using T4 GPU still a lot. And because GKE could provide us with enough T4 GPU, so mostly we still rely on on-demand mode to gather resources from GKE. But for alpha GPU, because it's newer compared with T4 and it's more popular than T4 GPU, so we use reservation and the DWS dynamic workload scheduler introduced by Nathan before. So that's a lot of the DWS. So I want to mention that DWS is actually one of my favorite feature provided by GKE. So we don't have a lot of alpha reserved, actually, because we need to pay the money, you know, for every second, even if we are not using the alpha GPU. However, when we have to run a lot of training at the peak time, we definitely need more alpha GPU than Euro. So in that case, DWS is one of maybe the best choice. So we are relying on DWS flex mode mostly. And we can scale up the resources provided by GKE easily. And then we can release the resources after our training jobs are done. And we don't need to worry about the further cost. The other type of GPU we use a lot at snap is a 100 GPU. So a 100 actually is very difficult to get from on-demand results. So currently, we mostly use it with reservation only. We can base it on the reservation. We can ensure our job could get started very quickly from GKE. The last type of GPU I want to mention is actually H100. It might be the most powerful GPU we are using right now. But we haven't adopted in our production a lot. So currently, we mostly rely on dynamic workload scheduler, too, so That we don't need to work with GCP with complex arrangement to get the reservation. Our job can be benchmarked very quickly so that we can see the numbers and Then make future decisions if we want to use H100. So I want to introduce more about dynamic workload scheduler, How we really use it in our system. So basically, mostly this is for L4 in our lab. So when we submit the job, we can check our GPU reservation. We can see if we have enough resources from reservation and Then submit the job to it at the very first. This is mostly for cost saving. We don't want to waste any money if reservation still has GPU to Support our job. If we cannot see enough resources From reservation, then our choice is to submit the job to our Dws cluster so that we can try to get this kind of on-demand Resources from GKE to support our job. So Dws is also a queue managed by internal GKE. So it can help us to figure out if there is enough resources to Write based on the priority we set in our job. If dws couldn't get started, actually we have our own handler. We will wait a little bit time and we see, oh, dws still has no Resource, then we will try to cancel this job so that we can Resubmit it to our reservation to see if reservation has enough Resources, maybe after one hour or two hours. So this is mostly about how we use, how we manage our job, CRD on GKE. So as I mentioned before, we are mostly relying on Qubitflow Training operator now to manage our jobs. We have a plan to support different types of job in the future. The most obvious one will be PyTorch job also provided by Qubitflow. We are also looking to how to use Reet before. But in our current production environment, we haven't started to Adopt Reet yet. Reet will be integrated with our System and it might have benefit for our data part. That's basically why we want to try it in the future. Our GKE scheduling system is mostly provided by Qubitflow. So this is also very popular feature right now. So without Qubitflow, it would be very difficult to ensure that Your job wouldn't deadlock each other. We are relying on Qubitflow to ensure that the job wouldn't Start until it can guarantee all the resources it needs. Next thing is about preemption. We also benefit from using Qubitflow with the preemption Start, preemption sighting. So we can set priority of our job. Some job can have a higher priority, which is our production Job, while some other experiments jobs can have lower priority. So we can always ensure that the higher priority job can preempt Lower operating job so that they can start first. This is very important for our production environment. Basically, we also want to, using Qubitflow, we can have multiple Team support. We can ensure that Qubitflow has Different quota for different teams to ensure that, you know, As snap, different teams may have different budget to set up Their resolution, the cost, how do they decide how many GPUs they can get. So Qubitflow helps us a lot to ensure We can allocate appropriate resources to each team. This is almost the last thing. I really want to thank To gke because after we scale up our training platform with gke, It gives us a lot of new benefit. The first one is about cost. I mentioned here it's up to 75%. It's mostly because determined by Different resources that you are using, like, you know, the Machines provided by gcp, like n1 machine, n2 machine, c2 machine. Different machines could help us to benefit different cost Saving. But the biggest cost saving we can get is actually up to 75%. The other thing is that after we start to use gke, we see Better stability improvement. It's mostly because we have more Flexibility to control our job. Like, we can decide the restart Policy for our port. We can avoid restarting the whole job if One, you know, this is tensorflow async training mostly for us. If one worker port failed, we can restart immediately from the Port level. We don't need to worry about the Whole job. And gke is very stable for us. Since we started to adopt using gke as our training platform, i can Say that after years, we only see one issue, only one issue with Dns from the gke cluster. And actually, this is also could Be handled by customer side. You know, if one cluster is Done, if you build up your system very robust, you can switch to Another cluster to continue your job. And you just need to ensure You disable the bad cluster to not accept new workloads for temporary. After gke is recovered on their side, you can easily bring your Cluster back. So last thing actually is our Future goals. We haven't adopted using Customer compute class yet. So mostly we are relying on Different cluster to use resolution dws. But in the future, we Want to group everything together. It's very important to ensure we Short our job scheduling delay even further. The other thing is Mostly for how the resources will be managed inside the snap. So inference team and training team are different teams, and They have different number of resources. And they have different Using user python, you know, like in the morning inference might Use a lot of gpu. We have a new plan to ensure that Different teams could share the resources from the reservation. So that we don't need to worry about wasting money here. We also want to have flexible quota management for this Sharing. Like this is very good feature provided by q2. You know, different teams can have different quota like 10 for team a, 500 for team b. But they can borrow. You can create your own siting to decide how many quota you Can borrow or lend from different teams. So inside one company, If you have a lot of gpu for different teams, this flexible Management siting can help you to save the cost even further. Last thing is we want to use gke to build our Infrastructure in the future with the specialized support. We work closely with gke engineering team. Even including the q team and a lot of other teams. So whenever we get issues, we can get immediate and great Support from gke side so that we can build up our infrastructure For other stuff like how we use re, how we use pychart In the future. All right. Yeah, I think that's all My slides. Yeah, thanks for attending this session. .