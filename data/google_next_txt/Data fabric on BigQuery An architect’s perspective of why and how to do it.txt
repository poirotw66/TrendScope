 Good morning, and welcome to day three of Google Cloud Next 2025. You've had two full days of sessions, conversations, meetings, maybe even an evening of fun and music at the Killers concert. So looking at this audience here, I know that you are a very motivated bunch. Either that or your friends and family of the speakers. So either way, welcome. Thanks for coming. We have an exciting topic for you all, building a data fabric using Google Cloud technologies. And we're going to talk about why it matters and how to do it best. My name is Sudha Srinivasan. I'm a product manager in the Google Cloud BigQuery team, and I'm here with Pedro Esteves, who is a data analytics solution lead from our EMEA team, our Europe team. And we are delighted to have Vinay and Chandu. Vinay Pai, Chandu Bhuman, who are our speakers from Virgin Media 02, one of the iconic telecommunications providers in Europe, and customers of ours for over six years. So I'm going to kick things off, talk about what a data fabric is, and more importantly, why are we even talking about data fabrics? Why does it matter? Who cares? What is the problem that it solves? I'll also talk about what Google Cloud technologies you can use to build a data fabric. Once I do that, I'm going to have Chandu and Vinay come up and talk about their journey in data transformation using Google Cloud technologies and BigQuery and what they have built over the last six years. And it's a fascinating story. Lots of great numbers, and they're going to share their architecture, their current architecture as well. Then we're going to have Pedro, Chandu, and Vinay come up and have a conversation about lessons learned, right? So we call it do this, not that, because along the way, if you've worked on something for six years, you've learned some things that you should not be doing, so they're going to be sharing that in full candid spirit. And we would love to take questions from the audience. If you have any questions, please bring it on, okay? Let's get started. What is a data fabric anyway? Now, to answer that question, we should start with what's the world like right now, right? It's not an exaggeration to say that AI is transforming the world. Over the last couple of days, you have heard from Google executives, product managers, engineers, as well as customers like you, about how they are using AI to turbocharge and change customer experience, how they're getting more agile, how they're getting more efficient using AI, right? But the one thing that we have to keep in mind is AI is fueled by data. Data is what fuels AI. So if you have poor data management practices, you're going to have bad AI outcomes. Why? Because you cannot use data that you can't find, understand, and trust. Because that is the foundation for using the data that you have. And that is exactly where data management comes in. According to Gartner, for companies that don't have an AI-ready data practice, over 60% of their AI initiatives and projects are going to fail or are not going to deliver the value that they expected from it. Right? So that's a very big stake. So this is why data management is important. So let's talk about why data management is hard. Okay? Because if it were easy, then everyone would be doing it perfectly. So why is it so hard? There are three main reasons. The first is data comes in many shapes today, and it sits in silos. Right? In the past, we had mainly tabular data. It was easy. It was structured. Now you have a ton of unstructured data. You have images, PDF files, IoT information, SAS applications. Your data is everywhere. And getting all that data, even having visibility across your entire data estate, is a huge challenge. And we all know that the real value of data is unlocked when you're able to combine data from multiple sources and use it together. Right? So that's the first problem. The second is things are moving very, very fast. Right? The rapid pace of change in terms of the regulatory landscape, in terms of actual data, movement data coming in, makes it almost impossible for you to use manual data management processes, like manual tagging, manual cleanup, and keep up with this pace. Right? It's just not tenable anymore. So that is a huge problem. So manual data management, which a lot of companies still do, is very hard. It is very time-consuming, and that is where a lot of our budgets are going. The third one is around the culture. Many organizations don't yet have a strong culture of data ownership. This notion of data as a product and making sure that the data you produce is maintained and accessible to the broader set of people in the organization. Right? And this is something that, because there is a poor understanding of what exactly governance is and, you know, not even an agreement on what governance is and how it should be managed, you see a lot of teams kind of taking different approaches to data management and governance. Okay? So three main problems. You have silos. The rapid pace of change makes manual data management very impossible to keep up. And the third one is culture around data ownership. So how do we solve these problems? This is where a data fabric comes in. Right? A data fabric is a data management architecture that enables organizations to provide unified access in a governed way to the data that they have. Okay? So it includes a lot of different things that are needed to make this right. A data fabric done right is going to simplify the way that your teams access their data. Okay? Why is that important? Because today, most companies are taking six to eight months just to get to that start line where they're going from raw data to data that their data analysts and scientists and engineers can use. Right? So that is the time that it's the prep time. And what a good data fabric can do is use automation to shrink that time significantly so that you're able to start using the data that you have more effectively. That enables you to get faster access to data and insights. What we call time to insight or time to first insight. Right? When people can start actually using it. It allows you to efficiently manage security, compliance, and risk in your organizations. And finally, it is the foundation for AI ML, as we just said, because good AI requires good data. So what are the components of a data fabric? Right? Data fabric is a term that I think people have thrown around a lot. Gartner has talked about it. Lots of companies are talking about it. So what goes into a data fabric? In essence, data fabric has four components. The first one is data integration. You need a way to bring the data that you have across your entire estate into a single unified view. We are not necessarily talking about moving all the data into one system, but we are talking about having a single unified view of all your data so that it is one part of one logical system. Right? So data integration enables you to do that. The second one is metadata-driven data management. Metadata is the foundation for a lot of the agentic capabilities that we have talked about over the last couple of days. Right? So if you want to build data engineering agents and governance agents and analyst agents, what do you need? You first need to have an understanding of what the data is. Metadata lets you understand what data you have. So metadata management, metadata-driven understanding of the data is important. The third is, once you have strong metadata foundation, you can actually do good governance. Right? Access control, making sure that the right people have access to the information, good observability, auditability, all the things that you need to do to make sure that the data is being used appropriately. And the fourth, once you have these three layers down where you can find the data, understand the data, and trust the data, is to actually activate it in a number of different ways. Right? So BI, analytics, AI, agentic capabilities, all of that can be built on a data foundation. So these are the four layers of the data fabric stack. So one of the questions that we get asked a lot is, you know, I've heard of data meshes, and in fact, we did a talk a couple of days ago about data meshes. So the question is, you know, should I do a data mesh or a data fabric? And if you really think about it, data fabrics and data meshes are not competing ideologies or competing concepts. In many ways, they complement each other. Okay? At its core, a data mesh is about federated data governance and decentralized ownership. Okay? The concept is, you cannot have a central team that is essentially owning all the data data, making sure that all the metadata is kept up to date, making sure that everything is being managed correctly. It is simply not going to be possible to scale that, right? They become the bottleneck. So a data mesh is about decentralized ownership because the data owners are the ones who know best what the data is about and how to maintain it. But you need unified data integration and data governance. Why? Because you don't want every single team going and building their own pipelines and duplicating data all over the place. That also makes downstream consumers kind of wary of whether the data is trustworthy or not. So that balance is essentially what you're going for. You're going for decentralized ownership and centralized, unified data management. And the centralized, unified data management is where the data fabric comes in. Okay? So that is conceptually how these two things tie to each other. So BigQuery is Google's unified data and AI platform. It is an autonomous platform so it has a lot of agentic capabilities built in. And BigQuery is designed to simplify the way that you do data management, simplify the way that you use your data. Right? It allows you to get going faster. Right? And use the entire range of data capabilities that you have. So BigQuery combines a multimodal data foundation. We've got capabilities to have structured and unstructured data within BigQuery. You've got a unified governance layer. I will talk more about that. So unified governance layer that is built right into BigQuery and allows your data users to go and find the data, understand what the data is doing, and start using it within BigQuery itself. And then it's got integrated AI capabilities. So a lot of the things that you heard in the last couple of days around AI-powered data preparation, AI-powered data engineering is all something that we are building on a foundation of governance and metadata that we have in the system. And then we have a number of different ways to activate this data. Right? Whether it is SQL processing or Spark, any number of open source engines, the AI query engine which was announced in the keynote, streaming processing, stream processing, all of that can be built on the same non-duplicated data store that you have. Okay? So you've got common data foundation, unified AI governance layer, integrated AI capabilities, and then activation on top. This is what BigQuery offers. And if you've been with us on this journey, you've seen this show up in a lot of different products. What we have done is now brought it all under the BigQuery unified data and AI platform. So let's go into each of those four layers very quickly. And then I want to pass the ball to Chandu and Vinay. First thing is data integration. Data integration is an incredibly complex domain and the complexity comes from the broad range and variety of data sources, personas, and types of data that you're dealing with. Okay? That's why a lot of organizations end up spending a lot of their cycles building and maintaining their data integration pipelines because it is a lot of moving parts and it's fairly complex. Google Cloud and BigQuery offer a full suite of data integration capabilities that cover the gamut of requirements that organizations have. Whether it is structured data, Spark workloads, unstructured data, or you're pulling data from applications, whether you want streaming or batch, whether you want ETL, ELT, or something in between, we've got capabilities in the product. I'm not even going to go over the full range, but you've got a lot of tools and services at your disposal which are built and tested to work well with BigQuery. We also work with a number of partners, so if you want to use partner tools to bring the data in, transform it, and enrich it, you can do that as well. The key thing here is integration is something that takes a lot of time and it is also the place where we can actually use AI and the power of Gemini to simplify things. So we announced general availability of BigQuery data preparation which uses Gemini to detect when there are things like schema mismatch and things that need to be corrected in your metadata and then suggests or makes recommendations around pipeline changes that you need to make. Right? This is one simple way, one first step that we're taking to make this process simpler because this is also where a lot of cycles go into just understanding what needs to get fixed. Moving on, unified data and AI governance. Are you all familiar with Dataplex? What we have done is brought the governance capabilities of Dataplex into BigQuery. when I say we brought it in, Dataplex still remains as a service and as a product. We are providing an integrated experience for governance within BigQuery. Okay, so now if you open up the BigQuery console, you can actually go and look at your catalog, enrich your catalog, manage your data governance from within BigQuery itself, and then the downstream aspects of that are exposed within BigQuery. So universal search, semantic search in BigQuery is powered by the metadata that we have, right, from governance. Similarly, data insights is a capability that the platform provides. That again is provided by governance capabilities. So BigQuery governance is unified in the sense that it is built right into BigQuery and provides you with end-to-end visibility from data all the way to AI. So we're actually giving you lineage for vertex models, for example. It is intelligent, so it is AI-powered, uses the power of the knowledge engine which we talked about, to actually go and enrich your metadata, suggest metadata. So it actually makes metadata generation easier. It provides data insights, just makes the life of metadata management a lot simpler. And finally, it is open. So we support open storage formats. It is something that you're going to be hearing about a lot more. Google's philosophy has always been to be open and embracing open source technologies like Iceberg. Underneath all the governance capabilities that we're talking about is the BigQuery, Universal Catalog. Okay? BigQuery Universal Catalog brings together your business, technical, and runtime metadata in a single catalog. So now you don't have multiple catalogs that you have to deal with. And the BigQuery Universal Catalog is automatically extracting metadata from all the different GCS sources that you have. Whether the data is in the databases, PubSub queues, GCS buckets, or BigQuery, we are pulling all the metadata, storing it in the catalog, enriching it automatically, discovering hidden relationships using the power of Gemini, and then using that to improve your experience, whether you're a data analyst, data scientist, or an ML engineer. Okay? That is what the BigQuery Universal Catalog does. It is foundational to all the governance and AI-powered agentic capabilities that you're seeing in BigQuery today. I will also close with this. We have support for multiple compute engines. Whether you're using SQL or you want to use one of the open source engines, we've got support for that. We've got support for, you know, computing, applying compute to unstructured data as well as structured data, all within a single platform. And finally, whether you're a data analyst or a data scientist or an engineer or you want to do AI, you now have one platform that you go to, one set of tools. You're not going to like five different places using five different things. You just come to BigQuery and you have the complete end-to-end experience within BigQuery itself. With that, I'm going to pass the ball to Vinay, to Pedro, to come and introduce Chandu and Vinay. Thanks a lot, Suda, and for the very good explanation of what is data fabric. So now we're going to talk about VMO2. VMO2 is one of the biggest telcos in the UK. And we have the data platform owners, Vinay and Chandu. But before we introduce them, I just wanted to explain what is VMO2. So we have a short video just to explain where VMO2 is coming from and what you're doing. I got a good, a good feeling. Yeah, I'm up on the cloud and coming back down. I got a good, a good feeling. Yeah, I'm up on the cloud and coming back down. I got a good, a good feeling. Yeah, I'm up on the cloud and coming back down. I got a good, a good feeling. I got a good, a good feeling. I got a good, a good, a good feeling. I got a good, good feeling. So we started the journey with VMO2, Google and us, me and Vinay, for the last six years. it's been a long journey. So we, the Vagifabric took a long time to build. So they're going to explain what we moved to Google Cloud. We moved a couple of platforms and we're going to explain how is it looking today and how is the Vagifabric is being built, right? So without further ado, Vinay and Chandu, you want to come to the stage? Thanks. Thanks. Thanks, Pedro. Thank you, Pedro. Good morning, everyone. Welcome to our session. Thanks for joining here. As Sudha mentioned, you know, there's a lot of commitment to wake up early at 8.30 morning. I've been waking up at 3 o'clock roughly every day, not because of commitment, but because I'm still used to the UK time zone. Well, we are from Virgin Media O2. That video which you saw, I just love that video because there's a marketing team doing it and I can't do slides like that. So it tells a very good story of what we do and what services we offer. We are one of the largest connectivity service providers in the UK, two iconic brands, Virgin Media and O2 that came together four years ago. We believe that connectivity is more than just a service. It's at the heart of the most exciting moments in life from unforgettable experiences to real world connections. And it's our promise to redefine connections and experiences so that our customers can be at the forefront of those magical moments. I'm Vinay Pai. I head of data strategy and architecture in Virgin Media O2. I've got my colleague. I'm Chandu Bhuman, senior manager for the data strategy and engineering. Thanks, Chandu. That's our customer base numbers. 5.8 million fixed line customers, about 15.8 million broadband contract mobile and around 45.7 total connections. We also provide IoT services in the UK. The data function in Virgin Media O2 is, as we call, unified by design and decentralized by culture. What I mean by that is we are a combination of both data mesh and data fabric and Chandu is going to talk a little bit more detail about it. But before diving into the details, I would just like to share a few more details about how we have come today in our current architecture. The last six years have been incredibly exciting in terms of our platform migrations. We migrated about six large data platforms from on-premises into GCP starting in 2019 with the Hadoop platform which we had got, which was a Virgin Media data-like platform. In about a year's time, we had moved that into cloud native completely. And after that, we started off with our NetEase data warehouse platform. Now, for those of you who know NetEase, it's a data warehouse appliance. It's pretty much like you chuck in into your data center, power it on, put the network and off you go. The one which we had, we had multiple of those instances was the largest which IBM could sell us. I mean, we had reached that point in scale where we couldn't grow anymore. And that was a big predicament for us because our data was growing, the estate was growing and we had nowhere to grow. And that was the reason we had to move out from the platform and move towards Google Cloud. That was a big program, massive program, two years in the making and we have decommissioned NetEase now, multiple NetEase boxes. When Virgin Media and O2 came together, I also inherited the O2 stack and one of the biggest data lake platforms we had, about 600 nodes of Hadoop cluster, was running the network data warehouse and this was the biggest platform in terms of size and scale. We were fortunate that just around the time when we were migrating this, one of a big program came in which roughly quadrupled the scale of the volume of data we were transforming. It's about 80 terabytes of data per day and 2.5 million events per second. That's the load which that event processing system takes. We couldn't have done that transformation if we were not on Google Cloud and it was only possible because we had just migrated that platform into GCP. We also did a few more platform migrations. Teradata Vantage is now running in GCP and our Hadoop cluster which is running in Cloudera has been migrated into GCP as well but depending on the type of migration we had to take different approaches. Some of them were lift and shift followed by cloud native. Most of them were direct cloud native transformation but it was not just a technology transformation. We had to make sure that we were adding business value as part of this journey as well. There's a couple of examples I can give in terms of what we did in that journey. One key program which we had was what we call a statical data ingestion. So our business wouldn't wait for five years, six years for all the data to be available or even one year for the end of those individual programs. What we had to do was we identified what are the key most critical business data assets which are required which are high value data assets and replicate that into GCP before the program completed. What that meant is our analytics users could use that data and they didn't have to wait for that platform migration to be completed. And once the migration was completed people just seamlessly switched over from the copied data to data which was now natively produced in GCP. We also did a lot of AI related programs for new products and services in GCP based on the new data which we are bringing into GCP and the key thing which we started just a couple of years ago was a big program on democratization of our data and this is a big story which we'll talk about as well in terms of how we have enabled those capabilities. It was not an easy thing. We had quite a lot of challenges but I just wanted to talk about what challenges we had prior to moving onto cloud as well. One of our biggest issues was the fact that we had as you saw we had multiple data platforms and they were all siloed. We had multiple data lake platforms multiple data warehouse platforms they would then go into an operational data store SQL servers and then at the end as you all know data ends up in an Excel file. So that was a challenge. I mean every time we copy data from one system to another it was a big task to keep that data quality in sync everywhere have the right governance on top of it and the amount of effort which was required to keep copying that it was just counterproductive from an IT perspective. What we have now done is moved all of that data into one unified data platform which is on BigQuery. And that itself has meant about 30% of our duplication of data we have been able to remove straight away. The other challenges we had was in terms of size and scalability. As I said the NetEase experience we had limited capacity to grow and from a data center perspective as well space, cooling and all those constraints we had to live within those things and it would take a long time for us to build and implement infrastructure whereas the business requirements were quite critical in terms of having that capability or capacity now straight away and we couldn't wait for that capacity whereas now we are more focusing on solving the business problem rather than on the technology which is required to provide that infrastructure. We also had huge capacity issues a classic problem would be where end of the month we would have to shut down access to our analytical users because the processing of that data at that point in time was more critical which meant key business capabilities about analytics had to be delayed because it was important for us to load the data and make it available inside our data warehouse but now data is pretty much available 24 by 7 for all our analytical users. So these were the challenges which we had but it's also important to understand what were the key business drivers why we went through this transformation program. One of the biggest business drivers for us was to improve our customer experiences whether it's personalized services or enhanced kind of call routing or it could be better fault diagnostics all of these experiences when our customers interact with us we wanted to make that better for us. We also wanted to improve the operational efficiency of our platform from a cost perspective and the key thing was for time to market as in how soon can we bring in new products and services into the market so that we can provide those services to our customers and finally as a large telecom company network services and service optimization was quite important for us. We spend about 5 million pounds per day in network investments and we had to make sure that that investment is optimized and in terms of where we roll out new 3G or 4G towers, 5G towers, where we expand on our infrastructure, where we dig the road so that we can put new cables and everything, that there's a huge amount of investment on the ground. We need to make sure that we are spending our money wisely, that's where data and analytics comes into picture. So this was a key driver as well for us. From a technology perspective, we always had a vision for building a cloud-native data platform and focus on managing the or building the solutions rather than managing the platforms as well. So that was what we have achieved by transforming our platforms into cloud-native. Moving on to the next slide. Okay. Thanks, Vinay. So as Vinay mentioned earlier, we are unified by design and decentralized by culture. So this is a logical representation of our architecture. So the way how we look at is we have like four key pillars in this architecture. The first one is engineering and the second one is modeling and the third one is data democratization and the fourth one is value creation. So I'm going to talk about all these pillars. So the first one is the engineering. And also we divided the engineering into the key building blocks, which is common platform and tools. So as an organization, when we were doing the cloud foundation, we have come together and agreed that we will use the common tools across the organization. And we have also centralized networking, security, privacy, and governance principles. They all are powered by the self-service capabilities. And we have an unified ingestion process. The ingestion, we have keep it as a unified one. The reason is, as Sudha was mentioning earlier, that integration or ingestion of the data comes with a lot of key challenges, like where we work with a lot of different sources, different types of data, different frequency of data, different formats of the data, different types of encryption. So there are various things in the ingestion. So we don't want to replicate that complex thing to each and every area. So we have unified it and kept it as a single entity across the board. So, and we, when we go to the, to the second one, which is modeling, we have layered into two layers of modeling. One is unified data layer, what we call it as, when we ingest the data, we keep the model of this very close to the source. So we can group this as a source data products. And second one is the business layer. So this is aligned to the business domains and business domain products. So that's how we have divided the modeling into the two layers. And the next one is how do we democratize the data? So now we have brought the data in and we have got a data in a single unified platform and we, how do we democratize and make the data accessible to all the, all the different teams across the organization. So we have set of common tools and the governance framework to democratize the data, which is a key function. And the last one, but it is where the value gets created. So the value in this area of, which is value creation area, the business and the technical teams come together and work together and each of the domains and the expertise of the domains is brought by the business and the technical teams will bring in the technical capabilities. They are combined and we create the value in this area. So how we have implemented this technically? So now if we, this is the technical architecture, the way, how we have achieved it. So the, the, the, the data integration. So again, if I go back to the integration, technically we have divided again into the three parts in the, in the, in the, in the or ingestion or integration. One is streaming, then the OLTP systems. How do we capture the data from OLTP systems using CDC? And the third one is batch ingestion. In the streaming, we use, we use the data flow and we use, we use PubSub for the streaming. And we do like quite a lot of complex streaming exercise. For example, when I was talking about one of the, the network capabilities, we process around 80 terabytes and on the fly in, in a day and 2.5 million records every second on the fly, we join the data. So it is, we, we, the data flow is able to handle the complex streaming capabilities. And the second one is OLTP systems. The CDC, we capture the data via the CDC tools. Primarily we use stream as, as one of the tool which we capture the data and we use the data stream. These are the two main tools what we use to pull the data in. And batch ingestion. So a few years ago, we, we have worked with PSO and built a batch ingestion in-house framework where we have large number of developers when they are ingesting the data, they just pass a configurable variables into the batch ingestion and, and this is our in-house framework what we have built. And the second key layer is, is data storage layer. This, this is where when I was talking about the two, two different models, business model and the unified data model. So this is where the data gets stored. One is like, we, we use BigQuery for the batch and analytics type of data storage and cloud SQL for API type of storage. And we use the, the ELT type of mechanism in BigQuery and to process the data, we first ingest the data and then we process the data in the BigQuery. And for processing, we also use data flow for streaming capabilities. And the, the, the last one where, where we use a business layer is where we use our data scientist teams and the analyst teams use BigQuery again for their deep dive analytics. And also we use a Vortex AI. And for the BI capabilities, we use Tableau and for the API type of data access, we use Apigee. All these are orchestrated from our automation layer, which is a central function across the board where we use GitLab, Okta, Terraform and Orca and, and the HashiCorp all for, for orchestrating and, and doing all the, all the capabilities across the board. So this is our, our current implementation, which we have framed the principles like six years ago during the cloud foundation. And we are all following the same tilt together and it has been quite successful for us so far. So that, that's where, and then Vinay will talk about the business benefits now. Thanks Chandu. So what have we achieved by doing this transformation, right? As Chandu mentioned, we are a combination of data mesh and data fabric. We have multiple domains in our organization and by making this data available to those domains, we have effectively enabled the business to create their own additional business logic and then add value on top of it. The examples I have given here are for the three key domains which we serve, which is commercial networks and operations. Now, from a commercial domain, one of the key problems which I always wanted to solve was around personalization and personalization could take many shapes and forms. It's about providing personalized marketing campaigns. It could be providing dynamic offers in terms of what the, what products and capabilities they can take. Or we have also built a lot of capabilities in what we call as our recommendation engine. These are all the capabilities which we have been able to provide or enable the services purely because all of our data is now integrated into GCP and provides the right quality and observability information on top of it. On the network side, it's one of the biggest and largest data sets which we have in the company, which is not surprising given that we are a telco. There's quite a lot of capabilities which we have enabled, but as I said earlier, the biggest one is around how do we ensure that our network expansion is happening in the right place. And that is both on the fixed side where we have to literally dig the roads and install fiber optic cables or on the mobile side where we install towers. And how do we make sure that they are in the right location and we're expanding in the right areas? That's a big, huge amount of investment which goes into it. On the customer experience perspective from a network side, again, we have enabled a lot of false diagnostic capability. How do we ensure that our customers get reliable services? If and when there is a connectivity issue, for example, how can we pinpoint where the problems are and solve those as quickly as we can? Those are kind of the services which we have enabled on GCP by virtue of having all our data in one place. And finally, on the operation side, again, there's lots of products and capabilities we have enabled about real-time routing and planning for field service agents. These are all the features which we have built in by having our data back in GCP. Lessons learned. Should we sit? Yeah. And we can. I can sit on this side. Yes. It was a long journey, as we all mentioned. It was six years building this data fabric where they brought lots of different systems from Adub to data warehouses and all into BigQuery so we can create that data layer for data fabric so we can build the business benefits. And I have a couple of questions for them because I've been working with them on this journey so I know roughly what they did. And it was a long journey and there was lots of things that went well but some things that didn't go so well. So this first question is about if you, what are the three lessons that you want to share with the audience of things that went well and that didn't go so well? Thanks, Pedro. So it's probably more than three but I'll go with three. I think the first thing which I would say is like, you know, think big but start small. As in, we knew exactly where we wanted to be and what we have as part of our end-state architecture. But we picked up small programs, delivered business value in that journey but consistently stayed to that track. That was one key learning. And as part of that activity, the other thing which we did was making sure that we added business value. It's not just a pure technology transformation. We are here to serve the business and we need to make sure that as part of technology, we are also adding business value. The examples which I gave earlier about doing tactical ingestion or providing those other capabilities via AI products and capabilities, that was part of our strategy to make sure that, you know, we are not just doing a technical transformation, we are adding business value as well. And the final point is make sure you have a good FinOps program. I think it's important to make sure that you have the right cost and value, you know, decisions in terms of when you are moving into cloud. Especially for a telco organization, it is all operating expense. And we had to make sure that each of our business case, when we are predicting that it's going to take X million pounds to spend versus what is on-premise cost, you have to stay true to that cost. And having a good FinOps capability is very important to, you know, make sure you are within those right boundaries. Thank you, Vinay. Yeah. It was more lessons, but we just have time for three. I think the second question is around, so you've seen they migrated lots of iDo platforms. There's two iDo platforms, one from Virgin Media and one from O2. There was two data warehouses. There was different approaches you took. Some of them were just pure lift and shift, other ones was transformation. This question is more about which one worked better, because we have lots of customers who try some of them lift and shift, other ones transformations. In your view, which one is the best one? No, we're... Let's answer that. Okay. Yeah, so I've been in this journey from last six years, from the day one, where we are doing the Cloud Foundation. That's where we started with Google PSO's help. From there, we have lived through these large transformation programs, and it has been quite a lot of activity happened in our area. So in my view, it depends on each organization to organization. But in my view, the most value-created situations or the migrations are the cloud-native transformations from the cost point of view, as well as from the performance point of view, and also the access management point of view, the principles of how do we manage the data and how do we manage the platform. The cloud-native transformations have given quite a lot of value. And also, we have done some lift and shift as well because of the transformation and because of the licensing constraints and because of the other reasons. But the value has been created by the native transformation. But each organization is different. So I think for us, the native transformation helped us a lot, especially the serverless capabilities helped us a lot. So we don't have to manage the platforms. That was the biggest, what I can say, in my journey, at least in the last six years. Okay. Thank you, Chandu. We have two more questions. The third question is around, I think, we showed a slide around data mesh versus data fabric. I'm just trying to understand VMO2. You followed both, right? So they don't have to be competing paradigms. Yeah. What's your view of how do you implement them both? What's your view on those different paradigms? So, yeah. So this, I think Sudha covered it very well, to be honest. So when we were starting our journey, our big bosses, we have come together and had a lot of brainstorming sessions about how do we go, how do we go about our journey because that was a big leap what we are taking from the on-prem world into the cloud world. So we have thought that we could be, we have read the Gardner reports and we had consultants who have come and tried to guide us. But both listening to the both areas, what we thought is we have to have a hybrid one. And data mesh and fabric, they are not competing with each other. They are always like they can live together and they can serve their purpose together. So that's where we were talking about, like the core principle of our, is in hybrid area. So I think they both complement each other. That's where we have seen that and we are running the show in the organization in that way. So it's pretty working well for us, to be honest. So that is where I see that they both can appreciate each other and co-lib together. That's our view, actually. Exactly. We see customers doing that a lot, right? The data fabric, but also the data mesh comes a little bit after. The last question, you've been on this long journey. What's next? What are you planning to do the next two, three years? Yes, big roadmap, Pedro. Last five, six years, as you have seen, we have been quite busy with platform modernization. And we are now at the very end of that journey. And now we are beginning in our second phase of journey, which is data modernization. And what I mean by that is getting more value out of that data. So there's a lot of key capabilities which you are looking in investing it. Building a data contract is one of them. They are enriching our data catalog so that we can make our data more usable for self-service discovery is the second key aspect. And finally, investing a lot of efforts in data quality and observability because without that, we cannot trust our data. And that will help us in building a federated data capability. So our end goal from an architecture and strategy perspective is to democratize access to the data. That's what we are looking for. is to watch the actual sensüe process as we go to. Very close together. Unfortunately, it is true! We all see that photologicalаний resume is such a small nasa and the same as we New York City as a крifery is dedicated to our towns. And also, over the amount of information now, if there is approximately we perceive aerei ан Biology will tell you that or however, good luck to all of you.