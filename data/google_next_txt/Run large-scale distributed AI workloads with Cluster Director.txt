 Thank you for coming. I really appreciate it. How's everyone doing? First day of Next 25? Everybody watched the keynote? Yeah. I like that as well. I think what really excites me and heartwarming for me, outside of those exciting new product announcements, really is about witnessing some of the really exciting success that we have seen for our partners and customers, just like all of you, are doing with our Google Cloud. So thank you for being here. I know many of you have not traveling across the country. In fact, some of you are traveling across the world to Vegas to join us at this event. And the fact that you're showing up in this particular session, I know there's a lot of other sessions going on. Great topics, great session across the event. But the fact that you're here really showing us your interest, your curiosity, and your trust to us on this particular topic. So we're grateful for you being here. So the session title is Run Large-Scale Distributed AI Workloads with Cluster Director. It seems to be a mouthful. Let me just break it down for you. We're going to cover three topics. First one, we're going to cover our AI infrastructure. Why Google Cloud? What is the product roadmap in 2025? And second, we're going to... Elias is going to take you into a deep dive journey to show you what this exciting Cluster Director is all about. Lastly, but not least, we're going to invite one of our favorite customers from CrowdStrike. Andre and KB will be on stage to share their experience and their success by leveraging some of the tools and capabilities with Cluster Director. With that, let's dive into the speaker. I'll introduce myself. All my fellow speakers will introduce themselves when they're on stage. My name is Chung. I'm a product manager in Google Cloud. I work on GPU-enabled AI infrastructure services on Google Cloud. We've already covered the agenda, so let's dive right in. First of all, why Google, right? I think many of you probably are familiar with some of the facts that's shown on this slide, but I think it's super important for us to look at what is the differentiated benefits when you consider Google Cloud as your infrastructure providers, particularly given the fact that now on the marketplace you see a wide range of AI or GPU service providers on the marketplace, right, from large to small providers. I think with the information in today's session, hopefully it will give you more context, and hopefully you'll gain a better, higher confidence in deciding to partner with Google Cloud. To start with, I think when we think about AI, in fact, at Google, we have been working on AI since the inception of the company. In fact, to our funders, when they founded the company, back in the days, they were doing AI research. In Google's sort of approach, we look at early from a five-pillars approach when we think about our AI journey. It all starts from research, right? And I think some of you probably are familiar with the Transformers research paper that's really coming from Google's research department, published by Google Research, and that it really is the foundation for most of the large language model evolution that we're witnessing today. From the research, we channel all those knowledge and academic work into the models that we can share with the rest of the industry, from Gemini to Palm 2 and many other models. Now, having the research and the models are not enough. As we all know that you need a vibrant and all-inclusive large ecosystems, Google has always been dedicated to contributing to open-source communities from, you know, Jax and to PyTorch to Kubernetes, you name it. Google is really devoted to promoting and building a vibrant open-source community so that you can leverage those frameworks and those tools to build your own AI workloads. Now, having an ecosystem is great, but what's really important is that you can take those knowledge and deploy that to large-scale production applications. From there, you can learn what needs to be built in terms of the infrastructure to make that, to support that scale of, you know, billions of users and consumers that your service eventually will need to serve. You know, again, Google has the fact that, you know, we have multiple search applications that hope many of you are familiar from search to Gmail to YouTube. So those experience and knowledge will really help us to build our platform and solutions a lot more scalable. Lastly but not leastly, all of these needs to be built on to the best infrastructure you can find. And when I say infrastructure, this goes beyond just GPUs, right? When you think about the infrastructure, you have to think about your compute, your general compute, your GPU compute, your accelerator services, and your network, and your storage, and your security. There's a whole sort of holistic infrastructure services that you need to consider when you think about building your AI workload. So speaking of that, it is super, super exciting for us to share one of the most powerful GPU roadmap that we have built so far in 2025. And I'll walk you through that. Start with our A3 Ultra VM. Essentially, that's built on NVIDIA's H200 GPUs. And it is really a workhorse sort of GPU offering in Google Cloud. Since we launched in end of 2024, we've seen large adoptions across different use cases across different industry and regions. One example that I'll share with you is that from a recent study that we did, testing that we did with DeepSeq R1-671 building model, if we were to run the inferencing test on that on H100 environment, like in our case, would be A3 Mega, we have to use 16 GPUs to sort of load that model. But in A3 Ultra with H200, you really just need one single VM, which is basically eight GPUs. So it's very, very powerful. It's available worldwide. Encourage you to give it a try. Moving on to A4, which is powered by NVIDIA's B200 GPU that was announced GA last month at NVIDIA's GDC conference. Since, again, since the announcement, we also received a lot of interest and demand for that. If you think about the A4, the B200 offering that we have, we are actually the first, in fact, I checked last night, just to make sure. I haven't seen any other public large cloud announcement for that. We're the first and also the only large public cloud, major cloud offering with live, you know, GB200 services and customer in production with. We'll talk about the actual performance comparing with H100 and H200 in a bit, but it is generating a lot of interest and we'll also soon support DWS and Spot very, very soon, so look forward to that. Next one is A4X, which is our offering based on NVIDIA's GB200 and the L72. This really is the highest performing NVIDIA GPU on the market. It is specifically designed for the largest scale-out workloads in the AI domain. Specifically, I think, if you think about some of the trends that we're seeing from pre-training scaling to post-training scaling to test time scaling, there's actually a tremendous amount of compute power that's needed where GB200 and the L72 is really shying to enable those reasoning models or test time computing or MOE, the mixture of expert models, particularly for large scale. So that's coming in Q2 very, very soon, so look forward to that. And wait, that's not all. If you've been to NVIDIA's conference earlier last month, you know that Jensen has announced quite a few really exciting new GPUs. So as a top-tier partner with NVIDIA, we are hard at work to bring those new and announced GPUs into Google Cloud as well. One of my favorite GPUs is what we call all-in-one, a general-purpose GPU, RTX Pro 6000, Blackwell. It can be used for graphics, visualization, inferencing, Omniverse, VDI, you name it, right? It's really, really versatile and powerful GPU offering that's coming into Google Cloud in 2025. And also, yes, GB300, right? And if you've seen the keynote in Jensen's talk, it is a beast, right? It's like 1.6 times more memory and two times more network bandwidth with CXA, you know, 800-gig NIC on it. So that's also coming in Google Cloud in 2025. Really, really exciting. The sort of the one-line summary for this slide is, you know, whatever your workload needs is, I'm sure that we can find something that will suit your needs and help you accomplish your AI workloads need. Of course, every AI workload needs a high-performance storage solution. I'm not going into the details, but I'm very excited to share that there are two new announcements that's coming up from our partner team on the storage site that's specifically designed to accelerate or to provide high-performance storage solutions for AI workloads. One is managed luster solution. That's essentially, you know, one of the persistent, sorry, parallel file systems offerings. And the other one is rapid storage as part of the object storage solution. If you'd like to learn more about these two new offerings, and there's a little bit of detail on the slide, but I would highly encourage you to go to the breakout session. I think it's BRK01-20. It's called the AI hypercomputer mastering or mastering your storage infrastructure. I highly encourage you to check out that session. Let's look a little bit into sort of the open of the box and look at the inside, open of the hood to see what's inside of those A4 and A4X. As I mentioned before, A4 is built with CX-7. It has really four times the network bandwidth compared with our A3 high. It supports 10 times more non-blocking cluster scale. And in our testing, we're also seeing that it provides twice as fast training performance for large language models. Obviously, it depends on your workload type. You may see different performance differences. We'll actually share a performance testing, performance projection based on our own data. And we'll show that later on comparing between our different A families. And if you look inside of the machine, inside the computer in our data center for A4, each A4 machine has essentially two Intel processors, ML Rabbit processor. each processor has 60 cores. And then inside each box, we have eight NVIDIA B200 GPUs, also four terabytes of memory and 12 terabytes of local SSD storage. On the networking side, outside of this NBL8 domain, meaning when you connect GPUs from this particular machine to other machines, the network bandwidth is obviously defined by the CX7 network adapter, which is 400 gig. But when you look at sort of the network bandwidth within this NBL8 GPUs, you're only looking at 1,800 gigabytes per second network bandwidth that's supported by NBL Link Gen 5. So it's a really, really powerful, highly, highly performing GPU. The north-south network interface that's denoted on the top right corner is our Google titanium NICs. Sometimes also north-south network traffic is defined as the front-end networking. So two by 200 gig NIC. Now let's take a look at A4X. Like I mentioned before, it's one of the most powerful GPUs available on the market. It has a massive, massive amount of networking bandwidth. With NBL72 domain, you're really looking at 18 times more network bandwidth than A3 Mega. It's with H100. So it's massively more performance from a networking performance point of view. Similar to our B200 has 10 times scale, actually maybe even more than that in our latest generations, and I can talk a bit more on that. It has three times better training performance in our training scenario as well, and particularly shines in the MOE models and reasoning models. When you look at what's inside of those machines in our data center for A4X, again, let me step back a little bit. Each of the NBL72 domain is actually composed of 18 machines, right, because you have 72, meaning you have 72 GPUs, and each box, each machine, physical machine has four B200 GPUs and also two NVIDIA's Grace, essentially ARM processors. Each processor has 72 cores, and because of this, again, NBL72 domain sort of design model, you have a massive amount of unified memory space. The total memory space is about 13.8 terabytes of memory space that's shared across the entire domain, and it's virtually transparent to you as you write your code in CUDA. You don't have to worry about which specifically memory you're accessing from that physical box. All you need to access is to say within this domain, and all the 13.8 terabytes memory is available for you, right? On the networking side, similar to A4, you have CX7, a 400 gig interface that's connecting outside of this NBL72 domain. Within this NBL72 domain, within this 18 machine sort of system, you have massive amount of network bandwidth, 130 terabytes per second network bandwidth. So, again, massive, massive scale that is really, really shine when we look at some of the performance projections for A4X. Now, the even better news is that, you know, I talked about A4, A4X. I didn't go into detail on A3 Ultra because hopefully many of you are already familiar with that, but what we're building really is a forward-looking and compatible network infrastructure that anything beyond A3 Ultra, A4, A4X, and the future generation of GPU services, it's going to be benefited by this backend network infrastructure that is designed to be highly reliable, performant, non-blocking, real aligned, and highly extensible. that's, you know, thanks to Jupyter OCS network fabric from Google. That's, again, something unique that we have built. Allows us really dynamically to add those ML blocks without having to sacrifice latency or bandwidth, and also it supports location-aware placement, right, which is super, super important for ML workloads. Now, this is the slide I was referring to at the beginning of the talk, to show sort of the performance differences between various generations of our product, from A3 Mega to A3 Ultra to A4 to A4X. The way to think about it in NVIDIA's terms, it's, you know, H100, H200, B200, and GB200. There are two graphs. One on the left-hand side is the dense model, with slightly smaller scale, 4,000 GPUs, with a 600 billion parameter model. I mean, it's still considerably a large model, but compared with the right-hand side, which is an MOE model, the mixture of experts model, with 32,000 GPUs cluster, and 1.8 trillion parameter. It's a relatively large model. As you can see that in two different use cases, one on the left-hand side, A4 really shines. That's, you know, projecting like two times performance than the A3 Mega H100 VMs. On the right-hand side, you can see that this is where GB200 really shines, right? With a large model, a lot of parameters, with MOE mixtures, sorry, mixture of experts, or reasoning model, you'll see similar performances. Like, you can easily see three times better performance than the A3 Mega. All right, so we just covered some of our products and product roadmaps that's coming in 2025. Next, I'd like to introduce cluster director, right? Before that, let's first take a look at the problem statement. As you all know, training workloads is fairly unique, and because they run typically as a highly synchronized job across thousands of GPUs, right? So a single degraded GPU could disrupt an entire job and sometimes cause significant delays, right? And so when that happens, obviously, it delays your time to market for your customers and also cause more cost. So the ideal path, obviously, everyone wants to see is where your time to completion of training is a linear function towards sort of the end of finishing your training. and over, you know, as you move to newer and newer generations of chips, you would like to see sort of an improved performance or shorter time. That's the ideal case, but we all know that this is far from reality. This one actually shows most of our customers experiencing today, either your own on-prem environment or in your whatever cloud environment that you run with. Sometimes you see a lot of different challenges on the right-hand side. We just call a few examples of that. And so the challenge actually may start right from the very beginning of when you're provisioning your clusters. It could be the VMs or the workloads were not provisioned close enough to each other to leverage the network bandwidth or to minimize the network latency. And then when you deploy the cluster, are you deploying the cluster with the target model that is optimized and have you consider the trade-off between cost and speed? And are you able to predict failures? Or when failure happens, are you able to quickly troubleshoot and even move on, even when failure exists? So all of these, including checkpoint solutions, what are you using to optimize that? So all of these could be really, really sort of challenging. And this is basically what we're seeing as well. So scaling AI from research to production turns out to be extremely challenging for everyone. Only few in the industry actually figure out how to do this. And so a lot of people spend significant amount of time to make the large number of VMs to work in concert. Google Cloud Cluster Director is a super computing platform that delivers exceptional performance with resiliencies at any scale, right? So you can run your most demanding workloads effortlessly. It includes innovations like topology-aware placement, single API deployments, and many others, right, to allow you to deploy and manage thousands of GPUs as a single unit, right? Without stealing too much thunder from Elias, I'm going to pass the floor to him and let him talk about Cluster Director next. Hello. Thank you again from my side on coming today. I'm incredibly proud. Last year, life has blessed me with a daughter and a product, and so far, it's been going okay on both fronts. I'm incredibly excited today to introduce the second part of my life, the product. So I'm Elias Calciardis, and I'm going to be showing how training large language models and other complex AI and HPC workloads demands incredible compute power, reliable infrastructure, and deep visibility. We're going to see today how we are addressing all of these problems with our new product. So think about the challenges for a moment. I'm sure all of you have seen it. Complex cluster configurations, uncertainty about performance, difficulty troubleshooting hidden issues, and the constant risk of downtime your for disrupting your training needs. We're here to eliminate those roadblocks. And today, we're introducing Cluster Director, built on top of our foundations around four key pillars. Effortless cluster management, simplified workload optimization, and deployment, comprehensive observability, and building resiliency. Let's just jump right into a demo. We will start by creating a new cluster using one of our predefined blueprints. First, we log in to Cloud Console and search for the cluster director for Slyon product. After enabling the APIs for the first time, and this is important because here's your own one-stop shop for all the APIs that you will ever need in your journey. So after you enable them for the first time, we land in the first page of cluster director. Here, we select create cluster and pick blueprints as the option. From there, we select the blueprint, specifically designed for ML training on A3 Ultra Instances in this case, and we will add for A4 and A4X and all the amazing things that Chang and his team are putting in Google Cloud. Our blueprints are validated by Google experts, ensuring optimal performance and reliability. They include everything from the compute instances and networking to storage and even the pre-installed software stack, including frameworks like TensorFlow or PyTorch, and all necessary dependencies that one will need to run the most popular available training workloads, or your own. While one-click deployment is fantastic for rapid prototyping, quick POCs, etc., many users need more control. That's where our guided creation comes in. So let's work through some of the options. So first, we define our compute resources. Because we started from an ML training blueprint, the optimal machine family, in this case A3 Ultra, is already pre-selected, along with the recommended resource plan, which basically instructs how you're going to consume these resources. But we have full flexibility. We can choose to use an existing reservation, create a new reservation from scratch, or even leverage our dynamic workload scheduler, DWS, to use basically calendar or flex modes and find available capacity on the spot. And of course, you can use on-demand or even spot instances where permitted by the machine family, for example, the H4D CPU. new VMs that we have. Crucially, as we adjust the resource plan, for example, adding more nodes or changing the reservation type, the estimated performance for our targeted workload, in this case, the Lama 70 billion model, updates in real time. This is based on real benchmarks that we have done behind the scenes. This gives you immediate feedback on the impact of your choices. We also provide you with a default configuration as per the workload that you have chosen. But we know that many of our advanced customers have custom needs. For that reason, we enable them to have full control on those settings as well. And if you see throughout the stack of compute network and storage, you have a lot. Let's deep dive. So first, for the networking, you can use an existing VPC if you have something that is already created for you already by your administrators. Or you can even create a new one with a few clicks. We provide recommendations based on the selected blueprint and workload, for example, suggesting appropriate MTU size and other settings. Next, storage. We support a range of options to meet different needs. You can use file store for persistent storage, of your data and models, and for high-performance crutch space during training. We also offer managed luster that we announced recently. Well, today, you can easily adjust the capacity for both. File store, managed luster, GCS, or whatever else from the list of the supported storage options. Last but not least, the core of the cluster, the orchestrator. We are announcing today a fully managed SLIM experience, which means that we will manage your SLIM cluster for you as well, the orchestrator. You can still tailor the SLIM configuration to your specific needs. You can provide a custom VM image, allowing you to bring your own pre-configured software environment. Additionally, you can upload or edit a startup script of extractions that you want these nodes to do on the boot up or every time that you are using them. And, of course, you can configure the core SLIM options via the provided UI. No need to deep dive into hidden, obscure settings. This level of control, combined with ease of management, is a key differentiation for our cluster director for SLIM. Finally, once you have finished, you review your choices, and crucially, you see an estimated performance for your workload or some of the workloads that are most popular out there, based on the number of the nodes and the other settings that you have done. This helps our customers understand performance even before they deploy their resources. Next part. Let's look at the management. It's significant to have a good day zero experience, but what about day one and beyond, right? After all, we are going to be living with this cluster for a while now. Creating it is only part of the problem. So, this dashboard gives you an overview of all your clusters. You can drill down, you can see a specific cluster, manage resources, adjust configuration, add, remove, edit, anything you would ever expect it, plus our integrated Slurman management tools now are in the product, including editing partitions, node sets, and a deeper dive for those Slurman configurations. We don't stop there. For the ones that want to actually use the UI to submit their jobs, install new software, and keep iterating through this UI, you do have the option to not have cluster director for Slurman B just about infrastructure. We provide a complete, optimized environment for your AI workloads. We offer pre-configured software stacks, including all the necessary frameworks and libraries, as we have said, and they're all automatically tuned for the maximum performance on Google Cloud. We have a recipe hub, brand new, with optimized configuration for popular models like Lama, Nemo, Megatron, and more. These recipes, of course, include the best practice settings from what we have seen and observed in our environments. We give you these recipes with a single click, but if you do want to customize further, we offer that ability also. Let's see how you can actually go and create an application. Let's jump to an example. Here we will install Lama, the 70 billion model. The moment we do that, our system is ensured that we deploy the optimal configuration with a single click. Now, let's actually go and submit our training job. Notice that the application is already selected, and we have pre-populated the recommended nodes and an optimized hyperparameter configuration. This is part of our recipe hub model. We have also pre-populated all the settings needed for checkpointing, making sure that you have a robust job submission from the very, very first minute. Of course, you can fine-tune and you can adjust, but we provide you a powerful starting point. Our next part, observability. With our cluster deployed and our workload optimized, the third pillar is how about you make sure that you can observe everything that is happening now, right? Especially the more we help you automate things, the more crucial it becomes for you to be able to see what we are doing and what's going on in your cluster. This is a brand new view. Our observability starts with this high-level dashboard, and it gives you a 360 view of your entire cluster. We show the health status of each node in real time. You can see all of the nodes as a single squares there. Nodes marked in yellow are the ones that are either potentially unhealthy or we haven't done a health check in a while. You can trigger a deeper health check for those specifically with a single click. If a node comes back as unhealthy, then you can remove the node from your cluster and get it replaced by a new one automatically. And lastly, and I will deep dive if time permits later on, we also have AI health predictor. In a nutshell, it leverages Google's AI to analyze patterns and identify indicators of a potential node failure, often up to an hour in advance or even longer, giving you time to prevent downtime affecting your actual workload. Now, something that it's not a joke about is when your machine is healthy but needs to go down for maintenance. After you keep using your cluster, this will eventually keep happening at some point. not with us. Many of our customers have told us that they do not like surprises. And for that reason, we're not only giving our customers a 90 day window for planned maintenance to occur, but we also give them an advanced look and tools to enable them further. Here, you can see nodes with upcoming maintenance, and you even have the option to trigger the maintenance proactively. Last but not least, in our last tab, you can view and see all the jobs currently running in the cluster and highlighting the participating nodes. So our customers can get a better idea of how their clusters are being utilized by job IDs. But we don't stop there. We're taking this a step further with another powerful feature, which is straggler detection. A single snow load, as you know, can basically slow down your entire job. Straggler detection is there to identify the subcomponents that are participating in this run. And if something is outside of the normal parameters, it will identify it as a straggler. And then you have the option to deep dive and identify if, indeed, there is an extra issue here. Let's return to our topology view, which is basically your command center. Here, you can filter by job IDs, and you can focus on the resources used by your current training run, or our detailed dashboard to analyze historical data. You can see a problematic job indicating that something was actually at fault because the KPI, you can see it dropping. Now, a key enabler here is that we can remove the node with our auto-healing processes and automatically resubmit the job so it doesn't impact your workloads. A key enabler for this resiliency is our multi-tiered checkpointing service. It automatically saves your training, job, state, at regular intervals. It offers flexibility with multiple storage options, both in memory and more persistent, so it has the perfect balance of speed and cost. And beyond recovery, our system maintains an end-to-end automatic health checks. We check the nodes every time before we submit a job. We check them at the end once the job has finished. And when they are not used, we're checking them again. This proactive and orchestrated integrated approach ensures that basically issues are identified and addressed sometimes before they can even impact your workloads, allowing you to focus on results, not failures. Here's where everything is coming back together. As Chang said, our cost is not to just offer you a faster GPU. It's to be able to allow you to have the total time to market being faster, not just from the one component, but from every sub component, which eventually is going to help you beat your competitors. And then we also try to do that with significant savings in the middle. So the first part is, as you have seen, we automate and make the cluster setup faster. That's the first step on how you can get on the road faster. Then we have optimized recipes for accelerating benchmarking and training strategies. That gets you up to speed running a workload a lot faster than you would. Then we make sure that maximum performance with dense deployments, combat placement, our titanium networking, et cetera. Last, fast checkpointing, intelligence pair strategy, fast repair with auto job recovery. And then once you finish with all of these and you still want to squeeze more performance, you jump into the deep observability. You get more insights about your workload and you can squeeze that little bit of performance in. Now, I'm not going to say a lot. I put a lot in the slide. But our main goal, ideally, is to forecast failure on machines before a disruptive event or failure even happens. Naturally, we're only able to forecast failures when they're leading indicators of these impending failures, such as thermal issues, memory issues, and creeping error correction code errors. We have the observation window, which is basically look back on our data center. We have a prediction window, which is basically from here on onwards on the data that we have done. And then basically we have six months of data that we use to train on 10-minute intervals. Out of all of that, we can actually do inference and we can predict if a node is going to be unhealthy in the new predictor window, which makes sure that basically once you keep running, we can detect the tiniest amounts of invariability in the nodes. And then at the same moment, we can give you a warning. Now, I'm not going to say how good or bad we do all of these things. I just said what we do. I will let our customers say how good or bad we did. I would like to welcome to the stage KB from CrowdStrike, a strategic partner for us. And we are very much believing in the strategy that you have to stop the breaches and leverage AI. So basically everything you've seen is like what do you do and this is the why you do it. Thank you very much. Thank you. Good morning, everyone. My name is Karan Bir Singh. I'm the VP of Product Management for Cloud Security at CrowdStrike. First of all, thank you for being here and thank you to the Google team for having us here. We entered into a strategic partnership with Google Cloud in 2023. Of course, we chose the platform for its trusted performance, resilience, and the manageability of AI infrastructure. Our recent collaboration with the cluster director team has already led to some very promising results that we are very excited about. We are already beginning to see 6 to 10x speed boosts when it comes to our models end-to-end training. As you can imagine, this is a material impact to this acceleration to our mission of stopping breaches at CrowdStrike. We are super, super in the interest of time. So we're looking forward to our continued partnership with the team over the next year and continued years. Of course, very excited about the road that we just saw. To actually get to the meat of this, I'm going to invite my partner in crime, Andre, who's going to come up on stage and share our approach, our implementation, and some of the results that we are seeing in practice thanks to the great work that the team just talked about. Andre? Thank you, KB. And hello, everyone. My name is Andre Preda. I will talk a bit about how we've been using the cluster director to train some of our LLMs at CrowdStrike. First, I want to mention that for us, this has very much been a cross-team project involving both data science and engineering. I am more familiar with the data science part, but I will try to do justice to the engineering part as well. Let's set some context. So I don't need to tell you that LLMs currently are making a very big impact across many industries. This applies to cybersecurity as well. We see adversaries using LLMs in various ways. On the protection side, we see companies using LLMs. At CrowdStrike, we stay ahead of these things. And I would say that our LLM research is quite expansive. What I mean by that is we have a very long list of very different projects spanning things all the way from training foundational models to fine-tuning agentic models, training smaller LLMs, larger LLMs, even multimodal LLMs specialized in cybersecurity. Recently, we are tackling more ambitious projects, so we felt a need to get a more powerful cluster, as well as remove some of the friction around experimentation because we need to do a lot more experimentation. So now let me tell you a bit about how we tackle this problem. So we started by just laying out our requirements for a cluster, and we like to make an analogy with a pyramid, starting with more fundamental needs at the bottom and working up. Let me read some of these for you. So if you want to train LLMs, of course, you need hardware. But on top of that, you probably want to have some type of logical separation between all these different projects. Of course, you need great observability into your cluster, like Ilias has showed you. And in the end, all these things are just meant to enable scientists to experiment for all these different projects. So let me tell you about our solution. Sorry. In terms of deployment, our engineering team has had a great time using what's called the cluster toolkit. Because it's infrastructure as code, it was very easy to automate the deployment, and it actually allowed us to start small with a small number of nodes, make sure the automation worked, make sure we knew how to use the cluster well, and then only then scale up to the full number of nodes. And at that point, because of all the automation, the scaling up was almost seamless. So this solution was very flexible, and we valued this kind of flexibility in terms of storage as well. So for storage, we had to consider things like what kind of models we wanted to train, how large our data sets were, how often they update, you know, usage patterns, and so on. After we came up with all these needs, actually Google's team helped us figure out the exact type of storage we needed for our use cases, and we valued mostly speed. We prioritized speed. Because when you train at this scale, really having good bandwidth, having low latency, impacts the end-to-end time for experimentation. In terms of storage, we also applied this same idea of starting small with, like, the bare minimum amount of storage we needed, and then scaling up only as we filled up the storage, which actually happened surprisingly often. It's really surprising just how much storage training large language models needs, not just storing the data sets and the model checkpoints, but also when your data sets reach, like, the terabytes, even operations like reprocessing the data sets and caching them to reuse later need to be considered and taken into account for your decisions. Okay, in terms of resource allocation and scheduling, we chose Slurm. Slurm is a bit of an industry standard. What we noticed here after using the cluster for a while was that our jobs tended to fall into one of two different categories, so you would have some longer-running jobs, like training LLMs. Those usually take a few days, a few weeks even, but they are mostly unattended, right? The computer just does the job. But we found that scientists also often needed to run some interactive operations, like exploring a large data set or, you know, testing out the new LLM that came out. So we found out that we really needed to have sort of a separate pool of resources always available on demand for interactive operations for our scientists. And Slurm really lets you partition your cluster in this way very easily. I mentioned observability at some point. We really love the observability solution available in Cluster Director for a few different reasons. One would be it allows creating plots like this. What we did here was to try to figure out the exact combination of hyperparameters we would need for some of our projects. We do this by just testing all these different configurations because they depend on the exact hardware you use, the exact project specifics, and so on. The second point would be that our engineering team has actually found it very easy and useful to use the API to create our own custom dashboards with real-time alerts based on the metrics that Google Cloud provides. Okay, I see I'm kind of running out of time, so let me just briefly mention my last point. It's about the support we've gotten from Google. So most of the time, you don't really need support because things are automated. They work well. But when scaling training from a relatively large number of nodes to a very large number, things were working, but we weren't noticing quite the speed up we expected. Because of our collaboration, we have access to all these Google experts, so we asked them about this, and they actually managed to find some minor tweaks we could make to our connectivity code, to our networking code, that really unlocked the kind of linear scaling that we expected. And in some small cases, like KB mentioned, we managed to see almost like an order of magnitude kind of speed up. So we really appreciate having this kind of support available from Google. Okay, so thanks a lot for listening to me. I will now hand it back to Ilyas to wrap it up. Thanks. So for the ones that stayed, we do appreciate the feedback, anything you can share or you want to share. The second thing is we do have some other partner talks that we think might be of interest to you if you want to take a photo so you can track these ones down the road. And since you already have, some of you, your phones out, if you also want to take... I'll wait for this. And then if you also want to sign up for the private preview of the Cluster Director and all of the features that we just announced, I'm happily taking submissions now and we can get back to you. You know, it should be an easy form for you to basically sign up for it and then we will reach out to you. And with that, from behalf of everyone, thank you so much. Thank you for spending the time here and for learning for the amazing things that we are doing. Thank you. Thank you. Thank you. Thank you.