 Good morning. How are you doing? Welcome to this session. Today we are going to dive into the exciting world of Gemini and we are going to show you how you're going to build a powerful AI application with Gemini and Gemini SDK. My name is Eric. I'm a developer advocate at Google. I am Chris, PM in Vertex. This is the agenda for today's session. First, we are going to start with a solid foundation, an introduction to Gemini 2 and Vertex AI. And next, we are going to get practical. So, we are going to look at SDK and explore how SDK can do for you. And next, my favorite part, we are going to dive into the code and look at some practical examples for customer support use case. And finally, we are going to wrap up with roadmap and next step. We are going to discuss what's coming next for the SDK so you can continue the journey with this exciting technology. So, all right, to build a solid foundation for the SDK, let's briefly cover the core concepts for Vertex AI and also Gemini. you know, the family of Gemini 2 come with two key models, 2.0 pole, 2.5 pole, 2.0 flash and 2.0 flash line. While all of this model are built on multi-modality and a huge contact window, but they are really built on multi-modality built for different use cases. First up, from 2.0 pole, this is the thinking and reason model we have. And this is the most intelligent and top tier model when you need to have a serious thinking power and complex problem to solve. It just goes way beyond the basic classification and prediction. It always thinks before responding, which results in better performance. So, it's fantastic for those multi-step, tricky, complex problems. So, whether it's in coding, map, STEM, and research. And next, we have Flash. This is for everyday tasks. When building a genetic experience, things need to be fast and responsive. This is your go-to model. It's really optimized for high performance, pure speed, and real-time streaming. Finally, we have Flash. This is the smallest model. But this model is all about maximum efficiency. It gives you the lowest latency. It's also the most budget-friendly among the models we're going to talk about today. So, with all this model here, so you can pick the right one, when you need to balance between the performance, course, and speed. If you don't know which one to start with as a developer, so I will suggest you start with the middle, the Flash one. This one is the most balanced and feature-rich model in the family. So, what makes 2.0 so practical and useful? So, from my perspective, it's always come down to two things. First, RD performance improvement across the pole. And second, just as important, it's just some amazing feature come with the model here, which unlocks its capability. So, let me just highlight some of them here for you. First, as I touched on before. On top of the multi-model input, the 2.0 Flash becomes the first all-in-one model that can natively create image and generate audio. And importantly, it also supports the native tool use. I think this is huge. It makes the model more powerful and capable. Having the access to the most up-to-date information through Google Search, and also connects to the enterprise system and data through custom function calling. Special for developers working on interactive and dynamic applications, so we are rolling out live API. So, this one is all about real-time. So, this is a bi-directional real-time multi-model in and out. So, with all these amazing features, it definitely can create a more complicated streamlined, agentic experience. And the good news is that the powerful model and also the amazing feature are readily available to developers through the Gen.AI SDK that we are going to introduce today. But before that, I really want to talk about Vertex.AI, because this is important. We understand that building a Gen.AI application is more than just getting the model inferencing, right? How are we going to ensure the response are granted, factual, safe, being enterprise-ready? How are we going to manage the eval, tuning, scale your AI application across company? So, making any production ready? Well, this is where Vertex.AI comes into place. So, you can think of Vertex.AI as your central AI platform, connect everything together. So, it provides you the model, not only just Gemini we talk about here, and a curated 200 more model in model garden. There's always one for your special use cases. And it also provides a whole suite of two in the model builder, agent builder, cover the entire end-to-end AI op lifecycle. And it also provides you the robust serving environment on the bottom here, for you to run your AI application at scale on cloud. And more importantly, it provides you the crucial enterprise-ready security, scalability, and reliability. So, somebody said, and I agree, when you are using Vertex.AI, you are not only getting access to a model behind your endpoint. You are actually tapping the power of an entire integrated AI system. So, Chris is going to be here to introduce the GNI SDK. And what is it? How are you going to fit into this ecosystem? How it actually do for you? All right, Chris, over to you. Thanks, Eric. All right. Let's talk about what the SDK can do for you. You can think of GNI SDK as a bridge connecting you seamlessly from playground of Google AI Studio and just getting started with an API key and to enterprise production environment of Vertex.AI. And we built GNI SDK to provide you with a single cohesive interface to accelerate your development process. Now, let's see what it looks like to transition from AI Studio to Vertex.AI. So, if you see on the left column, you will see how you can get started with an API key on AI Studio. And if you go to the AI Studio, type some prompt, and click the get code, that's exactly what you will see. And the only thing you have to do to use the power of Vertex is, if you look on the right side, is to provide mark Vertex.AI true and provide your project ID and location of your project, and you're pretty much good to go. So, wherever you choose to start building, GNI SDK will be there for you as a tool to accelerate your development process. So, it comes in four different flavors, and let's talk about what those are. And we did this because back in 2022, I don't know if you guys remember that far, when Gen.AI was starting to blow up and we see where it was going, we made a decision to not only speak Python, right? Because machine learning days, Python was pretty much the language of AI. And with Gen.AI era, we saw that any developers can do amazing things with just one API call. So, I want to share something personal. So, I'm an immigrant. I speak Korean as my first language. Who here speaks more than one language? Raise your hand. Wow, most of you guys, wow. So, you guys know what it feels like to express something that you want in another language that doesn't quite match, right? It takes like two, three, maybe even ten words to exactly capture what you want to say. And I think programming language are very similar in that way, right? And something I noticed is that when we speak the language that the users want to use, they feel seen and heard. And I call this speaking developer's heart language. So, we chose to speak to what the users want to hear in the language they want to hear. That's why we invested in three additional languages. And we've seen an explosive adoption for SDKs. And I'm very pleased and excited to announce that Go SDK is in generally available stage right now. So, if you are Go genius, you can go build your production apps today. Check it out. And I want to take you to the Python example of the Hello World version of this SDK. So, let me walk you through the left box and the yellow box there. So, it has four steps. First, you want to import the modules from the Google GenAI library. And second, you want to create the client object using genai.client. Third, calling the model. We're specifying Gemini 2.0 Flash, like as Eric suggested. And we're prompting it with Hello World. Surprise, surprise. And you can pass in different configurations as you need for your application. And finally, we're capturing the output with the print command. And you can see how simple it is to just get started. This is pretty much all you have to do. And, you know, I'm really, really excited to see what you guys can build. And I want to share a little bit more about the roadmap later. But let me turn it back over to Eric to show you even cooler examples. Thank you, Chris. Now, let's switch it to a live demo. All right. So, let's look at some practical example for a customer support use case. So, in the next 10 minutes, just imagine, I am a customer. So, I'm looking to buy a chair for my wife as a gift. You know, I kind of know that what kind of furniture my wife would like. So, I searched the internet. I find an image of this chair. It's such a good-looking chair, right? It looks elegant, comfortable, clean. I think she would like it. But the problem is that she likes red. This is not his favorite color. So, what I'm going to do? So, I'm going to reach out to my favorite furniture store. This next 25 Home and Design. And talking to an AI agent to see if the agent can help me out here. First, I start with our image of the chair. And then, ask a simple question. I think this is the problem with the live demo. So, let's wait a little bit. The image is not coming up to the agent here. Okay. It's checking the policy. Interesting. All right. Let's start again with the question. The question is that, do you have a chair similar to the one in the picture, but in red? You know, it's not very straightforward to human beings. But traditionally, it's been a challenge for machines. Because it has to reason across the image and text. So, let's see if this AI power agent is going to handle this. So, let's see if this is a chair. All right. All right. Very good. So, the agent was able to find a similar chair. And for me, not only one, two. It's very similar in the picture here. It's equally elegant, clean, comfortable, right? And most importantly, this is in red. That's my wife's favorite color. Remember? All right. All right. Just imagine that behind the scenes. What it actually does is that the agent process my visual query and then analyze the chair and identify the key feature in the chair and then search the product catalog and retreat the similar item and present the result here. So, it looks to me a very complicated process here. But actually, it's very easy to implement in the SDK, which I'm going to show you later. But here, I'm pretty happy. I like it. But I have to be 100% sure the chair will work. So, I provide further context. And this time, I'm going to provide a picture of my living room. This is where my wife is going to put the chair in. I would like to ask whether the chair is going to fit in to this living room to see the AI agent can help me out of here. Now, the question I have. I like it. Would it fit into my room? And attach the image of my living room here? And send it to the agent. And send it to the agent. So, what do you think? Do you think it will work? Do you think the chair actually fit into the living room? All right. So, that's the response from the agent. You see that? It gives me a very detailed assessment here. Scheme through that looks like a look at the chair from the style, the color, and the space of the living room. And the final conclusion is that, yes, it should fit. It definitely gives me a lot of insurance about my choices. And interestingly, it's also generally an image of the chair inside the room, but doesn't seem to be very, very fit into the room in here. But that's all right. So, we will continue working on that. But anyway, I think this is already very helpful. It gives me a lot of confidence that the chair is going to fit into the room. So, I'm excited. I would like to try it out. I might just pick it out on the way home tonight. Maybe a good surprise for my wife. So, the next question I'm going to ask in here. Is the chair available in the store near me? I am at Google account next. All right. So, convenient. The chair. It's available in the store in this conference. Joe Arang. But this just highlights the Gemini's capability to use native tool use. To connect to the most up-to-date information through Google search. And also connect to inventory system outside of the group, the model here. All right. So, it definitely highlights this seamless agent experience. And interestingly, the AI agent, having understood the whole context, is ready to bring up the conversation to the next stage. Transition from support to sales. It's very interesting. So, for me, I'm happy. I'm definitely delighted with this user experience. So, let's see how this is implemented using SDK. So, I'm going to bring up a notebook here. The goal of this notebook, it's going to show you how you can implement those key steps for every single key step in the conversation here. And the notebook is already popular, available in Google Cloud Gen AI GitHub repo. And you can see the link in here. And also, the link to the notebook. And Chris is going to share the link at the end of the session. So, you can get hold of this notebook now. Or you can look at it later. But inside the Google Cloud Gen AI repo, there's a lot, a lot more code sample for Gemini and for some other model as well. So, looking up after this session. So, let's go back to the notebook here. So, we are going to do some setup, some data preprocessing. So, that consists of the step to install the Gen AI SDK. So, the PI install Google Gen AI. This, the command to install the SDK. And then we authenticate the notebook to Google. And then we set up the Kyle project and location. So, in Kyle, there's a concept of location and project. This enables you to set up the security boundary for your data and workload. And we just create a very simple product catalog here. With a set of image and basic inventory information. Now, we are ready to follow the four-step process Chris just introduced. First step, just import some library, necessary module from the SDK. And then create a SDK Kyle. We specify the project and location here. Also, know that when you connect to Google AI for developer on AI Studio. You only need to specify the Google AI API key. And the rest of the code, pretty much the same. So, this is highlight the advantage of using SDK, which provide a unified experience for AI Studio and Vertex AI. Perfect. Now, we are almost ready to go into step number three. So, this is the question I asked before in the image I provided earlier. All right. So, preparing the request to the model to generate a content. So, a typical response to the model as Chris just mentioned. First component is the model ID for the model we are going to use. Second is the content for the model input. And finally, the optional model configuration. Things to know when you specify the ID, not only just the foundation model like Gemini, it's also the TUN model and also the one-party model. And in the future, the SDKs is going to support many more models here. So, in this case, we are going to start with 2.0 flash. Now, we get everything prepared. We are going to send the request with the model ID and contents. And also, the configuration to the model. But before that, look at the contents again. The contents here is where you can put the model input. So, that means that will include image, text, audio, video, and even the whole code base inside here. And this is where you can leverage the huge 1 million context window. You can put everything inside here. So, in this case, instead of using RAC for the search, which requires a complicated setup, I just put in the whole product catalog inside the content here. It simplifies the implementation. It might also improve the search accuracy as well. All right. Perfect. And optionally, you can also set up the system instruction just to customize the model behavior. All right. All right. We are ready to send out the request to the model. All right. So, it takes about two seconds for 2.0 flash to process the whole thing, which is pretty amazing. So, once we get the result from the model, you can bring out the response here. The response typically contains the model output and also the metadata. You also can bring out the markdown in a formative test. All right. So, let's do another comparison here. Let's go back to the model ID. So, this time, instead of using 2.0 flash, I just going to switch to 2.5 pull. And sending out the same request to the model. I don't expect a big difference because both of them are really good at multi-modal understanding. The notice difference here is the time. As I mentioned before, 2.5 pull is taking time to think. It kick out the thinking process before we spawn it. Remember, for flash, it took about 2 or 5 seconds. So, for pull, it's 12 seconds. It's also amazing for a thinking model. Another difference is the response. The response is a lot more verbal, a lot more rich here. And this comes down to a characteristic of 2.0. So, 2.0 flash actually is trained for conversational efficiency. That means the response is always short, on-point, and cost-efficient for serving. So, if you want to have a rich response for 2.0 flash, you always can set the system instructions. Let them give you more information. All right. So far, so good. So, one amazing feature that comes with Gemini is the control generation here. It's also called structure output, which ensures the model output always complies to a certain format. So, in this case, I really want the model to output the JSON file, which contains the model ID, so that I can use the model ID to connect to external system. There's two ways to define the response schema. You can use PyTentic. That's the case here. And you also can define a Python dictionary using OpenAPI schema. And you can see that model output. It's in a JSON file here. Contained the model ID and some other information. Really good. So, I'm using the model ID to connect to the inventory system to provide more information. So, now we have completed the first use cases here. So, in the last two or three minutes, we have explored the capability of multi-model input, long context window, structure output, and PyTentic as well. Now, we are moving on to the second example to answer the question whether the chair will fit into the room. So, this is the case to highlight Gemini's capability on multi-model reasoning and image generation. So, to do that, first, I switch the agent's role through the system instruction here. It was a self-representative, now it's an interior designer. In the little bit more complicated agentic system, it's very typical. You have multiple sub-agents, each one doing special tasks. So, in this case, I want to customize the model behavior to provide more custom response to the question that I ask. And the second thing is to generate the image. The only thing I need to do is to specify the response modality here, to request the model to generate an image based on the requirement above. So, this image is a lot better than the previous one, having the chair inside the room. Alright. So, very good. Now, let's move on to example three. This example to answer the question, where the chair available at a store near me inside the Google Car Next. So, this example highlights the capability to use the native tool used function calling to connect to real-time and business data. Certainly, making the model more powerful and capable. And to define function calling. You use the function declaration here. In this case, I define two function calls. One is to get product info. So, this is connected to my inventory system that gets the business data. And the second, to get the store location. This is actually using Google Search to find the information about where the Google Car Next is. So, you can define as many as function calls as necessary, depending on your business operation. And the model is going to automatically decide which function to call. And then generate the function call and the argument for you. And then you're using the function and also the argument to connect to external system. Going to Google Search and also going to the external system, get the most up-to-date information. And then you're using the function and the information. So, you see, this is the result from those real-time and business data. And then you incorporate all this information and output. The model responds in an actual language presented in the conversation there for me. So, alright. So, the next demo here is Live API. So, as I mentioned before, the GenAI SDK support the Live API. Support this is a multi-model, bi-directional multi-model in and out. So, unfortunately, for the notebook, it only can support text in and audio out in this case. And I'm going to use the Google Search example I used before and do a little test here. And the first question I'm going to talk to, Gemini, is that, Hello, Gemini. My name is Eric. I am doing a demo at Google Cloud Next. That sounds exciting, Eric. What will your demo be about? Alright. So, it's about Google GenAI SDK. The Google GenAI SDK provides a unified interface to Google's generative AI API services. It seems like you can use it to integrate AI capabilities into applications. Are you planning to demo it using the Gemini API or Vertex AI? Yeah. Yeah. So, I think you got the idea. Just some simple code you can lead into a back and forward natural conversation here. Alright. So, in the last about 10 minutes, we have explored a lot of Gemini advanced capabilities. So, that includes the multi-model input, long context window, structure output, native tool use, function calling, and now the live API. All using the GenAI SDK. But honestly, we only scratched the surface today. And there's much more that GenAI SDK can offer. But the big takeaway of today's talk is that GenAI SDK is going to make it a lot easier to bring the powerful model and the amazing feature to your project and to your application. It definitely empowers you to create more streamlined, agentic experience. Now, it concludes the demo today. We just can't wait what you can do with the GenAI SDK. And I really hope the chair will work. And if not, it's not even my fault. It's Gemini's fault anyway, right? Okay. Let's get back to Chris. Talk about the roadmap and next step. Well, thanks, Eric, for the demo. Can we give him a round of applause? Live demo is always hard. Thank you. That worked out like a charm. The chair and in the room example. That was pretty interesting. Alright. So, I want to share a little bit about what's coming. So, as I mentioned, we launched Go SDK this week to GA. In May, we're going to follow up with Java and Node. I would love to share a little bit more, but because of the cutting edge nature of what we're doing, we can't share quite publicly, but I can't promise that it's going to be exciting stuff that's coming. So, stay tuned. With that, I want to thank you for coming and spending time with us today. Here's a QR code for the GitHub repo and Bitly link that you can type in. If you want to play with the notebook that Eric just showed. Take a picture. There we go. I'll wait one second so that you can finish taking those pictures. Oh, I see somebody taking a picture twice. Okay, fantastic. Alright, and we appreciate any feedback. And we're going to stick around and answer any questions out in the hallway. So, please come find us if you have any questions. Thank you and have a great day. Thank you, and have a great day. Thank you, and have a great day. Thank you, everyone. Thank you, and have a great day. Thank you.