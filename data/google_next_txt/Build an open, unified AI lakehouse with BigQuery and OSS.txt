 We are going to talk about at a very high level what a lake house is, some of the key components that build a lake house, and then we'll hand it over to CME where they're going to talk about their journey and at the same time we will want to keep some time at the end for some questions and answers. I'm pretty sure folks are able to hear me and so with that let's jump right in. Twenty years ago when I was building a data platform for a media and advertising company, we did not have big data tools. All the commercial tools could not scale to the big data needs and at the same time Hadoop did not exist in a place where we could utilize it. So imagine building a lake house then. We had independent silos, we had a collection platform, we had an operational data store, we had a warehouse and then you had data marts so that all the things that can be done. I'm sure folks you can relate to it if you've been doing data platforms for a very, very long time. Imagine now in this state you wanted to add one more field all the way through from source all the way to the reporting platforms. It was very hard and God forbid if you wanted to carry forward a new event which would come in from the source all the way to the destination. It was a very hard and difficult exercise. So what we are looking at is twenty years later how has technology changed and made life simpler. I'm hoping it's simpler, I'm not sure that we have reached Nirvana yet, there is a lot more room to be done. But imagine instead of having multiple copies of the data in many different places and you spending too much time copying data, making sure it's secure, making sure people have access to it, you have single copy of the data. You have single place where you are defining what the data looks like, what are my tables, what are the columns in it, and imagine if you have one place where you have all the security policies that are available. So that you can access your data using any processing engine that you want. You want to use BigQuery or you want to use open source processing engines, you should be able to do that. You want to connect any of your favorite IDs. I don't know how many folks are old enough. I've lived through VI-EMACS wars and I'm EMACS by the way. So imagine everyone wants to bring in and plug in their own IDs to the data environments and start analyzing the data they want. So that's another interesting thing that we have enabled. And finally, you should be able to use any of the orchestration platforms to orchestrate a pipeline or a graph across the board. So that is what we have enabled as part of this. We have unified storage, which means you can have data in any place that you want. BQ storage, GCS, you can have a single unified meta store. Every processing engine which is sitting on top is able to access the data in the same and consistent manner. So you're getting single version of the truth. Imagine how simplified life is and that's basically what we have enabled as part of GCP here. The journey is not done. And I was thinking that there would be some claps coming in, but no jokes. The journey is not done. There is a lot more to be done, but we at least have a point where we can simplify our customers life very, very well. Here I wanted to take a side track and go into why are people talking so much about open source. So think of it this way. Coming to cloud extends the life of your legacy applications, right? So if you had capacity constraints, old stack, all of those things, life can be extended. Open source gives you zero vendor lock-in, which means you can pick up your bags and leave whenever you want. So it's not Hotel California anymore. We are talking about flexibility of tools. You can bring in any tool that you want and you can start using it. You will be able to innovate as fast the open source community is innovating. So there are many more people who are patching things, running into special issues and fixing all of those. And last but not the least, you have access to talent. There are so many people who are comfortable and familiar with open source, and that's the reason why open source is getting popularity. So from our point of view, what are we doing from Google Cloud when it comes to AI lake houses? So Spark, from our point of view, is the most used big data processing engine at GCP. And there are three different ways to do Spark, at least on GCP from our point of view. The first one, which is the simplest and the easiest way from my point of view, is serverless Spark. You focus on just Spark and nothing else. We will take care of the infrastructure, managing it, running the job for you and finishing it at the back end. So that's the serverless side. The second one is somewhere in the middle where you just specify what kind of infrastructure you need and what version of Spark you need. We will create the environment for you. You run your jobs on it and you leave. So which means you're doing some infrastructure management, but not too much. And finally, the last one is if you want to have full control on it, you manage the infrastructure, you manage the bits, you do everything. You can also do that on DIY Spark on GKE. Not my favorite way of doing Spark, but in case you want to do it, you can certainly do that. So here we are talking about now what is serverless Spark. So here, imagine, we have made it on demand. As simple as that, like I was mentioning earlier, you take a job, you just give it to us, give it properties as to what are the Spark properties that you have, and we run it for you. It's open and flexible. What it means is that you should be able to pick the version of Spark that you want and use all the open source integrations that we have with the iceberg formats or any other table formats that need the covers, and you should be able to use them. And finally, it's integrated into BigQuery. So that way, when you're looking at metadata, the same information that's present in BigQuery in tabular forms in BigTables in BQ storage is also available for Spark processing. So that's basically the simplification that we have enabled in Spark. There is a whole talk which is focused on that. So it's a shameless plug here. Please go to this talk. It's at 3 o'clock, if I'm not mistaken, where we are going to spend 45 minutes just on exactly all the things that we are doing to serverless Spark. Dataproc, it's basically a catch-all for us where we have so many different open source processing engines available to you. You can pick and choose any of the processing engine that you want and say, here is an environment I want. We will spin it up on demand. So if you say, bring it up, we'll bring it up within 90 seconds is what we have observed with most of our customers. It is secure. It is fully integrated into the GCP ecosystem. And finally, it is cost efficient. And one of the few reasons why it is cost efficient is we have invested a lot in performance in Spark. If you were to bring in your own DIY Spark, we clearly will outperform it. And we are probably the most price-performant engine in the market right now. This is a slide where I will step away. This is a photo opportunity. You don't need to take my pictures, but take a picture of the slide. The reason why I am flashing this up is there are a lot of things that we have added in Dataproc on GC. I will come back in. So the key being, what I am finding is that we are making lots and lots of enhancements to Spark and Dataproc. Customers are not taking advantage of that. So here are some of the key things which I would want to highlight so that way when you go back, you should be able to take a look at some of the things that you are doing in your environment to see how you can improve performance or reduce costs at your end. I talked about performance, so I won't speak more on it. We have added support for the new and the greatest VM families that are coming in. As they come in, we are planning to do shipments of integrations from Dataproc point of view as and when the families come out. Obtainability is going to be an interesting challenge in the long run. Stockouts do happen, unfortunately. There is not infinite capacity. There is finite capacity. So one way to avoid stockouts is leveraging flexible VMs. Imagine you create an environment and you say, this is my first preference. If that VM family and size is not available, this is my second preference. If that is not available, third preference. So now you imagine, you can keep the same scripts, everything working, and as things change dynamically, we'll pick and choose whichever is the right VM family that's available for cluster creation at that point in time. And this integrates very well with auto scaling also. We have support for Iceberg, BQ Metastore. We know there's going to talk more about that. We have lots and lots of IDE integrations. So you can bring in your own IDE, deploy it on a laptop, and access Dataproc environment. So we have opened it up completely and still secure at the same time. A lot of things going in multi-tenancy. So you can run multi-tenant environments which are interactive with distinct security accesses coming in, and also we provide secure lightweight images. A lot of customers are saying, your images are too big, so can we make them smaller? So we have done that work too. So with that, I'm going to hand it over to Vinod. He's going to talk about unified metadata management. Thanks. Thank you, Sushil. Sushil gave a very interesting overview on AI Lakehouse and how do you build an AI Lakehouse and key capabilities that we are announcing. One of the key things that empowers a lakehouse is actually unified metadata. And I'll walk you through why. So first, I'll kind of lay down the problem statement on why we need unified metadata. Second, then I'll walk you through some of the key capabilities we provide in Google Cloud on a unified metadata offering. And then I'll walk you through a user journey on unified metadata across an OSS engine such as Apache Spark, BigQuery, along with Apache Iceberg. So let's kind of lay the groundwork. Why do we really need unified metadata to enable the Lakehouse? In existing or earlier architectures, a lot of organizations have several analytical engines. So it's a multi-engine architecture. And so they have fragmented copies of the data, and they end up copying the data across these different engine systems to query them. There's no uniform governance that's applied across these engines when you query them. And then furthermore, to enable all of these organizations have to do custom workloads with custom solutions, increasing the cost of the project itself. Along with this, you get a lot of delays in time to market because if you have to copy the data, build these pipelines, it takes a long time to actually come down to do the actual analysis that the organization needs to make them successful. So the keys, like user journey, a problem that many organizations are trying to solve in this multi-engine architecture is think about a user like Alice, who's like an analyst querying data, say, on an Apache Iceberg table with an OSS engine such as Spark. Alice just wants to share that table with a different user like Bob in the same organization who's using a different engine such as BigQuery to do analysis and Alice just wants to share the table and Bob should just be able to query it. This particular user journey has been quite complicated with all the pain points that I talked about in the previous slide. So we at Google with a unified metadata strategy are really enabling this engine interoperability so that this particular journey is seamless for the users. Now, along with the problem of unified metadata, Iceberg as a table type has become popular in the industry and so on and so forth. And so if Alice wants to share an Iceberg table with Bob, you know, like we need an Iceberg catalog that can basically enable that. The catalog should manage the table creation, deletion and so on and so forth. So this is again like an interesting problem. So we at Google have heavily invested in Iceberg to enable this as a part of our unified metadata strategy and I'll walk you through how that's done. So what we really are providing at Google is a unified metastore called BigQuery Metastore that enables the engine interoperability. BigQuery Metastore provides for a unified metadata across different runtime engines such as Spark and BigQuery and Hive and Flick. Furthermore, the user can immediately start querying the data once a table is created in Spark, the user can quickly create in BigQuery. This enables faster time to market and coupled with this, you get out of the box governance and centralized management. So if you have any policies or governance rules, that's like enforced uniformly and you get fine-grained access control and other capabilities as well. So this is one of the key things that really enables the Lakehouse because now you are querying a single copy of the data and single schema and all the pain points that I talked about in the beginning are kind of addressed. And furthermore, we support open APIs such as the Hive API and also support the Iceberg Catalog and across different storage systems whether it's stored in BigQuery or an object store such as Cloud Storage. So here's, let's double click this a little bit. Let's look at a particular journey. So for example, Alice creates a table in Spark. So here's like a snippet from Spark. And so you create an Iceberg table. The table like namespace gets mapped to say for example a BigQuery data set immediately once you create it. And then once you create the table, it's a BigQuery table. And the magic is that now Bob can immediately query this. Like there's no copying the data, no multiple schemas. It's like immediately accessible in the other engine. And this is what is engine interoperability that enables the Lakehouse. So here's the thing. So you just have to run a simple select statement and with one copy of the data and one schema, the other user is able to query the data. So this is the AI Lakehouse at scale. And basically now you don't have to create the bespoke systems, the expensive costs that I talked about, and you have faster time to market as well. Now the other way is true as well. Like a user can access any form of BigQuery table through the metastore. So for example, a Spark user can now query any BigQuery table and also like make sure that it's accessible in their engine as well. It's also a good way to create a new engine interoperability in its true essence. So with this, I want to say that I'm super excited to announce the general availability of BigQuery Metastore. To summarize, it provides a unified metadata capability across key engines such as Spark, BigQuery, Hive, and Flink across different table formats such as Hive and Iceberg and BigQuery as well. It provides for governance out of the box. So it really gives you like a governance strategy across these engines. And across different storage systems, it is picked off, say it's stored in BigQuery or say on a cloud storage as well. And along with this, support for key open APIs such as the HMS API and the Iceberg catalog, enabling to have like engine interoperability with open standards. So we really request you to take a look at this because this really enables a lake house for any of your problem pain points that I talked about in the beginning and really unblocks your key use cases by enabling you to query one copy of the data with one schema. With that, I hand over to Zenil from the CME group who will talk to you about their journey on building a unified data platform in the lake house. Thank you, Inul. Good morning, everyone. Thank you all for being here. I am Zenil Pama, and I head the data platform at CME Group. If you have traded futures or options for any length of time, I'm sure you have heard of CME Group. CME stands for Chicago Mercantile Exchange, and we are the world's leading derivatives marketplace, helping institutions and individuals manage risk, and navigate global markets with confidence. We provide a highly regulated platform to trade widest range of global benchmark products across all major asset classes in financials and commodities. Our clearinghouse guarantees the integrity of every trade, helping reduce counterparty risk and also helping keeping the financial systems stable. Our market participants span a diverse range from banks and hedge funds all the way down to individual traders, all using our markets to manage risk and also access global opportunities. We provide real-time data and analytics to these market participants, enabling them to manage risk by making informed decisions. CME has been around for a long time. We have been around for more than 175 years, with routes going all the way back to 1848. For the initial century or so, CME operated primarily in the commodities asset classes. Starting from 1970s, we started expanding into financial products. Passed forward to 2021, we announced our partnership with Google Cloud where we are building the future of the next-generation markets. Since then, we already have more than 26 petabytes of data in Google. And I think it's safe to say that we are not even halfway there. Now that you know a little bit about CME and what we do, let me walk you through our journey of how we build a highly resilient data platform in Google Cloud based on our learnings. So our journey started long before our partnership with Google. Around 2015, 2016 timeframe, we started building our lake house in AWS while our warehouse remained on-prem on Oracle Exadata. That brought a lot of learnings and just as many challenges. We had siloed data across teams and systems. We had duplicated data sets. Data ownership was unclear. Discovering and accessing data was not streamlined and because of that, our self-service capabilities were quite limited. Because our data was growing very rapidly, managing cloud costs was also an ongoing concern. So we always knew that when we migrate our workloads to cloud, we have to migrate everything, including our warehouse. So GCP migration timelines were always an ongoing concern for us. So based on everything that we learned, we knew that we had to reimagine how we deliver our data capabilities on Google Cloud. Essentially, we had to build a lake house. I'm going to walk you through five areas that we focused on for designing and implementing our solution in Google. The first challenge that we had to solve was our migration timelines. We knew that based on our highly, you know, large and complex workloads, we needed to provide golden path for migration based on automated predefined frameworks and templates using best practices. We adopted the ELT approach, which is extract, load and transform where data is loaded first and transformed in place. Also to meet our varying requirements from business to handle, you know, various, you know, types of data freshness, we ensured that the platform supports both batch and real-time processing. The next focus area was unified data access and integration. We needed our users across the team to access data in a consistent and secure way. Know where the data lived or what tools they were using. This is the foundation of Lake House architecture. Google's Big Lake and BigQuery were a natural choice for us. It provides that unified layer of access to data through BigQuery, irrespective of whether the data is sitting in open file formats, such as Avro, Parquet, Iceberg or whether it's sitting in native BigQuery tables all without the need to move or duplicate any data. It also provides seamless integration with our BI and HML tools supporting everything from dashboarding to advanced analytics. The next focus area was governance. We needed to solve for the data ownership and also wanted to ensure that our platform was always trusted when it comes to security. So we implemented data mesh architecture where data is organized by domains. Each domain has data stewards and data custodians who are responsible for managing access, data quality and also business metadata. When we talk about security, we implemented fine-grained access controls using our Google-provided roles and policies and then we also enforce end-to-end encryption of data both at rest and in transit. Next focus area was discovery and accessibility. We wanted users across the organization to easily find and access data whenever they need it. We implemented Atlan as our enterprise catalog which is fully integrated with Google's Dataplex. It provides search and lineage capability which allows users to know exactly what the data is about, where it is coming from, how it is getting transformed and also how it is getting used. This level of visibility and streamlined access enabled self-service capability for us where users can query and analyze data independently. The last piece of the puzzle was scalability and efficiency which primarily focuses on how we can continue to grow while still keeping things cost-effective. By leveraging Big Lake and BigQuery, we reduced data movement, we avoided duplication and we also implemented tiered storage access in GCS to manage our costs. By using services like Serverless Spark and also managed data proc, we were able to scale processing whenever we needed without really having any overhead of managing any infrastructure. So now that we have spoken about how we designed our system, let's take a look at high-level architecture of what the implemented solution looks like. This is CME's Unified Data and Analytics platform. Let me break it down a little bit. When we talk about data coming into the system, we are talking about three main zones, landing zone, processing zone, and curated zone. Landing zone and curated zone defines our lake house architecture. All data in these two projects are organized by domains which is our mesh architecture. A domain, when we talk about GCS, is a bucket. A domain, when we talk about BigQuery, is a dataset. Landing zone is built on top of Big Lake which internally uses GCS to store files in various open file formats such as Iceberg. Curated zone is built on top of BigQuery. All consumption happens from this project via BigQuery. We have dedicated processing zones for each domain and it solves two purposes for us. One is data injection and the second is data curation. Next, I'm going to walk you through how the data flows in the system starting from ingestion all the way through consumption and we'll also follow up that with a demo which will showcase how this actually works. Data comes into the system from various sources such as our on-prem systems from our applications that have already migrated to Google Cloud from our legacy data lake in AWS and also from our partners and vendors. We have built templates and automated pipelines to ingest data in both batch and real-time mode. We heavily use services like Spark, Dataproc, Storage Transfer Service and Dataflow for ingestion and all of our workloads are managed by Argo. Ingested data lands into Big Lake in various file formats and our automated pipelines create authorized views for this raw data into curated zone so that it can be consumed through BigQuery immediately. Users can further curate this raw data they can apply data quality rules they can also apply metadata and also do various transformations using language of their choice. We use GKE for compute and we use Argo for workflows but the users also have the choice of leveraging Spark and Dataproc to do the curation. Curated data lands in curated zone in native BigQuery tables. Once again, all consumption happens from here. Let's dive into consumption. Users can search for data and also look at the lineage information in our enterprise catalog which is admin. Once they know what they want to access they can submit access request which is routed to data custodian for approval. Once they have access they have the option to query and analyze the data using BigQuery Studio or they can also use BigQuery connected sheets. Our internal applications consume BigQuery API for leveraging data from our curated zone. For our teams like Quants and Data Science we create a separate platform for them or project for them with capabilities like Vertex AI, Notebooks, AutoML and also Cloud Workstation to fulfill their needs. We also have a semantic layer that is built on top of Looker and Tableau which is fully integrated with BigQuery to support all of our dashboarding and visualization needs. For our external customers we provide them data in multiple ways such as WebSockets, Google PubSub and also through file shares. For our customers that have a presence in Google we allow them to access data directly through BigQuery through a Google data exchange service called as Analytics Hub. And to conclude to meet our SLOs we replicate all the data from primary region to a secondary region in Google to protect us against any potential regional outages. So hopefully this gave you pretty good insight into how CME's Unified Data Analytics platform is built on Google. Without any further ado let me turn it over to my colleague Ilangu Ganeshan who is going to walk you through a demo. Good morning everyone. I'm going to right slide. Okay. So I want to encourage each and every one of you to embrace three key principles when you are starting your Lake Coast journey in Google Cloud. The first one adopt security first mindset and that's going to help all of your stakeholders inside the organizations come on board and help you guys with the migration. The second one is to reduce the barrier to entry for all of your engineering teams with different skill sets. And that's going to help you with reducing the time to market. And the third one is to invest in self-service capabilities and that's going to help you scale horizontally across the organizations. I am one of the engineering leads for data platform team at CME and I have been involved with our data lake implementation in AWS as well as with the Lake Coast implementation in Google Cloud. So, with keeping in mind security, self-service and time to market we're going to do a two part demo based on the Lake Coast architecture that Zeno talked about. First part we'll be focusing on automated infrastructure provisioning to support our domain teams. And that will make sure the provision infrastructure is secure by default. The second part we will be focusing on deploying data pipelines using Golden Path, analyzing the data with BigQuery and visualizing in Looker as well as with Google Sheets. Now, let's use a business use case to run through this demo end-to-one. CME group business teams, market makers, researchers and traders require access to order book market data, which is bid ask price data to measure the liquidity of the market. To measure the liquidity, we need to analyze huge volume of historical data and then calculate the difference between bid and ask price. I just want to make sure this is not a trading advice. So, all of our historical market data is currently stored in AWS S3. So, let's go through the steps that are required to run this demo end-to-end by migrating the data from AWS S3 to the Google Cloud Storage and then analyzing it with BigQuery and then visualizing it in Looker Studio and Google Sheets. Okay. So, implementing a lake house in GCP requires provisioning resources across multiple projects, adhering to CME's security and governance standards. So, to simplify this process, we have abstracted everything behind Kubernetes resource model as API interface with our Github's principles. CME is heavily invested in GKE and all of our development teams are already familiar with Kubernetes resource model. So, choosing Kubernetes resource model or KRM as an API interface was a natural choice for us. Now, let's go through the infrastructure components that will be provisioned when teams onboard to the data platform using this process. As Zeno talked about the Lake Coast architecture, landing zone and curated zone are shared projects. And for each team, we create a processing zone project in order to support dedicated compute slots as well as BigQuery slots. So, when each tenant on boards, we create a KMS key in each zone. We create BigQuery data sets to ingest data and also manage big lake tables. We create cloud storage buckets and landing zone to ingest raw data. We create cloud storage bucket and processing zone to support temporary artifacts to execute data pipelines. And we create big lake connection in our landing zone to manage the big lake tables. We also deploy all the infrastructure resources that are required to execute a data pipeline in our processing zones. So, we provision a service account to access cloud storage and BigQuery. We provide GKE namespace to deploy the data pipeline. We provide Dataproc configurations if teams choose to run the workload in Spark. We deploy Argo to orchestrate and schedule our workflows. So, we also run security and governance policies as part of this deployment process to make sure provisioned infrastructure meets CME security needs using Open Policy Agent. Now, let's go through what we discussed so far in action. For the demo purpose, we are onboarding a new domain or a tenant called Next2025 using KRM specification. We are creating a YAML file and providing basic description of the domain, providing the data engineering teams contact information. We are also declaring billing identifiers for the FinOps purpose and then providing reliability configurations that will help the teams to replicate data across multiple regions to support high availability. Now, let's go ahead and commit these changes to Git and let's submit a pull request. Okay. Once pull request is created, it's reviewed by engineering teams to make sure the pull request adverts to the standards as well as via automation process to check for syntax violations. Okay. The pull request is reviewed and merged now. Now, let's go ahead and kick off the deployment. For the demo purpose, we are manually running the deployment, but in the real world, this is all automated. All right. Deployment is kicked off now and it's checking out the repositories. We can see that it is running Terraform initialization and Terraform plan and then it will execute OPA policy validation using open policy agent to run for security violations against the Terraform plan and then infrastructure provisioning is kicked off for the tenant onboarding. All right. The deployment succeeded, I think. Okay. Let's quickly switch to the Google console and explore some of the resources that are provisioned. We can see that in landing zone, a new bucket is created for the next 2025 domain and it is configured with default secure configurations for encryption keys with CMEK as well as basic lifecycle policies. Now, let's switch to the BigQuery Studio. We can see that a next 2025 data set is created in landing zone to support our big lake tables and next 2025 data set is in curated zone to ingest data natively into the tables. We can also see that a processing zone project is created to provide dedicated compute capacity for the teams. And overall, this entire process takes less than an hour to complete for the engineering teams. Now we have seen how the teams onboard to the data platform using this process. Let's get back to our demo use case to measure the liquidity of the market using market data. Like I said, all of our historical market data is currently stored in AWS S3. We will be deploying a data pipeline using golden path to migrate this data from AWS S3 to GCS bucket using cloud storage transfer jobs and then running a cleanup process and automatically creating big lake tables in landing zone as well as creating authorized views in a curated zone. And this is going to help our customers directly run queries against data that is migrated from AWS. We absolutely support transform and load steps to curate and load the data into our native BigQuery tables. And at the end, we also support capability to run data quality validations using AutoDQ and publish the results to Enterprise Data Catalog, which is Atlan. And finally, we'll see a simple basic visualization using Looker and Google Sheets. Let's go ahead and see this entire thing as in the demo. Okay, we can see that all of our market data is currently in S3 partitioned by date, exchange code and asset class. Now let's go ahead and onboard this data to our data platform in Google Cloud using KM specifications. We are creating a YAML file and then providing source type as S3 and declaring the bucket name and providing the prefix location to pull the data from. We are also providing the partition format, partition keys, and then the schema associated with the data. And that will make sure automated pipelines are generated for migrating the data from AWS and ingesting into the big lake tables. We are also declaring a target specification to point at BigQuery and that will automatically load the data into our BigQuery tables. We are also creating... All right, so we are creating data quality specifications to run the validation using AutoDQ. We are creating the source to run the validation against, providing the sampling percentage and the filtering criteria, and then couple of data quality rules. One to run the not null check, the other one to run the validity check against the values. Now let's go ahead and submit this to... Let's go ahead and commit this to Git and then submit a pull request. Okay, the pull request is again created. It is reviewed by the engineering team as well as the automation to check for syntax errors. Once it's approved, the automation is going to automatically merge the PR. Okay, let's go ahead and kick off the deployment pipeline again. The pipeline is checking out the repositories. Again, it's going to run the Terraform plan and then run OPA policy validation to check for any security violation against the Terraform plan. And then the data pipeline is deployed using the golden path. Now let's go to the command line and take a look at the execution parameters for the data pipeline. We'll be running the pipeline for a partition date of March 11, 2025 for the AutoBook data. Now let's go ahead and submit the pipeline to Argo. All right, the pipeline is kicked off. And it is successful. Okay, now let's go to the Google Cloud console and take a look at the data quality results in AutoDQ. And for the demo purpose, we have declared two rules, one to succeed and one to fail. We can see that not null check succeeded and the validity check against set of values failed. Now let's go to the BigQuery Studio and take a look at the data. So we'll start from the landing zone. So we successfully migrated all of our data from S3. Let's take a look at our landing zone, big leg tables to make sure the data is successfully loaded. Okay, we can see the data in the landing zone tables. Let's also take a look at the authorized view that we created in the curated zone that is exposed to our customers. We are able to access the same exact data in our curated zone as well from the customer standpoint. Now we have all the data that is required to perform the analysis. Let's go ahead and execute the analysis query to calculate the bid and ask spread for an e-mini SMP contract. We were able to successfully query the data and analyze it. Now let's go ahead and visualize it. We'll start with Google Sheets. With a simple click of a button, we are able to get the data from BigQuery Studio all the way into Google Sheets using BigQuery Connected Sheets. And we can natively visualize this data using charts. By customizing the parameters and again clicking on the Apply button, it is automatically going to go to BigQuery and pull the data in and visualize it. Now let's take a look at the same in Looker Studio. Again, with the click of a button from BigQuery Studio, we are able to take this data into Looker Studio and visualize it. So hopefully the architecture that Zenul went through as well as the end-to-end demo that you guys saw helped to connect everything together. We were in Google journey for the last three years and we learned a lot. And we want to start some of the lessons that we learned based on the decisions we made. The first part, which is on my, I think, right side, is the plan that we did initially. Some of the decisions we made were irreversible, meaning it's almost impossible to change or it's not cost-effective after the initial implementation. Now, if you are planning to migrate data from other hyperscalers like AWS, network cost is going to be a major factor. So, especially if you're migrating petabytes of data. So, we recommend working with the hyperscalers account team to make sure it is not adversely impacting your cloud bills. The second thing we want to emphasize on is if you're planning to create big-leg tables on top of cloud storage buckets, and if you don't know the workload's access pattern, we strongly recommend enabling auto class on those buckets. Now, if you choose to manage the lifecycle configurations on your own, you might end up paying data retrieval fees if the data is not stored in standard storage class. And later on, if you decide to enable auto class, you might end up paying data deletion fees or early deletion fees if the data stored in archive storage class or near-line storage class did not meet the retention requirements. So, when we initially created our automation, we went overboard with the number of KMS keys that we created, meaning we created a KMS key for every BigQuery dataset and cloud storage bucket. And that started showing up in our cloud bill. Later on, what we realized is whether you, I mean, all these KMS keys were actually used by cloud storage agents behind BigQuery and storage buckets. At the end of the day, whether you create a single KMS key or multiple KMS keys, you're granting access to the same service account. So, we recommend creating a key per service within a project to make sure, to find a balance between security and cost. And there are some decisions we made we were able to incrementally evolve and adapt. When we migrated all of our Spark pipelines from AWS EMR to Dataproc, we did a quick lift and shift to meet our migration timelines by making very minor changes to integrate with BigQuery using BigQuery storage APIs. After our production launch, what we realized is the BigQuery storage reads were getting limited to a couple of Spark executors. So, by working with Google, we identified that BigQuery storage read API sessions are rate limited within a project and the support has to increase the limits. So, support helped us increase the limits and that's a considerable improvement in the performance. The next thing I want to focus on is Dataproc. So, we migrated all of our Dataproc storage to local SSDs. By mounting local SSDs with the rate zero configurations, we significantly reduced all these Spark related shuffle issues. And even during last year's election cycle, when the market volumes were all-time high, we saw a consistent improvement in performance. Overall, this resulted in around like $1.2 million in cost savings. We are going to conclude with what we are focused on for 2025 and beyond. With more adoption, especially with sensitive data workloads, there is a heavy focus on fine-grained access control. Currently, we are working with Google to implement fine-grained access control using BigQuery column data policies. The next big thing we are focused on is seamless data distribution for our external customers who are not in Google Cloud. Now, when Vinod announced BigQuery Unified Metastore with Iceberg support, we are super excited about it. So, we are currently exploring opportunities to integrate the Iceberg metadata with Snowflake and Databricks to meet our customers where they are. Especially, this is going to help us reduce data duplication. And we are heavy-looker users. And currently, we are on looker SaaS. We are actively working on migrating to looker core to improve our security posture and bring all of our data into VPC service control perimeters. Everyone is excited about Gen AI, right? So, we are not an exception. So, we are actively reviewing our security and governance controls to enable Gen AI use cases on top of our data. We strongly believe metadata quality is super critical to enable Gen AI use cases on the Lakehouse. So, we are heavily investing in our enterprise data catalog to enable Gen AI use cases. So, hopefully, if you are in the process of migrating to Google Cloud for implementing the Lakehouse, or if you are planning to start the journey, so I hope this helps in planning your migration better. Some bonuses in to take in place in rooms will look at the damage of the today's buildings that may not be awesome. So, we are also going to be doing location lin Chicken Mar 남 ki. Our Lee here at N точно上 er and H wong K Cassis has brought a tribute on the layup decision that is fulfilled. Sa Native Americans are proud, and we are not talking about a crucial team at permite.