 All right. Hi, everyone, and welcome to our session. Today, we're going to talk about data warehouse and data lake modernization, and we'll also show you how BigQuery can help you modernize your data for the AI era as well. My name is Osman Ali, and I am data analytics solution lead. With me, we have Vishali Walia from Paypal. She's senior director of engineering from Paypal. And then we have Davide Korda, who is from Intesa Sao Paulo, working as a senior director of data. And last but not the least, we have Mohit Virindra, who is head of product of data analytics at Google. And together, four us are going to take you through these following four points. First, I'm going to talk about some of the migration best practices and some of the pitfalls. Then, Vishali is going to talk about how Paypal has migrated their environment onto GCP. Davide is going to talk about Intesa's migration journey. And Mohit is going to talk about some of the interesting product announcements. So let me get on with the first topic. But before that, very quick overview. And maybe some of the slides you might have already seen. So what I'm going to do is I will actually go through them very quickly, just not to bore you. So generally, you ask this question, like, what's stopping you from achieving this AI-driven transformation? Usually, you put these issues in three buckets. First one is you have data silos. You have structured unstructured data, data and different cloud providers, on-premise, across clouds, and so on, so on. Then you have lack of AI readiness is the second bucket. And this is usually where data and AI model live in separate systems, different vendors, again, different clouds as well. And then last bucket, there's an increased cost. So obviously, maybe your environment can scale, but the cost-wise, it also scales very linearly, which is not ideal when you, you know, really scale your data as well. So this is the third bucket. So with Google Cloud Autonomous Data and AI Platform, we have designed this to basically kind of mitigate all these issues. And it goes from multi-model data foundation to governance across data and AI, as well as it offers you integrated AI capabilities. And lastly, it offers you a wide variety of engines. So let it be Spark, SQL, AI engine, you can actually do all of these. And then the main message I want to leave with this slide is like, we are an open cloud platform. You know, we work a lot with different open databases, file formats, iceberg, and such, as well as we work with, you know, we offer a lot of incentives for both of our partners and customers, as well as we have a big list of technology provider partners who really help streamline and, you know, do these faster migrations. Now, you will hear this a lot. So BigQuery migration service. So this is one service I want you to take away for everything around migrations. So whenever it comes to data warehouse migration, data lake migrations, BigQuery migration service is one of the key service you're going to be using. And I will suggest to use that as well. And this service itself consists of four main services. Like first one is your data transfer service. Second is your assessment service. Third is the translation service, which supports around 15 different dialects to BigQuery dialect. And then fourth one is data validation service. And we have seen roughly 3x growth of this service year over year, which is actually, you know, a testament of how, you know, this is evolving as well. And then very quickly, this chart is showing you some of the migration costs on a timeline. So if you look at from left to right, you can actually see, you know, how these costs have evolved, how this, you know, this, you know, resources, let's say back in like 1990s, 2000s, I was part of one migration where we had to throw like roughly 150 people to do a Sybase to TeraData conversion. I used to work for TeraData at that time, you know, so it was a territory migration. And now, you know, similar migration can take with, you know, less fewer resources. And of course, it's faster. There is a less IT freeze. It's more lift and transform focused. And we're not talking about these as IT led projects anymore. We're actually talking about these projects to be more like business as well as IT driven both. And with tools like Gemini translating code now, maybe the next version will automatically figure out why the original developer wrote it that way. Or maybe that's not a good idea. Let's keep it that way because sometime you want to keep these mysteries as is. And talking about this migration service, of course, Vishali and Davideh is going to talk a lot about their experience with these services. But just to quickly highlight Vodafone and Metro, these are two customers I have worked closely together. And, you know, they have migrated. They're very, very complex environments. Let's say in Metro case, there were 20 countries. They were using their on-premise data warehouse. And they were actually managed to migrate this successfully using these services. And similarly, Vodafone, a large scale telco, very complex environment. And they were able to do this migration within nine months, which is pretty cool in that sense. Now, getting on to the four or five main points I'm going to talk about. These are just some of the guardrails and design principles. Obviously, you don't need to, like, follow all of them strictly. They are there. It depends on, you know, what environment you're talking about and what scenario and so on, so on. But basically, I'm going to talk about these five points very quickly. First one is kind of obvious, but we get carried away. So often you need to resist the urge to do a complete big bang migration. Because let's say you want to, you know, do too much and you want to kind of, like, do a lot of transformation. You want to change your ETL tool and so on and so on. This is very wishful sometime. And this can actually lead to literally calling it boiling the ocean. And sometime the only thing you will successfully boil is probably your project management's, manager's temper or maybe, you know, the budget of your company. So think about phases. And very quickly, if you look at this upper part, I know it's not visible for everyone, but first one is showing existing architecture. And then you're going to the target architecture. So you need to really think about introducing phases like transition phase where you can make your source and target into BigQuery. And then onboard your users, unlock your AI and ML workloads, and then introduce a transformation phase, which will be more focused on, let's say, you want to actually, you know, do some of our transformations there. You want to replace your ETL tool there. You want to do more like disruptive things, you know, and then you can go to the actual, you know, cloud native phase. And then schema and code optimization. This is also another interesting aspect. One key message I want to leave here is BigQuery is multi-engine environment. You might be coming from an environment which is, let's say, bolted on solution. You have probably not like a different engine for different things. But in BigQuery, we offer a SQL engine, we offer ML engine, and we offer a Spark engine. So you can actually really optimize based on these specific workloads. So you don't need to like basically think about as one engine and everything I need to optimize would have to be that way. And then remember, there's this principle called Pareto principle, which is like 20-80 rules. Sometime doing 20% of the change can give you 80% of the result. So in the optimization as well, you need to think about that as well. Okay. Now going to the third one, data integration. So think of using old tools on a new machine. This is a common pitfall as well, trying to adopt brittle, outdated ETL tools. You probably all remember like trying to download a Blu-ray movie using dial-up internet. You know, how would that experience be? So sometime using these old ETL tools could give a similar experience, you know, moving your data from one place to another. So what I'm trying to say is don't, I'm not saying that replace your ETL tools. If they work very well with BigQuery, just use them. But sometime they may not. So, you know, you need to also think about that as well. So instead, look at the modern cloud ingestion framework. And the diagram on the right side, it shows you like why you want to switch from ETL to ELT. Yes, there is a one obvious reason that, you know, you want to do push down into cloud and, you know, you want to achieve the cloud performance of BigQuery and stuff like that. And that's why you want to switch to ELT. But there's another big benefit of doing that is you can actually bring your transformation libraries close to your business users. So they can actually really apply different transformations based on different needs. Now, CI, CD. So lack of proper versioning of data and code, you know, this is also one pitfall. And BigQuery really helps you not to actually have a version control for your code, but also for your data. And one advice I want to leave here is with get everything. Okay. Like you have BigQuery repositories, they integrate really well with GitHub, as well as you have you have data form that also integrates really well with Git. Similarly, your developers' favorite ID tools integrate very well with Git. And we are working hard actually to bring that developer experience. Like your developers really don't need to actually switch to BigQuery Studio. Yes, BigQuery Studio is amazing. They should probably in some cases, but sometimes developers have different systems, so many different languages, and they want to prefer using their favorite ID like Visual Studio Code or JetBeans and stuff. So we have plugins for Gemini, as well as plugins for BigQuery. So that developer experience, as well as BigQuery Studio experience, can actually be part of, you know, their favorite ID as well. Okay. Now the last point is around data governance. One of the most important ones, sometimes we ignore because sometimes it's also probably the most boring one as well, arguably. So imagine your current environment, if it's riddled with quality issues, you know, simply moving it to the cloud will just create a digital data swamp. And it's a new place with the same old mess. Yes, that new place is probably has better view, because again, it's a cloud view, and you get a beachfront and condo and everything. But still, you know, if your data is bad, it's a data swamp, it's still a swamp. So the idea here is this migration is a key opportunity to really work on some of these things to do some of those cleanups, as well as think about three main BigQuery pillars around BigQuery access control, around CMEK, as well as AEID encryption, where you can actually apply encryption on the fly, as well as bring your own keys, and even integrate with external key manager. And lastly, think about using data loss prevention, and BigQuery, you know, profiling, as well as quality scans, because they can actually really help you find those anomalies. Okay, so with that, I'm going to stop here and ask Vishali to come over and tell us about PayPal's story. Hi, everyone. I'm Vishali, and I lead data platforms at PayPal. Thank you for being here to hear about PayPal's migration journey. Can we have a show of hands if you use PayPal, Venmo, Zoom for international payments? Wow, that's amazing. Thank you for being PayPal customers. So for those who may not know what PayPal is, what does PayPal do? PayPal makes sending and receiving money, selling and shopping simple, personalized, and secure. So the numbers that you see on this slide should give you some sense of the scale at which PayPal operates. Big data and analytics is at the center of everything we do. It's how customers experience our services, like the risk models, for example, that are trained on massive amounts of data to detect fraud. And it's also how we make data-driven decisions across every domain in the company. A few years ago, we had 400 petabytes of analytical data that was fragmented across a dozen or so data warehouses and data lakes of many different flavors. How did we get there? Because we had some fixed capacity platforms on-prem that were not able to scale, and we added new platforms through acquisitions of other companies that came with their own data ecosystems. Now, when you have thousands of analytical workloads that you want to run, there's a whole supporting cast of platforms that you need, like schedulers and BI platforms and data movement platforms, ETL platforms, and so on. So, clearly, we couldn't scale at the velocity that our business needed, because it was just too complex to scale our analytics ecosystem. Moreover, innovations in AI were happening rapidly in the cloud, and we couldn't leverage them on legacy platforms and on fragmented data. So, we had to do something about it. It was clear that we had to unify our data in the cloud to have the scale, the agility, and the access to AI innovations. So, why did we select BigQuery? Because it checked all of our boxes. It's a fully managed cloud-native data warehouse. It has powerful capabilities and performance at the scale that we needed. It has a familiar SQL interface, which makes for a much gentler learning curve for your data scientists, data engineers, analysts, and developers. It has BQML for model building, training, predictive analytics that makes AI very easily accessible and democratizes it. And it has integrations with Vertex AI, which provides access to the full range of MLOps capabilities, and the model garden, which now, of course, includes multiple LLMs. So, how did we do it? 400 petabytes migrating into BigQuery? This is our playbook. It's not everything, just a highlight reel, and hopefully, you know, this is of interest to you and can help you in your journey if you're on one. So, first and foremost, enterprise alignment. This has nothing to do with technology. Large-scale migrations are a multi-team sport. A Teradata platform we migrated was our enterprise data warehouse. It was one of the largest, if not the largest, Teradata platforms on the planet. It was our analytics workhorse used by more than 90 teams across PayPal in every single domain. And it took us 18 months to migrate it to BigQuery, start to finish, from the day we said we're going to migrate to the day we decommissioned the Teradata platform. This was possible because we made it an enterprise priority. Every stakeholder team had it as an OKR on their roadmap. Once we had that, everything else started to fall into place. The next important thing is discovery and analysis, and this is kind of obvious. So, we created detailed inventories of our data, users, workloads, and data pipelines, both inbound and outbound in our source platform. We discovered lineage. What are the data dependencies of these workloads and what are the inter-workload dependencies, and we created these detailed graphs. The other thing we did was analyze the usage of all of the artifacts. What is their business purpose and establish ownership for each one of them. Next comes strategy. And these are just key principles that we laid out. Some of them are listed here. One was we said we're not going to carry any tech debt forward. And I know, Usman, you said do not boil the ocean, but we did decide to boil the ocean. And we said anything that doesn't have a justifiable business purpose or based on the usage analysis. It's not heavily used. We were questioning, do we even need this? Right? And then if there were workloads that had a similar purpose or data sets with minor differences, we decided to consolidate them. Another decision we made was to decouple dependencies. What I mean by this is we said we are going to be able to migrate our workloads in any order. So a downstream workload can migrate ahead of the upstream or at the same time. It doesn't matter. And we did it by carefully orchestrating how we were replicating data and stopping it at the time of cutovers. Then we also created some opinionated execution rails. So we had a fixed number of platforms through which applications could run to use BigQuery. And this turned out to be really helpful to have a level of governance and cost control as well. Then the next big decision was always do we modernize or do we do a lift and shift? Now, there are many advantages to doing a lift and shift. You can move a lot faster. You can leverage a lot of automation. You're changing fewer variables so there's less risk of failure. But there are some cases where modernization is necessary. You need to update the code because of business reasons or performance reasons. Sometimes you have to change the underlying data model. So we dealt with it on a case-by-case basis. And then we also realized that training is going to be essential for success. We invested heavily in training. We worked with Google to design self-paced as well as instructor-led courses. We had hackathons in PayPal offices across the world to get our developers excited about this journey. And then finally, execution. So this, of course, is the most important thing. We automated everything that we could. If it was a lift and shift, we used BQMS that Usman was referring to, the BigQuery migration service, to automatically transpile code from a source platform to BigQuery. And it did a pretty good job for a variety of source platforms. We used it for Teradata, for Hive workloads. We used it for Snowflake, Redshift, many other platforms. We also automated testing. Now, you know, most of the heavy lifting of the migrations was done by PayPal teams. But we also worked with many of our consulting partners. Some of them are here in this room to help make this program a success. We also invested in building live dashboards to track the progress of every artifact being migrated. When you have hundreds of thousands of tables and hundreds of thousands of workloads, spreadsheets just become untenable. So we built a platform to keep track of our migration progress. And then finally, optimization. So optimization, we decided to include this at the time of migration rather than something that happens later. Because we are already modifying code and we're going to be testing code. To do it at a later point, you have to modify code and test it again. So we worked with the Google team to make improvements to be QMS. So it would generate optimized code. And then we also built a query optimizer more recently that is using LLMs to optimize queries at scale. So not everything was smooth sailing. There's a number of lessons we learned along the way. And one lesson we learned is moving targets are hard to chase. For our first migration, we realized business doesn't stop. New use cases keep being added to the source platform. So we had to have strict moratoriums on that and prioritize making data available in BigQuery for those new use cases ahead of existing use cases that were being migrated. The importance of architecture review. This was especially true for Hadoop workloads because we had a choice of multiple architectures in Google Cloud. We could rewrite the code for native BigQuery or we could use the Spark connector for BigQuery. We could use Big Lake or we could move the data into GCS and use Dataproc. So again, this was a hard lesson that we learned how to create the optimal architecture for cloud for these workloads. And then we created a decision tree and had a governance process to make sure we were reviewing every use case. FinOps cannot be an afterthought. This is really important to know what resources you're consuming, who is consuming them and holding them accountable. When you have unlimited resources in the cloud, it becomes very easy to start blowing your budget. So this is something we realized a few months into our migration journey and then started building our own FinOps platform. And then the long pole of data reconciliation. So data discrepancies will happen, you know. They're inevitable and it takes a long time to resolve them. So no matter how much automation you build, this step remains largely manual. When you have billions of rows in a table and your row counts are mismatched by 20 records, it's like trying to find a needle in a haystack. So it's important to factor that into your migration schedule. So the impact of migrating into BigQuery and unifying our data has been tremendous. We've seen huge gains in query performance by magnitudes for complex queries that are used by data scientists, anywhere from 2.5 times to 10 times better performance. Our data is more than 16 times fresher because we eliminated all of those hops and data pipelines that we had before. We have instant access to insights on stream process data in BigQuery. Reduced complexity. We virtually eliminated all of our data duplication because we got rid of all those many platforms. We reduced our operational burden because we have a fully managed platform now. And painless DR enablement. BQDR became GA this year and literally at the click of a few buttons you can have DR for your platform. Greater resilience. No more infrastructure incidents. Disk failures. Other hardware failures. Not our headache anymore. We have magnitudes better SLA compliance compared to what we had in the past. Built-in data recovery because we have features like time travel in BigQuery. And then, of course, the benefits to business enablement. We are rapidly onboarding new use cases, and able to scale them. We are able to offer AI-based personalized experiences like rewards and shopping recommendations. Check out your PayPal app. You know, if you haven't opened it recently, you'll see those rewards and recommendations in there. We've been able to reduce the complexity of our MLOps pipelines by using Vertex AI. And the data boost from BigQuery to the feature stores makes features instantly available for inferencing. And then, of course, AI agents have the potential to open up possibilities that we are only now starting to imagine that will revolutionize commerce. All this is possible because of clean, governed, and unified data that we have in BigQuery. And with that, I will ask our next speaker, David de Corda from Intesa San Paolo to share his story. Hello, everybody. And a pleasure to be here telling you more about the story of our migration journey to BigQuery. So, a few words about Intesa San Paolo. Intesa San Paolo is a top banking group in Europe and leader in Italy in all the business areas. And the scope of our migration is what we call Data Service Hub. Data Service Hub is our enterprise data platform. It's not a data warehouse. It's a much more complex data ecosystem of tools and processes that is the single point of truth for all the data governance and data usage within the company. It's basically a data lake integrating more than 200 heterogeneous sources, a semantic area, and then data products. So, this is the system that is a critical system that feeds every day all our business channels, the marketing campaigns, data labs, and in particular, also the governance processes that we run in risk and compliance, financial and supervisory reports. So, it's a critical system and it's based on technology built 15 years ago. Teradata do best in class at that time, but that now is showing, was showing some limits similar to what PayPal was experiencing, of course, like scalability, problem with performance, and also the need to always duplicate data for their full usage in other databases. Also was not really suited for the new AI use cases, unstructured data, all of that. So, based on this scenario, we started thinking about how to modernize this platform, how to change the setting. And of course, there was a lot of skepticism also in the company because this is a system that runs very critical processes. It's not something we can stop anytime, any day. So, there was a bit of a nervousness about migration and was seen almost as a suicidal mission to do such a big migration without really stopping the service. So, what we did was to try to rally support from the different stakeholders. And the key element for that was to build a business case. So, to show really the benefit of doing such a migration. And so, we did a TCO, five years full-scale business case that will show to the CFO, to the other stakeholders in the business, what is the value of doing such a migration. And actually, after doing that, we start gaining much more support and more attention. And we move into running a selection phase between four different options of cloud providers. So, first we selected Google BigQuery. But then, of course, the game was not done. There were still people, hmm, is it going to perform as well as our platform? Is it really going to do what you said in the business case, etc.? So, we had to run really a thorough testing phase that was very important to really get the full confidence. So, scalability, performance tests. So, we took the full power of equivalent to our Teradata platform and we created an environment that could process a similar workload in BigQuery. And we actually showed the 30% improvement on the mixed workload that was exactly the workload representative of our peak day that is generally end of the month. So, not happy of that, not happy of that, we did a three-time scalability test that proved that actually linearly the performance could stay consistent. This was really amazing because we could see the value of the change we were proposing. And then also some functional testing. Finally, we could start planning the migration then. And then we organized it on three major drivers. So, first of all, guarantee, number one, guarantee that our processes that are today running on the platform will not stop and actually will have no regression in the new platform. This was a requirement number one, absolutely necessary to not cause any regression to the business. Number two, was actually to try to kickstart new projects that we have in the bank that could immediately benefit from this key enabler. So, this, for example, our core banking transformation that is ongoing. So, those processes need to run without extraction of data. They need to follow a native approach using the cloud. And then number three, modernize. Try to find ways not to do a lift and shift, but really improve the technology. So, for example, our ingestion, we redesigned it into a PySpark parametric engine that could run actually multi-cloud and then substitute tens of thousands of ETLs that every day were feeding the system. So, a lot of changes. And also on data catalog, for example, we implemented data plex and all the capabilities, replacing other tools that are legacy tools for us. So, what are the results that we are? So, what are the results that we have achieved? We are halfway through. We are still alive. Not bad. And it's a long journey. It's a three-year journey. You need to keep pace. You need to keep attention. And we focus on organizing the plan with a foundation phase to run all the automation, to invest on the foundation capabilities for paths that are really essential to build the framework and the blueprint that you will need to build solid foundation. And then a pilot wave that basically vertically will test the migration on a slice of our data from lake up to data products. And then we started, once we have done that, we were able to start parallel waves of migration and we are now progressing well. And in parallel to new projects that are enabled natively on the platform, like supporting our core banking transformation for all their data components, AI compliance use cases, and also with the integration between BigQuery and Salesforce, we're able now to run campaigns using data natively, not moving data at all. So, this is also a very effective achievement. In terms of performance, we are delivering the data earlier. This is already a massive achievement. Having the data earlier for the users is helping them to focus more on analysis, to get better SLAs on their processes. And this is already, even on the process that we have not transformed, the one that we are guaranteeing no regression, this is already a big result. And then, of course, we are seeing improvements in cost, in performance, in general, time to market, to kick off new projects. And then, it's always a good time to do data lake rationalization because is everything really necessary? So, we did an analysis of all the data items and we saw that at least 30% of them were not used in the last 10 years. So, this is something that definitely we have decided not to bring forward, but has been, of course, a complex analysis and a socialization with all the data owners and data users of those data anyway. That has to be done, but I think it's a worth investment. And then, migration of the scale are also good occasion to try innovation. And we were able to use some of the tools that were described before, like SQL translation from Teradata to BigQuery and also some Gen AI to automate some of the text to SQL capabilities. So, let me conclude with some key success factors and lesson learned. And, of course, key success factors. This is a big teamwork. It's a teamwork between, of course, Intel Sao Paulo stakeholders all together in the different parts of the business and technology. And also, with the support of Google and the other integration partners that are there to help us because it's a very complex journey. But, it's very founded on keeping all together the same planning. And the focus on planning is very, is absolutely key. So, we need to plan in advance user and data owner availability and try to minimize their involvement because they are very busy on many projects. And we need to help them to focus on the right issues on this migration. To do this, what we did is an automated reconciliation on a granular level, day by day, of all the data elements that we are migrating. So that they can focus on which are the key points where they need to investigate and support us. And often what it comes is actually that what doesn't reconcile is what is wrong on-prem. So, it's a good chance also to start again clean in the new environment and fix those data quality issues that you had there for 10 years and nobody noticed it. So, it's the right time to put things in order. So, lesson learned, of course, as also we saw in the other presentation, FinOps is a critical thing. You cannot really predict always what a configuration or design decision will imply in terms of consumption. But you need to monitor it. We create a FinOps now granular on a daily level that is very, very much capable to tell us where we are spending, where we are consuming more and more. So, this is something to invest really upfront. And then we also did identify static call data. These are big portion of data because our system has 10 years history. So, a lot of the data is never changing. So, those big parts, we have moved them in advance already in the first year, regardless of which wave they were planned to. So, that moved already the gravity into cloud and helped us to gain a bit of time ahead of the end. And then the foundation and automation already described. These are things you need to plan in advance to keep a journey of the scale running and be successful. So, thank you. Thank you. And I leave the mic to Moit for some. Thank you, Vishali. Thank you, Davide. And thank you, Usman. So, hopefully, if you're still sitting undecided whether to move, should we move, what will it take to move, this would have at least given you some credibility. So, I'm coming up in the last. I lead the product management side. We want to show you the tremendous investments that we have made in automating, in taking away some of the pain and decision making, making it easier for you. And we have learned in working with the great customers who just presented. So, without further ado, we've heard these points as a recurring theme from both our customers. Should I modify? What's the risk? How will the journey look like? So, we took that as our guiding principles and we implemented them in BigQuery migration services. Two things I want to call out here. One, we don't do data warehouses alone. We now do data lakes, Spark, Hadoop, workloads as well. Number two is this is a very partner-friendly service. So, we have API first and we are free to use. So, if you're a partner in the audience, you can pick and choose what components of the service you want and use some of them or use all of them. We don't obligate you to go from end to end. That way, you can white label BQMS components in your accelerators wherever you see fit. Here are the announcements. Here are the launch announcements from today. Quite a few of them. I apologize for the ugly, busy slide, but these are kind of the key things that we want to talk about. We are making our Gemini-enhanced batch and API translations generally available today. And I have a slide if I can get to it. We are also announcing a preview of Gemini-enabled translations. The difference between the two is the first one puts Gemini after our compiler. So, it takes away the toil from a human. You've done the translation. You've done translation at massive scale. Now, what happens to the last 5% of the work? So, the first addresses that. The second puts Gemini in front of the translation engine. So, what that does is it allows you to submit unclean input, mixed input, XML-embedded SQL, things of that nature. It takes away the burden of submitting clean code. You can submit old code. This is, again, 15, 20-year systems. Very common for us. So, this is to address that. We are also announcing in preview source lineage for all the 15 sources that we support, which is very powerful so that our customers like PayPal don't have to build the lineage themselves. We've had assessments which give a view of landed state of your workloads on data analytics for a while. We're introducing a new class of assessments. These are assessment lights, which means that they're supposed to run in minutes. They don't extract anything out from your databases. They don't query your database. We export a signature, and based on that signature, we are able to calculate how much your expected cost of landing on BigQuery will be. It's quite powerful. Happy to talk about that outside after the session. We're also announcing three major data migration connectors, Cloudera in preview, Snowflake in preview, and Teradata has been out there for a while with Redshift, but we've added something called Delta compression in Teradata, which means that now we can replicate your data, continuous replication, with the smallest possible size of data being carried over the wire, and we guarantee the integrity. And last but not the least for Cloudera, we are also doing metadata and governance. I've already discussed these just in the interest of time. I wanted to show this view. So BigQuery migration services, we heard that name a lot. Everything in black has existed before today. Everything in green is announced today, and everything in red is coming up shortly. And with that, I will say if you're interested in learning more, scan this QR code. One is for what BigQuery migration services, and you can start using it right away. Scan this code. The second is we have incentive programs both for our customers and for our partners. So we want to take the pain out of an already arduous process, as our customers told us, and we want to make it as easy, as cost-efficient for you as possible. Things in the incentives include removing the double bubble cost. You're running two platforms at the same time. We don't want to saddle you with that cost. But with that, please provide your feedback for the session. Thank you very much. Thank you to our esteemed customers.