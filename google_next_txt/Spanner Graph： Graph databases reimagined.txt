 . Hi. Hello, everyone. Super excited to be here. I'm Pratiba Suryadevara, VP of Engineering at Google Cloud. The next 45 minutes, we're going to dive into the world of graphs. We're going to talk a little bit about how Spanner is uniquely positioned to run your graph workloads. We'll talk about the latest capabilities that we are working on. We cannot not mention Gen.ai within the first one minute of the session. Yes, we'll talk about Graphrag. We'll talk about Gen.ai capabilities. And another exciting capability, the hybrid search techniques that we can use to get the tailored contextual responses for your Gen.ai applications. So, it's going to be an exciting thing. So, joining me with me today is Bay Lee, engineering manager, founder of our Spanner Graph. Delighted to have Spurthy Ravi, engineering manager at Snap. The next 45 minutes, we'll do just recap of what graphs are. We'll talk about how we've reimagined Spanner Graph, what we have done to make it the most reliable, most suitable environment to run your graph workloads. Then we'll have Spurthy walk us through how they're using Spanner Graph to run their identity workloads, which has billions of users at Snap. We have an exciting live demo from Bay. And then we'll talk about a little bit of capabilities that are coming up. And hopefully, we'll have time for Q&A. If we run out of time for Q&A, we are always there outside. We'll spend some time answering your questions. Let's dive in. Let's just spend a minute or two on what graphs are, right? So, graphs are basically data models optimized for relationships. We think of graphs as just nonlinear data structures containing nodes and edges. Like, for us, when we're building these, it's pretty much nodes are the entities and edges are the relationships between the two. This is a just simple example of one of the graphs. In this graph, we have, like, people, the skills they have, the places they live in, and we can build applications like recommendations around what member groups they want to be in and stuff. I think graphs are everywhere. If you look around whether these are retail graphs that we have, where they think of relationships between who's buying what products, what recommendations we want to give them, or the transactional ones around financial systems, right? Who is doing what transactions from what country, what locations? Are these fraudulent transactions? How do we want to think about these? So, we think the use cases for graphs are, like, endless and infinite, right? So, we think that across multiple verticals, right? Whether it be retail, financial, telco, supply chain, drug discovery, network security, I think the use cases are pretty infinite. So, then the question is, hey, I understand graphs. I understand it's used for representing relationships. Why can't I use existing data structures to represent it, right? You have all the relational models today. Yes, we can. You can use foreign keys. You can use joins to represent that. But suddenly, like, very quickly, it becomes very, very complicated, right? I have a very simple example where you have maybe trying to build an application where it's friends of friends, and you're trying to figure out what posts these people have liked. People are across different countries. These are friends and friends. If you're trying to build a SQL for this, right? If you represented this in a simple relational table and built a graph, built a simple SQL query, it becomes very complicated very quickly. Compare that with what you can do with GQL. It's a simple few lines of code that you can write to understand the relationships between these. So, I think so far we discussed what graphs are, what we want to use these graphs for, and we think that graphs are the right way to model these relationships. And these graph databases, because of their query specialization, because of the graph languages they support, and also at times the memory structures that are set up, are the ideal tools for running graph systems or systems or building applications where you want to understand deeper relationships between. We talked about graph. Graphs are amazing. We talked about they're applicable everywhere. But we all know graphs don't have their adoption, right? Then if it's so amazing, why don't we see graph adoption the same way we see the adoption that we have for other relational NoSQL or document stores? About two years ago or a year and a half ago when we started building Spanagraph, we talked to hundreds of customers understanding why don't we see this adoption? What are the challenges for you? It kind of summarized into three set of challenges, right? It started with operational overhead. We all know I think graphs are not the single source of truth. They're usually backed by another relational store. What it means is you're building pipelines to duplicate that data. You're managing those pipelines. You need to worry about the security compliance of this. Yes, we put all this effort. We are ingesting into a relational store. We have built the pipelines. We've scaled. We put everything together. End of it, you would be disappointed with the SLA that you're getting if the SLA of a graph database is lower than that of your relational database. The second one is the scalability limits, right? We all know these graph queries are very compute intense, and the compute needs are not very predictable because generally they span out. A simple example, if I have a two-hop query, and I'm looking at the performance of a query, if the node is a celebrity versus a regular person, the performance is going to be very different. The way some of the systems tried to solve this problem is, hey, why don't I actually scale the writes vertically, and I'll scale the reads horizontally? But we all know vertical systems have a finite scale. So what that means is you can't build large graphs. So that has been one of the challenges that most of our customers highlighted. The last one is a learning curve. Yes, we said graph languages are amazing, easier to program in. But before we had standards, SQL was available for graphs only later part of 2023. What we did with label property graphs was not a standard. Only after we have GQL is where we are actually getting to the standards. So with this mind of these are all the challenges that we have, I'll invite Bay to walk through the approaches that we took to build Spanagraph to solve some of these challenges. Okay. All right. So this is Bayley here. I've been leading the Spanagraph effort since the beginning. So like Pradeepa mentioned, right, so since very early days, we try to understand why you are using graphs in the first place, what kind of value it brings to you. But even more importantly, we want to understand why you are not using graphs. What kind of frictions are there? So essentially we want to keep the good things and remove the bad things for you. So in addition, we actually looked into, you know, Google internal systems and learned lessons from there as well. So if you remember, you know, Google actually has a very long history with graphs. So even 25 years ago, our company founders invented the PageRank algorithm. PageRank algorithm is actually a graph algorithm. It projects the whole web into a citation graph. And then essentially nodes are web pages and edges are hyperlinks between the web pages. Then you propagate the scores through these links. And when you convert, you get the centrality scores, which are essentially how important this web page is in your entire web graph. Right? So and then this centrality score is used in the ranking. Right? So you all know how well it works. Right? Google becomes very popular. Another example is actually knowledge graph. So more than a decade ago, so Google introduced the knowledge graph. So knowledge graph is essentially the knowledge of the whole world. Right? So it has places, things, and people. So essentially when you search in Google.com, you will type in search queries. We actually translate the natural language you type in into graph query, into many graph queries, actually, to fetch the data from its knowledge graph. Then we produce the answer for you. So essentially, knowledge graph turned Google.com from a search engine only to an answer engine. Right? So I presently actually spent six years writing code building knowledge graphs. And I had a lot of fun there. So essentially, we took the lesson learned from you. Again, you know, thanks a lot for the feedbacks. Right? And we took a lesson learned from the Google internal systems. We folded them into a span graph. So this is actually decades in the making of sense. Okay. So what is span graph then? So it has three main pillars. So the first pillar is the native graph experience with SQL interoperability. So the second pillar is the search on graphs. And then the third pillar is the scalability and the performance. So let's actually dive right in. So first of all, span graph is a graph database. So we want to make sure the experience is smooth. For that, we provide you native graphic experience with GQL. So GQL stands for graph query language. It's a new ISO standards, you know, released, I think, April last year. So Google and many other graph vendors actually got together for past six or seven years, actually. So, you know, working on these standards. Eventually, you know, luckily we published it. So Google actually one of the early adopters of GQL. So if you look at the GQL syntax, you probably don't feel it's very foreign. So if you are familiar with the core language called OpenCipher, it's mostly evolved from OpenCipher. So there are a few differences. For example, in the first line, you need to specify a graph to query. That's because Spanner actually supports creating multiple graphs within your database. So you have to specify which graph you want to query. And the rest, you can see there's match statement. So that's a core part of, you know, GQL. It's a graph pattern matching. So you can see the syntax. I hope even if you don't, you haven't learned any graph before, I hope it's easy to understand. Essentially, you use parentheses to denote nodes, the arrows to describe edges, right? So essentially, you traverse from a customer named John, traverse its node edge, one to three hops to reach a target customer. Right? I hope it's very easy to understand. And then the rest of the query is mostly, you know, similar to traditional queries, right? It's order by, limit, and return. But how are these queries executed? So we all know graph queries are very performance sensitive. So for that reason, we actually optimize these operators quite a bit. For example, like match, optional match, quantifications, pass modes like any trail and walk. And path finding like shortest path and cheapest path. Right? So we adopt multiple, you know, optimizations, for example, vectorized processing, factorized processing to essentially process your edges in bulk or process them in a very compressed form. Right? So there's actually a lot of optimization into this. I will give you a quick example. For this query, like remind you, like any, if you see any right beside the match, the second line. So the any, the semantics of any is actually a reachability. Right? So let's say you have two pairs of nodes. So you want to just keep one path instead of all possible paths become this one pair of nodes. You just keep, keep one, right? You want to dedupe. So when you run this query, there are three main operators to support this query. There's the scan, like starting nodes. You want to find like the first node is John, right? You want to find that node. And then, you know, you'll find that node through that operator. Then the recursion operator keeps track of the node you already visited. You want to make sure you don't do like duplicate the computation. Then you expand from J to A and B, like two more nodes it connects to. And again, the recursion keeps track of A and B, right? You want to dedupe along the way. Then, okay, you keep expanding from A and B. They reach a common node C. But if you recall, the semantics of any is to keep like one path between a pair of nodes. So now between G and C, if you don't remove the one from B to C, you will have duplicated path. So we remove that one. The moment we find out the duplication, we remove it. So you don't like waste your computation and then come back and remove. We do eager pruning like versus late pruning. Then you traverse again, right? So from C, you reach A and B. And because A has been already visited before, right, we also train that path. And then you have essentially the result. So you can see as we go, we remove all these duplications, right? So that's one optimization among many other optimizations we had. And okay, for the rest of the query, you can see like order by, limit, and return, they are not really graph-specific, right? So in SQL, you solve a similar thing. So for that reason, we didn't try to reinvent the wheels, right? Because like what's the value of doing that? We want to focus on the graph problems. We don't want to reinvent the wheels for no values. So we reuse all these relational operators. So besides the operators, we actually reuse the data types, functions, and expressions, too. Because like, for example, string, string in a graph is a string in SQL. Like what's the difference in these data types? So we didn't find any value over reinventing all this. And because of this deep fusion of this relational technology and the graph technology, we can do a lot more powerful things. For example, here's one example. You can, you know, you have full interoperability between SQL and SQL. This rectangle, if you can see, that's the same graph query we showed you before. But because of this deep fusion of the technology stack, you can essentially embed your SQL and SQL together, right? So you can use the graph to do what graph is good at, like traversing relationships. But you can use the rest for the SQL, right, if you are more familiar with it. So imagine you have a group of data scientists who are really familiar with SQL and you want to introduce graph to them. Like how do you do it? Right? Instead of asking them to, okay, so now you have to do SQL for everything, you can just tell them you can just keep using your SQL. But only for the parts you feel it's very hard to do in SQL, you can embed this little SQL there, right? You don't have to do everything all at once. So you can gradually, you know, expand your SQL knowledge and do more and more SQL, right? So this is essentially making sure the learning curve is as, you know, as shallow as possible. And search on graph is also very key, right? So sometimes because graph is, you know, is for relationships, right, traversing relationships, if you want to find a rich information about something, let's say, like, like shoes or something, and you can not only look at the shoes itself, you can traverse this relationship to find more, like, producers, buyers, or brands. You know, you can just traverse a few steps deep to find the richer information. But how do you get started? Like, how do you find the starting nodes, right? You have to find the starting nodes to traverse the graph. So search is actually for that purpose. So let's say if you want to find more information about hiking shoes, and you can essentially just use search to find the starting point, and then you traverse the graph, right? So this is actually very common, especially now the JII age, right? So you have natural language as an input, and you want to ground your foundation models with a knowledge base. This is actually the thing. Like, this is the glue between this foundation model and your knowledge base. So how a graph created in the first place. So we have this, the opposition is there's already a lot of connected data, but they're stored in the tabular format. So if you remember the entity relationship modeling you learned in the undergrad school, it's about modeling graphing tables, right? Entity and the relationships, they're essentially nodes and edges. But right now we want to give you the reverse mapping to essentially produce the graph from the tabular that you already have. So here's one example. You have tables and then you use the schema to reverse them back to the graphs on the right-hand side. There's a lot of benefit. There's no ETL, no data duplication, no data lag, data governance is no longer a problem. Okay. So I will just quickly go through how data is stored because the format is important, right? So in Spanning Graph, we actually store the nodes and edges together so that when you do traversal, it's a simple I.O., right? So you don't have to, like, use your multiple reads and, like, from the disk or from the storage or from the buffer pool, we just minimize the number of pages you have to read. And all these are stored in actually clauses, which is our clause storage. So the disaggregation of the compute and storage is very key, right? For example, you can access the storage without really going through the Spanning servers. You can essentially decouple this. Isolation is really good in that sense. In summary, like, we have a native graphic experience. We have interoperability between GQ and SQL. And we have a search on graphs. We have virtually unlimited scale. And we have a competitive performance. And it's on top of Spanning. It's very quickly, like, why we choose the Spanning as a foundation. So many years ago when we started this effort, we looked into the technical stack of a graph database. So there are many parts of that database is actually graph-specific. But there are so many parts which are not really graph-specific. For example, transaction management, right? You want to commit your data atomically. So it's not really graph-specific. But data import, export, you know, load balancing. So monitoring and alerting, resource provisioning, and the control plan, all these are not really graph-specific. So we want to save our energy to focus on the graph thing. And Spanning, you know, on the other hand, it's very, you know, used widely in Google, right? All the payment, you know, search, ads, they all use Spanning. So it's unlimited scale, you know, very high availability. There are so many benefits we just build on top of this platform. All right. So, okay, so much about Spanning graph. It might sound good on paper, looks good on paper, but how does it work in reality? For that, I would like to invite Sporsi on the stage to give us the insight of how Snapchat uses Spanning graph. Thanks, Vic. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Hey, everybody. I'm here to talk about a few graph use cases at Snapchat. Snapchat itself can be thought of a large graph where the users, their friends, their content, ads can be modeled as nodes, and the relationship between them can be modeled as edges. Our communication graph has over 1 billion user nodes and 10 billion communication edges between them. Today, in particular, I want to cover the Snap Ads Attribution use case, which involves identity matching. So what is identity matching? It involves the process of identifying users and matching them with events that happen both on Snapchat and off Snapchat. We utilize various signals to identify these users, such as cookies, hashed emails, IDF fees, et cetera. Why is this so important for Snap? This directly improves our direct response ads because we can then attribute a conversion to Snapchat. This also increases the availability of third-party signals so that our ad ranking, targeting measurement models can get better. So let's go a little into what the current system looks like and what are some of the issues with it. The current identity matching system is built on top of KV stores, which works well up to two hops. Here, for example, if you get a conversion event, you can see a bunch of KV stores here, and we try to match that event. If there's a match, great. If there isn't, we do some offline processing to get a probabilistic match. At the end, we can only go two hops. We need to be able to go multiple hops at scale to better improve our matching. Additionally, another issue is staleness of data. So if there is a linkage that comes through that we can't immediately match, let's say, like, hashed email and a cookie, and there's no direct match on these tables, we aren't able to augment this graph immediately. We need to do offline processing. It takes a while, and then we're able to augment this graph. So how can graphs help? Graphs have the ability to add real-time connections to the graph, both edges as well as nodes, ability to traverse the node more than two hops at scale. Also, we're able to compute graph analytics on top of it, multiple algorithms like PageRank, that we can then augment the graph with node and edge features. So by modeling the identity data as a graph where the personal identifiers I mentioned, like hashed email, cookies as nodes, and the relationship between them as edges, we're able to traverse the graph more than two hops at scale. We're also able to give our ML engineers the ability to run different query models to improve our match rate. So a little bit about the size of the identity graph at Snap. So we have over 20 billion user nodes. So these are the personal identifiers I talked about, like hashed email, cookies, IDFEs, edges. These are the relationships between them. And these come from both real-time events as well as linkages that we get after offline processing. We have over 10 node and edge properties, and the total size of the graph goes over 10 terabytes. So let's take this example to kind of understand a little bit more of how a graph is better than a KV store for identity matching. So looking at this, let's say the current graph has like three linkages. A user ID to cookie1, cookie1 to cookie2, and cookie2 to a hashed email. And we get a new event which contains two PIIs, which is a hashed email and a completely different cookie, cookieX, saying, you know, this email and this cookie bought X, right? So given the existing linkages in the graph, we are able to match cookieX with user ID, as you can see. The graph query would look like this. Retrieve the first matching user ID starting from cookieX or this node with a max path length of three to four. So why spanner graph? So a concern with graph DBs have been availability, scalability, and especially for a system like ads attribution, where we do need ads, the identity data to be persistent. We need a system that's fault tolerant, highly available, and can scale. So spanner offers sub 100 millisecond latencies for three to four hop queries. It also allows us to scale horizontally with social workloads not being predictable. During NYE, Black Friday, we have a lot more events than normal days. And we need to be able to immediately scale without having any downtime. Additionally, it's available, it's managed, secure, and also we have in-point recovery. So we built the identity graph on spanner and we see some really great results. The really low latencies, three to four hop queries where P99 is over around 50 milliseconds. It can horizontally sail with traffic increases. We're able to add new events and linkages in real time. We're also able to capture relationships that we couldn't before because they needed so much more offline processing. In the future, we would want to run graph analytics and graph algorithms on subgraphs so that we can further enhance the node and edge properties. With this, I will hand it back to Bay for a demo. Thank you. Thank you, Sikorsi, for the great insight of the recommendation of the identity resolution in Snapchat. So that is one of the key use cases of graphs, but that's not the only one. So let's say we have a recommendation, we have a fraud detection, we have a root cause analysis, and many more impact analysis, for example. So here I will just show you one example of a recommendation using a spanner graph. So let's say you are an engineer working on a retail website, and you want to build a recommendation engine. So the experience is going to be when a user comes to a website, if they do a search, not only you want to return the product they are searching for, you want to return other things that are relevant and they might be interested in. So you want to, like, cross-sell and upsell. So let's say to begin with, you want to start simple. Let's say you just want to return things they are searching for. Like, remind you, a spanner supports a search. So this quick example, you can see I'm select star from product table using this search function to return the product. So this is a simple demo. So if I run this query, you can see there are multiple product returned. They are essentially all some kind of hacking shoes. So this is a great start. But you don't want to stop here because you have so many other things people or your user might be interested in. And you want to try luck with graph. So because graph, again, graph is about relationships. By finding all these connections in your retail graph, you might be able to find more things people are interested in. So let's say you want to try the graph. And, again, you want to start simple. You don't want to go, like, too far each step. So you can do the same thing with search, right, on graph. It's essentially the same search function just like in the GQL syntax. If you run this simple query, you can see you have these three products returned, right? So they are all, again, the same three products. It's a waterproof hacking shoes-related product. And then these are rendered as nodes in a graph. And now you want to go a step further. You want to follow the relationships and to find the things commonly bought together with these waterproof hacking shoes. For that, you use graph. So in this graph query, I hope it's very easy to understand. So you still do the same search to find the product, which are waterproof hacking shoes or boots. You traverse its purchase edge to find which customer bought it before. Then from there, you traverse the purchase edge again, but in the forward direction to find the things they bought, right? The users who bought these waterproof hacking shoes, what else did they buy? So you can see this is a simple relationship traversal using GQL. If you run this query, you can see there are many, many things got returned. So before you only had the three product or returned, which are essentially what you searched for. But by following the relationships, you can see there are like 20 product or returned. So if you can see them, they are hiking shoes for sure. They are like hiking poles, hiking socks, right? So backpacks, headlamps. So they are essentially the, you know, the essentials for you to have a nice, enjoyable hiking trip. And you are a good, solid developer, right? You don't want to stop here. And you remembered, so there are tables which record the past, you know, price points of this product. You want to surface this to a customer or users to make sure they have an informed decision if it's the right time to buy. But because the information of the price history is actually in tables, like you are using graph and there's a table, like what do you do? But with Spanner Graph, you don't have to complicate the things. If you remember, in Spanner Graph, we can actually combine SQL and GQL together. Let's switch to this query. It's essentially the same query, right? So this parts, this graph parts, you can still remember, right? So there's match and there's a product and there's a search. But that's exactly the same graph part for recommendations. But after the graph search return all the recommended product, you can do a drawing of the price history table, right? And essentially you bring graph and relational and graph together. So if I run this query right now, you can see, so all these products are recommended before, but now they are enhanced with the price points. Right? So now you can surface this to a customer so they have a more informed decision. Here's a very simple demo. So I hope you like it. If we switch back to the slides. All right. Okay. So since our public preview last August, right, so many of you showed interest of Spanner Graph. Right? So of course many of you launched on Spanner Graph already, right? So there's a lot of good feedback. So essentially you say you want more, right? You like what we offer right now, but you want more. So and we'll be working closely with so many of you to see what's the right thing to build, what's the priority. Right? So GraphRack is one of the things, right? So there's so many requests about GraphRack. Essentially you see the limitations of a rack because rack is based on the text trunks, right? So it doesn't have all the rich information in the trunks, right? You have to use vector search to find the relevant trunks, but you cannot follow the relationships. So for that reason, we launched, you know, Spanner Graph integration with Lanchain. Right? Right now you can use Lanchain to essentially turn a text document into a graph backed by Spanner Graph. You can actually ask natural language questions using Lanchain, right? We have some fine tuning or prompt tuning to make sure the translation is correct. Many of you actually are already using it. So visualization is a key experience, right? So because Graph, again, it's for intuition, right? It's very intuitive, easy to communicate if you realize them as Graphs. For that reason, we launched the Studio experience, which you just saw. And we also have a notebook, right? So it's open source. And also we heard from many of you that you have a lot of data in BigQuery and you want to serve it at the low latency. So we have a reverse ETL from BigQuery with one simple command. You can essentially bring your data to Spanner Graph for fast serving. Performance optimization is a never-ending journey, right? Because, like, because, again, Graph is what we do ships. So you want to traverse deeper and deeper, right, to find more and more things. So we are always tuning our performance. Emulator, right? So you want to test your things before release. Graph now is supported in Spanner Emulator. So the last item, we're going to enter the preview very soon. So especially in the GNI use cases, many of you said you don't necessarily know the schema beforehand. You want to handle this open data. So we are now adding the open data modeling support into Spanner Graph. Again, which will be previewed very soon. And furthermore, so Graph is such a critical piece of the Graph journey. So we've been working with many, you know, partners closely. Linkuris, Graph history, Canvas, and G.V. They are already supporting Spanner Graph. So actually many of them are actually here. Right? So if you are interested, please talk to them. Right? So I love the product they build. Okay. So here's one more thing. So, again, since the very early days, we tried to understand what you really want. Right? What kind of problem you have at hand. So we heard very early on that you have operational workload. So you have, in fact, you have very diverse workload. Right? Besides operational workload, you also have analytical workloads. So in the very early days of building Spanner Graph, we keep that in mind. Right? So we wrote code in a way that is mostly reusable. Right? And lately we've been working with our BigQuery partner to bring the BigQuery Graph to the picture. Right? So the Spanner Graph, you can see that's one stop of the journey of the Graph journey on Google Cloud. But that's not the last stop. Right? So the journey continues. So they support a full spectrum of operational and analytical workloads. They have a unified graph scheme and the query experience. Realization is also a key part. It's almost the same. So there's a very small data flow between these two systems. Query Federation, Graph Fabric, and ETL reverse ETL. We want to make sure this is really a unified solution for you. Right? So on Google Cloud. Okay. So Spanner Graph is already GA. Right? So many of you are already using production. So you can scan the QR code here to get started. There are, like, user guide, collabs. There are, like, more examples of blog posts. You can find more about Spanner Graph. And there are two sessions. Two more sessions. But Equivax is getting on the stage to share their experience with Graph. And there are more sessions. Right? And, you know, your feedback is already in the cloud. So besides the product feedback, you know, feedback about this talk, right? What do you want to hear? So there's just so much to talk about, right? The same deck, I can talk to you more, like, two more hours. There's no problem. There is so much to talk about. Let us know what you want to know. We will definitely do that next time. Okay. Thank you. So right now it's QA time. I hope you like this. Any questions we can? I think there's a mic going around. So. Can you talk more about this reverse ED? I think there's a mic going around. So. Can you talk more about this reverse ETL from BigQuery? Yeah. Go ahead. Go ahead. Okay. So essentially reverse ETL tells you. Mm. So essentially it's just one command, right? So you specify. So which tables you want to report from BigQuery, you need to number two for a millionpp. So comments-based, which helps move into that way to optimizeüllt. Technology can decide to compete on the very熟 car. you want to port from BigQuery to Spanner, you specify the essentially destination, the instances, and the tables you want to, you know, migrate data to. So that's one command you can essentially, behind the scene, the data will be like continuous streamed into Spanner. Yeah, I think the way we look at Spanner and BigQuery is as a single data systems. So when we talk to our customers, they talk about ingestion use cases where they're ingesting data into BigQuery, but they have some low latency real-time analytics they would want to run on Spanner. So we've created a seamless mechanism where you can actually stream that data back into Spanner and serve that data from Spanner. So you can use that for graph, you can use that for native analytics, you can use for any of those kind of use cases. So the idea is single system, you can figure out where you want to serve that data from. Yeah. I have a question. Thank you. So for like a typical RAG use case, you have all this, like say company documentation, and you have this doc and you need to parse out relationships. I don't fully understand how you parse those relationships. Is that something that's manually defined? Yeah. Okay. Good question. So essentially, there are different approaches. So we find the approach where you roughly specify what type of relationships or what type of nodes you are interested in, almost like a soft schema. So that is more efficient. Because IOM, they can read the document, they don't necessarily know where to focus. So if you just give them a hint, okay, this is a set of nodes and edges that are interesting. The quality and recall, we observe it's better. So based on that, we will, like with Lanchain, for example, we will create a Spanner Graph schema and import the documents, the relationships extracted into Spanner Graph and for query. I think there was a question here somewhere. Yeah. Kind of a new question here, but, sorry. Kind of a new question, but do you guys, how does a Spanner Graph handle hot nodes? Ah, that's a good question. So essentially, there are multiple technologies. So first of all, hot nodes are the ones with a lot of finding, a lot of find out, right? It happens a lot in the natural graphs. So essentially, we have an optimization, I kind of mentioned on stage, why is, you know, vectorized processing? So for the hot nodes, we don't process, even for the regular nodes, in fact, we don't process this ongoing edges or incoming edges one by one. We do them in bulk. So it's essentially like a scan almost like hundreds or thousands of them at the same batch and we process them. So another thing is called vectorized processing. Because hot nodes, let's say, if you have a very high-fine-in, high-fine-out hot nodes, we traverse across it, your data becomes, intermediate data becomes very large. So it's almost like exponential in a condition product. So we introduced something called factorized processing. So when it goes through these hot nodes, we make sure we compress the intermediate results so you don't kind of exponentially explode the results. So also, we are building something called sampling. So very often, you don't always need all the information about these hub nodes, right? Because, like, what you do is, like, 10,000, like, one million edges. So, like, say you are doing machine learning or something, very often sampling is good enough. So we actually have a built-in sampling operator as well. So, like, tailored for machine learning use cases. But there's, like, because there are so many about this topic already, you can always mark your hot nodes. Like, if you don't really care about there's so many edges, you can, like, trim them. Yeah. That's the thing we have right now. I have a question. Yeah. There it is. Hi. Thanks. This is awesome. This is really exciting. Obviously, talking about graphs, we immediately start thinking about Neo4j. And they've got a couple of analytics out of the box, things like page rank and community detection and those kind of things. So is that on the roadmap? Yes. When is that coming? And then additionally, in the same sense as well, shown the full-text search. Yes. When do we get multimodal search, like image search? Yeah. That's part of it. I can answer the search one. And I think, yes, ML algorithms are the next thing. We didn't want to put it on this thing because it's, like, we were told not to put roadmap things here. So we can follow with you online and offline and tell you what it is. I think one of the things in Spanner that we are super excited about is this multimodality that you can bring between full-text search, semantic search, and vector search. The Spanner full-text search is actually built on top of the same advanced search queries that we do for Gmail and our Google search and docs. So the use case that the demo that Bay has shown actually was actually token-based search that you're merging along with graph to find the node in the graph. So full-text search on Spanner is already GA'd. Semantic search, both for vertex integration, ANN and KNN, is also GA'd. So now you can build these very exciting AI applications that are hybrid search first. Then you find the node in the graph that you want to start with and traverse. So it's opening up really very exciting and different use cases for us. A quick question. Yesterday in the BQ graph session, Candice mentioned a query where we can merge both Spanner and BQ, like an external source. Yes. So is that the other way around as well, using external source as BQ and trying to merge federated query? You want to take that? Yes. So essentially, the thing you mentioned, I can give you a little bit more context. So we realize there are a lot of use cases where you don't necessarily have all your data stored in one place, but let's say due to governance issue or ownership issue, but we want to build a virtual knowledge graph across many data sources. So for that reason, we are actually supporting something we call graph fabric. So essentially, again, building virtual knowledge graph or spending multiple data sources. So BigQuery already, like yesterday, you saw the slides, right? You can essentially create a graph using multiple, you know, tables, you know, some are stored in Spanner, some are in BigQuery, so you can create a graph out of it. So these tables can be backed by, you know, different data formats or like iceberg or some Parque files or something or stored in Spanner natively. You can do that. Yeah. And we have more. Yeah. Thanks, everyone. We are at time, but we'll be outside there in the hallway. We can take more questions. I think we are at time. We'll meet you all outside. Thank you. Okay. Thank you.