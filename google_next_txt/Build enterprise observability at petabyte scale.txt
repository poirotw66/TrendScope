 Music Music Music Music Music Good afternoon. Welcome to our session. Hopefully you had a wonderful lunch in a box and really enjoyed the food. So today, we're going to talk about how do you build enterprise observability at extreme scale, both from a Google perspective and from a customer perspective. I want to start and I want to thank Yahoo for the partnership. It's been a tremendous amazing last year as we've been scaling the environment and I look forward for the next year's, but thank you for partnering with us in this journey together. So today, as I already mentioned, I will be providing the Google perspective and Shereek will come up and provide some background on Yahoo, what the environment looks like and and, you know, kind of the journey to Google Cloud. You know, how do you get there? We'll answer religious questions, you want to run an hotel sidecar or not, and get into lots of detail and we'll be sure to leave some time at the end for questions. So let's start with a quick background on Google, observability at Google, and a little briefing on open telemetry for those that don't know. So Google, no, we're not going to talk about AI. This is getting ridiculous, okay. So as a Google employee, I'm required to tell you how wonderful AI is, how transformative it will be to your life, and how life-changing it will be for you. So now that we have that out of our system, we will not talk about AI today. But I did want to provide a little bit of history for Google. It's amazing to think that it's been 20, nearly 27 years since Google was founded. You know, I always think of Google as an upstart company and everything else. And if you think about our business, we have 10 businesses with over a billion users each. So as we had to scale, we had to develop a different way to do things. You know, the SRE model, we've literally written a book on it, and everything else. But interestingly enough, to be able to do what we've done, we had to have rock-solid observability. We had to be able to ingest massive amount of telemetry at scale, and make it usable very, very quickly. So there's probably a few things you may or may not be aware about Google. And we're going to be able to do that. And we're going to be able to do that. And some of the things you may or may not be aware about Google. You know, one single Google data center has six times the bandwidth of the whole internet combined. And, you know, during the dot-com bust, and all the fiber providers going, we bought all that capacity. Plus we have ships going around the world at any given time, and we literally own the fiber capacity surrounding every single continent, attaching every data center. We have some of the most scalable systems in the world, and it's designed to allow us to do what makes Google, Google. So I wanted to provide more details on our internal observability system. But of course, Google being Google, the lawyers got involved and said, you can't use exact numbers and everything else, so I'll give you at least a scale of the observability systems at Google. From a monitoring perspective, we literally have quadrillions of time series in memory at any given time. We ingest hundreds of millions, over a hundred million data points per second. Exabytes of logs per day. So just massive, massive scale. And from an observability standpoint, that's very, very important, because we have to be able to handle all that telemetry, and we've had to do things in a different way, because we need the ability to be able to process, you know, and I'll use logs as an example. They need to be able to be processed and actionable within seconds. You can't wait 30 minutes. You can't wait an hour for a pipeline to complete, then to index the data and everything else. We need the ability to do everything in near real time. And so we've obviously done that for monitoring, logging, and trace, and that's what powers Google. And the good news is that most major services within GCP are already instrumented. You don't have to worry about this. It works out of the box. But what many people probably don't realize is a majority of our traffic actually comes from outside of Google. You know, whether it's a large retailer monitoring all their stores globally. You know, a lot of our customers run in a multi-cloud environment. You're still going to have on-prem environments, and you need to be able to watch everything in near real time. And that's kind of what Google's designed to do. And to make this work, we had to figure out a way to be able to ingest that telemetry at scale and make it actionable. But the other thing we had to do is we had to break the paradigm of, we have separate logs for the security team. We have separate logs for the operations team, for the network team, for the application team, for the data team. It just is not a scalable model. It becomes extremely expensive. And unfortunately, I'll show you in a second, with AI, it's very important. Yes, here, I said it after making a joke. Shame on me. So, architecturally, what we've done is we can bring any type of telemetry in. We don't care. Believe it or not, we still have a ton of customers running FluentD, FluentBit, Spanza, OpenTelemetry, CollectD, completely unstructured logs. It doesn't really matter. You know, the goal is to be able to handle that telemetry at scale. And an interesting thing that you might not realize is about 18, 19 years ago, Google had to figure out a way to handle our internal logs at massive scale. So, how do you process these and make them actionable? We had to build a near real-time SQL query engine. Internally, it was called Dremel. You probably better know it now as BigQuery. What's ironic is the technology has come around. We've had to spend the last five years to make it work for customers to be able to support JSON format logs and to be a much more usable experience. But think about it. We're able to bring any type of telemetry from any location, use the most scalable data platform in the world, BigQuery, and, of course, AI. Why is that important to have it all on one platform? And it comes down to a very simple fact. You know, Gartner coined a term a few years or eight years ago, to be exact, AIOps. And the theory is we'll take all of our telemetry, we'll put it in a data lake, run unsupervised machine learning, and none of us would need to be employed. And it would make our lives easier. But the reality is that has not proven to be true. And it turns out that you need to keep your data in place back to that one copy of data. You need to be able to run your machine learning on that data in place. And by being on the BigQuery platform, we're able to do that. And then furthermore, we're open in. We don't care about the format. And we're open out. You know, so you can use, obviously, our tools. You can use Grafana. You can use whatever you want. We don't care. So I want to provide a little bit of background on open telemetry, because that's going to be important in our story. How many people here have heard of open telemetry? See, this is great. It's getting better and better each year. About three years ago, I would ask about open telemetry. Someone would half raise their hand, like, I don't want to be the only person raising her hand. Last year, a little bit more. And now you'll see more and more use cases, like Yahoo using open telemetry in production today. Google is actually re-instrumenting a lot of our back end to be open telemetry. And what's crazy is, obviously, I'm biased and focused on observability. But to see a chart like this, that open telemetry, and believe it or not, there were a couple months last year that open telemetry was even more popular project than Kubernetes. So think about it. Observability, more popular than Kubernetes. And by the way, the number three, Cilium, is also observability related, eBPF. So that shows how important this has become for all of us. And very simply, we need one way to instrument things. It becomes very complex if I had to deploy. Here's a logging agent, monitoring agent, trace agent. I had to do manual instrumentation. Very, very painful process. And what open telemetry allows us to do is very simply instrument once with a common semantic conventions, common protocol, and then it gives you data portability. So you can use it everywhere. And this has really, really been industry changing because to be able to put a slide up like this, that every observability platform now supports open telemetry, is something that, you know, even five years ago I would have doubted that everyone can come together with a common goal, but the market is clearly spoken. So a few words about Google and our view on open telemetry. I was inspired by the lunch today. And our strategy could be summed up in one thing, forks. And very simply, no forks. We've heard loud and clear from our customers, do not have your own distro. Whatever you do, John, please, please, no. Because it becomes, once again, creates a management issue if I have to have, each platform has its own distribution. Very difficult to manage. Everything we're doing is a no-till core. And, you know, now we have either in preview or GA native OTLP endpoints for any environment, which is great. You don't even need a Google exporter anymore if you're using the new OTLP endpoints. We'll have native pipelines later this year for both GC and GKE. So fully managed pipelines, which simplifies this and makes it much, much easier. The other thing, though, that was interesting is, especially a lot of our European customers are highly regulated. You know, we love open source. We believe in open source. We just can't use it. So what Google's done is we're not, don't worry, it's not our own distro, but we are, through our assured open source program, we're now going to offer the ability to have a Google vetted, goes through all the security process, fuzz testing, everything else, guarantees that there's no supply chain injection, and supported by Google, is a huge thing. So now I'll let you have the best of both worlds. Fully open source, but have the security of Google to do that. And then I'm not going to spend any time today on APM, but you might have noticed themes about application-centric observability and some of the keynotes, and you'll see lots and lots of sessions in AI-assisted, and open telemetry makes it super easy to now start to get a unified view of the environment. So with that, I'm going to turn things over to Shari. All right. Thank you, John. I really appreciate that. Good afternoon, everyone. My name is Shariq Anwar. I'm an observability engineering manager for Yahoo Mail, and today I'm going to walk you all through how we implemented Google's observability suite, as well as adopting open telemetry. Before I go into further details about that, I wanted to quickly give a few brief metrics into providing how big our Yahoo Mail environment is. So Mail consists of 130 different components that are distributed around the whole globe. These components do send about 190 million metrics per minute, and in some use cases, we do have these metrics can have dimensionality that goes above 150. So that means we have more than 150 different attributes per metric. Currently, we are using an on-prem system that's built in-house, and it's a custom platform for our metrics environment, and it's built on top of OpenTSDB. From a logging perspective, we use Splunk on-prem. We have a Splunk enterprise environment that is extremely large. That index is about 1.5 petabytes of data a day. It's, I think, over 2,000 or 3,000 machines distributed, and our goal as we're moving to Google and Google Cloud and throughout this whole digital transformation strategy is to reduce our logging volume. I'm sure I'm not the only one, and we're not the only one in that boat. It's probably everyone in this room has some sort of strategy that they're looking at for cost optimization. From a tracing perspective, we currently do that with our logs, and we have it implemented within our logs. It's not ideal, but we are looking to actually implement native OTLP tracing by the end of the year, and that should also significantly reduce our logging volume once we get that implemented. Next, I want to quickly just highlight why we chose a central observability model and what were the requirements around it. For mail, we wanted to have a project that essentially stores all of our logs, all of our metrics, and traces into a single pane of glass view that gives us an end-to-end flow of our application. From a day-to-day perspective, our engineers, they need to be able to troubleshoot, respond to incidents, and as well as get down to the root cause. A central observability environment is pivotal for that. Moving on to some secondary requirements, we did want to have logging separated out and accessible per team or per project. And in those use cases, that came down to providing role-based access control on specific logs to allow for least privilege access. In Google, you can use log views to actually accomplish that. And the last piece is separation of knowledge objects and search artifacts. And what I really mean on that is having role-based access control on your dashboards and on your alerts. Coming from a Splunk environment and other tools, they have these types of features already built in. And whenever, you know, team A logs in, you don't want them to be able to edit team B's dashboard and vice versa. There is a feature request currently that we're working on with Google. Essentially, it will use Google's cloud resource tagging mechanism, and you'll be able to tag dashboards and alerts with a label that is then also tied back into an IAM principle. And that IAM principle has a binding, which whenever a user logs in, they'll only be able to edit or read the particular dashboards and the particular alerts. Okay. There's a lot happening on this slide, so I want to kind of break this down a little bit. Before I move on, I wanted to kind of highlight three key aspects of Google logging. One being a log bucket, which essentially stores all your log data. A log router, which is the central component responsible for managing the flow of logs generated by your resources. And the last one being a log sink. A log sink are destinations where your log entries are routed and stored. So let me get behind the podium real quick because I did want to turn on my laser pointer for this. So I'm breaking this down into three different pieces. The first piece is here that I'm highlighting, which is listed as a GKE cluster project. The GKE cluster project is where all of our applications and our services are deployed. As part of onboarding to this GKE cluster, we also create a tenant project associated with it, which is just a GCP project. And within this tenant project, an application can run their managed services that they may need. And on the far right-hand side, you can see the central observability project. And that's where all your logs get stored from your infrastructure, from your managed services, as well as your applications. So let's break this down first by looking at the GKE cluster project. We have our infrastructure logs, which are highlighted in blue. You'll see that there is a logging sink configured as part of that project. And the destination for that logging sink is the central observability project. Once those logs get routed to the log router on the central observability project, there's a rule that says, route these logs to the appropriate log bucket, which is this infrastructure log bucket. At the same time, we have an OTEL sidecar running as part of our GKE cluster and our applications. That sidecar has a configuration to route the application logs back to that tenant project that was created. And within that tenant project, there's a sink configured to route your managed service logs, as well as your application logs, to the centralized observability project here. And that log router also has a rule to route the logs from your tenant, as well as your managed services, to the appropriate log bucket. One question that probably is going to come up later on is, why do you route your sidecar logs to the tenant project and not directly to the central observability project? There are two main reasons for that. The first reason being, if we wanted to really group the application logs and the managed services together. And the second reason being, when routing the logs directly from the sidecar to the central observability project, the configuration was going to become extremely complex and we want to keep it as simple as we can. And so now you can see a team can log in to the central observability project and they'll have access to all of their infrastructure logs, as well as their sidecar logs, and as well as their managed services if they're running any. One quick note that I did want to mention is, we are using cloud identity on the central observability project for a more seamless integration. We were using workforce identity and we ran into some limitations with that. So I know the identity team, I believe a few folks from Yahoo are here that are presenting on workforce identity. You can listen more about that in their session, but we, from an essential observability project, did implement cloud identity. Okay, moving on to the metrics architecture. Before I go any further, two concepts, again, that I really want to highlight. One being a metric scope. A metric scope is a configuration that stores time series data within the project. By default, a project's metric scope only includes the project itself. And the second concept being a scoping project. A scoping project hosts the metric scopes. So having those definitions out the way, we'll focus again on three different aspects here. One being the GKE cluster where the application is deployed. The underlying infrastructure, your hypervisors, your nodes that you're running over, and the underlying sidecars. And all those CPU and memory metrics, the system metrics, get routed to a metric scope that's stored as part of the GKE cluster. And again, there's an open telemetry sidecar that's running. And that sends your application metrics back to that tenant project that we discussed earlier. And stores them in that metric scope. If you're running any managed services as well, those metrics are also stored within the metric scope as part of your tenant project. So now you have your infrastructure metrics, your CPU, memory, disk, all part of your GKE cluster project. And then your tenant project includes your application metrics as well as your managed service metrics. Now on your far right hand side, you still have your central observability project. And the metric scope for that project goes and adds a scoping project for the tenant project as well as the compute project, right? The underlying GKE cluster projects. Now when a user logs into the central observability project, they'll have access to all their metrics from the managed service as well as their application metrics and the underlying system metrics. All right. So how is this all done? I talked a lot about the architectural diagram and really want to focus on the onboarding process. As I mentioned before, the tenant first has to be onboarded to the GKE cluster project. So once they're onboarded to Google Kubernetes, there is a request submitted to my team, the central observability team, which then we check to ensure that the tenant has actually enabled the appropriate permissions for the central observability Google service account. And that service account is required to have those permissions to create those resources that we just discussed earlier. Your log sync, your log bucket, your metric scope. So one, my team also developed a Terraform module that you can actually enable. So every team that's onboarding, all they have to really do is just enable a Terraform module. They get the appropriate permissions that they need for the central Google service account. And then from there, the central observability team, which is my team, submits a PR to a very simple YAML configuration file, which just lists the Google project ID. Once that project ID is there and listed and somebody approves that, there's a CI CD pipeline that kicks off. So that CI CD pipeline underneath the hood, it's actually just going through a bunch of different Terraform modules and creating the resources that we just discussed. It will create the metric scope, create your logging syncs, modify your logging router rules, as well as create your log bucket. Once that CI CD pipeline completes, if it's, again, a tenant that's onboarding, all the logs will send, end up getting sent to the central observability project within the appropriate log bucket. And you'll also have access to the metric scope as a scoping project. At the same time, if it's an infrastructure project, which is our GKE cluster projects, you'll have all the logs getting routed to a GKE cluster log bucket. And then you'll also have access to the underlying system metrics as well. All right. Moving on to OpenTelemetry at Yahoo and how we adopted it. This is a very opinionated slide here that I feel can have a lot of different opinions on which model you should adopt. You can go with a sidecar approach or you can go with a daemon set approach. It really comes down to your use case and your requirements and what best fits your use case. From a sidecar perspective, it has its advantages. It also has its disadvantages. From an advantages perspective, application-specific configuration is ideal within a sidecar approach. So if you're going to have each application requires its own custom collector and having its own tailored data collection and data processes, that's one reason you would want to use a sidecar approach. The next reason being isolation and scalability. Since each pod has its own sidecar, you can manage application-specific data collection independently and scale pods without affecting other applications. And the last one being reduced overall impact. And what I mean by that is lower blast radius. If a sidecar fails, it only fails on the associated application and not the whole underlying cluster. The other approach you can take is a daemon set. It has its pros and cons, one of them being efficient resource utilization. So a single collector is actually deployed to the node that minimizes resource consumption compared to having multiple sidecars running. Simplified management. It's also easier to manage and deploy a single configuration across the entire cluster. And the last one I have listed is cluster-wide visibility. And what I mean by that is it's really ideal for gathering system-level metrics. If you have that type level of use case, you probably want to deploy a daemon set. The two cons listed here are custom app config. Again, having... You're limited to tailor the data collection based on individual applications. And the second one being there's a single point of failure. And if there's a failure on the daemon set of the OTEL collector, you're not only just impacting one application, but you're impacting all applications that are running as part of that node. So those were the two main reasons that Mail decided to actually go with a sidecar approach instead of a daemon set. Now I'm going to dive into OpenTelemetry's custom app logging. OTEL has a concept of file log receiver which tells... Which tails and parses log files. The file log receiver also has the ability to support includes and excludes. So you can go and actually put log file paths that you want to read and log file paths that you want to exclude. And as part of that, you also have different parsers for logs such as regex, key value pair, and JSON. Coming from a Splunk environment, most of our logs are in key value pair. Splunk has automatic key value pair extractions as part of their tool. We did start off with using key value pair extractions within OTEL, but we quickly learned that it's very limited. The limitations came down to the whole event actually has to be in key value pair format or the extractions don't work at all. So if half your event is in key value pair format and half the event is in just regular text, the key value pairs won't get extracted. What you'll have to do in that use case is actually use the regex parser and extract those fields using that regex parser. A side note on the key value pair feature, we've been working very extensively with Google in developing a key value pair extraction feature. And I think right now it's in private preview and we're using it actually in our production environment. That feature I'll get to a little bit later towards the end of the deck. I have a slide that kind of showcases all the different features that we requested that were really important for us. But I will say that made our developers' lives a lot easier for migrating to GCP because they didn't have to spend a lot of time in reformatting their logs. A few learnings that I want to kind of highlight is one size does not fit all. And what I mean by that is every team logs differently. Every team has different volumes. So do your proper open telemetry performance testing to ensure that you can provide a standard for how applications should consider actually deploying their sidecar with the appropriate CPU and memory limits and requests. The second piece being there is a 64 limit label limit on the Google side. And what that means is if you're using a JSON extractor as part of OTEL, the native extractor will store the data in a payload called label. And that will show up on the GCP console in Logs Explorer. And within the labels field, if you pass 64 labels and you have 64 different extracted fields, you have more than 64, the event actually starts to get dropped. So it's not ideal, but there are use cases that we have at Yahoo where applications are writing actually have more than 64 JSON fields being extracted. And we ran into that issue actually recently. One way to get around that is to use mapped attributes. OTEL has a transformed processor which you can implement and take the body of the event and map it back to an attribute field. And that attribute field, when Google ingests the log, it actually stores it within another payload called JSON payload. And that does not have a 64 label limit. So you will be able to get your full extractions if you use that transform processor. Okay. Moving on to OTEL custom app metrics and application metrics. There are two different Google metrics exporters available. One being the Google Cloud exporter and the second one being Google Managed Prometheus. Yahoo opted to use the Google Managed Prometheus exporter. We've been using that for, I think, about a year now. We are looking into moving over to the native OTLP endpoint. I think John said it's either GIA or it'll be ready soon. The other thing that I would mention is the GMP exporter using the GMP exporter significantly reduced our cost as well from an application metrics standpoint. I think it reduced it by almost 90%. So if you are interested in using GMP, that's one ideal reason to use it is the Google Cloud exporter versus GMP. There's a huge cost saving. A few learnings that I want to highlight. Exponential histograms currently have a rebucketing issue on the Google query side. It's a work in progress. We do have a bug open with Google around that. One side note in terms of what an exponential histogram is, it's a developer's best friend. Pretty much you give a configuration for a bucket count as well as a scale factor. And based off of your metrics data point, OTEL creates the histogram buckets for you. So you don't have to actually build in the histogram buckets within your code. It actually does it for you. So it's really a developer's best friend. I would highly recommend using that when that gets supported. But in the meantime, use static bucketing if you are running exponential histograms in Google. The other piece I want to mention is just time series collisions. Those happen pretty frequently in Google when you're using Google Managed Prometheus. The way to get around that is to ensure that you're using labels and that uniquely identify the time series. So that means it's using project ID, location, cluster, namespace, job, and instance. Make sure you define those attributes and you won't run into those issues with the time series collision. One side note on that, service namespace and service name actually get mapped to job on the Prometheus side. And service instance ID actually gets mapped to an instance. So your service instance ID could be your container name, your host name, whatever you may want to use. And the last learning that I want to highlight is histogram counts and sums are only queryable from PromQL. So if you're using the native Google's cloud monitoring UI, you're not going to see the drop down won't actually contain the histogram sums and counts. You can switch over to PromQL and query them that way instead. That's something that we recently learned and I just wanted to point that out. All right. This is one important topic and I want to thank John for the partnership throughout all of our requests. I know we've been bombarding him and all the Google product managers here with a lot of requests. The engineers that request everything, they're sitting right here if you want to talk to them. But a few key features that I want to just highlight. I mentioned that Google introduced a key value pair extraction feature. It's currently in private preview, but there's a screenshot here as you can see. Once you get that API enabled, you'll be able to actually extract key value pairs based off a pair delimiter. You define the pair delimiter. It can be a space. In my screenshot here, it's a space. So you'll see that there's a log level. I think there's a conf and then I put together a parser field and you can see how it's being extracted. So if you're interested in using that, definitely opt in to use that. Granular snoozing on incidents. This is something that is required from our standpoint when we were going to production. We want to be able to snooze on incidents on per resource label. So what I mean is if there's an issue or there's a change happening in US East or US West, we want to be able to actually snooze those incidents. And that became GA recently. Next one is log deletion. You know, every application sometimes end up having changes and within those changes, PII data starts getting logged or sensitive information gets logged. We need a way of deleting sensitive logs. I believe there's a feature coming out soon that you can actually submit a request to support and support and give them a query and support can run that query for you in the backend and delete all the sensitive logs that might have been there. Currently, what we had to do would actually delete the whole bucket. And it's not ideal, but that's one of the reasons I would highly recommend managing your configurations through Terraform. It makes it very simple in case you do have to do that. Increasing the alert limitation per project. I believe the default alert limit is 500. As I mentioned before, we do have a central observability project and all of our alerts and dashboards are going to get stored there. So we needed to increase that drastically. And I believe we got an increase all the way up to 10,000. It's upon request. I believe if you have any use cases that you need to really increase that alert size, feel free to talk to your account team about it. And the last one is continuous reliability and performance improvements. As I mentioned earlier, we send a ton of logs and we send a ton of metrics. The main important thing for us is to make sure that the backend and the frontend are performant. And so Google's been working with us very frequently and heavily to ensure that their backend and their frontend are more reliable and continue to add different features that help us get to that goal. There's an experimental feature that I want to call out, which is query isolation. So if I'm running a query within a project and I run a really bad query, if I run a really bad query, then, you know, that should not impact another user coming in and running a query, right? So there's a query isolation feature that's in the experimental phase right now. And I think that was it for me.