 Good afternoon, everyone. Thanks for coming. Despite it being a Friday afternoon in Vegas, I'm kind of shocked at how many people have showed up. Thanks for your patience. Thanks for showing up for the conference, for making it a success, and for being here. So the conference is winding up. I hope you had a good time. In this session, Megan and I are going to be talking about... Megan's over there. So Megan and I are going to be talking about a few different architectural approaches that you can adopt to build and deploy RAG applications on Google Cloud. Okay, so that's the topic. So here's the agenda. The first item might be a little bit of a refresher for many of you. We're going to talk very briefly about what RAG is, why it's useful, and how it works. And then we're going to look at a few design decisions that we as architects and developers need to take when we build RAG applications and deploy them on Google Cloud. We're also going to be looking at some design factors that influence some of those architectural choices. And then we're going to look at four different architectural options, prepackaged architectural options, that you could use to get started with deploying RAG applications in Google Cloud. We're going to follow that up with a demo of one of the architecture options by Megan. And we'll have some time at the end, hopefully, for questions, right? About 10 minutes or so. Right? Okay? So, what is RAG? Why is it useful? And how does it work? I think we're all familiar with the abbreviation and what it expands to, retrieval augmented generation. So you retrieve relevant data from data sources that you trust and that you specify. you add that retrieved data to the context of a prompt that you give to an LLM. And then you use that enhanced prompt or augmented prompt to generate responses that are meaningful and that are factual. Right? So that's what RAG is. And if you're someone who likes definitions, then here's one for us. It's a technique. There are a lot of techniques like RAG for grounding. So it's a technique to ground the responses that an AI model generates by using prompts that you enhance or augment with context that you include in the prompt. Right? So that's kind of one way to define RAG. So without RAG, I think even the most optimistic amongst us, optimistic in the AI world amongst us, would acknowledge that there's a fair bit of hallucination hallucination. Right? And the reason models hallucinate, probably, is because they are built to respond, even if, in many cases, the data that they are trained on predates the context of the question that you're asking. Right? In many cases, they are built to please human users. Right? So they will try to respond. And the other thing is that without RAG, the responses might often be not relevant to the domain that your question is about. Because, you know, the data that the model has been trained on might not be specific to your domain. It might be more general. With RAG, you see that the responses are grounded in data that is domain-specific, and that's also more recent. And, for example, if you have, like, a private corpus of data that is known only to you, right, not to the LLM, the LLM is obviously not pre-trained on the data, you can use that data to enhance the responses from the model. So that's RAG. How exactly does RAG work? Now, without RAG, there's a response, there's a prompt that comes in, and then the LLM generates a response. Now, there's a lot of stuff that goes on within the LLM. We're not going to look at that. As far as a user is concerned, and even to some extent as far as a developer is concerned, that's a little bit of a black box without RAG. Now, with RAG, a few different things happen in between when the prompt comes in and the response generation happens. We're going to be looking at that part very briefly now. So let's focus on the box at the very bottom of the green square over there. As architects and developers, the first thing we need to do is identify the sources of data that we're going to use for RAG and then upload that data, the relevant data, okay? And then what happens is the data is prepared, the data is parsed, the data is chunked, divided into small pieces, and then for each of those chunks, something called embeddings is generated, right? Think of embeddings as numerical vector representations, an array of numbers in a multidimensional vector space, right? So each data point that you're uploading has a particular point in that space. And depending on the proximity of that data point with other data points, you either find a similarity or a difference, right? So that's the point of embeddings. Those embeddings are then stored in a vector database, a vector index. So this is the data ingestion part of the RAG flow. On the serving side, when a prompt comes in, the prompt is also converted to an embedding using the same model that was used to generate embeddings of the source data, right? And so those embeddings are in the same multidimensional space. They get the same length of embedding arrays. And those embeddings are also used to then do a semantic search, a similarity search, in the vector database. The retrieved data is then used to augment the context of the prompt, and the augmented prompt is then sent to the LLM, which now generates a response that makes a lot of sense because it has context, right? So this is how RAG works. There are a couple of useful links that I'd like to provide here. The first one is a Google Doc that talks more about RAG and the use cases for RAG. The second one is a very interesting primer on how embeddings work, right? So I recommend that if you're not familiar with embeddings, or if you want to deep dive into the mechanics of how embeddings work, please make a note of that second link. Okay. So we've looked at what RAG is, how it works, why it's useful. Let's look at some of the decisions that we need to take as architects when we want to build RAG in Google Cloud. There are a lot of decisions, right? This slide only shows some of those decisions, the important ones. First, we need to decide what those sources of data are going to be. Where are we going to collect the data from? What format is the data in? What preparation do we need to do to make the data useful for a RAG application? We need to obviously prepare the data. There are a lot of tools that you can use to do the parsing, the chunking. You need to decide what is the size of each chunk, what's the overlap that the chunk should have, and so on. We need to decide what kind of embedding model we're going to use to generate the embeddings, right? So not just for the data, but also for the prompts, right? The same model needs to be used. So it could be text models, multilingual, multimodal, a lot of options over there. Again, decisions that we need to take. Once the embeddings are created for the input data, we need to decide where the embeddings are going to be stored, right? The vector store. I've shown a few options on this slide. There are multiple options, right? So here we're talking about Vertex AI vector search. You could even use Cloud SQL with PG vector enabled and so on, right? They are open source and third-party databases as well. And then we need to decide what model we're going to use for the inferencing, what AI model we're going to use, what LLM we're going to use, Google, third-party, open source. And also, we need to decide how we're going to serve the model, right? Where is the model going to be hosted? Are we going to run it on Vertex AI? Are we going to run it on containers in GKE? Are we going to use Compute Engine VMs? We've seen customers do that. Are we going to even run it on Cloud Run, right? Recently, we announced support for running inference servers directly on Cloud Run. So the serverless option also exists. And then we need to decide what runtime we're going to use for the app itself, the generative AI app, right? Is that again going to use Cloud Run, GKE, Computer Engine, whatever? And then, of course, we need to make the most important decision, which is how the app is going to work, what logic it's going to use to talk to the inference server, right? How is it going to interact with the embedding service, and so on? Now, what complicates things even further is that for each of these decision points that you see here, there are multiple design considerations that we need to take into account, right? First and foremost, we need to decide for a given use case and a given workload, is this vector store ideal or should I choose something else? If I require more configuration control, then do I need to go for an option that is more DIY? Or should I look at maybe something that's easy to set up and manage, right? Those are, again, trade-offs. You need to look at the effort to operate the system, the entire pipeline. There are things like cost, performance, availability, trade-offs among all of these options, right? So this is, the scenario is very complex, right? It's getting even more complex as new tools are being announced every day. In fact, even during Next, over the last couple of days, a lot of new AI related tools and products were announced makes things very complicated and difficult for us as architects. Fortunately, Google provides some opinionated architectural approaches that you can use if you want to get started with RAG on Google Cloud or even if you want to deploy to production. So we're going to look at a few of those architectural options. I've presented these architectural options along the spectrum. At one end of the spectrum, we have options which I'm calling fully managed. You can call them out of the box or almost fully managed or whatever, right? There are a few examples there. We'll look at those a little later. At the other end, we have options which give you a lot of control, right? I'm calling them, you can even call them DIY kind of options, but not all of them are DIY, right? So for example, you could set up a third-party topology completely with third-party AI tools on GKE clusters in Google Cloud, right? So that's an option. In between these two ends, we have options which are partially managed to some extent, right? One example here is Cloud SQL or AlloyDB with PG vector enabled as a vector store, right? Now the database is managed, but you manage the indexes in this case. If you use Vertex AI vector search, now that includes a vector index, right? So there are options there. So over the next few slides, I'm going to present each of these architectural options in brief, and we're going to provide links to the documentation that is available that you can use to read more about those architectures. I'll start with the fully managed option. The option that I picked here is Vertex AI RAG engine, which a lot of you might have seen. It's available only in a few regions as of yesterday. Things are changing very fast, so please do check the documentation link that I provided there. Now, this is an API that you can use to do everything that RAG needs to do, right? From data ingestion to parsing to chunking, indexing, storing it in a vector database, preparing the input prompts, retrieving relevant data, ranking the responses, and then serving the responses. All of that with a single API. Before you start using it, please do check the documentation to make sure that it's available in the region that you're interested in. Now, this is close to the fully managed end of the spectrum. Now, we're swinging back to the other end of the spectrum. This is a mostly DIY kind of an approach, right? I'm calling it DIY. You still use a managed product like GKE, right? So in this case, you're using a GKE cluster, a regional cluster, to run the inference server, to run the front end, and also to run something called the embedding service. Now, let's start at the bottom, right? So data comes in, relevant data that you want to use for RAG comes in from external sources to a cloud storage bucket, and then cloud storage fuse CSI driver in this case is used to pull the data from the cloud storage bucket and then process the data, sharding, chunking, and then the embedding service also creates embeddings of the data that's ingested. We're not showing what model is used to create the embeddings, but again, that's why it's DIY, you can choose the model. And then the embeddings in this case happen to be stored in the cloud SQL database with PG vector enabled. It could be an alloyDB database as well, for example, with PG vector enabled, right? So that's the data ingestion side. And then on the serving side, at the top, the application users send in their requests, and then in this case, we're showing a chat interface as an example of a front end server that's running on a GKE node. And then there's an inference server which also runs on a GKE node which processes the prompts and then uses an LLM to serve the responses back. It's mostly DIY kind of an architecture. There's a link there which you can use to learn more about this architecture, including design considerations and recommendations for how you can configure each of these components. components. Now we're getting into the middle area of the spectrum where it's partly managed. So here, I use Vertex AI for the LLM inference and the serving. I use AlloyDB for the vector database, but the rest of it, the application itself, the runtime is unspecified, right? So it's not fully managed yet. So what happens again here is if you look at the top left, there's the data ingestion flow. Data comes in again through a cloud storage bucket, pub subtopic triggers an event which then is picked up by Cloud Run to launch a processor job. And then in this case, it might use Document AI. You could use other parsers. And then the Cloud Run processor generates embeddings using Vertex AI in this case, maybe embeddings for text API. The embeddings are stored in AlloyDB. So that's the data ingestion side. On the serving side, we have a front end. So the application runtime in this architecture is not specified. So that's up to us. And then there's a backend which takes the inputs and then constructs enhanced prompts, sends the prompts to Vertex AI, and then generates the responses and sends them back. So this is slightly more on the managed side when compared with the previous architecture. Again, there's a link to the reference architecture guide that you can use to learn more about the components in this architecture and the design factors that you should consider. Moving on, now this architecture is even more fully managed. I'm saying that because in the middle of the slide, you see Vertex AI, which is used for all of the AI-related actions. And then you use Cloud Run for hosting the application itself, the front end, the back end, and for the data processing job. Again, there's a link to a reference architecture guide, but let's quickly walk through this diagram here. So data comes in to cloud storage again, so that's kind of common across all of these architectures. Again, PubSub is used, and then a Cloud Run function in this case is triggered, which divides the data into chunks, creates embeddings for each chunk, and then builds and deploys an index. That index is stored in vector search within Vertex AI. So that's the data ingestion flow. And on the serving side, the requests come into a front end, which is a Cloud Run service in this case. The front end passes the request on to the back end, which is again a Cloud Run service. The service takes the input request, converts it to an embedding, and then runs a vector similarity search on the vector search index, which we had previously created. And then the response from the vector search index is used to augment the prompt and then generate a response, which is more contextual to our use case. And then the response is sent back. Again, feel free to use the link there to learn more about this architecture. At this point, I'd like to invite Megan over to do a quick demo of how you can deploy this architecture on Google Cloud. Over to you, Megan. Thank you all. Thanks. Go sit over there. Sounds good. Hi, everyone. My name is Megan. Right. So let's see here. So as Kumar just kind of has gone through, there are so many ways to deploy RAG on Google Cloud. This architecture specifically is what we are about to run through. So Kumar just kind of went through the way that text and embedding sort of flows through this architecture. architecture, let's kind of go back to some of those decision factors and pick a use case for our demo here. So to actually show this whole situation in action, we need a use case. So let's start with the model, actually. So let's say we want to use Gemini 2.0 Flash. In our model evaluation within our org, we had some good success with it. Every model has a knowledge cutoff date. So as Kumar talked about, the value of RAG is in extending a model's training and all of its knowledge with more up-to-date or sort of specialized knowledge. So in this case, the knowledge cutoff date for Gemini 2.0 Flash is August 2024. So we tried to think about some use cases of, okay, what's a specialized field where a lot has happened since August 2024? One example is quantum computing. I don't know about you. I am not an expert in quantum computing. I would love a chatbot that could help explain what the heck some of this new stuff is. You see huge chip developments from Microsoft, Google, and others just in the last few months. It's all very exciting, but Gemini 2.0 doesn't know about any of this. So what you're seeing here is a really cool tool called Gnomic Atlas, and what this can do is take certain hugging face or custom data sets and sort of visualize them in vector space. So in this case, to get some up-to-date article information about some updated quantum computing developments, we look to Wikipedia, and specifically the Wikipedia data set on hugging face. And so what you see there is a search on quantum mechanics, and Gnomic Atlas is cool because it will kind of cluster certain topics for you, and that's what you see in the colors there. So yeah, here's just an example article that we're about to put through the RAG architecture I'm about to show. In this case, it is the Willow processor from Google launched in December of 2024. What we did was we grabbed a very small, small subset of some of those quantum computing articles and downloaded them into plain text, and that's what you're seeing is that article as plain text, ready to be chunked and embedded and up-certed into Vertex AI. Right, so let's walk through what we're about to show live here. So there's a Vertex AI vector search index. That is what all these articles will be sort of stored in, in embeddings form. And then we have three Cloud Run services. We have one for ingestion and two for the chatbot itself. On the left there, you see the Cloud Run function representing the ingestion pipeline. I'll show that in a second. What this does is it watches a cloud storage bucket for new articles coming in or uploaded, and it's triggered through EventArc on a PubSub topic, and it handles all of the chunking and upserting. And then on the right there, you have the chatbot. All of this is Python. You have just a simple streamlet frontend handling the sort of chat UI. And then on the back end, you have a FastAPI server, that's handling the actual RAG flow. It's doing the vector search, augmentation of the prompt, and the inference into Gemini 2.0. We are going to share a link at the end of this to all the code and the infrastructure as code for all of this setup, so you can set this up tonight if you want in your hotel room or on the plane, because that sounds like a good Friday night to me. So with that, let's switch over to the live demo, and let's see this in action. Thank you. So what you see here, so our code is sort of split into two parts. The first is the infrastructure as code, which is all of our Terraform that sets up the sort of substrate of this whole architecture. So for example, if I hop into vertex.tf here, this is the Terraform code for the vertex.ai vector search index. Now, you might be asking, why are we using infrastructure as code to set up a sample? Well, one, for us as Google Cloud, it helps us get these easily packaged architectures to you so that you can easily deploy them. And even if you're just playing around, I highly recommend, if you can, defining all of your stuff and infrastructure as code rather than clicking around the console and then being like, wait, what the heck did I do? How many dimensions is this model? And so big, big fan of this. And so, yeah, you have an index endpoint sitting on top. And then if you go into a file like ingestion, you can see that cloud storage bucket I mentioned where we're going to be uploading those Wikipedia articles and the whole trigger event-driven architecture bit that allows that ingestion function to run. Now, we're not going to sit here and watch Terraform deploy as much as I would love to do that. So what this is is a sped-up version in my Google Cloud project of that Terraform init plan and apply workflow. And so this is a sped-up version of that. It's telling us, okay, these are the resources I'm about to deploy into the project. And if we fast-forward through a little bit here, we can see it's successfully deploying 24 different resources, everything from the PubSub topic to the cloud storage bucket to the vertex AI vector search index. And so once this is deployed, we can take a tour around that Google Cloud project and see, for example, here's our vertex AI vector search index, has our configuration as defined. And then we can see our cloud storage bucket here that is ready for files. I was practicing this morning with a couple of Wikipedia articles. But what I'm about to show is uploading a new one, and we're going to watch it flow through this architecture. So here they all are. Let's pick the Sycamore processor. This is the processor, I believe, that predates the Willow chip. And you can see one file successfully uploaded. What we can do now is this is the cloud run function logs for the ingestion pipeline. And we can see here that just now a cloud event flowed in. It's saying, I'm downloading the Sycamore processor dock. I'm chunking it into 25 different chunks. We are then calling vertex AI to create embeddings using the text embedding 005 model. And we're upserting those embeddings into vertex AI vector search using the JSON-L schema that vector search expects. And then we're doing a little test query just to make sure we're getting some nearest neighbors back. Now remember, when I'm in my chatbot in just a second now, it's going to take my prompt, convert that to embeddings, and attempt to do a vector search in that vector space within vertex AI vector search. So let's hop over into our chatbot. Again, this is just a really simple streamlit front end. And we can ask a question like, what is the Sycamore processor? Question mark. And we can see here that an answer is coming back. Let's go back to the logs to see exactly how this response was generated. Now here is one of our cloud run services representing the back end. And if we scroll down here, you can see that the request came through, what is the Sycamore processor? And it's saying, okay, I'm going to retrieve some context. Vector search returned three matches. And you can see in this case, these were the three matches right here. So the very top of that article was returned, as well as some other information. And then the full prompt goes into Gemini 2.0 Flash. If I expand here. And an answer sort of comes back. So again, what you're seeing here is a sort of simplified view of what that last architecture is designed to achieve, which is a nice balance between managed and sort of configuration, where we do kind of get to pick, you know, certain things like what embeddings model we're using, what programming language are we building our RAG app with. Cloud run has a lot of benefits, as you may well know. Scale to zero. You pay for, you know, what you use. And I'm a fan of Cloud run, so I use it when I can. All right. If we head back into the slides, please. I think, Kumar, if you want to come back up, we have some closing thoughts here. We want to leave a ton of time for y'all's questions, if you have them. And so, yeah, that's a quick demo there. Thank you, Megan. Thank you, Megan. So it's awesome to see things work, even on a Friday afternoon. I've seen a lot of demos kind of, you know, get started, but this was awesome. Thank you. So we'd like to wrap up with questions, but before that, a few, you know, takeaway thoughts from us. We saw a lot of architectural options, a lot of decisions to be taken. One recommendation that we both have today is before you look at tools, before you look at models, before you look at inference options, specify your requirements as granularly as you can, right? Spend the time spent with planning is going to help us down the line. Think of specific goals that you have around performance, cost, operations, and so on. So once we have those requirements defined, we need to assess the trade-offs. There are going to be trade-offs, right? We choose one thing, we're going to give up something, right? So, for example, if I'm going to use a DIY approach, I'm going to get a lot of configuration control, but then it's kind of a pain, right, to manage the whole thing later on. So consider managed options. And speaking of that, my recommendation personally is to start with the fully managed option and start small and be prepared to experiment as you go along so that then you can start fleshing out problems with the end of the spectrum that you started with, right, the managed approach. And then you can start getting a little more DIY depending on which component you want to make DIY, okay? So I just wanted to share those recommendations. There are also a few links that Megan had mentioned earlier. The first link is to the Google Cloud Architecture Center, which has a lot of useful references for AI ML-related workloads, not just for RAG, but generally as well. Please do take a look at that. And then the second link is for the code that Megan demoed, right? It includes the Terraform. I think this link points to the application code, right? Okay, but in the application code GitHub repository, you should see a link to the Terraform infrastructure repository as well, right? So with that, thank you so much for being here. There is still some time for questions. I believe lunch is still available until 1.30, so in case you're hungry, you can maybe have spent some time with us to discuss your thoughts. Feel free to share your expert opinions. A lot of you have used RAG before. We'd also like to learn from that. And before we go, just make sure that you complete the survey in the mobile app. We would appreciate any feedback you can provide. Thank you all. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.