 . Please welcome Cox Communications Director of AI Strategy and Product Management, Samantha McConnell, and Capgemini Principal, Google Cloud Center of Excellence, North America, Jennifer Marchand. . Welcome. Thank you. So today we're going to talk about generative AI, and I want to start with painting a picture. Imagine a world where AI not only understands but anticipates your needs. Welcome to Google Cloud Next 2025 where we'll explore AI in business. I am Jennifer Marchand with Capgemini, someone who has witnessed the transformative power of Gen.AI. I'm excited to share some of these insights with you today. Today we'll delve into the practical lessons on making generative AI truly valuable for the business. We'll hear from Cox Communications on their big wins, stumbles, and lessons learned along the way. I invite you to think about how AI can impact your business as we discuss this. Joining me today is Samantha McConnell. She's a visionary leader in AI strategy and product management at Cox Communications. She's been a business leader in the enterprise conversational AI space since its launch at Cox six years ago. And for the last one and a half years, she has been leading Gen.AI programs, some of which we'll speak about today. Samantha, thank you for being here. Thanks so much for having me. Before we get into some of the sessions, let's set the stage a little bit here around really the session last year. So we were here last year with Cox Communications. We were in a different phase. We were just starting the journey. Last year was really more around exploration. I think this year the pivot is more widespread use at scale. A lot more stakeholders now engaged and Cox is really ready to make the big step functions. So I'm curious for your experience last year and this year. What are you excited to see and how has it changed? One thing that's really stood out to me so far today is just seeing the rate of change accelerating. Last year there was lots of excitement. There was lots to look forward to, but you could just feel tangibly how the rate of innovation is just accelerating along that exponential curve. So that's really stood out to me. Great. And can you talk a little bit about how Cox's engagement and participation in Google Next has changed? It's grown. A much larger group of us here this year, and it really represents the increase in interest, enthusiasm, and just momentum behind our Google implementations at Cox. All right. So before we delve into the specifics, can you talk a little bit about really your process for selecting use cases and then making the decision on which ones to prioritize and move forward on? Yeah. Even before we get to selecting use cases, we have been working through a significant change in mentality from finding ways to accelerate existing processes and solutions with AI to instead finding new ways to solve both old and new challenges with AI-infused solutions. So rather than having a brownfield mentality where we're inserting AI to speed things up, we're taking a greenfield mentality to solving problems. All right. So let's start with partnerships. This is an extremely fast-paced changing area. I'd like to take a moment and understand really what you view as critical success factors in choosing partners in this area and to keep pace with the speed. Yeah, absolutely. It's been really critical for us to find and select partners that are able to innovate quickly and learn quickly with us. You know, the group in this room knows that this is something you've got to wake up every day ready to learn the latest, right, the newest, and keep changing. And it's been really critical for us to find partners who are comfortable on that journey. Another key aspect has been finding partners who will work with us through agile cycles. So given that rate of change I mentioned, year-long waterfall dev cycles just aren't effective, practical. And so being able to approach things with an agile mindset and finding partners who are comfortable there has been really critical. I know from our experience last year we started down a path early in some of the use cases as deterministic. And really as soon as Google launched the agent builder and generative flows we quickly pivoted. And through that change in addition to leaning on Google to learn we also benefited from some of our co-located teams in Nashville, one of our co-delivery centers. So what I personally have found is really just leveraging multiple channels of knowledge sharing to really cover all your bases. So speaking of some of the new areas of Google's product base, can you tell us what you're exploring now? I can. I think maybe even before I get into some of the newest things I can talk about how we selected what we're now building on, if that's helpful. Sure, absolutely. Yep. So one real critical aspect of our approach in the Gen AI space has been to have an innovation budget and to use that bandwidth to run some proofs of concept on promising AI capabilities. This space is so fast moving. It's not mature and solid enough to move straight to the rock solid business case stage of things. And so we've been running proofs of concept to better understand how capabilities work for us and what are reasonable metric assumptions to put into credible business cases. That also gives us the opportunity to have a checkpoint to say how should we be prioritizing the different initiatives, right? Not everyone will rise to the top. And in our first wave of Gen AI innovation, we had three different Gen AI work streams with distinctly different outcomes. So the first was generative AI for knowledge management. We launched both a search-based capability as well as a content generation capability. We went through proof of concept there. It was highly successful. We said this is something we want to scale and move forward with right away. Our second work stream was AI-infused proactive communications. It was successful. We put it in the backlog, but it hasn't risen to the top quite yet in terms of prioritization. And then our third work stream was an immersive Gen AI-backed conversational sales experience. And we had great learnings there, transferable learnings, but we said we're not going to move forward with this one as is. So we had those three work streams that gave us really great examples of different ways to approach learnings from a proof of concept. And now we're building on some of those things with the latest Google capabilities. So for that knowledge management search, initially it was entirely text-based in our data store. And now we're incorporating video into our data store as well, which has been really exciting. Another kind of tip of the spear thing we're working on with Google is partnering with their Delta team on a billing predictor capability, which is allowing us to understand when a customer is going to ask a question about their bill before they even ask it. So rather than fielding those as assisted interactions, we're able to anticipate them and provide helpful context up front. All right. So let's talk a little bit about some of the organizational impacts that this technology has had on the organization. Specifically, the blend of data and AI is becoming much more important and quite honestly has appended well-laid plans, has redefined roles in the organization, and caused a lot of confusion as people figure it out. However, it is essential for building these conversational customer experiences. How has the AI Center of Excellence helped scale the leverage and clarify the use of data in AI across solutions? So our strategy at Cox so far has been to have distinct but very collaborative data and AI organizations. So we have a centralized AI org that you mentioned. We also have a data org. But we kind of approach it as sibling organizations that partner very closely due to the very connected nature of data and AI. What an impact that this has had, though, or AI has had on our data strategy has been a change to multi-cloud. So there's a diverse spectrum of AI capabilities. There's different strengths, different platforms, models, and we have shifted to a multi-cloud data strategy to support that need, right? What additional trends is causing shifts or rethinking of the operational model? Another big factor in our operating model considerations has been just a more nuanced operating model. There's been a need for different types of expertise. Historically, we were able to have a very clear-cut tech lead, business lead, and then the rest of the team built out from there. And at this point, it requires more different types of expertise. So we certainly need the subject matter expertise for a particular use case. We also need the technical expertise of the existing infrastructure. But then we also need AI expertise. And so it's become a more nuanced, more collaborative operating model as opposed to that very rigid one that we were used to historically. Great. Let's shift to actually dig more into the Center of Excellence. So I want to start with really what prompted the formation of the Center of Excellence and what that journey has been to defining and writing the book. Yeah. Well, I think the biggest catalyst was that rapid rate of change. So this group knows, right? You wake up every day, you read the news, there's exciting AI announcements. And we said, if you don't have your eye on that ball constantly, it's going to be an impossible task to stay up to date. So while not every technology has a centralized organization, at Cox, we have centralized our AI expertise. And so that we do have folks who are able to focus on that kind of with that singular focus and share that expertise with the organization. So what was your path to go about defining the journey, defining the roles and goals? Well, so our centralized AI organization is a new org, but it's also kind of a new type of org. So I mentioned, you know, not every technology has its own centralized organization, but we think it's appropriate here. But we have been very purposeful upfront to get really crisp on our values, our goals, our approach, our operating model, and to define those things upfront and to help give that clarity and vision to the rest of the organization. So, you know, the transformative nature of Gen AI really takes me back to agile transformation. That, that to me resonates as really, truly a mindset shift that required retraining the mind. And the most effective organizations that I saw adopt agile, embedded agile coaches in working teams to challenge the working models. I feel like Gen AI is equally as challenging here. Let's talk a little bit about, with this new organization, really how leadership, sponsorship, and support has really paved the way and made this organization successful. So, obviously there is growing pressure to act now or get left behind. What have you seen with your executive team in their embrace of Gen AI and specifically, again, how that has changed between last year and this year? Yeah, absolutely. Between last year and this year, I have just seen so much growth and enthusiasm, excitement, support. And I really think a lot of that has come from having some of these Gen AI implementations live and being able to see the results of those things in production. And so, it's gone from, you know, an exciting promise that also probably had some healthy, understandable skepticism around it to now it's a part of how we do business every day. And it feels real and tangible and something we can build on. What are some of the challenges and best practices that you found to really educate the executive team as they've learned to embrace Gen AI? Yeah, I think one thing that we've really learned is that our executive team and even, you know, the broader organization, right, it takes a little bit more time to talk about some of these ideas that it might for other subjects. Because there is so much to learn from a foundational standpoint to make the details and the particulars of that conversation make sense, right, to be grounded. So, that's something we've focused on. And the joke is you can't have a 30-minute meeting or maybe even a 60-minute meeting about this. It's sometimes a multi-hour endeavor because there is so much groundwork to lay to then introduce whatever the particular topic is. I feel that. And I know others feel that, too. That is great. Can you talk about some of the challenges that the executives have faced as they've learned Gen AI and really how to support the organization and teams through the journey? I think that, for one, I'll give them credit to say they've invested so much time and energy in coming along. But I think for our executives and for all of our audiences, sometimes it can be kind of where to start and how to start organizing around this. And so, one thing we've been organizing around, we call our four R's. So, those are risk, return, readiness, and reimagine. Risk comes first. You know, there is so much potential and excitement and value with AI. But also, you know, one really negative risk incident can undo all that, right? And so, risk has to come first. That's something that we're always protecting against. Then, return. So, our AI implementations need to have tangible business benefit. I mentioned we do the innovation, we do the proof of concept up front. But then, we're looking to tangible business benefit from that point forward. We have readiness, which speaks both to our team, our centralized AI org readiness, as well as the broader organizational readiness. And then, we have reimagine, which goes back to what I mentioned around that greenfield mentality. So, rather than plugging AI into existing processes and just kind of accelerating them, instead, we're saying, is there a better way to solve this problem? So, that wouldn't have been an option when we solved it the first time. So, I think having those four R's has helped a lot of folks kind of organize around our approach. I love the idea of the four R's. And it strikes, to me, a close resemblance with Google themselves, really, with AI at the core. So, this reimagine, specifically, approaching every problem with the greenfield, I think Cox is truly embracing an AI first mentality. One thing we did skip over as well is the software development lifecycle. And really, the fact that Gen AI involves a comprehensive tech stack. So, while the experience part of Gen AI is a top layer and you can move very fast, it doesn't negate the need for all of the complexity of the integration and plumbing underneath. I know last year, with one of our Gen AI pilots, we specifically chose a very difficult use case on purpose to test the limits of the platform. But what we ended up doing was spending eight months building the integration and the plumbing for the tail end of the experience. So, the conversational sales, really adding items to the cart, checking out. After we were all said and done, the learnings from the pilot was that the beginning part of the process was the most valuable. It was really about educating customers, making a recommendation on a right sized plan for them. So, I think the key takeaway is really striking that balance of iterations as you build this out and choosing what the goal for each iteration is. Is it experience? Is it proving the data or the integration? Or is it really educating the stakeholders around kind of the art of the possible in the context of, in this case, Cox Communications? Yeah, I think you hit the nail on the head with this idea that even AI solutions have a lot of components that aren't AI. And they all need to be considered and accounted for and there's work associated with them. And I think the opposite perspective too is starting to be interesting around even solutions that aren't AI solutions are beginning to have AI components almost inherently. And so, rather than there being this kind of dichotomy between AI or not, it is increasingly, it's a mashup of both, right? All right. I think one of the other large shifts that I'm seeing really across, at Cox, but really across customers is with the investment of Google Cloud and the other hyper scalers specifically in this space. And, you know, conversational AI, knowledge, work, customer experience being ripe use cases for this technology. And, you know, there's a change in the ecosystem and the landscape where traditionally there were a lot of point solutions, a lot of times selected by business themselves. And then IT left to figure out how to integrate and make it all work together. And with the hyper scalers leading in this space now, it's really shifting to technology-led, riding on the coattails of players like Google Cloud to streamline the ecosystem as well as rationalize the portfolio. What are you seeing in the space? Yeah. I agree with you. So, I think that a proliferation of point solutions isn't the answer. I also don't think it's one platform or one model to rule them all, right? It's a purposeful ecosystem. This crowd knows, right, AI is an umbrella term for a number of different capabilities. And I think it warrants that purposeful ecosystem. But, you know, certainly partnering with Google, we benefit from the rate of change of having partners with the resources to innovate at that rapid rate. So, in some of our generative AI work streams, we'd be working and we'd say, you know, maybe a potential capability gap here. And then, you know, even within days, right, we'd see that was a roadmap item, right? And the gap's been solved and it's a new capability. And so, benefiting from that rate of change has been really beneficial. What recommendations would you have for other organizations that are adapting to this changing landscape? I think a key aspect of our approach is design with as much flexibility as possible. We're all, you know, it's evolving. I think even the best of us, right, are going to be surprised by how some things work out. And so, as much as possible, designing knowing that the model will change, right? It'll be Gemini 2.5 instead of 2 or 1, you know. And so, keeping that in mind, knowing that things are going to keep evolving. To that point, can you talk a little bit as well about really how you have evolved your thinking on measuring success, measuring value realization in this technology? Yeah. I think another point I can bring up here is this shift from a project to a product mentality. So, and it goes along with that idea of avoiding point solutions, but instead developing extensible enterprise capabilities where we can develop that capability and then, as much as possible, make minor adjustments, but then deploy it across different user groups, different parts of the business, and really maximize the value we're seeing, minimizing the risk and cost of those implementations. And we're able to tie it to very tangible business benefits. So, I can mention the example of our knowledge management generative AI search. In that particular capability in our initial user group implementation, thousands of agents across four different countries, we saw agents' time spent searching for information in our knowledge base decrease by about half. It's a very tangible business benefit. In terms of content creation, the SLAs for those authors to create the content that's in our knowledge management system, went from weeks down to hours, sometimes less than a day in order to create that content. So, having the tangible business benefit and then maximizing it through those extensible enterprise capabilities. I think, to that point, too, getting it out there early for early learnings continuous feedback is proof, too. I mean, for me, right, even the personal productivity tools, we're only a year and a half in, but it's hard to imagine life before those. So, yep. Any specific feedback from your interaction with business as well and the response from users? So, the general feedback we've gotten has been so positive. It has just been so encouraging. I think the prevalent sentiment as we launched our first generative AI tool was, why didn't this exist ten years ago? But it's been wonderful to see such a positive reception. All right. So, I think we will maybe talk about a few key takeaways here and then we'll give some time to hear some questions from our audience. So, I'll let you maybe start with some key points you want to leave with the audience. Yeah. I think product, not project mentality, is a really important one. Developing those extensible enterprise capabilities. And I think also tying them to tangible business value is really critical. Great. I think for me, I think there is as much complexity of obviously the stakeholder management here. Like you said, everyone is learning in this space. And at the same time, it's a race to be the first to launch. So, there is this balance of pressure but taking the time to really educate and manage stakeholders. Yeah. And one thing we didn't touch on but I think is so important is organizational readiness and how upskilling and knowledge isn't just critical for the folks who are full-time practitioners, but also the folks who are interacting with these solutions. Because if you don't know that temperature and top K are things you need to worry about, right, you wouldn't know to seek out a partnership on that or collaboration on that. And so, I think this idea of readiness both within practitioners and then also within the many stakeholders we partner with. That's a great point. I think given that this is not an industrialized project, product, et cetera, this you, we don't know what we don't know and we don't know what we know as well. But there is a lot of uncertainty and you just have to be adaptable and flexible. Thank you. Thank you. Thank you.