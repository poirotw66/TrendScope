 Deepak Kalakuri Group Product Manager Group Product Manager Group Product Manager Group Product Manager Hello, everyone. Welcome to the recap session. This is Deepak Kalakuri, Group Product Manager at Google Cloud. I'm excited to talk to you about troubleshooting issues with Gemini Cloud Assist investigations. Let's get going. There are a few issues that stop customers from getting most out of their applications today in the cloud. It's hard to acquire expertise when you're troubleshooting issues. The diversity of the cloud stack and the resources that customers deploy their applications on is ever growing. The applications themselves are complex, coming from a variety of possible libraries and sources and a different choice of architectures they could be running on at the infrastructure level. And to add to that, there are new feature launches both in the cloud and the resources and your application as well that keeps adding to the complexity. The second big issue that we see is that users and their Sari teams have a hard time analyzing the excessive data that comes into from a troubleshooting standpoint. When an alert goes off, they do have to typically go through a collaboration Slack thread channel to figure out which data sources they have access to and where do they pull data from across a variety of metrics and logs or past runbooks to related tickets and try to piece together a story of what could have happened and how they could troubleshoot the issue. Now, this usually takes about a couple of hours is what I learned from my customers. What we want to change with today is a fundamentally rethink of how this process should work. We believe, or more, I believe, the troubleshooting cloud environment should not need deep domain expertise or unnecessary risk to proprietary data. With Cloud Assist investigations, that's exactly what we bring to you. We give you a higher uptime for your applications because you're able to troubleshoot issues in under a minute or so. For most of your issues, it drives developer velocity and keeps your data safe. Think about your users today who are probably taking log error messages and putting that in ChatGPT or some custom built AI which exposes your customer data or your company proprietary data to the public Gen AI tools. With Cloud Assist investigations, you don't have to worry about that. Let me walk you through how that would work. Here's a quick overview of the core engine that's powering the investigations. There are three parts to it. There is the discovery of the application topology and the resources that are related to the issue at hand. GCA goes through the specific issue that you pointed out and identifies the resources that could be impacting it, collects logs, events, configuration changes, metrics, and errors across all these related resources, groups them together to do the second thing, which is to understand and identify what observations it is able to pull that are relevant to the issue at hand. And finally, based on these observations, the AI synthesizes hypotheses as to what could have happened and how you could troubleshoot the issue faster. And it gives you about one to three hypotheses, the top ones that are relevant to your issue. There's a variety of data sources that we pull from, and we do that very securely. We act on behalf of you, the end user, work with your permission set, and we don't copy the data outside of your project. All of your investigations and the hypotheses stay in your project. And there's a lot of data sources we pull from, including grounding the issue in relevant public documentation and knowledge sources that GCA has access to. Let me walk you through a quick example of how this could be done for an application. In this case, we are going to CloudHub, looking at the alert and the relevant log messages. Investigations automatically copied everything into the investigations form for your specific log error. And what it's doing is it's going through all the relevant resources, and in this case, identified from the logs a deployment failure that went from 3.4 to 3.5. It formed a relevant hypothesis as well that you could have made a recent code change. And in this case, as you can see, you could go to the error message yourself in the log and look at the actual committed URL from the GitHub in your metadata that's available to you as a user. And in this case, once you go to the GitHub, you're able to see the difference, and there is indeed a memory leak that was pushed in the recent code change. That was a quick 60-second take on how you could investigate a complex issue from CloudHub or log error messages. Now, we believe that fundamentally, again, investigations or troubleshooting should fit into your current workflows. And let me walk you through a couple of quick examples on how you can troubleshoot straight from your GKE console or your Dataproc service that you use today. In this case, the user is on the GKE workloads page. And as you can see, there's an error. And they're able to click investigate. And usually in about a minute or two, the investigation completes. It identified level and relevant observations, as you can see on the screen, for why the pod crash is happening in this case. And it also identified that there's probably a node affinity and anti-affinity conflict that is preventing the pod from coming up. And that's a fast way to troubleshoot for the user without having to figure out where to go next. Here's another example. Dataproc has integrated investigation straight into their failed batch jobs, in this case. And the user is able to get to the result for the troubleshooting process very quickly.