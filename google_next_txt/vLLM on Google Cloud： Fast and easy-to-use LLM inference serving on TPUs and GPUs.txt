 Welcome to the session for VLM on Google. I'm Brittany Rockwell. I'm a product manager here at Google working on VLM on TPU. I'm thrilled to be joined by Wizzit Kwon from GDM or Google DeepMind, also the founder of VLM, and Robert Shaw, top contributor of VLM and engineering director at Red Hat now, formerly Neural Magic. So I'll pass it off to Rom, who will kick off our presentation. All right. So we have three topics today. First is why VLM? What problem is it solving and why should you care about it? Then we'll talk about the new announcement that was made this week, which is VLM support for TPUs, which is one of the most exciting initiatives that we have inside of the project. And then we'll cover best of VLM, where we'll talk about some of the features that we support and how we make the models go fast. So the world really changed in November of 2022. We have a conference like this, where AI is the only thing or one of the only things that gets talked about. It's hard to have a conversation in IT today that doesn't include a conversation about AI. And Gen AI, you know, really was what woke the world up to the power of deep learning. You know, I've been working for a startup called Neural Magic prior. We had been working on making models run fast on CPUs. We've been working with BERTs and YOLOs and some of the older models that definitely have a lot of use and a lot of adoption for various use cases. But we're never really a board level, you know, CEO level type of initiative. And this fundamentally changed with the advent of generative AI. But at the time, open source models, especially when generative AI started with, you know, in November of 2022, open models were much, much worse than proprietary models. And this is actually a slide from the Neural Magic board meeting. At the time, we were a company that was selling infrastructure to support running open source models. And so this was a huge problem for us that we had, you know, a world where the best open models were either, you know, weirdly licensed with things like the original llama that only had a research license or were much, much worse. So models like OPT was the best Apache model at the time. And so I remember making this slide for our board meeting. And, you know, I think this was late February, early March of 2023. And it had this fundamental crisis of, you know, are open models going to be something that people use? Like, will they ever be able to catch up to where the, you know, the closed source API market was? And what we've seen is that over the course of the past two years is just tremendous progress and an absolute explosion of capability of open models. You know, obviously, we've seen the llama series, you know, obviously culminating with llama four this past week. But beyond just meta, we've seen a explosion of the number of companies, even Google with Gemma, Databricks, Mistral, Nvidia, Microsoft, DeepSeq, Alibaba, all these companies investing in open source models and continuously pushing the pace, making the models better and better and better, evolving the architectures to improve the quality and performance of open models. And we live in a world today where we have state-of-the-art models that are freely available to download and use and adapt for enterprise use cases. And these open source models have lots of advantages, whether it's cost, running it on your own managed infrastructure can enable you to, you know, especially at high utilization, to, you know, minimize the cost of deploying, customization by fine-tuning for specific use cases, you know, control over the model life cycle and control over the amount of resources that you're putting behind the model, and of course, you know, security by running the models in your own secure VPC. And so we really believe that open source models have a huge role in the enterprise AI landscape, and this trend has been accelerating as the capability of the models have gotten better and better and better. It's really showing no signs of slowing down. But with the introduction of all these open source models, if you're a company that's seeking to leverage these models to, for your enterprise use cases, you need a way to run them. You have to build infrastructure to support the deployment of these models. And this is really where VLLM comes in. VLLM is that piece of software that you can rely on to run all of these diverse models with all of these complicated architectures that are improving, you know, year, week on week, not year on year, week on week, and really is that piece of software that you can rely on and build your platform on top of and know that you're going to get the best performance on the day of the release with all the features that you need to build a performant inference service. And really, our goal with VLLM is to build the fastest and easiest to use open source LLM inference and serving engine. And over the course of the past, you know, almost two years now, a little less than two years, VLLM has evolved from a research project, you know, started at UC Berkeley with the original page detention algorithm into the real industry standard for how to deploy all of this innovation that we've seen over the course of the past two years. And I always think back to, you know, watching the project and, you know, how it became this behemoth that it is. And, you know, page detention was obviously an amazing innovation that fundamentally solved the core problem of building a continuous batching inference server. But really importantly, VLLM has evolved much beyond a research project. It was set up with an open Apache 2 license that welcomed all of the different participants in the ecosystem, whether it's model providers, companies like ours at Neural Magic, hardware providers, to have a single place where they could invest to make sure that their component of the ecosystem was well supported. And this has really created a true community flywheel around the project. You know, one of the things we always think about at Red Hat is working on industry standard technologies and how these help to drive innovation, standardization, and allow various different ecosystem participants to make sure that their piece is well supported in a common stack. This is what we've done around Linux. This is what we've done around Kubernetes. And we really believe that VLLM is this for enterprise inference serving. So whether it's model creators, you know, with key models like Lama, you know, launching on VLLM on day zero, you know, all of these model providers are actively engaged in the project, are contributing back code to make sure that their models run well on day zero. And this is an incredibly important aspect of the VLLM community because of the pace of innovation that we're seeing, where, you know, almost every week we have a new model coming out that's pushing the boundaries forward. You know, when we were working at Neural Magic prior to joining in VLLM, we used to have a proprietary inference engine. And every time a new model came out, it was like the worst day because we had to spend all this time to, you know, port the newest model into our stack. And so it's a huge advantage that VLLM has the position that it does because everyone sees it as the place for how to launch their models. And so the community makes sure that the architectures are well supported, which is continuing to drive improvements in the project. Next is hardware diversity. You know, we'll obviously talk in a second about the TPU backend, but the existence of a common inference engine that the ecosystem is standardizing on provides a very clear and obvious place for new accelerators entering the market to invest behind. And VLLM has set itself up in such a way, and we'll talk through, you know, how we've integrated the TPU backend that allows various hardware accelerators to enter the project. And again, provides that standardized layer for folks to invest behind. And you can see it on the right in terms of the contribution trajectory. You know, in orange, that's the UC Berkeley team. In blue, that's the Red Hat team. And beyond this, we have a huge array of red, which is community. As various users, you know, fix bugs related to their specific problem, implement XYZ feature that you would never even think of as a project maintainer that solves a particular problem. We have this advantage in VLLM, and it's pushing the project forward at a very rapid pace. And so we really see VLLM as the Linux of Gen.ai inference. That single place to run all the innovation in the model space on all the innovation in the accelerator space. And this is ultimately the vision that we're trying to drive. And so we'll talk about this in a second, but I think importantly for this cross-platform inference engine, we've really tried to design the system for a consistent user experience across accelerators. And we do this through PyTorch. So there's lots and lots of key features that VLLM is implementing, whether it's scheduling algorithms, open AI API, you know, quantization, you know, Prometheus metrics. All of these things are shared across all the different hardware backends. And we have PyTorch, which is a single implementation of all the models that allows all the different models to run on the accelerators. And this provides us with one single model execution integration point that we can fit in new accelerators into the market. And so Brittany will talk through now a little bit of the journey for VLLM on TPU. Thanks, Rob. Okay, so the three key points that you can take away from the session about TPU is that TPUs are now available in VLLM and we're super excited about it. The second point is they're very performant and it's really easy to use, easier than any usage of TPU in the past that, you know, that we've probably ever delivered. So we're really, really excited about this because we think that this is a way for us to bring TPUs to the masses. And, you know, what better way to do that than with partnering directly with the open source community? The third is there's a lot still to look forward to. And so Ironoid, you saw the big announcement. You know, Ironwood will be ready from day one in VLLM. This is our commitment to you all. And so we're super excited about that. But there's a lot to be proud of today with TPU and this announcement, but there's a lot of work ahead of us as well. Okay, so if you are using VLLM today, why would you want to use TPUs? So first and foremost, perf per dollar is very competitive, extremely competitive. We've been working really, really hard to bring it up to speed, literally, in the last two months. And we're really happy about where we've landed, although that will continue to improve. The second point is improving obtainability. And so this doesn't just mean, like, adding more capacity to whatever pool you can draw from. But more importantly, we're releasing features like custom compute classes in GKE that allow you to essentially launch a VLLM workload that utilizes both GPUs and TPUs simultaneously because of the hardware-agnostic Prometheus metrics that Rob had just discussed. The third, arguably most important thing, is near-zero developer friction. So Wizzuk had added TPU to the back-end, or as a, as a, I think the first back-end in VLLM actually, sometime last year. And we really ran with that. That was like a, such a great proof point for us. But really at the core of that, and I think what Wizzuk has really done from the start with VLLM is that user experience was so, so, so important, right? Like all of these really competitive, hyper-performant features, making that really accessible to, to people that, that just want to get started really quickly. And so we really wanted to keep that at the core of TPU as well. And so with these three different features, the fourth is, that you get is minimal workload disruption. So fungibility within your workloads using GKE, you retain the same user interface, the same developer experience, the same telemetry that you use in your production workloads. That makes it really easy to basically port it across different back-ends, including TPU. So we're really excited about it. If we dive into developer experience, if you're familiar with VLLM, if you're familiar with Vertex, for instance, you might already be using the existing VLLM container image in Vertex. So it would just be a question of using the TPU variant of this from Vertex, subbing this in instead, and then, you know, defining accelerator type, machine type, and accelerator count to fit whatever, you know, same friction that you would have moving from one GPU generation to another, you would have the same with TPU. So we think that this is, this is great. If you've ever tried TPUs in the past, you might have had a very different experience. So we're really happy about this. The second point is kind of retaining this, like, swap out the image for a TPU specific image is the same in GKE, GCE, and data flow. And so if you are already using the Docker Hub images, like the pre-baked images that VLLM provides, you can now also get the nightlies on TPU, and we're going to cut our first hardened release imminently. And so with all this, it's very, very easy to sort of move back and forth, but you're retaining the same payload, the same API signature, the same telemetry to use in auto-scaling, for instance, and then the same load. And then you can extend your load balancer as well. So these are big wins. Super easy. That's sort of at the core of what we do. So because of this, like, hardware agnostic approach to designing or integrating Prometheus metrics within VLLM itself, this opens up a ton of ideas from the orchestration perspective. Like, if you're familiar with horizontal pod autoscaler, which uses Prometheus metrics to scale workloads up and down based on some, you know, it could be workload-specific or workload-centric Prometheus metrics, like request wait time or cache utilization. And so those things are hardware agnostic. And so we've come out with something called custom compute classes that allows you to define a set of preferences. And so you might, for instance, define TPU, like you might have a TPU reservation. So you define that as your first preference, then you would deploy that workload. And then based on demand, triggered by or being monitored by Prometheus metrics, you could then scale up. And then the moment you would hit, let's say, a stock out, you could then fall back to GPU and spot because, let's say, the price of spot is really competitive and you would rather, and you have a fault-tolerant workload. You would rather fall back to spot. And then if that doesn't work out for stock out as well, you can try on demand because those are two different capacity pools. And the cool part about all this, too, is that there's a reconciliation step. And so once it will continue to try spot, let's say, so let's say all nodes are running and humming and you're running on both GPUs and TPUs, your third preference will continue to try and spin up spot if spot wasn't successful. And then it will kind of start to spin down as maybe your volume, your application's volume starts to go down as well. So this is super cool. We're really excited about it. And it's coming soon. So the team of engineers that we've been kind of working around, although it started in v0, we're excited to land the most performant variant of TPU in VLM in v1. The team has almost 3x performance on single chip and I think 1.7 at this point on multi-chip. These are just the LAMA 3 models that we have as examples, but we have a team of, like, you can name it from the lowest part of the stack, like compiler level engineers, really, really talented engineers that really don't know much about VLM, which is why we worked so closely with the Red Hat team and was to kind of guide our development progress because, you know, we were new to this too. And so we couldn't have done it without them. The other new features that we also landed with the help of our partners was multimodal model support, which is becoming really important when we see, like, GEMMA 3 models that are being announced and LAMA 4 models that are being announced with multimodality as, like, a core feature. Structured decoding, multi-LORA, which is really important for GKE inference gateway, then optimized sampling because that's a tricky problem. And then all of this is in the V1 integration. And the folks at Neural Magic also helped us land hardware accelerated quantization. So all of this is available across all modern TPUs. So V5E, Trillium, V4 and V5P. A lot of this work happened on Trillium. But whatever you have available, you should be able to use VLM up from V4 and up. So another really cool thing about Trillium that we realized with VLM is it works really, really well on compute-bound workloads. And so compute-bound workloads tend to be, as far as large language models go, where pre-fill heavy dominates. And so when your input sequence is really, really, really long, or really long relative to your output tokens, you're going to perform- Trillium is going to really shine in these use cases. So up to 2.6x times better than on 50-50. And so what that means concretely is that use cases like summarization, metadata extraction, code completion, and agentic workloads will really shine with Trillium. And then document generation, creative content generation, you know, less. So we would recommend starting, obviously, on the right side of the graph if you're looking to get the best performance. And then, of course, Ironwood was announced. You've probably heard it multiple times over the last couple days. Like I said, this will be optimized fully for day one in VLM. At some point in October, we'll be starting to look for private preview participants. And so we welcome the community to come on board and help us. Ironwood is this really exciting chip. So 5x more peak performance per chip, 6x more HPM capacity per chip, and 2x more power efficient than Trillium. So, yeah. Really, really impressive chip that we're really excited about. Our focus area is looking ahead in 2025, not only with Ironwood, but there's all this. So we really optimize single chip and multi chip. The next step is really just pushing the boundaries, basically, on some of the more SODA models like DeepSeq. So expert parallelism becomes really important. Context parallelism is super important for extremely long sequences. So those are top of mind for us. Dynamo persistent cache will make startups really fast on TPU. Cascading attention, too, for certain use cases that can benefit from it. Host and kernel auto-tuning is something we want to add so that it's just really, really easy to get started on TPU in the most performant way possible. Speculative decoding, if you're already using models that are spec decoders, we want these to also run on TPU really efficiently. And then Ironwood, as I said, and then we want to further optimize multimodality because of how important it is in the industry now. And then KV cache offloading for kind of pushing the boundaries on pre-fill heavy workloads. If there's something that you don't see here, we definitely want to hear from you. We welcome contributions from the community. So yeah, thanks so much. I'll now hand it over to, sorry, I'll hand it over to Wuzak now. Thank you. Hi, everyone. I'm Wuzak. I'm a co-lead of the VLM open source project and also a research scientist at Google DeepMind. Today, I'd like to briefly talk about some additional values that you can get from VLM besides the broad model support and broad hardware support in either GPU or TPU. So if you don't have experience with VLM, then you may wonder, like, why you need something like inference engine at all, right? Like, why is it not enough to just write your model code in PyTorch or JAX or whatever and just run it on your GPU or TPU? It seems working. Like, how is VLM different from this? I think this is a very fair question, and I have two answers to it. Actually, VLM has more values than two, but today I'd like to highlight two things. First is inference optimizations. So there are many inference optimization techniques invented by industry and academia that can accelerate your model without losing your model quality. So if you don't leverage them, then you're leaving a lot of performance and efficiency on the table. And VLM basically efficiently implements this and easily integrates this together so that you can leverage it for your models. Okay. Second is distributed inference. As you know, like frontier models are getting larger. To serve these large models efficiently, VLM supports various parallelization schemes and a lot of optimizations. Okay. Let's first dive into the inference optimizations. Oh. So it turns out that there are many, like, inference optimizations. There are many inference dimensions you can actually optimize in LLM inference. First, we can optimize the model itself by, for example, using quantization. Also, we can optimize the prompt processing of the inference, particularly by something like previous caching. Finally, we can optimize the token generation pace of inference by spec-lip decoding. Let's look through these individual items quickly. So quantization is a technique to use low-bit precisions to represent the model. You know, like model parameters or model weights are basically a bunch of floating point values. And instead of using 16-bit data types for these values, you can use 8 or even 4 bits to save the storage or compute for the model. Specifically for the LLMs, there are three axes you can quantize. First is model weights. This typically takes the largest amount of memory, so you quantize it first. And the common practice here is to quantize the model weights from Bflow 16 to FP8 or INT8 without losing model accuracy, model quality. And by doing so, you can have the number of chips you need to deploy the model, like, say, from 8 chips to 4 chips. Second is KV cache quantization. The KV cache sometimes takes a huge amount of memory, like, sometimes even larger than the model weights, especially in the lone contact scenarios. And similarly to the model weights, you can quantize it into, like, 8 bits, like FP8 or INT8. Okay, last is activation, which is the intermediate tensors produced during the execution of the model. So the activation tensors are typically small in size. But interestingly, if you quantize them, you can leverage the specialized low-bit tensor core instructions, tensor core the hardware units, to further accelerate the compute. And also, like, depending on your parallelization scheme, you can also gain some benefits from the communication by quantizing the activations. In VLM, we provide the LLM compressor library, which is actually mainly developed by the Red Hat team, to quantize your model weights with your own data sets. Then you can efficiently deploy the pre-quantized models with VLM. We support a wide variety of quantization schemes, like FP8, INT8, GPT-Q, AWQ, and more. And also, we have a bunch of optimized kernels for these quantized operations, particularly the quantized matrix multiplications. Okay, the next optimization is automatic prefix caching. This is similar to the prompt caching in Gemini API, but we do it automatically. Basically, the idea is we keep the KV cache of the previous recast, and if the same or similar recast comes next, then we reuse this KV cache for that recast to skip some of the computation. And there are two major use cases. First is the share system prompt. The system prompt is a text at the beginning of the context where you can condition the model to behave in a certain manner, like be a helpful assistant. For example, Llama 4, which released recently, also has a similar system prompt of hundreds of tokens. With previous caching, you don't need to recompute this system prompt for every recast. You can compute it once and reuse it over and over again for every other recast. For every recast. Another important use case is multi-turn conversation. So with previous caching, you don't need to recompute the past conversation history over and over again. The KV cache of the past history is kept in memory, and this enables much faster response because we don't recompute all of the conversation. Okay, the last optimization technique is speculate decoding, where you use a small model to generate, say, next four tokens as a draft. And then uses the original large model, the target model, to verify the draft and accept them if it matches the original distribution of the next tokens, or reject them if they not. This is faster than the regular token generation with the large model because the model can process these draft tokens, the four tokens or five tokens in parallel, instead of, like, generating these tokens one by one. Okay, so I think what's special about VLM is, of course, VLM supports all of these well-known optimization techniques. You can easily benefit from it. But more importantly, we can combine them together to get the best performance and efficiency. For example, here, in this example, the model is quantized, the KV cache is also quantized, use prefix caching for the system prompts, and, like, spec decoding for generation, and more. And actually, besides what is covered today in this talk, there are more optimizations coming, like CPU, KV cache, or, like, jump decoding for structured outputs, and they will also be combined together with the existing ones to further optimize the performance. So please stay tuned. Okay, let's switch gear to the distributed inference. So we believe LLM inference is going distributed. While there are still many good models runnable on a single GPU or TPU, and they will even in the future, but we believe the most popular models, the most strongest models in the future will be run on single-host multi-device, or even multi-host multi-device. Okay, so why do we need distributed inference in the first place? The first reason is that, obviously, the frontier models, the strongest models, are too big to store it in a single-device memory. Take DeepSeq V3 as an example. The model has 671 billion parameters, which means even with FP8 quantization, you need 16 H100s to serve the model. Basically, you need multi-host inference. The second reason is that you may also want to use more chips for higher aggregate flops and memory bandwidth to eventually reduce the latency and increase throughput. Yeah, VLM supports various model sharding schemes for distributed inference. Actually, most of the, yeah, we support most of the well-known sharding schemes, including tensor parallelism, pipeline parallelism, expert parallelism, data parallelism, and also desegregated serving experimentally. And the key idea here is that there's no one-size-fits-all solution. Each sharding scheme has pros and cons, and the optimal setting actually depends a lot on the model architecture, your hardware and cluster spec, and your workload characteristics. So you need to understand these parallelisms and pick the best one for your usage. What VLM does is it provides various options to you. You can easily explore them and efficiently deploy any parallelism you find the best. Okay, the first one is tensor parallelism, which partition the model weights along the model's hidden dimension. For transformer models, like, this means you are doing all reduce for every layer output. Tensor parallelism is most commonly used at small scales, like, typically up to eight devices. However, it doesn't scale very well beyond eight devices. It's partially because the all-use communication is too expensive, or sometimes the model architecture doesn't allow splitting more than eight pieces. The second is pipeline parallelism, which distributes the model's layers into different devices and execute them in a pipeline fashion. A key advantage of pipeline parallelism is that you can avoid the expensive all-reduced communication in the tensor parallelism, which basically requires, like, the communication between all devices. You instead do point-to-point communication between two devices each. However, the key limitation of pipeline parallelism is that the performance is often bottlenecked by the load imbalance between different pipeline stages. For example, if one pipeline stage is processing a long input prompt while the other stage is processing a short input prompt, then the stage with the later stage, the stage with the short prompt, waits idle until the other stage finishes. This may lead to low device utilization. Okay, the third is expert parallelism, where we place different experts to different devices, for the MOE model particularly. And this typically leads to lower communication overheads than the all-reduced in tensor parallelism, because you can use, I mean, it actually depends, but you can often use the all-to-all communication, which is more lightweight than all-reduced. But it also has a load imbalance problem. Basically, you know, the number of tokens routed to each expert is highly dynamic, and it can be also very skewed depending on your model and inputs. Okay, the fourth is data parallelism, where you split the inputs instead of the model weights, and you replicate the model weights to different devices and routes different inputs to different devices, different replicas of the models. And here, similarly to the expert parallelism, data parallelism leads to low communication overheads typically, but it has a load imbalance problem because you route different inputs to different devices. So if the different inputs have different lengths, for example, then you may have the load imbalance. But more importantly, you have a higher memory pressure with data parallelism because the weights are replicated instead of sharded, and each shard goes to store it in the different devices. Okay, finally, desegregated serving, which we experimentally support with the third-party integrations. This is an idea to partition the inference in time, in a sense. Basically, you have two sets of devices. One set of devices are dedicated for prompt processing of the inference. Basically, the recache is first routed to this set of devices, and after the prompt is processed and KVCache is generated, then you send the KVCache to the other set of devices which are dedicated for token generation. This way, you can achieve some separation of concern between prompt processing and token generation, which is useful when you have a very tight latency bond. However, this, as you can see, involves some overheads of transferring the KVCache between these devices, and also, this may lead to low cluster utilization overall when one of the two sides are much easier than the other. Okay, again, yeah, VLM supports all these parallelisms, and more importantly, you can mix and match them for best performance. For example, like, our recommended setting for, like, Lama 3 and also, like, Lama 4 Maverick is to use tensor parallelism for the devices in the same host while using pipeline parallelism for those in different hosts. For DeepSeq 3.3, on the other hand, because of its unique model architecture, like, we use data parallelism, you can use data parallelism for the attention layers while using the expert parallelism for MOE layers. Okay, so finally, VLM is a project to enable all of these optimizations and parallelisms easily and efficiently. You can enjoy them, enjoy all of them with a single command. Basically, you can start from VLM serve model name, and you can use either pre-quantized or unquantized models. And also, you can specify this, like, different, like, combinations of parallelism strategies. In this case, eight-way tensor parallelism plus two-way pipeline parallelism, which means you're using, like, 16 devices in total. Okay, yeah, you can also use, yeah, FPA-KV cache easily. Yeah, and also, you can use different speculative decoding methods to further accelerate your model. In this case, for example, you can use N-gram spec decoding, which you can use without any trained draft model, which may serve as a good default. And, yeah, like, prefix caching and other optimizations also enabled by default. Okay, yeah. Yeah, finally, you can use VLM. You can find VLM in many places. It's a fully open-source project, so you can find the code in GitHub. We are also actively maintaining our blog to share our technical improvements, and also, we tweet a lot. Yeah, also, particularly, you can also find a lot of VLM-related offerings on Google Cloud. Yeah, in GKE, Vertex AI, Dataflow, Cloud Run. Yeah, please check it out. Oh, yeah. Thank you, everyone. Thank you. Thank you.