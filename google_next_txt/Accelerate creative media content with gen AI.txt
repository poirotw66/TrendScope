 Hi everyone, welcome. We hope you had a great time at the keynote. We hope you attended the VO Deep Dive. That was just a few minutes ago. If you haven't done so, don't worry, we're going to go over a lot of the Gen Media models and everything to do with them today. Many of you are probably here to learn a lot more about the models, but specifically how it can help you in your business workflows, creative workflows that is. So you've come to the right place. We're going to go not just a little bit of a deep dive on the product, but we're going to be showing some real life industry use cases. My name is Tomas Moreno. I'm the outbound product manager for Imagine and VO and Gen Media on Vertex. And I'm joined by my co-presenters here. Hi, I'm Khan. I'm a product manager working on Genative Media in Vertex AI. Hi, I'm Justin Thomas. I'm the head of digital experience and growth at Kraft Heinz. I'm Jeff Goodby. I'm the co-chairman of Goodby Silverstein and Partners in San Francisco. And I'm Martin Pay-Lutison. Jeff is my boss and I'm the director of AI at Goodby Silverstein and Partners. We're really, really honored to have our co-presenters are going to be showing some real life use cases and how they use our models to really create content. So let's get started. Not only are we going to provide you with an update on product, like I said, but we're going to do more of a deep dive on the new features, the workflows, and how all these models come together to help you across all these use cases. So with that said, let's get started. Generative AI is really, really taking off in terms of creative workflows. It's taking the media, ads, creative applications, creative tools industry by storm. A lot of use cases, a lot of adoption. that's going on. And it's really because it's doing two things. The first thing is, it's really democratizing the process. It's letting everybody become a creator. Now with these generative AI models, anybody can really generate videos, images, generate music, or generate voiceovers, as opposed to spending a lot of time in the past before. So now you can do it much faster, much more efficiently. The second reason is really it's expanding the possibilities. Things that you couldn't do before, you're able to do now. So things like doing special effects, things like working and building videos with different backgrounds, editing videos, doing all these other creative workflows. You can do those things now much, much faster in seconds, and everybody can do them. So we're very excited about this. We're going to guide you through the whole journey. But what does it take? Well, the main thing, there are a lot of barriers really for these models to be adopted. And there have been traditionally. But we're at that point where all these barriers are really coming down. The first one is, our customers need high quality. Quality is number one. What does that mean? Having very low defects. Generating the right image and the right video in the proper way that you want it to come out. It needs to be scalable. You don't want to spend a lot of time creating your own models, hosting them in your own cloud or on-prem. That takes a lot of effort, a lot of engineering effort. Customers want a managed platform. There's a shifting landscape. These models are changing. Features are changing. Quality is changing. The numbers of platforms are changing. You want the customers want a platform that where they can come and use one platform for everything. And then it has to be safe. Safety, especially for images and videos and music, is number one as well. That's the other number one. You have to be able to generate with confidence. And this is why we created Gen Media on Vertex. Gen Media, we anchor on a couple of things. We have a full collection in all the modalities of the models available in a managed platform that allows you to scale. You don't need to spend a lot of time engineering and hosting models anywhere. You start using them right out of the box from our platform. We maintain the highest quality, not just in terms of prompted hearings, but also visual appeal. And then safety is at the core that we do. So let's dive a little deeper on what this is. First, we are the only platform now with Lyria that has all the modalities. You can generate music. You can generate voiceovers. You can generate video with VO2 now. I think some of you saw the demos on the keynote. We can do a lot more than that. So let's... We're going to do a little bit more of a deep dive in here. And imagine, that was the first-gen media model that we have. It started last year, early last year, and now we're in our third generation. So we're going to go into these models a little bit more. What does quality mean? Quality means a couple of things. Number one is high visual appeal and low defects. So you want to make sure that the image that you're generating is appealing, that it has the right and the right style, and that there are no defects, meaning things that you don't want on that image laying around. Because the problem with not having the right visual appeal, low defects, or even prompted hearings is that you have to keep generating images and videos over and over, and it wastes time and it wastes money. And with that said, prompted hearings, how close does your image or your video or your content align to what you put on the text? That's also as important. So we have a lot of metrics that show that we have a very, very compelling leadership in this space. The other one that I talked about is safety. Copyright indemnity. We provide copyright indemnity. We don't use data, images, videos that we're not supposed to be using to train our models, and we provide copyright indemnity for that. You have to use those models, and as long as you use it in a responsible way, we also cover indemnity for that. We have also SynthID, which is an invisible watermark, invisible even to the naked eye, or even to the pixel level. It's a technology by Google DeepMind. It's actually available across all our models. And then we have configurable safety filters. You can use different filters to allow end users to be able to generate across different things, like, you know, things like toxicity, you know, vulgarity, all those things. You can actually regulate those filters, so you can let your users use these models with confidence. Okay. So that's the platform. Let's get into a little bit of the product before I hand it off to Ken. He's going to go over some really cool demos and use cases. So, going to go over a few products. VO2, it's our video platform. It's now GA. Text2Video is GA with allow list. Sorry. Text2Video is GA, public GA. Anybody can use it. And Text2Image2Video is GA with allow list. Okay. We released this in Q1. Really, you know, enhanced realism, wide range of styles, and really, really redefines phases and quality and control. The length of each video clip is five to eight seconds. VideoExtend is also now available. We didn't make a big fuss in this conference about VideoExtend, but it is also now a feature that is available with Vio. And there are two aspect ratios that we're starting. We're taking product feedback on what other aspect ratios we're going to be generating. Another really exciting piece is our customers really want control of the video. Just by putting in an initial frame, a prompt, sometimes you don't get it to do the right motion. Right? And so now we've announced today that we have advanced camera controls. The first and advanced camera controls in interpolation. A lot of us know it as first and last frame control. You put a first frame, you put the last frame, and then you're able to generate a video that aligns both frames together. So you have extreme control because you control the first and the last frame. Camera movement presets. Sometimes you'll want a video that will move to the right or move to the left as you see there in that shot. Right? You can, instead of prompting, now we have greater control because we have camera presets. Other camera presets that we're bringing on are move back, move down, move forward, move right, pan up, pan down as well, or stay fixed. Sometimes you'll want to stay fixed and generate a video. Now you have much, much more control. Why is this important? If you have more control, you can generate an output. You can generate the first time right. You save time. You save money. The next set of features that we're excited about is, the first one is editing, which is outpainting. Sometimes you'll need to generate a video and expand it to a certain device. There are a lot of devices out there. You have mobile devices. You have desktops. You have TVs. Different form factors. So you want to be able to expand and we call it outpainting, right? So in this case, we brought in a really cute image of a pancake jumping in maple syrup. And you can see we turned it from a desktop form factor to a mobile form factor in this case. So now you can actually use it across both devices. The next feature is video in-paint. This was really helping with video effects. So, you know, in this case, for example, you can see how we were taking off the girl from the person. It looks like he's flying. So it's going to help a lot of creative workflows, especially around video effects. And many other advertising video effects. You could think of a lot of different use cases. Imagine 3. We released Imagine 3 last year. Very, very high quality. It's the highest quality model out there in terms of visual appeal, prompt adherence, and defect density. Very low defect density. On top of that, Ken's going to go over some really cool demos. Imagine, we have editing where you can insert, remove objects. We have customization workflows. And as well as what I really like is instruct editing, where you can actually chat with the images. And you can insert or remove things just by typing a prompt with no masks required. So it's maskless editing. We also have that in GA as well. What I'm actually excited about now, two, three more slides, which is audio. And so we just recently announced a preview of Lyria, which are text-to-music-lm. Now you can actually generate video tone. I'm sorry, not video, audio tones and music, which is prompts. And so, you know, you can generate things like, you know, an instrumental pop song. Like we see here. I'm just going to click on this. And it's nothing. Well, let's play the other one. Irish fork music. Okay. And then one of my favorites is, you know, West Coast Hip Hop in the style of Oakland. Hy-fi music. Okay. The short clips, they didn't load very well, but you get the point. Should we keep playing it? Okay. Should I dance? Okay. So now you can generate music, you know, just with a simple prompt. So up to 30 seconds, this product is in preview. And last but not least, we also want to announce Chirp, which is our model for text-to-speech and speech-to-text. And so this is really groundbreaking audio. Let me play. Let me try to play this audio. Hopefully it plays. And we're just going to let Chirp narrate this audio. Our groundbreaking audio generation and understanding model, Chirp 3, now includes the following features. HD Voices allows you to generate speech from text and captures the nuances of human intonation. Like, hmm, making conversations more engaging and immersive. HD Voices is GA and available in 31 languages and 8 speaker options for a total of 248 distinct voices. Another feature is Instant Custom Voice, which helps you generate realistic custom voices based on a 10-second of audio sample. It can even be your own voice. Instant Custom Voice is in private GA and can be used to personalize call centers, develop accessible content, and establish unique brand voices. To ensure responsible use, Instant Custom Voice includes built-in safety features and usage permissions. The third feature we're announcing today is Transcription with Diarization. If you've ever tried to transcribe from a single channel recording, you'll have seen that it is very difficult to determine who is saying what. Transcription with Diarization is in preview and accurately separates and identifies individual speakers in multi-speaker recordings and significantly improving the clarity and usability of transcriptions for applications like meeting summaries, podcast analysis, and multi-party call recordings. Back to you. So, that was, you know, we basically typed that up and we had, there's different types of tones. Very, very cool. The last thing I want to leave with is we're bringing all of this together. It's all about these models coming together and as our guests are going to speak here, they're going to be talking about all these models coming together to really generate content. So, we have a media studio, it's on our console, you can use all these models in a bunch of different ways together right out of the box. So, with that said, I'm going to pass it on to Khan. He's going to show us some really good use cases. Thank you, Thomas. So, the quality of AI-oriented media content has improved significantly in the past few years and it has enabled many new use cases in many different domains. Now, I'd like to show you some popular use cases that Google Cloud customers have been using our generative media models for. We'll focus on three domains. Image editing apps. Marketing and advertising. Video storytelling. Let's start with image editing apps and I'll show you what Gen and I can do to power up these apps. So, I love travel and I took a lot of photos when I travel to keep those sweet memories. But many times when I look back at those photos, I wish that it looks a little bit better. Let's take this photo as an example and now I'll show you many useful features that a photo editing apps can leverage vertex generative media models to offer to their users. Let's start with removing things that I do not want to see in the photo, such as the people in the background or like the hat on the table. I can do that very easily using the image and in-painting feature. Then what about changing the food on the table and make it more eye-catching? Like maybe turn it into a plate of salad? Again, I can use the in-painting features to add objects to my image. And what about changing the photo background so that I'll be eating breakfast on a beach front rather than at a pool? We can do that very quickly using image and in-painting. But a portrait photo might not look very nice on Instagram. So, I want to expand it to become a square photo. And here I can use the image and out-painting features for that task. And because it's a beach photo, I want to change my shirt to a beach t-shirt. Another easy job for in-painting. Then I can use image and three customizations to stylize my photo into many different types of styles, like 3D art, oil painting, many more. And last but not least, I can use Vio to turn the stylized images into a funny Vio like this one. And those are just like a few examples of how a photo editing app can benefit from vertex and media models. And the possibility is just endless. And we are looking forward to seeing what you're building with these models. So, next let's talk about the second domain, marketing and advertising. A picture is worth a thousand words. So, as a marketer, having a good visual for your ads is very critical. And historically, marketers have relied a lot on stock photos to create their ads. But finding a stock photo that perfectly match what you have in mind sometimes can be very challenging. In that case, image and can have you quickly turn your visions into a perfect picture. You can use image and to create images for a broad variety of ad campaigns, like this one for a restaurant ad, or this one for a fitness ad. An image will be extremely helpful when your idea is just too creative, such as creating an ad for an electric scooter featuring a Viking man. Or a bonsai service featuring a person in a diving suit right in the middle of a desert. In a lot of cases, marketers will need to put their products into different scenes. And this will cost a lot of time and budget to arrange the actual photo shooting. With background editing, you can get it done very quickly. You can put this perfume bottle in a lot of different scenes. And you can do similarly with appliance and furnitures. You can put them in many different room settings. And it's not just limited to images. You can also turn your image into an engaging video. And you may have a banner ad like this one. Which you can use via image to video features to turn it into an engaging ad like this one. And you can also add music and narration to the video. Premium coffee. The taste of true distinction. And just to be clear, this short ad was created entirely with AI. Including the video, the narration, and the music. And you can repeat that process to create different scenes and change them all together to create a longer video ad. Here's a storyboard for an ad for the same coffee brand consisting of six scenes. I use our generative media models to generate the visual and the audio for each scene. And then stitch them all together. Let's take a look at the final video. Premium coffee selects only the top 1% of fair trade Arabica beans. Slow roasted, perfectly brewed. Releasing notes of chocolate and spice. Premium coffee. The taste of true distinction. Yeah. So I hope those examples gave the marketers in the room today some inspirations on how you can leverage these Gen Media models for your business. And now let's talk about the last domain. Creating long form video content with AI. So the quality of AI-generated images, videos, and audio has significantly improved over the past year. Which now allow us to create long form video content entirely with AI. And the typical workflow for an AI-powered content creation will look something like this. You start with an idea for your video. And then you use Gemini to refine it. And then you ask Gemini to turn it into a storyboard with different scenes. Or you can just write a storyboard by yourself. And then you use Imagine to create a still frame visualizing each scene. And iterate on that until you get the visual that you like. Then you use Vio to turn those still frames into video clips. And use Lydia to add the background music. And use Chirp to add the narrations. And finally, you use a video editing software to stitch everything together into your final masterpiece. And now let me show you a short documentation video that I created using this workflow. Winter held the land in its icy grip. Every surface, every branch, was etched in frost. A world silenced under a blanket of white. But even in the depths of winter, a change was coming. A subtle shift. A gentle surrender. As the sun began to reclaim its dominion. And then, life stirred. Tentative at first. A whisper of greed against the white. A promise of what was to come. And then, with a burst of color, spring announced its arrival. A single flower. A vibrant testament to the enduring power of life. A touch of spring. A promise of renewal. The land awakens. And life blooms anew. So, that was all the generative media use-cases I would like to show you today. to show you today. And next, I would like to invite our customers to share their stories about how they are using vertex-oriented media models in their business. Let's welcome Justin from Kraft Heinz to talk about how they optimize the marketing workflow with Gen.AI. JUSTIN THOMAS- Thank you. So good afternoon. My name is Justin Thomas. Again, I'm at Kraft Heinz. And at Kraft Heinz, we want to be the most creative, consumer-obsessed food company. And when we think about food, just like creative, it becomes a matter of taste. And taste is shaped by culture, by intuition, experience. It can be very personal. And as technology, and specifically AI, continues to accelerate in its ability for production, we look at it as this connection to taste. Right? The human and the technology, that's going to be critical. And so Tastemaker is our AI platform, specific to our marketing community at Kraft Heinz, empowered, obviously, by Google. And we're looking at this as a way to augment and accelerate the power of our creativity at Kraft Heinz. And so what I'm hoping to do today is give you a little bit of a taste of the, a taste? Of the journey that we have been on. And it hasn't been that long, actually, for us. So we had our first pilot in August of 2024. And it started by just a question from somebody in our marketing community. A problem. With an ask if we could tackle it. If the work that we were intending to do, and that we were doing in AI, could help solve it. And so what started as that pilot has continued to snowball, has continued to build momentum, as we've created new capabilities, as we've created new use cases, and ultimately, as we've scaled our users and the platform. And so starting off, the very first thing that we did was brief generation. A very manual effort, not necessarily a loved task. And so how could we expedite that? How could we accelerate the use case of that? And this was the first question. The very first pilot. And really quickly, it came with an ask of, can we incorporate images? Let's take our brief and our images and bring them together. And this was a major, major pain point for early creative, early concepts. And so with Imogen and Gemini, we brought these together. And as you can imagine, people got really excited when they saw that first pilot, that first integration of brief and concept imagery. And then we got peppered with questions and feedback and wish lists and ideas. And can I do this? Can I change it to this format? Can I change the background? Can we put people into the shot? Can we put people into the shot? And so for us, it was very clear that our marketing community was hungry. And they wanted to be able to do more. And so we had to keep pace with that. We had to keep going. And another part, as we moved through it, was creative evaluation. So we have a lot of brands. And we operate in a lot of markets. And so you can imagine, we put a lot of work out into market. And a lot of work that we don't actually bring to market. And so one of the questions was, could we look at how we use AI to pre-test whether the creative is right for our brand, our consumer, right for culture, before we go and allocate capital, before we go and put a media plan against something, whether it's a new campaign, a new brand, a new innovation? And again, the company, the marketing community came to us and gave us a problem, and we looked to tackle it. And then lastly, how we bring our brands to life. So food styling can be very time-consuming. It can be expensive. And you imagine, right, Heinz is everywhere. And so we have markets all over the world that will say, I need a new burger shot. And they'll go and produce that content. And so we thought, how can we bring the dynamism into food? Again, through Imogen and Vio here. And so this has been a really exploratory journey for us. And again, it hasn't been that long. And what started as a pilot has quickly scaled to, how do we think about operationalizing this? How do we think about workflows for our marketing community? And now we have users across marketing. We have users across innovation, R&D. We're across many markets already, and we're continuing to grow. And I believe our success lies in two factors here. One is how we think about our brand intelligence. And then the second part is the user centricity. What are the problems that our marketing community is trying to solve? And so let me start with brand intelligence. Our brands are one of our most critical assets as a company. And we have the benefit of bringing that asset, bringing that IP into Tastemaker. Generating a really rich reference library that includes all of our best campaigns, all of our brand IP, our visual style guides, our consumer data, how we think about our brands in the role of culture. And use that in a way that we can quickly reference, and then also analyze the effectiveness of that. The other part is, as you can see here, we chose RAG because we looked at it in two ways. One is we knew that we could save time. We didn't need to train specific models on proprietary data. We can just pull in and inject as we needed to. Second thing was money, right? Again, we didn't need to fine tune. We didn't need to spend time training on what our brands are all about. But then lastly, as I said, we've been building quickly. And so the ability to stay flexible and bring in frontier models immediately has been really beneficial for us. And then it came to our user centricity. What was really clear is that people are still finding AI daunting, intimidating. And so how do we think about promptless experiences? Building optimized workflows, or what we refer to as apps here, that are trying to tackle specific problems for our marketers. And getting them to the point of value as quick and as easy as possible. And so this is a quick snapshot of the UI that we've put in place. And what you'll hopefully notice is just how simple it is. We're not trying to complicate things for our marketing community. We want them to be able to click, point, and then edit as they see fit. And these are the type of things that our marketing community are actually producing. We're not touching this. We have people coming in every day and producing their own creative, producing their own content. And what you're seeing here is some examples of text-to-video or image-to-video in what we've done. And some, you know, it wasn't that long ago, trying to dip a french fry into ketchup would have been really difficult. And that's the type of, when you think about stylistically, right, as a brand, as a brand manager, I'm going to be very specific in the way that ketchup needs to move or pour. Or how mac and cheese needs to scoop onto a fork. And again, that's that brand intelligence that's been really important for us. And so we're able to now take one generation and think about, okay, how do we bring that dynamism into our creative? But ultimately, we're trying to speak to many different consumers. And so one generation can quickly scale into hundreds or even thousands of versions for us. And being able to pair that with that brand intelligence, that consumer demand landscape, and think about how we best resonate with our consumers. And it's resonating with our community. These are some of the quotes that we're hearing. I love the last one here. All this without writing a prompt. How do we make things super simple for our marketers? And here's another quote, essentially proving what we've been trying to solve for, which is how do we save time, how do we save money, and how do we generate better outcomes for our users? And it's not just a nice tool. So this is what we're seeing in the results. We have creative processes, creative workflows now, that have gone from eight weeks to eight hours. The promptless experience that we're able to put in place has allowed us to put non-technical users, getting them to a point of value four times faster as well. And so we're really excited still. We're still early in this, but we're already seeing a huge amount of adoption. And we're continuing to learn and get that feedback on what we build next. And with that, I'm going to pass it back to Tomas. Thank you. Thank you, Justin. It's always the real-life world examples that are most exciting. So this is really, really exciting. Thank you, Justin. And so now we're also going to talk a little bit more about how to use all these models to create some really good production content. And so here's Jeff is going to kick us off. Thank you. They don't trust me the best to do that. I feel like I'm in the middle of a bunch of miracles here. I'm a guy that actually cut type by hand and cut film and pasted it together and ran it through spools in a machine. So this is really more amazing for me than it is for you, if you can imagine that. How many people went to the Wizard of Oz thing last night? A lot of you. That shit was amazing, wasn't it? My God. I'm going to show you another film here that we generated. I think it's equally amazing. We plan to have it done before that film is done. Martin's going to take care of that. We are lucky enough to work with the Dali Museum in St. Petersburg, Florida. It's a single artist museum. So they've basically got a bunch of Dali stuff to put up. And we make things for them that actually take the work of Dali and put it in new forms that I call them new rides. It's like new rides at Disneyland. Great technological things that make them into a lighthouse and kind of reimagine what a museum can be. In 2016, we let visitors in the museum actually step inside one of Dali's paintings, the archaeological reminiscence of Malay's Angelus, in virtual reality. So that experience went live before the first consumer version of the Oculus Rift was actually available to the public. This is still in the museum. You can do it now. In 2019, we trained a GAN network to remind the world that Dali still lives. So we made him. We put him in a box in three places in the museum. And he actually tells you what's in the museum today. He reads you the paper, which gets remade every day. He tells you how to get to the bathroom. And in the end, he says, do you want to take a selfie with me? And he turns around, takes a selfie, and sends it to you. And people just lose their minds when they see that. Finally, a couple of years ago, we explored the collective dreams of everyone. With visitors in the museum, they would put their dreams into an ever-growing tapestry. So we took everybody's dreams, made one big tapestry out of it all. It was the first time that a diffusion model was used in a large-scale museum experience. And this thing was also mind-blowing. It was like a collective dream of America on that day. Oh, and last spring, we installed a telephone. This is Dali's aphrodisiac telephone. And you could pick it up, and it let visitors actually speak to Dali. You could ask him questions, ask him where to get a taco locally, or ask him about your own dreams. And it combined a speech synthesis model and a large language model, and for the first time, a lobster telephone together. That will not happen again. So this tradition led us to something. This is the film that we're making now. It's a film that was envisioned by Dali in 1937. He left a little plan for it. It was never made. He took it to MGM. They said, we can't make this. It's too crazy. And it is now being made. And I'm going to have my man come out and explain how. Thanks, Jeff. Thank you. So realizing Dali's vision from 1937 required more than just hitting generate. We needed to interpret his surreal vision faithfully. So I want to take a minute to look at a traditional filmmaking process, which is, you see an example of that here. It is a fairly linear path. We start with concepting, brainstorming the core idea. Then we move into writing the script, laying out the narrative and the dialogue. Next up is storyboarding, where artists visualize the script shot by shot. And then we go to the physical shoot, capturing performances, locations, practical elements with cameras, good old-fashioned cameras. And after the shot, we often have a VFX phase, coloring phase, adding digital layers, effects, and environments. And finally, editing is where everything comes together, where we refine the pace, integrate the sound, and assemble the final film. So it's a proven sequential process that we know really well and that has specialized teams focusing on each distinct stage. Now, one of the big challenges with this is that going backwards in that sequential process is very expensive and time-consuming. In fact, it is often impossible. So that brings us to a more modern AI-driven process. And this is what we use to bring Dali's vision to life. So how is it different? Some pillars in this project remain familiar. We have high-level concepting still, understanding Dali's intents and our goals, as well as the final editing stage, where human craft creates the narrative flow and the impact of this film. But the traditional middle production stages, the detailed storyboarding, the physical shoot, and the complex VFX builds, have fundamentally been transformed into what we call an iterative AI storyboarding and content creation loop. So instead of drawing each shot, we used Dali's script and our concepts to work with Gemini, crafting detailed prompts. Imagine 3 then visualized these as keyframes and VO2 generated motion directly from those keyframes, creating the core content that would normally come from a shoot and extensive and expensive VFX work. Crucially, this AI process is non-linear. Unlike traditional production where reshoots are costly or even impossible, here we can instantly generate variations or at least quickly generate variations, review them quickly, jump back to refine the prompts, and regenerate a scene with VO all over. It takes minutes. Sometimes it takes hours. We can't even do this in the editing stage. It allows for a rapid iteration and experimentation that simply isn't possible in a traditional filmmaking process. So let's take a look at what that actually means here. Because I like what Justin just said about not having to write prompts because a couple of years ago everyone was supposed to be a prompt engineer. But you know what? Gemini is much better at writing prompts than people are. So we take an example from the original screenplay, a man with a lobster on his head and caressing a swan in the middle of a huge forest of harps, and use Gemini to unfold the layers in that script segment. And that becomes a much more elaborate and interesting prompt, a surreal and dreamlike underwater scene. A woman with red hair and a lobster-like crown holds a white swan in her arms. The background features what seems to be underwater harps-like structures with long vertical elements that resemble seaweed or coral formations. The lighting is soft and diffused, creating an ethereal and somewhat haunting atmosphere. The color palette is dominated by shades of teal and brown, contributing to the watery ambience. All right? It's poetry. What does that look like? Next step is Imagine 3, and this is what that looks like, the elaborate prompt that we developed over so many iterations. And it became this. So this is the first still that anyone sees from our film that we're so proud of. But, of course, this is just a still. So let's see what happens after that. We then add the VO elements, describing the camera motions, describing the action, turning into a beautiful scene in our film. And this was, in fact, your first glimpse of a scene from the final film. This process has been repeated a lot. And I want to, you know, it still takes a lot of time to do this. The prompt took a long time to get to. We iterated over and over on just that one scene. And so far, we've done that for 198 scenes in the final film. So there's still a lot of work here. But so making the film is time-consuming. But the time spent in front of the computer is less costly than the time spent on a film set and the time spent in VFX. So the traditional approach is very costly. This is simpler for us to work with. All right. So this is an exciting moment for us. Giraffes on horseback salad, inspired by Salvador Dal√≠'s screenplay. And it wouldn't be possible without Google Cloud. So I'm very excited to give you a glimpse, the first people in the world who get to see this, the trailer for Giraffes on horseback salad. Enjoy. Let me tell you about the strangest movie never made. In 1937, I wrote a story about a man and a surrealist woman. An enigmatic, almost mythical figure. It was about a world where reality burns away with flaming giraffes and impossible dreams. That was my vision. A movie full of wonder and strange beauty. But they said it was too wild, too impossible for its time. I agree. The world wasn't ready until now. I called it Giraffes on horseback salad. Prepare yourself, because the impossible is coming to life. Thank you, Martin. And thank you to all our guests. This is really exciting. A real-world example. So, yeah, let's continue the journey. If you haven't gotten your hand on these models yet, contact your sales associate or Google Cloud FSR. They'll get you a lot listed. We have a process for that. Keep an eye out for these sessions, the bottom two sessions. The middle one is ours. The middle one just finished, but keep an eye out for the recording. That's a VO deep dive. And tomorrow, Jeremy is going to be hosting a session, no lights, no camera, AI-driven creativity with view and imagine. So, take a look at that one as well. So, with that said, thank you so much for attending. We have about two minutes. Maybe we take one or two questions before we get off the stage. Anybody have any questions? Oh, there's one question. I see one. So, there are various companies using these technologies inside of their products. How does that vary from what we can get straight through the new Media Studio from Google's platform? Yeah, the Media Studio is a UI, a console, where you can actually take advantage of all these models together. And it's mainly for testing. You know, our models actually get integrated into the platforms for our customers. The Media Studio is mainly just for testing. The API, we actually sell the API. Okay. One more question. Can you share a little bit about the iteration, how you built for Heinz, your top layer, integrate? Like, what was that process and how big the team, like, how did you actually pull that off? Good question. So, again, this was very exploratory. And it started with one person who's sitting over there. We had one person. And it was a question and a problem. And we started to go from there. And as we realized that we were creating value, as we saw that sweet spot, we started to add more people. I think what was really important for us, too, is we didn't just go straight to scale. We were very purposeful in how many and which users we were going to let use this. And it came with the commitment and the promise that I'm going to give you something, and you're going to give me something back, which is feedback. All right, you're going to tell me what you want, what you don't like, what you do like, and we're going to continue to iterate and optimize and ultimately deliver based on what we're hearing from you. And so it created this opportunity for us to build with the business, not for the business. Great. Thank you. Okay. That's a wrap. Ken and I are humbled. And thank you to our guests, Justin, Martin, and Jeff. And thanks to Jeff for all the contributions to the creative space. So thank you.