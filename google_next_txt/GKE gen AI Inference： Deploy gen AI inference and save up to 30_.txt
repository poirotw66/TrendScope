 . Hi, everyone. Welcome to this session. My name is Akshay Ram. I'm a product manager with GKE. I'm going to be joined by Shubh Srivastava and Vinay Kola. And we're super excited to chat about Gen.EI inference on Kubernetes and why you should use Gen.EI inference if you really want to. Today's session, we're first going to briefly talk about the positioning of GKE. You have plenty of choices to run Gen.EI inference in Google Cloud. You can use Vertex.AI. You can use GKE, of course. But why would you actually want to use Gen.EI and Kubernetes for Gen.EI inference? The second part, we'll obviously work backwards from customers' requirements. What are we hearing from customers? And what specific features they would like us to build? Then we'll obviously look at some of the solutions we've built working backwards from customers. And then Vinay will talk about Snap's Mento ML platform team and how they're thinking about Gen.EI as a platform team. To get started, there's obviously a lot of choices. You can use Vertex.AI. Vertex.AI has Gemini, Cloud. You can obviously run your own Vertex endpoints through Vertex predictions. Why would you use Kubernetes? You have many options. And most of our customers tell us the same reasons why they use Kubernetes in the first place. It applies to Gen.EI inference as well. Like, they want portability. Kubernetes is open source. It's an open API. It gives them a lot of flexibility. And that flexibility is true across clouds. So that's one of the reasons why they choose Kubernetes. And it stays true for Gen.E inference because that's just another app. Of course, performance and cost. I think Kubernetes is you operate your own infrastructure. You just get the orchestration. So you're able to tune at layers of the stack, which gives you a lot more control. That's some of the reasons we hear from customers. Double clicking further on GKE, we've made a ton of investments both in open source Kubernetes with Red Hat as well as GKE itself to really optimize our product for metrics that matter to customers. And for customers in running Gen.EI inference, it's usually cost savings, it's throughput, and it's state latency. And as you see, we've benchmarked ourselves to make sure that GKE is performing much better than OSS. But we're all built on OSS, open APIs. None of the portability changes. We contribute back to open source. That remains the same. You just get a better Kubernetes experience on GKE. And lastly, one, I can't end with the positioning without this, is that we hear from customers that GKE or Kubernetes is a platform for all sorts of applications. It's web apps, batch, and EIML inference is no different. And if you look at EIML inference, it's not just inference. It's training as well. It's fine-tuning. And Kubernetes acts as a platform. And on GKE, we've made investments both on the training side and the inference side. For example, yesterday, Anthropic spoke on how they run 65 keynote clusters on GKE for their training jobs. So both of this is possible today. And it's one of the reasons why customers choose Kubernetes, to get a standard platform to run their infrastructure. So we've taken a look at, you know, if Kubernetes being open, portable APIs, and a platform where you can run many apps and fine-tune performance, and you get a lot of agency of choice to tune how you want. Let's take a look at some of the customer challenges. The first, obviously, is cost efficiency. But with Gen AI inference, it's actually slightly different. Because if you take a look at traditional inference, you can run them on CPUs. You can run them on L4 GPUs. But for Gen AI inference, you need high-performance accelerators. And high-performance accelerators, obviously, you have to pay for the performance. So they tend to be expensive. So customers are really asking for, hey, how do I really get the bang for my buck on, like, running Gen AI inference? Because the cost per accelerator has gone up significantly because you're getting more performance. The second reason, or second big pain point customers talk about is obtainability. Obtainability is slightly different here. It's not like consuming CPUs. Again, you're consuming accelerators. So you need to think about obtainability. Like, do you want to do reservations? Do you want to do spot on-demand, EWS? So there are a lot of choices. And customers are looking for a strategy to obtain capacity for inference. Then third, if you look at Gen AI inference right now, there's a lot of churn across different layers of the stack. At the model server layer, at the accelerator layer, at the Kubernetes scaling layer, as you can see, there's a lot of changes happening there as well. So customers are really looking for time to value and really get the most bang for their buck without having to really go through a ton of learning on all different layers and really simplify their lives and their time to value. And finally, the fourth reason, I think this comes back to just being Kubernetes customers. They really want portable and open standards because they tend to run across clouds, on-prem and in the clouds. They really want that flexibility to remain, even with Gen AI inference. Double-clicking a bit further, we asked customers, OK, so how is your AI ML inference journey? We broke it down into three phases. In the first phase, customers really told us, hey, I need to learn about all of the different accelerators in Google Cloud. Be it all the GPU instances, all the TPU instances, and I need to understand what's the best fit for me for my model. So there's a learning curve there. Additionally, there's like, what is your procurement strategy for your capacity? It's not just like you spin up an on-demand instance. You can, but you need like a little bit of nuance because accelerators capacity planning is something you need to be slightly more involved in. So that's, customers are asking us for strategies on how they think about procuring capacity. Additionally, for TPUs, there were a few other nuances in terms of, you can use the deep mine stack with Pathways, Jetstream, that we have an entire serving stack there to like benefit from how Gemini trains and the technology they use. Or you can use the more open source stack, which is the whole VLM stack. So it's an additional choice that customers have. So clearly customers have a lot of choice during evaluation. But with choice comes a lot of qualification and understanding what is the best fit for the use case. So they're looking for some guidance. Second, during onboarding, I think onboarding is a phase where you're really benchmarking, making sure that they're getting the two, if they're latency bound, they're looking for latency metrics. If you're throughput bound, you're looking for throughput metrics like token per second. So they're really looking for the metrics that matter and they're comparing and evaluating performance they're getting from other environments. And if you're running TPUs, it involves an additional learning curve of, you know, XLA optimizations to really drive the most of what you want from TPUs. And finally, when you're scaling out in day two, I think here it's very natural. Where customers have, you know, scaling load balancing. How do I do load balancing for LLMs? What are the best practices? How do I balance cost and availability? How do I monitor my metrics across my accelerators? It's something which customers want good data operational capabilities for Gen AI. And they're asking us for best practices about those. So let's take a look at some of these Gen AI inference capabilities, starting with the evaluation phase. I just told you that, you know, this evaluation phase is all about learning and navigating a sea of choices. And customers were really looking for data driven decisions to make educated and informed choices. And that's why at Google Next, this Google Next specifically, we launched something called GKE Inference Quick Start. What GKE Inference Quick Start is, is Google runs benchmarks and exposes data for you to query. And what is that data? You can tell us the model of your choice. You can tell us your accelerator of your choice. And then we will give you the benchmark data, which is effectively the saturation point of the accelerator. What's the HPA metric you want to scale at? So what it gives you is a great starting point to run near optimal infrastructure. So then you can fine tune as you go. Let me quickly show you a demo about this. So this is the quick start. So effectively, you can get all the list of accelerators. And you can see all the popular open models here. One of the nice things here is you can see that every model is launched. You get a new model available in this list. And you can see the accelerator stack for a given model. There was a previous screen. And now you can generate a manifest. And what this manifest is, is basically like the Kubernetes HPA preferences, the accelerator configuration, the model server configuration, all configured to your performance needs. The specific command said you want, it was a latency bound application. So we generated a manifest for you. And this is all based on the data which we have run on our internal benchmarks. And then you effectively then take that YAML and just apply it to your cluster. So you go from a model to a deployed Kubernetes infrastructure running in an optimal way with just a few clicks. And as you can see, the pods soon will start to run. We set up best practices like pod monitoring. So we already have it enabled, so we don't need to enable it additionally. And then you'll see that, you know, your pods are starting to run. It's already running here. And we just didn't stop there. We also, like in this demo, we simulated with a bunch of traffic. And we have out of the box dashboards. So you can then measure, okay, I used Google's inference quick start to like get a blessed template, ran it myself, and how is it performing? So then you can then take it and further fine tune and modify as you need. So this is available starting to date in public preview. And it's just a simple API call. You can list them. We already have qualified LAMA 4 there. So you can also get the best practice templates for LAMA 4. You can get data across all the different accelerators and the saturation point on latency and throughput. And you can ask it to generate a manifest and deploy it yourself. So we took a look at, you know, how we simplified qualification. The other big pain point we have is acquiring capacity. And what we launched a couple of months ago is custom compute classes. This is generally available. And what custom compute classes allows you to do is specify priorities. Those priorities can be across two dimensions. It can be across pricing plans. So you want to be across spot, on-demand, DWS, reservations, depending on, you know, which strategy you want to follow. You can also specify multiple accelerators. Let's say you use inference quick start to qualify different accelerators. And you can specify multiple accelerators here. So you get, you basically cast a pretty wide net and acquire capacity within the region. So this is a good strategy for you to use custom compute classes. And we see a lot of customers use custom compute classes. One more feature we added within custom compute class is the ability to use DWS Flex Start. This we found is going to be very popular for inference workloads. Prior to this, to use DWS Flex. DWS Flex is where you can acquire capacity. And that capacity is available for seven days. It has a time to live for seven days. And it's usually available at a slight discount to on-demand. So it's a good way to procure capacity. So it's not like spot where you can get interrupted at any time. But it has a time to live. So it's a good in-between capacity option for you. And that's also available to the same interface through CCC. So what this means is as a customer, you can say, hey, use my reservations first. Then use DWS. Then use some spot. And depending on your risk profile, you can mix and match based on, you know, which application. If it's dev test, you want to be more opportunistic on DWS spot. If it's prod, you obviously want to have more reservations there for you. So you have this nuanced way of specifying your risk profile. And since this is an abstraction on top of NodePool, the GKE scheduler will make sure that, you know, it follows the priority list that you've specified here. So we've taken a look at, you know, the AIML inference journey from an evaluation standpoint. From an onboarding standpoint, a lot of it is benchmarking work. But one pending feature request here from customers was I haven't, it takes me additional work to get TPUs to be very performant. And to get to a place where I feel comfortable using TPUs. So just last month or a couple of months ago, we launched inference with VLLM on TPUs. So what this enables you to do as a customer is you're using a popular model server, which is VLLM. And underneath the hood, Google contributed, is working with Red Hat on this VLLM server to effectively bake in a lot of the PyTorch XLA bits. So you don't have to learn about PyTorch XLA optimization. It's completely portable and it's baked into the container image. All as a customer you have to do is change your container image from the VLLM GPU image to the VLLM TPU image. So this gives you seamless access to TPU performance without having to go through the learning curve of all of the XLA optimizations that you would have to otherwise. And the best part is, again, tying back to CCC. That's what I feel. CCC is becoming very popular amongst our customers because it gives you the ability to mix and match. We effectively allow you to take the same CCC template and say, hey, I want to use some TPUs and I want to use some GPUs. And because of VLLM, you're now in a place where you can port across GPUs and TPUs, not care about any of the switching costs because it's completely baked in and managed. And all you care about is making sure you get the right performance at the right cost so that you can run your inference applications for your business. So I have a quick demo here. And in this demo, it's super short, but all it says is you have a Kubernetes deployment here running for you. And what you'll see behind the scenes is there's a node pool which has both Trillium as well as L4 GPUs running on G2 standard machines. So it's a pretty large mix of accelerators. And as you'll see here in CCC, you're starting to use Trillium. You're going to use then V4, which is a TPU V4 spot, and you're using L4 GPUs. So it's like a pretty fungible mix of accelerators. And as you'll see, as we scale up traffic, you see all of the accelerators start serving traffic and the pending requests start coming down. So all of this with a very usable interface and compute classes. And because of the investments we have made with VLLM and TPUs, you're now in a place in Kubernetes and GKE specifically where you can extract the best accelerator for your use case without having to worry about all of the different nuances of accelerator architectures. Finally, if you're looking to move beyond the VLLM architecture, we're also making investments in the whole whatever we learn from Google DeepMind. So facing it through GCE and of course making it available through GKE. So we just announced at Google Next in preview Google DeepMind technology called Pathways, which allows for a lot of distributed disaggregated serving and multi-node host serving through leader worker set. And that effectively allows you to run even the largest scale state of the art models on TPUs. And all of this with some of the technology that has been built by Google DeepMind underneath the hood. So we took a look at the evaluation and onboarding. Now I'm going to hand it over to Shubh who's going to talk about data operations and the best practices you think about load balancing, observability, and all of the above. Thank you, Akshay. All right. So like Akshay said, we already have gone through evaluation, onboarding, and now is the time where we are going to take a look into data operations when we have already productionized the workload and we are doing the production ramp. So what we are going to do for this particular section is go through some of the customer trends and some high level industry trends that we see and challenges. And kind of go through all of the innovation advancement that we have included in GKE to kind of combat some of these challenges. But before we go there, I just want to know with a show of hands, how many of you are already running your containerized Gen.AI inference workloads in GKE? All right. So we do have a few people in the audience. Good to know that. Awesome. So, yeah. So basically the challenges that we have right now, the first one is the load balancing traffic, right? And we have heard from our customer that traffic management, traffic load balancing is inefficient. And I want to highlight why this is efficient and how it is different from like general, you know, web serving or application serving versus when you're serving a GKEI inferencing workload. Because for these workloads, we need low latency, high throughput, especially if you have LLM models, you need high throughput for token generation. And also at the end of the day, we want your end user to have a good performance while serving these requests, right? So that's why traffic management and load balancing, it's a paramount, you know, priority for our customers. The second thing is balanced availability and cost efficiency. So when I say balanced availability, it essentially means the ability for our customers to have availability of these accelerator across different zone and region. And it's not just the availability, right? But also, one thing that we have noticed with our customer is the fact that the utilization of these accelerators, because they are expensive and we want our customer to get the maximum utilization of these GPUs or TPUs. The problem lies, the challenge lies on the fact that when you are kind of like benchmarking or choosing the accelerator, it's not easy, right? Like as a developer or as a platform engineer, there's so many things that goes under the hood. Like you need to know what kind of model you're using, how many requests you're going to be serving, what are the kind of requests, and also what kind of efficiency you want. So taking all of these things into account, GPU utilization or kind of like knowing how to utilize your GPU or allocate GPU effectively remains a challenge. The other thing that we see our customers struggling with is accelerate the data loading, right? Like you have your model, now you want to kind of update your model data weights as quickly as possible and kind of load that data into your GPU and start inferencing. So that also is one of the challenges that we see. And lastly, performance and monitoring, obviously better performance, better observability, you're able to like fine tune your workloads in a better way. So the first feature that we launched in Cloud Next is called Inference Optimized Gateway or Inference Gateway. And what it essentially does is it gives you a really efficient way for AI inferencing. So what it has is the fact that it is very much like model-aware, right? For example, if you have LLM workloads, it kind of takes into account what are the workload patterns you have. It lets you serve your request with low latency. For token generation, it's going to give you that sequence token generation with high throughput and ultimately increases the performance. Another feature that comes with Inference Gateway is the fact that you're also able to route to LoRa adapters. So if you have a lot of like fine tuned models, you can actually kind of like densely wind pack these models into a single GPU or TPU, which again makes it more efficient and cost optimization. Another thing that I would like to call out, and that is kind of like differentiating factor for Gen.AI serving versus normal serving, and it's the fact that for Gen.AI serving, when you have your patterns, your workload patterns, which are kind of like lasting from a few seconds into a few minutes versus your normal web serving, where the requests are some millisecond. If you're using the traditional way of serving these workloads, for example, round-robin algorithm, what's going to happen is when you're on the decoding part of your serving, where you're continuously sequentially generating token after token, your GPUs can become very much memory bound, right? They're very much memory busy. And in that point in time, if you receive a request in that GPU, then it's not a good customer experience because it's busy and it's not able to serve that request, right? So there's going to be some queue building up and just not a good performance for your end customer. So what inference gateway does, it takes into consideration some additional matrices like KV cache utilization and also queue depth. So it has that intelligent routing on top of, you know, just other things that we discussed. So it makes sure the request goes to the GPU, which is idle. And I'm going to show you some of the stats I have at the end, and then we're going to go through other features. I want to show you, which I just talked about, the utilization for KV cache, right? So on the left side, you see, this is the benchmarking we did. It's like a load test we did with a lot of, like, publicly available data across different model servers. And what we have seen that without inference gateway, there was, like, a lot of, like, variance that you see in the KV cache utilization. And also, there's a lot of saturation for KV cache as well. Whereas with inference gateway, it is pretty seamless. It's pretty, you know, it's not as variant as you see here. And also, there's no saturation for your KV cache utilization. These are some other metrics that we have. Without the inference gateway, we have seen there's also increase in the queue buildup from your client side. Versus with inference gateway, we haven't seen any kind of, like, queue buildup for serving inference requests. And the same for the time to first token latency and the average output token latency. So all of the data points that it provides you a really good performance for serving your Gen.AI workload on GKE. There are two other features that come with inference gateway that I'm going to quickly highlight here. And that is one which we just talked about, like, balance availability, right? So if you have an application, your application stack, your serving stack in one region, and due to some reason, if there's, like, stock outs or observability challenge in that region, it seamlessly takes your traffic from that region to another region to make sure there's reliability and there's no disruption for your workloads. So it's pretty powerful for all of your load balancing when it comes to, like, multi-region capacity chasing. And last but not the least, safety and governance, right? So it comes inbuilt. At Google, safety and security is our top priority. And it's not just a one-time thought. We want to have all of our security tools kind of, like, inbuilt in every stack of our infrastructure, starting from the gateway level. So it supports integration with first-party security services like Google Armor, as well as other leading industry solution for AI. So, yeah. Okay. So we have gone through this. Okay. So kind of now double-clicking into the storage acceleration for inference, right? So, like I said, our customers want to kind of, like, pull the model from storage as fast as possible into the GPU and, like, begin inferencing as fast as possible. And in order to do so, what we have seen the common trend with our customers, like, they tend to over-provision their pods, right? And that's not a good way of kind of, like, optimizing your storage. So let me kind of, like, talk about how this problem manifests in two parts and what are the solutions we have in our toolkit for storage with GKE. So first one, when you're using your, you know, like, when you're creating your workload on GKE, you're creating a pod, you're scheduling a pod, then you're getting a node for your pod, you're mounting a volume to your pod, and then now the thing that remains is, like, you have to pull your container from container registry or artifact registry, and you've got to, like, boot up your container image, right? And what we have seen for, like, popular model servers like VLLM or NVIDIA Triton, they could be, like, really large files, up to a few gigabytes, and that can really add latency for your model, for your container startup time, right? So that is the first part of the problem. So once you do that, now you want to kind of, like, download your data, your model weights, and you want to start inferencing. Again, these files are very large. I'm pretty sure you guys know, like, how much time-consuming it is to download all of the model weights and kind of, like, start inferencing. So this problem is twofolds. Now let me kind of, like, go through the solutions that we have. So the first solution that I have here that's called secondary boot disk, and as the name suggests, it is a secondary boot disk for your node. And what it essentially does is that it comes preloaded with your container image, right? So instead of going to, like, another data source to kind of pull your image, you can just get it, you know, preloaded in secondary boot disk. And what we have seen that it has really, really lowered the latency and made your container boot up start time 29 times faster. In our benchmark data, it shows that usually the time it takes is around four to five minutes, and we have dramatically brought down the time to around, like, 9.38 seconds. So that's pretty powerful. I'm going to quickly go through the next two options we have. One is the cloud storage fuse, which is nothing but, like, it gives you that mount, actually, like a volume mount on your pod for your GCS bucket, right? So it kind of acts like a local storage for your pod. And, again, it helps you for accelerated download of your model weight, and it makes it pretty easy for you to kind of, like, optimize your latency there. The last one that I have here, actually, I have one more feature for storage. But another feature that I have here is called HyperDisk ML, which is, again, it is a block disk storage. Again, it makes your data downloading even more faster. It increases it up to, like, 11.9 times relative to, like, downloading from, like, model registry. So it's pretty powerful. One thing I would like to call out here, though, this particular feature, like the HyperDisk ML, it's zonal, right? So what we have seen our customers say that when they're kind of setting up their production workloads, they want their compute to be in different zone, not just for redundancy, but also to be able to, like, chase capacity in different zones. And also if they want to use spot or preemptible VMs, they can pick that from two different zones, right? So this being zonal, we have launched another feature with HyperDisk ML, which is called as HyperDisk ML multi-zone. And what we do is, like, we manage, we clone the disk for you in different zones where you will be running your compute. And it's completely managed by us so that you have the cloned data, not just in one zone, but multiple zones. Another relatively new feature that I want to quickly call out before I hand it over to Vinay is called Anywhere Cache. And the problem that I just talked about, you know, that we face with our, you know, with your compute being located in two different zones, and you would want your storage co-located with the compute, and at the same time, you would also want storage performance to be accelerated for your AI workloads. And what essentially this does, it creates SSD-backed local read-only cache for your GCS bucket in different zones, right? So, for example, if you have multi-region GCS bucket, let's say in U.S., and you have your compute running in U.S. East 5A and U.S. Central 1B, you can also create caches in these two zones. So you can now co-locate your compute as well as your storage into different zones. Again, this is fully managed by us, and it always returns consistent data. So you don't have to worry about making sure your cache is consistent. Yeah. This is exactly what it is. Now, some of the benefits from using Anywhere Cache is the fact that you get faster data access, reduced time to first byte latency for reads. You're avoiding any multi-region data transfer fees because you're not actually traversing the network, backbone network to go to a GCS bucket, but you're just taking the data from the cache. So it's pretty easy. And we see 70% of our customer with multi-region buckets seen costs, you know, savings with Anywhere Cache. All right. So the last topic I have for me is observability and monitoring. And one of the, you know, the latest feature that we have, this has been introduced by NVIDIA for quite some time, but we have done some improvements on this, which is DCGM. This is Data Center GPU Manager from NVIDIA. And it's a tool from NVIDIA that lets you kind of, like, manage and monitor your GPU. So it gives you that, you know, out-of-the-box monitoring for your GPU utilization, performance, and health. What we have recently launched is that out-of-the-box one-click capability, so that it's default on. We collect the metrics from GKE once you enable the option, and it takes it to cloud monitoring. So you have dashboards to kind of look into GPU utilization metrics, performance metrics, and I.O. metrics like NVLink and PCE as well. So that kind of gives you provision to do, like, performance monitoring, cost optimization, as well as do, like, capacity planning for your accelerator as well. When it comes to observability, we wanted to give our customer not just, like, hardware-level observability, but also application-level observability. So what we have for application observability is out-of-the-box observability for popular AI inference that you see here, the six model servers that we have. And, again, it's default on. It's free. It's one-click. And as soon as you enable it, it goes into cloud monitoring. It's integrated with GMP. That is Google Managed Prometheus. And you can query these metrics using PromQL as well. And, finally, we also have out-of-the-box monitoring for TPU for core utilization, bandwidth utilization, memory, and multi-slice latency metrics as well. All right. So I want to quickly summarize with some of the key takeaways. So we looked into evaluation onboarding, what was necessary for evaluation. There was, like, procurement of capacity, having availability, having, you know, tools like inference quick start for kind of, like, picking and choosing what kind of models you want, as well as features like CCC for, again, it gives you the flexibility to kind of fine-tune your workloads and choose your workloads. And services like DWS to have that extra capacity on top of on-demand capacity. For onboarding, we looked through different tools, like inference quick start for benchmarking, VLLM for TPU stack, as well as, you know, like model server benchmarking as well. And last one for data operation, we have gone through our load balancing innovation with inference-optimized gateway and some of the storage option and observability and monitoring. So with that, that takes to our last part of our session. And for that, I would like to invite Vinay Kola, engineering manager, Snap, to talk about inference for their ML platform. Thank you. Thanks, Shobhan Akshay, for that introduction. I'm Vinay. I'm here to talk about Snap's ML platform and how we're having to evolve it for the world of Gen.AI. Cool. So a couple months ago, we published a blog post about Snap's ML platform. It's kind of mind-boggling to go through the numbers sometimes, but we're training 100-plus models a day, each of which can be 100-plus gigs. And in order to do this, we're processing a trillion events per day, ultimately resulting in a billion predictions per second. So if you just think about it, like that in and of itself is a great achievement. However, the AI industry doesn't stay still. It's evolving, and we need to evolve as a platform with them. Now, Snap isn't new to Gen.AI. If you haven't used these Gen.AI-powered lenses in the app before, you should definitely try them out. You can add your furry friend of choice to your snaps and send them to your friends. So we've been using Gen.AI in our apps for a while. However, what has changed over the past year is that we're seeing a lot more appetite from developers wanting to experiment with Gen.AI for all of their use cases. And so that rate of experimentation is rapidly expanding, and we need to evolve our platform to help with that. So you might ask, like, Snap already has an ML platform. Like, what really needs to change? Why is Gen.AI different? And here's how I think about it. For our recommender system workloads, it's relatively easy to figure out if it's good or bad. There's precision, recall, AUC. We know how to tell whether it's good or not. In the world of LLMs, it's not that straightforward. There are open source benchmarks, evals, but they may not work for your use case. And so what we really need over here is use case-specific evals, and we need to help customers within Snap to run those evals. The next point is price and performance. For our recommender systems, again, we have optimized these workloads over the past five years to run optimally on CPUs and GPUs. But the world of Gen.AI just made that a little bit harder. CPU inference, at least right now, is not really feasible for Gen.AI. And so that limits our options in terms of running these in a cost-efficient manner. Lastly, with latency, that also changes things. Generally recommender system workloads, the latency is measured in milliseconds versus with Gen.AI workloads, it's going into multiple seconds, sometimes even minutes for video generation use cases. So that causes two sets of problems. On one side, from a user-facing point of view, we need to think not only about the latency of the whole inference, but just the time to first token, because that's what the user perceives. Additionally, from an infrastructure point of view, having requests that take, like, multiple seconds changes the way you need to think about traffic routing. So that's why things like inference gateway, et cetera, become useful. Spoiler alert. So let's look into how Snap is solving these problems. Here's an overview of what Snap's LLM inference platform looks like. We have a couple categories of production use cases. So we have things like content understanding, ad understanding, user understanding. We also have developers who are essentially iterating on their laptops using Jupyter notebooks for these production use cases. And the reason I wanted to highlight those specifically is because that's a really, really key use case for Gen.AI. Given the rate of change in the industry, we really need to empower developers to iterate rapidly, and that's why I wanted to call that out specifically. We also have a couple, like, developer productivity use cases, Enterprise.AI. All of these finally go to an LLM gateway under which we run API-compatible models. So this allows developers to, within Snap, to swap out models to see which works better for the use case. It also allows us as a platform to potentially optimize things in the future. So with this overview, I will go over the customer journey of how a developer at Snap starts experimenting and eventually launches something to prod, and how we're trying to accelerate that journey. So as Akshay mentioned, the first phase is evaluation. So the problems here are, again, it needs to be fast. You can't ask developers to wait weeks to experiment with their models. It needs to be within hours. We need an API abstraction so that they can, like, swap out models without having to redo their evals. And they also need access to GPUs, especially for larger models, and that's often a bottleneck for fast experimentation. So we solved that with a couple of, with help from our friends at Google. Inference Quick Start, this is a really quick way to figure out if a certain model is feasible for a given use case. It helps us figure things out, you know, again, in the order of minutes. DWS has been useful from a capacity point of view. Like, we can't wait for GPU reservations because those are often in the order of, like, weeks, but having timed capacity can help us run experiments without having to wait weeks. And as I mentioned, we're trying to have API-compatible models running internally so that internal customers can swap them out and test without having to change things. So let's say your testing is going well, like you're unblocked for experiments, and now you want to put this as an A-B test in production. Now we have a new set of challenges. Especially for larger models, you might need multi-host inference. This may not have been a blocker at the experimentation stage, but now they're putting in prod, it might become a blocker. Additionally, cost feasibility is another big thing. Once you're putting things in production, we need to make sure you're not burning your money. So how do we solve these problems? Leader worker set, this has helped us, like, run multi-node inference without having to rebuild the entire architecture ourselves. So we're able to ride on the industry momentum to run these models. Additionally, again, just on top of the, in terms of performance, again, using VLLM and TPUs, again, helps us make our production inference cost feasible. The name of the game over here is instead of Snap trying to reinvent this stuff from scratch, you want to try to utilize open source solutions so that we can get our production inference workloads in front of our users as fast as possible. So now, let's say you experimented locally, it's working fine. Now you put your new model in front of users, that's also working well, and you're launched in production, right? The journey does not end there. You need continuous cost monitoring and optimizations, and this needs to go through the journey. The journey never stops, basically. Otherwise, the finance team is going to come knocking on your door and ask you to, like, save some money. So in order to solve this, inference gateway is another key solution over here. This helps us keep the GP utilization of our clusters up. CCC is another way for us to experiment with various types of compute and keep our bills down. As platform owners within Snap, we need to invest in observability to give the other use cases within Snap info about how much money their inference is costing so that they can think about optimizing it over time. So now, through evaluation, onboarding, and, you know, what we call data operations, that's the journey of a new model eventually getting to prod at Snap. But before I let you guys go, I wanted to zoom out a little bit and walk you through how Snap's inference platform fits into a broader content understanding platform because ultimately that's one of our key business use cases. My goal at Snap is to make it easier for Snap's developers to experiment and launch their models to solve business problems. So what that means is I can't stop at just inference. I need to integrate with the rest of Snap. Otherwise, it just doesn't work. So for content understanding specifically, there are three things that we do beyond just inference to make it easy for our developers. So one, we integrate our inference platform with the pubsub streams that contain all of the content that's being uploaded to Snap. So this makes it easy to run near real-time inference on any content that's being uploaded and generate the LLM output out of that. We also make it easy to access LLM inference via Jupyter notebooks. Like I mentioned, just to make it easier for developers within Snap to run experiments. The last key point is we provide a way for developers to store the outputs of LLM inference in a feature storage, could be any storage. The reason why we do this is because it prevents repetitive computation of LLM inference on the same content. And this is especially useful at a place like Snap because the same features could be useful for multiple different use cases. You can think like content moderation, content search, they're all probably going to use a similar set of features. And so being able to store the output in a single place and have all of the downstream customers reference those features saves a bunch on cost. The key point I want to drive home over here is that you have to meet the AI or ML engineers where they are. we have to shorten the feedback loop from weeks to hours because that's the only way you're able to ship fast. That's all I had. Thanks, folks. Apl cerveza. . Thank you.