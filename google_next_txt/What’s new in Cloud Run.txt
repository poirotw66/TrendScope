 Hello, welcome, welcome. Thank you. Welcome to What's New in Cloud Run. I am Steren Janini. I lead product for Cloud Run, and I'll be joined by Yunong, who is a director of engineer. We'll have two customers with us on stage, Scott, DP of engineering from Replit, and John, senior software engineer at Wayfair. So today, we'll first recap what Cloud Run is. Then we'll take a look at what's new for developers, where we'll hear the Replit story. We'll take a look at new workloads you can deploy to Cloud Run. And then, because it's 2025, what's new for AI apps, where we will hear from Wayfair. So, let's go. Cloud Run. What is Cloud Run? Who knows about Cloud Run in that room? Yeah, I figure if you come to a What's New in Cloud Run talk, you probably know about it. But to recap, Cloud Run is a fully managed runtime to run your code on top of Google's scalable infrastructure. It provides a simple and automated experience with top satisfaction scores from developers. But it is also a very capable on-demand runtime. Hyper scalable, as we say. It scales from zero to 10,000 container instances in 10 seconds. And when you don't need it, it just scales down to zero. No infrastructure management, no pre-provisioning, nothing. All of that with a pay-per-use pricing model, where you only pay when your code is running. You have a free tier, and if you use it a lot, you should purchase committed use discounts. One thing about Cloud Run is that it is available everywhere Google Cloud is available. That's that simple. Just recently, we've added Mexico and Stockholm, I believe. And you can trust that Cloud Run will be here in any future Google Cloud regions. All right. Now, let's take a bit of a look at what's new for developers. Well, Google Cloud Serverless had Cloud Run and Cloud Functions, and developers had to pick between the power of Cloud Run and the simplicity of functions. Well, we've just merged the two as Cloud Run Functions. Simple. Now, developers have a new deployment option when they deploy to Cloud Run. The function, alongside containers, Git repository, source code, you can now deploy single-purpose functions. So one thing I want to highlight about Cloud Run Functions is the notion of per-instance concurrency. So this is a very important notion because traditionally, function-as-a-service products, including Cloud Functions v1, but also other very popular functions-as-a-service products, only send one event at a time on a given instance. So that means that if, for example, you receive three events at a time, three function instances will be started, and you will be paying 3x the price of an instance. But if you're like me, you know that many programming languages can process multiple things at the same time. Node.js has been designed for that, for example. That is why on Cloud Run Functions, by default, you have per-instance concurrency, meaning that if your instance can process multiple requests or events, it will be sent to it at the same time, reducing your latency and your costs. So if you look at the two charts, you find that when concurrency is set to one, to handle the same load, we needed like 550 instances, while when it was set to the default, only 150 instances were needed. This is quite powerful. We've seen customers drastically reducing their cost when moving from a traditional function-as-a-service product to Cloud Run. So that was Cloud Run Functions. A nice thing about it is that it comes with a built-in online editor, which now has built-in Google Code Assist, where the Google Code AI assistant helps you write the code of your Cloud Run Functions. But the best way is just to take a look at it in action. Let's switch to the demo, please. All right. This is the Cloud Run UI. I'm going to be demoing from the UI. I click write a function, and here, I'm just picking a name, next 25 demo function. That's all I have to do. You know, I select my runtime, Node.js. I'm a JavaScript developer. I'm going to pick a trigger, because of course, I can have HTTP functions, but I think functions are really useful for event-driven programming model. So I'm going to trigger on Cloud Storage. Here, Cloud Storage, all I have to do is provide a bucket, and this bucket, this function will be triggered anytime a new file is uploaded to Cloud Storage. I want to show you something else, which is that I can mount this Cloud Storage bucket in my Cloud Run functions. That's something new that you couldn't do before with Cloud Functions. So I'm going to mount a new volume, again, I'm picking the same bucket, on my function file system. So I've created a volume, now I need to mount it at a specific location. So I select the volume we just created, and I'm going to be mounting it at slash data. Then I hit create. Here, my function is created. Next thing we have to do is write the code for it. So as you see, we have a built-in editor in the Cloud Run UI. I'm more of a, a Node.js module person, so I'm going to be using modules. For the sake of the demo, I've copied already my demo, but let's take a look at it. So we import the Google Gen AI SDK, and I'm also using the read file and write file built-in Node.js functions. Here, I'm initializing my AI SDK, my Google AI SDK, and then my function will listen for Google Cloud Storage events. When it receives an event, it's going to be reading the file that the event captured. So the event is basically triggered any time a new file is created, so I'm going to be reading the file at slash data, which, remember, slash data is mounting the Cloud Storage bucket, so just by reading that file system, I'm actually reading in Cloud Storage, and the name, here is the name of the file. Then I'm going to be using Gemini 2.0 Flash to summarize the file, the file content. Once I have done so, I'm going to be writing back the summary back into Cloud Storage. Last thing I need to do is to add the Google AI SDK as a dependency. Google, can I type Gen AI? That's the new SDK, by the way, to do Gen AI. And then the version is, that's it. And because we are using ES6 modules, I need to do type equal module, type module in my package.json. That's a Node.js thing. We should be good to go. Google Gen AI is the name of the package, and here we have our code. Let's hit save and redeploy. So right now, this function code is quickly going to be built into a container image and redeployed as a new revision to our Cloud Storage, to our Cloud Storage. So you see, I can click and seal the build logs, but in the meantime, let's go to Cloud Storage. Here, you see there is nothing in it. And I have here a text file. This text file is basically a copy-paste of the Cloud Run overview documentation page. So you see, it's very, very long, and it tells you everything about Cloud Run. This text file, we're going to be uploading it to Cloud Storage, and it is supposed to be summarized by my Cloud Run function when it has been deployed. So let's take a look. It has built into a container, Cloud Run being a container runtime. Now this container is being deployed, and you see the traffic has been routed to that new revision of the Cloud Run service just now. So, moment for action. Let's drag and drop this file into Cloud Storage. Pretty nice. If you didn't know, you could do that. And now, the function is going to be triggered by event arc because this new file has been uploaded to Cloud Storage, and the function reads the file, calls Gemini, and writes back a summary into Cloud Storage summaries folder. Let's take a look. Here it is. I didn't cheat. This folder was empty. Can I download it to take a look at the summary? Yes, here it is. One line. Cloud Run is a managed, can I do zoom in? Cloud Run is a managed compute platform that lets you run container-write applications or build from source on Google's infrastructure, scaling automatically and offering both services and jobs for long-running tasks. Perfect summary of a very long page. My demo actually worked. Let's switch back to the slides, please. Thank you. We can switch to the slide deck. Thank you. So what you've seen in my demo is Cloud Storage volume mounts, which are available to Cloud Run services and jobs. Those are developers' favorites because they allow them to mount a Cloud Storage bucket like if it was a local file system. And so the use cases here include reading large media files they include also loading simple config files. You put your config file like YAML or Cheson on Cloud Storage and then you just mount it at the location where your application expects the config file to be. What we've seen is an event-driven Cloud Storage handler. That's the demo that I've done. And we've seen people using Cloud Storage volume mounts to load static assets, for example. another thing new about Cloud Run is that we've partnered with the Firebase team to create Firebase app hosting. So if you are looking at deploying Angular or Next.js or Astro or Next, go to Firebase app hosting because that will really give you a tailored experience for those frameworks. Firebase app hosting is directly built on top of Cloud Run and under the hood it is just a Cloud Run service, but there are some optimizations for those specific frameworks that the Firebase team has delivered. And finally, if you watched the keynote this morning, you might have heard of Gemini Cloud Assist. So Cloud Run is tightly integrated with Gemini Cloud Assist. First, it helps you design architectures where you can use a visual canvas to create your Cloud Run, your applications which include Cloud Run services. Gemini Cloud Assist can also generate Terraform config and G Cloud CLI for your Cloud Run resources when you chat with it. Google Cloud Assist helps you optimize your Cloud Run resources and here, Cloud Run is integrated into this new centralized dashboard which is called Cloud Hub. And finally, Google Cloud Assist helps you troubleshoot your Cloud Run resources. And here, in addition to playbooks that will appear when you have issues with Cloud Run helping you troubleshooting, what we've launched is also that Gemini Cloud Assist when you use Cloud Run function will actually send you a patch for what to fix in your function. And you see here the Cloud Run UI where Gemini Cloud Assist has made a suggestion for what to fix because the function was broken. Pretty nice. Now, I'm going to pass it on to Scott from Replit who built a platform for developers on top of Cloud Run. Okay. I'm going to try what Steeren did. I don't know if it's going to work as well. Hand up if you know what Replit is. Hey, pretty good actually. Pretty good. All right. I'm Scott from Replit. Our mission is to empower the next billion software creators. So we want anybody to be able to create apps and deploy them. So if you don't know Replit, our flagship feature is the Replit agent. That's where you can go in, describe the app you've always wanted, and then our agent will go and build it for you. So here's me describing an app I decided I wanted on the flight over here, which is just points of interest around Mandalay Bay that I might be able to walk to. And this is our agent proposing a simple MVP to get started and then kicking that off. Now after you have your MVP or prototype, you can add features, you can add user login, a database, payments, or just change the color of a button. And once you have it working how you like, you can deploy it to the whole world, whether you want just a few friends and family to see it, some coworkers, or maybe you have dreams of a million users. And you can do that with just one click. So who uses Replit? We see a wide number of use cases. Lots of people turn out to want custom software. A couple that I like to talk about, Kelsey, who works in PeopleOps, she built a new headcount tracking tool and she felt like she was able to build something on Replit pretty quickly that was better than what her HRIS gave her, which charges a lot of money, I might add. And then Ari built his whole business on Replit. So he built Merge Club. It's a place for entrepreneurs to find each other and get funding and work together. Now when we sat down to build the deployment side of this product, we had sort of a number of requirements that were really important to us. One, we had to be very efficient at small use cases like a headcount tracking app only the HR department works, and also power someone's dreams of having a startup that goes viral. We didn't want to be bound to any particular framework and it was really important that users could trust what they were building, that, you know, they put their dreams on there and as you move into businesses, they're going to trust it with user data. And so you can see we chose Google Cloud Run and we sat down with Steeren and team a couple years ago and we set out on this journey because it really hit all of those requirements. It was important for us to try and do this all in one platform because we don't want to teach users how to do migrations between deployment types. And if I could pick just a couple that really ended up mattering the most, I think that awesome scale to zero and execution time billing mattered a lot. It encourages people trying things and building softwares about trying many things and seeing what takes off. And then I think on the security side, we really appreciate when we go and talk to big enterprises who want to bring in Replit, knowing that we're on GCP is very important to them. It's a very trusted name in security. And we chose a particular scheme which is where every Replit user gets their own GCP project. So if you're considering Cloud Run for Platform as a Service, I would definitely recommend taking that approach. It makes our life easier keeping user data safe, but it's also a concept that people understand and trust when you're in those conversations. There's one more requirement, actually, that I didn't list there, and it's probably equally important to all of them, and that's we need to move fast. So Replit's a startup. We're in a fairly competitive space. And so one of my favorite Cloud Run stories is we were at a company hackathon. The team decided, hey, we'd like to have Replit scheduled jobs. I think it's like crons for Replit. And within a week, they had a prototype working. They could demo it at the Friday hackathon demo. And then in six weeks, we had something we could GA to all of our users. It was steady, reliable, and it's been running great since. And then the real test. The users actually like this. And this is where we've actually been incredibly impressed ourselves. This has got to be the most steady-growing product I've ever seen. Every week it comes in, and it just keeps the exponential going, and you can see our CEO shared this recently. But we now have over 150,000 apps hosted on Cloud Run. And again, it's running very smooth and steady. So thank you, Steren and team. In product management, when you see a chart like that, that's what we call product market fit. Congratulations, Replit. All right, so that was what's new for developers. Now, what new workloads can you run on Cloud Run? We've made many improvements to workloads which connect to your VPC networks. You know, I think it was last year we launched direct VPC egress, simply allowing you to put your Cloud Run services on your VPC. Simple. However, up until today, the Cloud Run instances were actually using a lot of IP addresses on your VPC network. So we fixed the glitch. We actually lowered the number of IPs needed when connecting a Cloud Run service to a VPC. We've also added direct VPC to Cloud Run jobs. So today, if you want to connect Cloud Run to your VPC, really use direct VPC. It should fit your needs, and if it doesn't, let me know. Next, a user's favorite. This feature has been pretty popular since we announced it. Identity-aware proxy built-in integration. So if you want to build an internal Cloud Run service, internal to your company, for example, well, now, it's one checkbox. Simple. You just check the IAP checkbox in the UI, dash dash IAP in the command line, and then, when users try to access your Cloud Run service, they just get a logging screen where they can log in with your company's authentication system. So in the past, that was possible, but you had to use a load balancer. Now, this is directly integrated into Cloud Run, protecting every single ingress path of the Cloud Run service. Next, to run the most secure workloads and enterprise workloads, we've collaborated with the Security Command Center team to create Cloud Run threat detection. So Cloud Run threat detection continuously monitors your running container instances for manitious actions taken in the container, like remote access or unusual behaviors or known vulnerabilities. So when these threats occur, they are surfaced in a centralized security command center dashboard. Next, a lot of customers need to be resilient to the loss of a region. So in case you don't know, Cloud Run is a regional service providing zonal redundancy. So that means that if Google Cloud loses a zone in a region, you don't even see it. We seamlessly migrate your containers to the healthy zones. However, customers might also want to be reliable in case of a regional outage. So this is where you can combine multi-region deployments with service health to do cross-regional failover. And in that case, your Cloud Run service will be deployed to multiple regions. and if one of those regional services goes down, then the load balancer will stop routing traffic to it until it is back to an empty state. Really allowing you to build high availability apps on top of Cloud Run at no additional cost. Next, as I shared, you know, Cloud Run was launched for HTTP or event-driven workloads. That's the Cloud Run service. Later, we launched Cloud Run Jobs. Cloud Run Jobs allow you to execute tasks to completion. Well, today, we are happy to announce the Cloud Run Walker Pool. Cloud Run Walker Pools are here to do continuous background work. They do not process requests. They are a set of instances which can just do things. And so, what can they do? We've seen customers using Walker Pools for pool-based workloads primarily. For example, you want to host Kafka consumers on Cloud Run including scale to zero. Well, now, the Walker Pool is the perfect use case for that, the perfect resource for that. And we are actually also going to be open sourcing a Kafka scaler that allow you to scale your Walker Pool based on your Kafka queue depth as well as other parameters. We've seen customers using Walker Pool to pull from PubSub. Like so far, Cloud Run services were PubSub push, where every PubSub method was pushed to your service as a request. Well, now, your Walker Pool can pull in batch your PubSub subscription. Here, you can auto-scale on CPU utilization or just manually, potentially on a schedule. It's up to you. Finally, we've seen people using Cloud Run Walker Pools to host GitHub Actions runners, where the Cloud Run Walker Pool will be the resource that builds your software triggered by GitHub. And much more use cases are to come once we add direct ingress connectivity to the Cloud Run Walker Pool instances. So these where all of the new workloads you can now deploy to Cloud Run. And we have to talk about AI. We have to, right? But also because you can do pretty nice things with AI on Cloud Run. First, we've partnered with the Vertex team to offer a one-click deploy from Vertex AI Studio to Cloud Run, where you prototype your, you tweak your prompt in Vertex AI Studio, and when you want to share it as a mini-app, you just click one button and it just deploys to Cloud Run. Scales to zero, so if you don't use it, you, of course, do not pay. Then, it's something I want to share is that Cloud Run is actually a good fit to run AI agents. So what are AI agents? It's a new type of application architecture where they use a model to reason. The model is their brain, but they also have access to memory. For example, you can use Firestore or Memorystore to store that memory. And the most important part is the orchestrator part, something that runs as a Cloud Run service and orchestrates coding tools, coding vector database, coding memory, and, of course, the large language model. So that orchestrator and serving actually can run very well on the Cloud Run service using frameworks like Lane Graph or the recently announced Google Agent Development Kit. So the benefits of Cloud Run for AI agents is that you can actually run any AI agent framework that you want. At the end of the day, it's just a container for Cloud Run. Cloud Run gives you out-of-the-box an HTTP endpoint which does streaming. And if you've used those chatbots, you know how important it is to stream the response back to the user for an optimal perceived latency. Finally, to call Gemini API from Cloud Run, all you have to do is just one API call. There is no API key to worry about, no credentials to worry about. It's all built into Cloud Run. And finally, as we've discussed, Cloud Run is scalable, cost-efficient, and reliable. But now, that AI inference can also work in Cloud Run with Cloud Run GPUs. Let me invite you, Nong, to talk to you more about this. Thanks, Nong. I'm excited to talk to you about Cloud Run GPUs. Like Steren said, Cloud Run is a great platform, predominantly in the past just for CPUs, and we thought it would be a great idea to bring GPUs with the same value proposition for Cloud Run. So what do you get with GPUs? On demand, scale to zero, fast starting, and pay by the second. We really want to call your attention to fast starting. It preserves the same sort of mechanics and performance characteristics as CPUs. I'm also very excited to announce that we're now generally available in four regions today with Cloud Run GPUs. So please go ahead and give that a try. So what can you... Thank you. And so what can you do on Cloud Run GPUs? Well, you can deploy and run your own model, open source models, or fine-tuned models. In fact, we even have customers who are doing fine-tuning of their own models right on Cloud Run GPUs itself. Don't worry, there's going to be a couple of demos that I'll be doing later to really show you the power of Cloud Run GPUs. So what are some of these use cases? Predominantly, we're targeting real-time LLM inference. Right? So think about the agents that Saren just talked about, custom assistance, on-the-fly documentation summarization, image generation. You know, we have a customer that's a global OEM vendor of smartphones, and a lot of the AI features, the image eraser, the translations. That's all done in Cloud Run GPUs today. Similarly, for media, for video processing, and 3D rendering. And this is where code starts are really, really important. You know, you're all big fans of Cloud Run. You know that it's a dynamic system. It dynamically scales up from zero to 100, back to zero. In order for us to do that, as your requests come in, we need to scale these GPUs very quickly. And so, our performance is five seconds from when there is no GPU or container to a container running with a GPU fully booted. And this is several orders of magnitude faster than anything that's available in the competition. And you'll get to see this in action in just a little bit. I'm also really happy to announce today, also, that Cloud Run jobs with GPUs is shipping preview. So just like services, jobs will be available with a GPU attached. So this is really good for things like fine-tuning, for a batch AI inference, for media processing. Again, all the great goodness of jobs, being able to execute them for a large amount of time, having different kinds of triggers. That's all available with Cloud Run GPUs. But now, really, the moment you've been all waiting for, which is some demos to show you how this works behind the scenes. So if we can switch to the demo, we still have the script from before. Okay. So most of you have probably seen the Cloud Run UI. Hands up if you have. Wow, the whole group. This is great. Awesome. I don't have to explain everything. This is perfect. So we're going to go deploy a new Cloud Run GPU service. I really want to show you, you know, today two demos. One, this one is really around zero to one. How easy is it to go from zero to one and build your own LLM app running on Cloud Run, right? That's production ready. And you'll see how fast it is. And after that, we're going to do a scaling demo, which is like, I've deployed my app. I want to scale up. My business is successful. Not quite as successful as Replit, but maybe getting closer there, and I want to scale. How is that going to work in Cloud Run? So let's start. I'm going to deploy service. Most of you have already seen this. Before I do that, though, I'm going to actually open the command line and install the OLAMA toolkit. So we're going to reconnect to my Cloud Cia terminal here. OLAMA is one of the easiest ways to interact and deploy LLMs. We have a really strong partnership with them. I do want to shout out that tomorrow at 11 a.m., we have a deep dive session on Cloud Run GPUs with Frank and Sager, who are sitting in the front right there. So you definitely should go to their session. OLAMA will be there along with our customer, Vivo, and they're going to go through a much thorough deep dive with GPUs with you there as well. But just a quick shout out here. So here we've got the terminal up and running. I'm going to install the OLAMA CLI. So while that is installing, let's see who wins, right? Is the installation going to finish first or are we deploying a Cloud Run service? Okay, cool. So let's do this. I'm going to use the OLAMA container image. So this is just an open source OLAMA container image. We're going to give it a name. We're going to call it Cloud Next 25 GPU. And then all these configs you've seen before, right, our region is US Central 1. I'm going to allow all authentic invocations. Here's the IAP proxy that you just talked about. We're not going to use that today. I'm going to switch my billing to instance-based. And then I am going to change the container port to 11434, which if you didn't know is LAMA, elite speak, so that's why. And then we're going to change the resourcing here for my CPUs. And I'm going to click this button, which is GPU. So here is where I can attach a GPU to my Cloud Run instance. Now, we support NVIDIA all fours today, and we'll talk about the road mapping a little bit, but we're going to support other GPU types that are coming soon as well. Okay, cool. So now the next thing I need to do is I need to bring in a large language model into my container. Now, I can do this in a variety of ways. One of the easy ways is just to build a container image with your model in it, and that just gets loaded when you deploy. Since we haven't done that, I'm going to bring that in through a volume mount. So I'm going to create a volume mount, and then, again, I'm going to use a GCS cloud storage bucket, and then I just happen to have a bunch of models in a GCS bucket. So I'm going to pick this Gemma bucket and select that, and then we're going to now mount it into our container. So we're going to mount this volume. I'm going to pick the volume we just made, and we're going to say root.olama. Okay, cool. The last thing I want to do is I want to shout out the networking team. As Sharon said, we do support VPCs, especially outbound VCPCs. So, for example, here you could do connect to a VPC for outbound patchwork traffic. I can route all my traffic to a VPC. So, cool. I think that's everything. Last time I did this, I forgot to add the GPU, which is hilarious, but now we're going to click create. What's that? Oh, that's right. Here we go. And we're going to pick no zonal redundancy. Actually, one thing is we also support zonal redundancy for Cloud Run GPUs. This is a good callout. So this is the same sort of ability to provision a regional service for your Cloud Run GPUs. So if a zone were to go down, we'd automatically fill over for you. So, again, increasing the reliability of service. Today, we're just going to use a no zonal redundancy option. I'm going to hit create. And we can see down here Olama has finished installation and my service is being deployed. So we're going to give it a second. And in the time that we just spoke, which is about five seconds, I have now deployed a large language model on Cloud Run using a GPU. And because we selected min instance equals zero, if I click the metrics here, we will see that my instance count is zero. Nothing exists. So, cool. So now what I'm going to do is I am going to now call my Cloud Run service that I just deployed. So we're going to do that using the Olama CLI. So I'm going to set Olama host to the new service that I just deployed. And say Olama run Gemma 2 2B. All right. So what is happening now is that we're actually experiencing, you're all experiencing a cold start with me live, right? So there's no container that exists. We're having to boot the machine, boot the GPU, install the drivers, install the large language model framework, install the large language model into the GPU, right? And so think about all the steps that has to happen for this to work. And so it takes about five seconds to install the GPU and get that up and running. And it takes a few more seconds, typically on the order of 10 or 20 seconds for the entire container to be ready in a stat that we call time-to-first token. And so boom, here we go. We now have a model run up and running, so we can say hello. Right? What is your name? Right? And so, you know, tell me about Cloud Run. Right? And so this is me going from zero to one on a production-ready large language model service that's serving my requests on Cloud Run. It's production-ready. You can all sign up and try this today. So we think this is the best way to deploy an application in Cloud Run. Thank you. We really encourage you to all try this out. You're all Cloud Run fans. This should be very, very easy for all of you. So cool. And now let's say my service has gotten somewhere popular. Again, not quite as popular as Replit, but getting there. And I need my service to scale. So the next demo we're going to do is a one to a hundred scaling demo. Right? So we just showed zero to one from a developer ergonomics perspective. The easiest way to deploy the GPUs. How about the quintessential question, does it even scale? Right? So let me authorize my Cloud style again. All right. So I'm going to go back to services. This time I won't deploy a service live because it's a little bit more complicated. But what I have is a stable diffusion application. And so what you can see here is, if I go to revisions, you will see that I have two containers here. I have a sidecar container and I have a TorchServe container running pod. And the sidecar container is what's handling the HTTP request. And the TorchServe container has an NVIDIA GPU attached to it. And what you also see is sort of the min and max instance size. Right? So I've got min instance size set to one and max instance count set to 200. And so if I go back and look at the metrics, you will see that, for example, we were doing some testing earlier, but we were at 153 instances and now we're back down to one. So there's nothing running. We're at the baseline. Right? And we're going to now send some traffic to my stable diffusion service. And I just happen to have a load generator here. That one of our team rolled up. So this is my load generator for stable diffusion. And so this is a separate Cloud Run app. Again, this is just a CPU app that's running the UI here as well as a load generator. So the first thing we're going to do is generate one image. So we can randomly generate a prompt and click generate image. So again, this is now calling out to that stable diffusion service that we just saw. And there's a lovely picture of a fox. Right? So that's interesting. And so now we're going to do or we're going to run a load test. So I'm going to say, hey, start, use 100 clients starting at zero and increment a client every two seconds and ramp that up. So click start. And you should see sort of on the right-hand side is the image gallery of the images coming in. And here we have some metrics. So what you're seeing here is essentially on the left, we have our infrastructure and our load generator. And you see that the load generator is incrementing every two seconds. Live demo, folks. And then on the right-hand side here, we see the number of back-end GPU instances as our system reacts to scale. Right? And so given that we set million instances to one and max instances to 200, we should expect to see the system scale along with the demand. So this is a really good simulation of the diurnal cycle as you're spiking up or a viral event. Right? Suddenly your app becomes very popular. You need to scale while Cloud Run GPUs will scale along with you. And so while we wait for this, actually, it's important. Maybe I'll take a beat and walk you through some of the back-end infrastructure and architecture of Cloud Run and what we're doing and that infrastructure and our ability to go support this sort of scale. Right? And so you can see, for example, our scaling system's now realizing there's a lot of scale coming in and the GPU instances and the GPU instances are now climbing along with it. So Cloud Run runs on a system internally at Google called Borg. And Borg is our production-ready planet-scale compute orchestration system. And it's also used by our other properties at Alphabet like YouTube, Search, Gmail, and Ads. And so that power and the flexibility and scale of Borg is really what allows us to run Cloud Run as a multi-tenant system that supports all of your workloads. Additionally, our autoscaler is built on top of a decade-plus of engineering effort from the Google Cloud team that's able to support this sort of scale that we're seeing in our customers. Right? So while this demo is running, I'll talk briefly about sort of a couple of different parts of the autoscaler. So one is what we call predictive autoscaling. And so what that does is it looks at the historical uses of your application. And so if you have a very nice sort of diurnal periodic function as your RPS comes in, we'll try to predict that and scale along ahead of sort of your scale on a daily basis. The other one is what we call reactive autoscaling. And this is what's happening right now, right? There's been no traffic and suddenly we've sent a whole spike of traffic. The system is very smart and realizes that it needs to be in reactive mode and so it very quickly starts to catch up as more clients get generated and the capacity of them and the number of instances on the back end will scale up much, much more quickly. So sort of with these sort of two different kinds of scaling algorithms, this is how we're able to enable our customers to scale in sort of the heaviest times in terms of traffic. In fact, just last month we onboarded a customer who is routinely scaling to one and a half million RPS in one region on Cloud Run. Right? And so that, again, shows you that power and the sort of the hardcore engineering effort that's behind Cloud Run that's able to support this sort of use cases. So as we're seeing this low test run, you now see the number of images that are being generated every second is going up to 10, right? You see sort of we're almost at 100 low generator clients and the back end CPU instances are keeping up, right? And sort of we're going to continue to scale as we go on and our client here is obviously pulling in the massive sets of images that's never been sent before to humankind because of stable diffusion, right, into the UI. And so I think this is a really good demonstration of not only is Cloud Run great for developers to go from zero to one, right, to get your app on running, but really, really, it really works incredibly well for your production-ready use cases. You know, we'll have someone, we'll have John from Wafer talking a little bit as well about how Wafer is leveraging GPUs for their production use cases, but certainly here you can see we're now at 100 clients on our GPU instances are climbing really quickly, right? Wasting all of TK's money. So I'm glad that, you know, this works out. So we're going to flip over. The other thing that I wanted to show is I'm actually going to click stop here because when I click stop, what you'll see is my low-generated clients immediately go away and Cloud Run will also then start to downscale your GPU count as well. So from a cost perspective, right, from an ROI perspective, you know, where you also get the cost savings as we scale down. So that was my demo. Next, I want to invite John from Wayfair to come up and talk about how they use Cloud Run. Thank you. Thank you so much. Hi, everyone. My name is John. I'm a software engineer at Wayfair. And today, I'm going to talk about how at Wayfair, we're leveraging Cloud Run plus GPUs to enable our next generation of customer experiences. So first of all, hopefully everyone here is familiar with Wayfair, but if you're not, we have a large e-commerce presence. We have iOS and Android, mobile and tablet applications. And then we have a small but growing physical store presence. And we really, again, aspire to be the trusted destination for all things home. And so one thing that I wanted to point out with regards to Wayfair's business is that shopping for the home is different. It tends to be more browse-based as opposed to search-based. Customers value individuality and we all, you know, have a highly emotional connection to our homes. And by and large, many products in the home category are unbranded and the category overall is very fragmented. And so for Wayfair, we have a bit of a challenge where we need to think about how we can enable a seamless browse and product discovery experience for our customers. And we also want to support new experiences for our customers like, for example, being able to search, buy an image and find products in our catalog that are very close to what you're looking for if not exactly what you're looking for. And so if you're not familiar with this concept of semantic search, I'll briefly outline it. It's quite similar to the concept of brag as well. But effectively, let's say you start with an image and you take an image. What you can do is you basically do some preprocessing of that image. You can do things like resize it to a standard size, maybe crop it or pad it with white or some background color or something like that and basically create a standardized input tensor which is basically RGB values for every pixel in that image and input that into a visual transformer or embedding generation model. And so an example of this in the open source world is Open Clip and you can basically choose different model sizes all the way from a smaller one which may have produced something like 256 dimensions as an output all the way up to larger sizes like 1024 dimensions or higher. And so a generated embedding basically represents the semantic meaning of that image and it's an array of like floats basically. And so you have this step where you generate an embedding and then you can do something like search an embeddings database or vector database. So in the Google context there's a number of options these days. One that we have opted for is vertex vector search within vertex AI which uses scan and is super scalable and we've basically generated embeddings for our entire catalog of products and all of our images which is well in excess of 200 million images and we can go and search that database for products that are most close to the input image. And again using embeddings is really the magic that supports that. So some examples of the types of experiences that this approach provides from left to right here. in our application now our iOS and Android apps there's a little camera icon and you can take a photo of any product and basically search our catalog using that image. And so in this example on the slide I have the coffee table at my house which I happen to know is from Wayfair but I searched for it and was able to find that exact product in our catalog. Similarly in the top right hand part of this slide on any of our product pages we really want to make sure that it's possible for you to browse for similar things to what you're looking for that maybe are slightly different. So maybe you're on a product page that's really close to what you're looking for but it's not quite there. We want to have a tray that makes it really easy for you to find something along those lines but might better meet your exact needs. And then the final use case at the bottom right here is something that's really exciting called Project Muse where basically as a customer you can describe the type of space that you're looking for your sense of style and sort of what you're looking to achieve and maybe even provide an image of your space. And then we'll use Gen.ai to generate a Gen.ai produced image of your space for you that if you like we can then go and actually show you products in our catalog similar to what you see in that generated image to be able to browse and shop further. So thinking about those types of experiences and the challenges that we have there's basically similar to what Scott of Replit was saying for their requirements the requirements that we had at Wayfair to support this type of use case there's basically three and again just to recap I'm specifically talking about this embeddings generation step which can be quite computationally expensive. So first of all we want a really strong developer experience within Wayfair we want to be able to maintain deploy, monitor, observe etc. that model that we set up. Two, we want that step to perform well. One point to call out is this type of search that I was talking about and in many of those use cases that's directly in the line of customer traffic and customer activity and longer searches mean less happy customers it's really important to get that response time down as low as humanly possible. And then the third point is cost where something like those open clip models that I was talking about earlier that in general they're doing matrix multiplication steps and there might be hundreds to thousands of layers of doing that and so using a GPU for the parallelism that it provides is highly beneficial but now all of a sudden you're managing cost and you're managing utilization across your CPU your GPU and trying to figure out how you can balance that most effectively. So at Wayfair we landed on using Cloud Run plus GPUs for this use case and so just going across and talking about why we decided to do that from a developer point of view if you're familiar with Cloud Run already it's exactly like that just now you have GPUs available to you so it's really easy to deploy you know at Wayfair we've got a more monolithic GKE deployment so it would involve a little bit more jumps to or hoops to jump through rather to get GPUs set up in the exact configuration that we need similarly you know GCE you'd have to do a lot more ongoing maintenance so with Cloud Run there's none of that it's really fast to scale up as Yunong just showed and we have great observability integrating you know in Wayfair's context we use Datadog and so we get correlated traces and all that sort of thing separately from a performance point of view because we have full flexibility to bring our own image into the Cloud Run and GPU instances that we set up we were able to really take a hard look at the actual code of how we're generating these embeddings and optimize it as much as humanly possible so our sort of naive deployment or our initial deployment was just using Python Pillow on CPU for basically image preprocessing and then PyTorch and TorchServe and we were basically able to go and say okay how do we optimize the image preprocessing on CPU maybe we can use Go for that where there's a lot better concurrency or overall it's easier to manage concurrency and then for the actual inference step we were able to convert the model from PyTorch and TorchServe to Onyx and Onyx Runtime Go with CUDA and then TensorRT and floating point 16 precision under the hood as well which really boosts the performance of the actual inference step with minimal accuracy loss and so we were able to see our per image basically our embedding generation response time improved by 85% at the P99 level and basically go from 500 milliseconds to 70 milliseconds which is really exciting and then similarly as Yunong mentioned currently with the L4 GPUs that are available they offer really strong performance at a reasonable cost profile and so combined with the fast auto scaling up and down we're really able to optimize our costs and we also saw an 85% reduction in cost so we've been very excited about the availability of GPUs on Cloud Run with that I'll pass it to Staren thank you so much thank you Scott so that wraps up what's new in Cloud Run what have we shipped this year a lot and if you want to take one picture for that talk that's the one I would take the team has been very busy what are today's highlights Cloud Run GPUs are now generally available serverless GPUs fast starting on demand that's Cloud Run you can build high availability applications by combining multi-original deployments and service health and finally a new worker pool resource allow you to deploy pool-based workloads to Cloud Run thank you for joining us