 Good afternoon, everyone. We are the opening act for the Killers, the concert later today. So thank you all for being here. My name is Brandon Royal. I'm a product manager on the GKE team. Really, really thrilled to be here in front of you. And thank you all for, I know today's been an incredibly long day, lots of content, lots of sessions. So thank you all for being here. You know, we've been on an incredible journey together over the last year or even a couple of years, how much this sort of ecosystem has evolved around building models and building agents. And so, you know, as a GKE team, we're really focused on helping to democratize AI, allow more customers to deploy their AI models, to deploy agents, to leverage the vast open ecosystem that is constantly evolving to help you build more powerful AI-powered applications and agents. So we're excited for the content today, and we've got some great speakers also lined up. So looking forward to diving right into it. So, again, my name is Brandon Royal. I'm joined by my good friend Abhishek, who's going to be joining me here on stage here in a little bit from NVIDIA. And we're really going to talk today about sort of this next level of scale. So, you know, we've been on, like I said, a long journey in the Kubernetes ecosystem. Kubernetes is, you know, 10 years old. And increasingly, we're seeing more AI models being deployed as well as other sort of non-AI models. And now that we've sort of gone past this first phase of, you know, building and prototyping, we're now at this level of sort of scale. So that's what we're really going to talk about is, you know, how do we deploy multiple models at scale and how do we leverage the efficiencies of the NVIDIA stack, top to bottom, to deliver some of that scale? And then how do those things work together with Kubernetes and GKE? So about a year ago, I actually stood on this stage, and I talked about the pace of innovation of AI models. And this is actually one of those areas that I always surprise myself. Every time I talk about, you know, we can't seem to get any faster in terms of the number of models and the power of those models and the efficiency of those models, they even can, you know, I keep getting surprised over and over again. Look no further than just the recent releases of things like Llama 4, like DeepSeq, like Gemma 3. So there's tons of very powerful open models out in the ecosystem, as well as great first-party models that are available directly from DeepMind. And so, you know, as a platform builder and as an AI engineer, you now have an incredible array of choices that you can build powerful agents with. And the ecosystem, of course, is also evolving around a lot of these models, and we're really excited to help enable customers to build powerful agents on GKE. Now, speaking of agents, you know, this is probably no surprise to anyone here in the room. The agents are coming. This is probably the one word you've maybe heard more than any other word through the course of the week here at Next. And our good friends, actually, at KPNG had this amazing survey. They really talked about, you know, how agents are becoming sort of the forefront of IT teams and, you know, organizations that are deploying AI. You know, 51% of organizations in that survey are exploring building AI agents, and another 37% are already piloting AI agents. And so this is moving incredibly fast. So, you know, this is really an important emerging technology that we want to all embrace. Now, the other consideration here is there's really no simple answer to, like, okay, which model do I pick? It's not a simple equation of, you know, just pick the most powerful model or just pick the most recent model. You need to think about sort of accuracy and reasoning. You need to think about, you know, what's the speed and latency that I want and throughput that I want? What are your underlying hardware requirements? What kind of licensing and usage? How much fine-tuning can I do? Right, there's a whole lot of considerations for, you know, how you pick the right model for the right use case through the right agent. And so, you know, our point of view is there really is no single answer. The future is really multi-model. So for platforms to provide multiple models to teams that they can consume via their agents, whether it's, you know, Gemini-based models or whether it's open models, whether it's the latest and greatest, whether it's a fine-tuned version, you want to be able to have choice as an AI engineer, as an ML engineer, to use the model that's most suitable for your use case. And so, you know, the GKE team has been investing quite heavily in providing the best platform for models and enabling technologies so we can do multi-model deployment, management, and governance. That starts, first and foremost, with superior price performance. This has been a huge area of focus for us. We want to deliver the best cost performance, price per token, overall training performance, inference performance across the board. We also want to help customers embrace an ecosystem of automation. And this is, of course, very important when we think about multiple models. We need to have automated pipelines where we can, you know, manage the deployment, the iterations of those models. We need to have cost controls and governance and all sorts of other things. And so, you know, we want to be able to leverage the vast open ecosystem that's been developing for over a decade to manage those models in production. And then the last is we need some intelligence around how we route those, you know, these requests to various models and even across multiple inference servers. So that's an area we've been investing in as well. And no doubt you've heard some of the announcements around things like GKE Inference Gateway and a bunch of other routing technologies that we're bringing to market. So for today's agenda, we're going to talk a little bit about some of the challenges of multimodel deployment. We'll talk about some of the technologies that we're bringing to market to help unlock velocity and efficiency. And then I'm going to have Abhishek from NVIDIA come up and talk a little bit about some of the work we're doing, or they're doing around NIM microservices and how we can deploy that on GKE. And then we have a very special guest, Jeremy from Major League Baseball, who's going to share some stories about how they're deploying agents for some really interesting use cases. So I'm excited for that as well. So let's talk a little bit about some of the challenges. So, you know, if you're, if you sort of believe me that, hey, this is a world of multiple models, we need to sort of serve multiple models out of a multi-tenant platform, what are some of the things that you need to be thoughtful of or prepared for? What are those challenges that you might ultimately face? So let's maybe break this down through the lens of an agent architecture. So if we have some kind of agent, like what are the architectural components that we need to sort of consider as we're scaling out the platform? So, you know, at the center, what we have in the agent itself, we have things like, you know, tool calling services. So we might extend those APIs so we can do native tool calling, whether it's an existing database or, you know, maybe it's a REST API to be able to essentially provide interfaces by which the agent can call those services. And there's some great protocols. And I'm sure Jeremy's going to talk a little bit about some of the work they've done on enabling that. Then, of course, there's, you know, memory and context and grounding. So these are services that are available. And, you know, the orchestration of those core components, like this is actually where Kubernetes and the cloud native ecosystem shines. So we can actually do this quite well today. We've been able to do this for quite some time where we can package up our artifacts, we can ship them through automation, we can have sort of microservices talk to one another in a very cohesive kind of way. Where things get more challenging is actually over on those green boxes. So this is actually where your large language models start to come into play. And now we need to think about, well, routing to multiple models. So we need some intelligence around how we route from model to model. And then we also need to think about how do we spread the load of multiple requests across multiple large language models, right? Which becomes quite challenging when we think about the agentic sort of context here. One request might take seconds, one request might take minutes. And so we need to sort of think about that architecturally as well. Now, you've probably seen reasoning models and maybe you've played with some reasoning models like DeepSeq as an example or some Gemini native models. You've probably noticed that these models take quite a long time to respond. They're doing quite a bit of work behind the scenes. And while all that work is happening, essentially that resource is fully utilized. And so we now need to think about how do we route requests appropriately to ensure that we're utilizing our resources most efficiently. We also need to think about sort of the inconsistency. We might have one small short request, one long request. And then we need to also think about that in terms of horizontal scaling. If we have any kind of horizontal scaling of our underlying nodes or our workloads, we need to sort of take that into account, which is a bit more challenging with large language models. So some of the areas that we're really focused on is helping organizations to do faster model onboarding and optimization. So as you're looking at maybe a new model, you want to onboard LAMA 4 as an example. How do you do that effectively so it's pre-optimized? So you don't have to do a lot of trial and error work in order to get to the model that you ultimately want or the performance that you ultimately want in terms of throughput or latency. The second is around for governance. It's like if you've got a shared platform and a fixed amount of resources, you need to have some kind of governance around how you share those resources, quota those resources. And then you also need to, as I mentioned earlier, have some automated pipelines so you can push those changes out to production rather quickly. And then the last is around how do you efficiently route requests and then do so with constrained resources. So making sure you can get access to the resources like GPUs when you need them. So in terms of the model onboarding loop, this is a sort of pattern that we're seeing increasingly from customers that are deploying many open models where we have sort of the discovery phase. We want to understand the performance of the model. We want to test the model. We need to do some kind of deployment. Maybe we're going to do some optimization, whether it be quantization or configuration of the inference stack. Then we need to sort of benchmark it and loop that over and over and over again. And our objective is we want to reduce those iteration loops down to effectively zero, if possible, whereby we can basically start with a pre-optimized model that's going to meet your performance objectives. And one of the ways in which we're doing that is we are introducing inference quick start recipes. And the idea of an inference quick start recipe is you can essentially provide your objectives in terms of throughput and latency performance. And then we will provide a set of configuration recommendations, including the underlying accelerator in order for you to meet those objectives. And this is all based on open benchmarking that we provide. We not only provide sort of the benchmarks and the recommendations, we also provide you the tooling so you can reproduce those benchmarks on your own. Now, in addition to providing the sort of tooling around it, we also have a nice user experience that we're launching as well. So you can actually access this directly within the GCP console. This is actually what it looks like. So now on the left panel, we have our AI ML section. You can see we've got a whole bunch of foundation models. There's around 3,000 models that we pulled directly from Model Garden. So we can go and look at the details. This one is an optimized model, which is LAMA 3. So we can go and do the deployment. And now you're going to have this new sort of deployment experience. We can see some specifics around the model itself. And you'll notice this section in the middle where we have sort of recommendations. So this is actually where we can use this small little slider to say, this is the performance that I want in terms of latency. And you can see how the selections change of which accelerator and which underlying configuration I need in order to meet those objectives. Then I'll just set some other configurations like my existing cluster, my namespace. Go ahead and hit deploy. And since I'm deploying this on a fresh cluster, this is essentially taking care of everything else for me. So my underlying infrastructure, my nodes are being provisioned, my deployment, my horizontal scaling, my observability, all of that is wrapped into this experience. So it doesn't actually go this fast. We're just speeding this up for demo purposes. And then we can see, you know, it takes a few minutes, obviously, to deploy. But then we can see the running model. Now, the other area that I mentioned before is this sort of idea of horizontal scaling. So if you have a single instance and you're making a request against it, relatively straightforward. But as you deploy multiple instances of that, you need to effectively route each one of your requests evenly so we can ensure we get a consistent quality of service for our end users, for agents in one example. And the way in which we're doing that is we recently introduced inference-optimized gateway. And the idea of inference-optimized gateway is instead of doing sort of round-robin requests and response, what we're doing is we're leveraging custom metrics that are being exposed by an inference server, like things like, you know, KV cache utilization or, you know, maybe the queue size or the batch size of a particular instance. And then we're routing those requests intelligently. And so that ultimately just means we're providing better performance out of the box for multiple instances as we scale. And you can observe that performance gain with some of the new observability of tools that we're providing out of the box as well. So you can look at your, you know, your overall, you know, throughput as well as your performance. And you can see that all at an aggregate level. So not only a single instance, but also all of your serving stacks all in the same place. So with that, I'm going to hand it over to Objek to talk a little bit about Nim. Thank you, Brandon. All right. Hello, everyone. My name is Abhishek. I am a product manager in NVIDIA Enterprise team. And I closely work on Nim integration with Brandon and team. Nim on GKE. Nim stands for NVIDIA Inference Microservers. This is the last time I'll say it out loud because Charlie here will kill me. We just call it NVIDIA NIM, the acronym. But essentially, when we talk to our enterprise customers, they mention these three pain points along with the performance of the model and deployment as well. The leakage of domain IP, they want their deployments to be in a private environment with their own enterprise data settings, right? They need secure access to the data and secure deployments within their own VPC. And the biggest pinpoint that they mention is getting from POC to production, right? Especially around the performance of the models once they deploy on NVIDIA GPUs. To address these issues, essentially, the customers out there have two options. These are not mutually exclusive options. On the left-hand side, you have managed API endpoints. For example, Gemini API, right? It's ready to use, ready to get started kicking tires quickly, right? It's the fastest path that you can use to get started with new, latest models out there. But then, there are certain limitations around infrastructure management, privacy, and limited control on the compute infrastructure that you can use for deployment. On the right-hand side, there is open-source approach, which is more DIY. So these models, especially the open-source models, right, are available on Hugging Face, and anyone can download it, run it, anywhere they want. They can manage it themselves. They have more control on the infrastructure. But a lot of times, these models are not optimized in a way to deploy on NVIDIA GPUs. So taking a look at both of these deployment options, NVIDIA has come up with something called NIM, which is an optimized way to deploy these large language models and not just LLMs, but also Gen AI models in a more performant manner on top of NVIDIA GPUs. So NIMs are essentially containerized models that can be deployed on the Kubernetes cluster. It can also be deployed on bare-bone compute on GCE, Cloud Run, or even Vertex AI. These are portable. You can choose any of the favorite servers on GCP and deploy it as a container, right, and it comes with prepackaged optimized libraries and optimizations that are needed to run these models. So for example, if you want to run a LAMA 3.1 70B model and get the best performance in terms of throughput and lowest latency, NIM is the way to go because it will give you out-of-the-box best performance out there as opposed to just using the open-source model downloaded from Hugging Face. We offer it as enterprise-supported, so we patch the security vulnerabilities and have a stable API for enterprises to use it in production environments and also guarantee performance improvement over time, right? This is one of my favorite slides which showcases the overall breadth of partnerships we have with original model providers. We work with all the mainly targeted open-source models out there in the Gen AI space and we nimify sort of these models to run in the most performant manner, right, on NVIDIA GPUs. These are hosted for experience on what we call as API catalog. It's simple, ai.nvidia.com. You can visit it today as well. It's a website where you can interact with these models. You can download the NIMH and take it and deploy it on whichever service on GCP you want, like GCE or GKE or Vertex or Cloud Run and leverage the underlying compute infrastructure available within these services. Now, we have a case with LiveX. They tested NIMH and got 6.1 times the performance. This was on LAMA model on A100 compute instance as a NIMH as opposed to just using the open source alternative out there, right? So, out of the box, you get the most performance out there and that results in better user experience for your downstream application. So, this is a simple demo that just showcases side-by-side NIMH off versus NIMH on. On the left-hand side, you have NIMH off, the open source solution out there, whereas on the right-hand side, it's the same model, same infrastructure, but as a NIMH. You can literally see side-by-side the perf difference between these two in terms of tokens per second. NIMH here in this case was six times more performant. Awesome. So, why have we collaborated with GKE team is mainly because of the points that Brandon mentioned. GKE provides a lot of flexibility and control to the enterprises, right, to manage their overall infrastructure, to deploy it as they wish and in their own private VPC. They can take a downloaded NIMH, right, deploy it within GKE, manage the cluster themselves, and also leverage the features that Brandon mentioned about, right, for deploy, managing the endpoints of NIMHs. Within GKE, you have a bunch of NVIDIA GPU compute instances offered, and NIMH runs on all of the supported compute instances for the specific models. This is a quick demo example of how NIMH runs and has been integrated within GKE. So, within GKE console, you have certain sample configurations and NVIDIA NIMH is also part of this. You can take a look today as well. It's been launched and available. There is description around what NIMH is, its value prop, and stuff like that, but you can click to deploy, takes you to a UI form where you can set your cluster parameters. You can select the specific compute instance you want to run in a specific region. Now, this UI has been updated lately for GCP Next, but once you accept and select a NIMH as well, just hit deploy, and the cluster gets created in the background. Now, this has also been sped up, just like Brandon's demo example, but once the cluster is up and running, you can quickly start interfacing with it. So, it's a seamless one-click type of deployment on GKE, which gives you the best performance out of the box. Now, the entire conversation right now was just along NIMH, which is highly focused on inference, but now we are having a new microservices called Nemo microservices, which will be more targeted towards fine-tuning use cases. You can use these microservices like Curator, customizer, evaluator for your fine-tuning operations and have like a data flywheel that connects your inference and fine-tuning portions on GKE, right? And on top of that, we have something called AI blueprints. These are use-case specific sample applications, right, which are targeted towards specific industries, specifically using certain agents, right, for your workflows. So, for example, blueprints will have reference architecture, some customized reference code. It's available on GitHub repo as well. You can check it out today. And certain tools that help you build these agents and deploy these agents on Kubernetes and other platforms as well. This is also one of my favorite slides. These blueprints, which are use-case specific, are available on build.nvidia.com as well, and it spans different verticals. So, we have certain blueprints for certain verticals, like healthcare-specific blueprints for drug discovery use cases, but there are horizontal blueprints like PDF extraction, for example, which could be used in different vertical use cases. There is an experience page on build.nvidia.com. Try it out. We have downloadable reference code for it as well. You can get up and running quickly leveraging NIMS under the hood. This is one of the blueprints that will be launched pretty soon. It's called IQ Blueprint that uses reasoning models for agentic workflows downstream. It's for deep research and reporting. You can deploy it on your favorite service on GCP, and it provides the overall observability and transparency right around what's going on in this flow with the best performance available out there. Awesome. I will hand it back to Brandon. Thank you, Abhishek. So we've talked a lot about sort of single model or single agent deployments and how to optimize and how to get started really quickly. But the question then is, okay, what happens when you want to deploy your next agent and your next model? How do you continue to accelerate your pace of innovation? And that's really where we start to talk about, well, now you have multiple clusters potentially that you need to manage as well as multiple models and all the sort of routing and sort of management that comes along with that. So when you think about sort of, you know, multi-cluster, right, you now have multiple models that you want to manage across maybe your production environment, your non-production environment, maybe multiple regions. We need to think about things like resource scaling, holistic optimization, and obtainability. So ensuring that we are, when we're deploying a model or an agent that it's effectively being routed to the right place to ensure that it's going to give you the best performance and optimization. And one of the ways in which we're doing that is through a new capability that we're calling multi-cluster orchestra, which is actually an open source capability that we're officially launching this week. I think actually technically we launched it last week. And this is actually super exciting because this is now a capability that you can essentially point to a single place that allows it to essentially route to multiple clusters. So now if you have, say, an agent and you want it to be deployed to the right place based on where the, you know, where those resources are or where it's going to deliver the best performance, we can essentially use custom metrics in order to do that routing and orchestration. And that's going to deliver better resource utilization for things like GPUs as well as better performance for your end users. The way in which this works, this actually leverages the underlying fleet management that's available directly in GKE. So GKE basically allows you to manage not only a single cluster but all multiple clusters together so you can have sort of policy and configuration essentially all in one place. And multi-cluster orchestra essentially allows you to say, all right, now we're going to use this admin cluster, which is the sort of central cluster that manages all the other clusters, as the point in which we want to route our workloads. So now I can say I want to deploy this cluster to the, or I want to deploy this agent or this inference end point to the right place, right? So however you want to ultimately define that, that could be based on performance objectives or obtainability objectives and essentially multi-cluster orchestra will take care of it for you. Now the other way in which we can approach this is through custom compute classes. Now the idea of a custom compute class is you can say for a workload what are my priorities of compute that I ultimately want to use? I'm starting from the highest priority to the lowest priority. So we can say maybe the high priority is reservations and then maybe on-demand versus spot or maybe different machine shapes. And that's ultimately going to give you a whole lot more control when it comes to obtaining GPUs for these workloads. And we also introduced what's called FlexStart. And this is great for inference workloads where you have sort of challenges around getting the GPUs that you ultimately need. And with FlexStart what you can say is I want this resource for this amount of time so maybe I want to run an inference workload for seven days for a certain traffic spike. And I can actually configure that all directly within my compute class so it can go through the priority and ensure that you can get the resources that you need. Now with no further ado I want to bring over Jeremy to talk a little bit about how they're deploying agents on GKE. Thank you Brandon. So hi my name is Jeremy Shulman. I used Gemini to make a younger version of me. I am a senior director of solution engineering and what that basically means is I take this amazing technology that all of our vendors and partners provide and I turn that into valuable business outcomes. So the team that I run is responsible for things like replay. So if you watch baseball you see instant replay. All the systems that are involved there. Network security broadcast engineering. So when I say the word infrastructure those are the systems that I mean. I think everybody might know about MLB. You know we bring baseball. I think the really interesting point here that I wanted to call out is that we're one of the most connected sports properties on the market which means we use technology to bring a genuinely amazing experience to our fans. Now when we peel back the onion of how how that happens with technology we look at our network as essentially the critical business connectivity. So we do things like WAN interconnects to all of our ballparks. We service ticketing. We have stadium Wi-Fi for our fans. But all of the game day operations that you might see like the actual broadcast of video streaming. The archival storage of plays so that we can play them back later. And then all the people that use data that comes out of all of the content. This is for stats, cast and statistics. And the many partners that use our data. They rely on our connectivity systems. So I'm really privileged to share our first agentic solution for infrastructure. I've been in automation systems for 10 years. I spent 20 years before MLB in vendor land. And now I'm bringing solutions in the real world. And what we've got, what I'm showing you here is a really complicated network diagram. Don't try to read it. But really we have a very complex network that interconnects all the ballparks and our data centers and our business operation systems. And what we needed to do was create agents that give us proactive response to any form of issue that happens on the network. We literally needed digital coworkers that rather than stare at, you know, our humans stare at dashboards and they look at Slack channels. We needed something that would instantaneously respond to issues, react to those issues, recommend what we should do next, and in some cases take action accordingly. We also need to build agents that allow access and collaboration for other operational teams throughout the entire organization. So if ballpark operations needs to do something or network operations needs to do something or security operations need to do something, we want those teams to be able to work cohesively through agents so they're not waiting on other members of the organization to do a thing to solve a problem. So when we talk about translating, you know, business objectives to business value, we really look at two problems. And the story that I'm going to talk about solves these two problems. One is the, what we call the aha to cha-ching, meaning we've got some idea of an automation solution that we need to build, and cha-ching, we have that. Now traditionally, we would have to spend, you know, months developing solutions. And what we wanted to look at was, could we take agentic technology and reduce that aha to cha-ching? The second big problem that we need to solve is the oh crap to all clear, which is, oh crap, something's going wrong in any one of these, you know, in our environment, and how can we get to all clear, the solution is resolved. So the particular use case that we're going to focus on are two of these. One is what I'm going to call connectivity agent. And this is a system that interacts with all the other systems, you know, all the physical properties of our systems, and then in real time, takes that information and makes a prediction, or it makes recommendation outcomes, and then we'll take action on that. So that becomes our digital co-worker of operations. We also have a network operations agent, which was something that we built last year before we got into the agent world, and what this allows is for a natural language interface so that other members of the operations team can interact with these systems rather than, you know, another way that we had been doing it. This is a natural language approach. So how did we build these systems? What does our stack look like? You know, on the top, what we are using is GKE to deploy the LL models that you've heard about and how Brandon's talked about. We've built a connectivity agent that's built on top of open source toolkits that also runs on GKE. And then the network operations agent we actually built using Dialogflow, which is, again, another Google technology that allowed us to drag and drop conversational workflows so that we could get that natural language experience without having to write code. And, again, that was the aha cha-ching. How quickly could we build a natural language interface for the network operations team? That was a critical piece of technology for us. In the middle are some of the SaaS systems that we use to interconnect. So we, as an organization, we use Slack as our means of communication throughout our lines of business. We have a system called Selector that takes in all of our metrics and logs and change records. That's the eyes and ears of our network. And it allows a natural language interface to query about what's going on. So that's our grounded information. And it has a lot of ML, RAG, all the things that you want out of a system like that. And then we have another system called Netbox, which is our really system of record that talks about what circuits we have, devices we have in our network. It's essentially our inventory system. So our main thesis, our main goal was to say, we have engineering playbooks. Like these are, you know, wiki pages, Google Docs, whatever. These are the documents that the humans look at. They say, when I see this problem, do these things take these actions? You know, and I think every infrastructure organization probably has a collection of these playbooks. Or they call them runbooks or whatever they, you know, used for that term. The idea was, is could we take an engineering playbook and turn it into a series of API prompts? And if we're able to do that, we can then shorten the amount of time rather than spending months creating bespoke, you know, AI bots or bespoke bots, and we can create an agentic solution. So we wanted to go from playbooks to prompts to a product, right? And so this is kind of the user experience that we wanted. We wanted to say, hey, we've got this agent. It responds to, say, a message in Slack. You know, maybe that comes from Selector. It says, hey, this event happened. It wakes up. It goes to Selector, gets all the information about that event, correlates that information, reasons about what happened, produces a report that a network engineer can look at. And then from that, the agent can then take an action like maybe swap over to a failover link or so. And it will then also open up a ticket with the service provider. It can do other things like, you know, provide hourly updates and create, you know, reports back to the management team. So the idea is this is a digital coworker that we want. So this is a demo. This is not sped up. This is in real time. So on the left, and don't try to read this. I'll narrate this. But on the left is the Slack. And so an event comes in. And on the right side, what's happening is, and this is really logs from our agent. It basically extracted the information out of the Slack message so we have the context of the event. And then it starts communicating in English, you know, basically an agent-to-agent communication between Selector and NetBox and this agent to determine what is the operational state of the network, answering like eight or ten specific questions that we would need to know, and then evaluates that answer into a set of yes and no qualifiers into a report, which is what's happening right now. And once it has all those yeses and nos, it's going to produce essentially a little report that says this is what happened. This is the event. These are my yeses and nos. And then it's going to say I'm going to open up a ticket with the service provider in this case. And this is kind of the report that a network engineer could look at. So something that would normally take hours to do is now down to, you know, less than a minute. And that is a half-million-dollar problem, you know, for somebody who has, you know, hundreds of circuits throughout their organization and deals with this problem. This is a really big problem. The other demo I'll show you is the agent, the network operations agent. And here we're just doing, using natural language. Again, this was built on Google Dialogflow. And in this case, somebody wants to bounce a port, which is a very common thing in networking that they want to do. And the idea, and what you're going to see here, this is, again, in real time. It's like, this is what I'm going to do. Go ahead and do it. So there's a little human in the loop there. It's going to perform the action to the network, and then it's going to be done. So, you know, going forward, the idea is connecting, you know, the connectivity agent with the network operations agent to make this whole workflow end-to-end and seamless. You know, why did we go on deploy on GKE? Lots of really great reasons. I mean, we have this amazing strategic relationship with Google. They're helping us on our journey. I run a very lean staff. They're helping us in many, many ways to come to the right solution with the right technology and really build our investment in agentic solutions. Again, time to product is always very important to us, and it allows us to go from, again, aha to chain. And in our type of organization, we always want to know how much something is going to cost, how we can grow and scale our operations, because this is just the first of many types of agents that we are thinking about building. So, you know, when we look back at, and again, we were able to build an MVP of the solution in a week. Like, I wrote a whole bunch of docs, a whole bunch of specifications, and we translated those specifications and the engineering prompts into an MVP product in literally a week. I was really amazed how the AI engineering, prompt engineering, can make that transformation. That was crazy. We can deploy and scale this in a way that we haven't been able to do before. And most importantly, we're able to build trusted and secure autonomous agents. And this has to do with the way that we're getting grounding data from our external systems and rationalizing them through very complex decision-making patterns. And really, this is transforming the way that work gets done. We have never done this in the infrastructure industry. Like, I think we're pioneering some very exciting work. We're really excited to be working with these great technology partners. And, you know, it's really just an amazing time to be in infrastructure engineering. So, I'm really happy about it. Give it back to Brandon. Thank you. Yeah. Thank you, Jeremy. It's so cool to see how quickly you were able to deploy a solution given the stack. And, you know, hopefully it's clear, like, how we're kind of building things up from the bottom to the top. Right? At the end of the day, really what we want to do is enable, you know, customers like Major League Baseball to deliver agents, to deploy models, to innovate incredibly fast. You know, while they're working on the first set of agents today, we fully expect that, you know, the rate of deploying agents is going to only accelerate over time. And so, we're super excited about the partnership and are looking forward to continuing to work with them. So, just to reiterate, the future is multi-model. And so, we see a huge opportunity for building platforms and infrastructure that supports multiple models so you can then, in turn, deploy multiple agents and bots that allow for the business value that Jeremy talked about earlier. You know, some of the key takeaways from sort of my perspective. We really want to help organizations to embrace the tools and technologies that allow them to bring agents to market faster, as well as bring the underlying models to market faster. And you heard some great technologies from Abhishek as well, you know, how NVIDIA is innovating in this space, how they're making it easier to, you know, provide optimized models, some of the work that the Kubernetes team is doing on, you know, deploying those recipes. We really just want to help you to bring those models and agents to market faster. And really, the second is just embracing this ecosystem. As you heard from Jeremy, there is no single solution. There is no single integration. We really need to think about a vast ecosystem of integrations and tools that we can use in order to build agents and to bring agents to market. And so, really embracing that technology and using open source wherever possible is, you know, something that we see quite a bit and we recommend. And then the last is really plan for tenancy. So, if you're in that, you know, category of customers that I talked about earlier that are sort of in the early exploration phases of deploying agents, definitely plan for tenancy. Think about not only what your first agent is going to look like, but what are going to be the requirements for the 10th agent, the 20th agent, the 100th agent. Because as soon as you start to establish this pattern, just as you heard from Jeremy, the next use case is going to quickly emerge and you're going to have to start thinking about this. So, think about tenancy, think about sort of governance and control and automation along the journey. So, we've got tons of great resources for you to go and check out. So, we'll kind of leave these up so you can look at the links. If you want to get started with the inference quick start recipes, you can go ahead and check those out. So, those are all available on GKE today. You also, you can see the NIM demo, exactly what Abhishek did earlier. You can actually check that out directly with this link as well. So, you can get started with NIMS today directly from the GKE console. So, super easy to use. And then if you're interested in multi-cluster orchestra and you want to try that out, I link to some of the documentation here as well. you