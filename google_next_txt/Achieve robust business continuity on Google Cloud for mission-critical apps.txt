 Tim Isaacs Hello, everyone. My name is Tim Isaacs. I'm going to do a quick summary of our session, Achieve Robust Business Continuity on Google Cloud for Mission Critical Applications. All right, so we're going to cover two things today. The first is key considerations for business continuity. We will cover the architectural as well as the operational things you should be thinking about when constructing your business continuity strategy. And then we will hit upon a very interesting case study with Charles Schwab on how they were able to take a lot of these principles and apply them in practice. All right, so what are some key considerations here? We know that mission critical apps need to be tolerant to various different kinds of failures and interruptions with little or pretty much no downtime. And that normally translates to five nines of availability. So the first step in ensuring this is to make sure that you're actually deploying appropriately. Multi-region deployments, where you deploy your application across multiple zones in one region and multiple zones in another region, are typical for achieving this. There are two types here. The first is where you have a primary region, where you're serving your application, and then you have a standby hot or warm region. And then the second type is where you're actively serving out of both regions at the same time. Active-active deployments tend to be more complex upfront, understandably, but they do simplify recovery, especially disaster recovery, where a full region were to go down, then all you're doing at this point is redirecting traffic versus trying to actually do a failover. The second key piece here is some other architectural considerations, such as picking the right services. Of course, you would pick the services based off of other business requirements as well, but one thing to note is that services have different reliability characteristics. If you're a zonal service, you can tolerate failures of certain kinds. If you're a regional service, you can tolerate more failures. If you're a global service, you can tolerate more still. So it's an increment, it's an increasing level of reliability you get as you go to higher scope services. The second consideration here is capacity planning. I think, generally speaking, most people do a good job of capacity planning for steady-state workloads, i.e. steady-state traffic. But you should be accounting for peaks, and sometimes even more so than what your peak traffic might look like. A second consideration here is how have you capacity planned for a disaster or a failure scenario. Typically, in those scenarios, you're going to lose some serving capacity, and then you have to fail over to another zone or another region. Making sure you have the available capacity there is going to be important. And the third piece here is backups. Of course, you probably have backups, but it's important to note that off-site backups here would be more resilient because it's in a different spot. And backups, of course, ultimately are your last line of defense. If all else fails, you still have your backups. Let's talk operational practices. Without good operational practices, you can't really support a good architecture. And there are many things here, but I'll just focus on three. Safe changes, super important. Make sure you have a strategy that thinks about canary testing, thinks about blue-green deployments, where you actually roll out to a portion of your fleet before you broadly roll out. And you also want to think about a good, robust release strategy. Second thing here is failure testing. Of course, you should be doing testing at all stages of your development process and your rollout process. But failure testing is an important call out because here you want to actually fail components of your application in a pre-production environment to see whether your application can actually tolerate these failures. So it's the true test of all the hard work you've done thus far. Last thing here is incident detection and response. Doing this with some SLOs in mind will actually help you know whether you can meet certain business requirements when outages or incidents actually happen. Last thing I'll say here is all this is ultimately a quality game. So the more you can left shift quality in your development process and do more testing at each stage, the better your ultimate outcomes. So now let's take a look at what Charles Schwab did for their login application. Login is a mission critical application for Charles Schwab and they did an excellent job of bringing some of these architectural and operational practices to life. So let's talk resiliency first. The login app is deployed across multiple zones in multiple regions. There's some self-architecting of replication and of course there's off-site backup. So a pretty solid architecture when it comes to redundancy and resilience. The second dimension here is security. And as you can see, security is fairly end-to-end, all the way from infrastructure security for their GKE clusters, to identity, to firewalling, to making sure the software supply chain is secure, to encryption, to regulatory compliance. Third dimension is ensuring safe changes. We've discussed that already a little bit. And here there's a fairly sophisticated strategy that has a progressive approach to rolling out changes. And there's a whole lot of automation in how this is done and there's supervision along the way. Incident management is SLO driven. The whole idea is you should be able to reach and provide an SLO for things like how quickly do you acknowledge an incident? How quickly do you resolve it? How quickly do you mitigate it? And then how are you able to actually get to root cause so that you can go fix the root cause and this doesn't happen? So incident management strategy here too is fairly sophisticated. So what are some practical takeaways? I think many a time we build architectures on paper, we test them out in our test environments, and everything works out well. But then we find bottlenecks in production. So one way to make sure that doesn't happen is to test your applications with real-world loads and do so at scale. It's sometimes easier said than done and sometimes costs a little bit more money, but it is worth it. Because, you know, that means you're actually catching some of these big issues up front. Change management. You can't gloss over this. This is a very important area where you have to plan strategically. And there's everything from how you want to plan out your releases to how you want to do rollouts. So a lot of things that go in here, and Charles Schwab has done an excellent job of this. One very interesting thing that Charles Schwab does is the notion of game days. And this is effectively bringing all of the stakeholder teams together and bringing all the pieces of the application, which is not just, you know, software, but it's software, it's processes, it's people. So all these things are being brought together to see how it works if something bad were to happen. And it goes without saying, testing should be at every layer of your development process, and as well as your deployment process. All right. So with that, thank you so much for joining me today. It was a pleasure. We have a full recording of the session as well. If you want to look into the details, I encourage you to take a look at that. Thank you again, and have a great day. Thank you.