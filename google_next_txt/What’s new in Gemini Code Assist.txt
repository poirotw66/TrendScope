 We're going to talk about Code Assist. My name is Amir Abbas. I'm one of the product managers on Code Assist with Google. I'm going to have some help with my friends. We have my friend and colleague, Andrew Hockman, another senior product manager on Code Assist. He's going to talk about some cool new things you can try today for free. And then we have our friends from Sentry. They're going to talk about the integration with tools and Gemini Code Assist and all the cool stuff that they're doing. Lots of stuff to cover. Before we can talk about what's new, we're going to just take a quick retrospective back in 2024. We've come a long way. There's a lot of lessons we've learned. 2024 was a good year of learning for education for us. We're going to share some of those insights. We're going to talk about some of the industry trends. Andrew's going to talk about some of the key product themes and some of the cool features that you may or may not have heard of, some new models, some new things that you want to try, give you a sneak peek at what's coming for the rest of the year. And then Cody from Sentry is going to talk about some of the cool stuff that they're doing with their integration with Code Assist. We're going to go back one year from today. Same casino, same everywhere. Back in 2024 at Next, we relaunched or rebranded this product as Gemini Code Assist with the first sort of generation of Gemini models, the 1.0 Pro. Models don't last very long, so just a few months later, we upgraded the model to 1.5 Pro, which was at that time sort of the latest and greatest thing. Around September, October, we forked the product into two editions. We had Standard Edition, and then we also had Enterprise Edition. And Enterprise Edition had these features like code customization, which gives you contextual code suggestions and code completions based on your Enterprise code. I think we're going to talk a little bit more about that. And then we ended the year by announcing Code Assist tools, where you can access to commonly used developer tools. We announced Code Assist agents. Andrew's going to talk a little bit more about that. I'm sure everybody's very excited about that. And then we also upgraded the model to, again, back then, Gemini 2.0, which was the latest thing. So lots to, you know, there was a pretty good journey in terms of model improvements, how we've sort of grown the product. It was also a year of learning. Our friends at DORA that have been gracing us with these state of DevOps reports for over a decade, 2024 was the first year that they included the effects of AI in software development. And we had some sort of some key insights. And what this is really saying is that we found a lot of positive forward momentum towards AI initiatives, towards developing AI. That first number has nothing to do with software development. All it's saying is that majority of the enterprise are saying it's imperative for their business to start developing AI-infused applications. Check. That's a good, you know, good step in the right direction. The second number is basically saying that, you know, three out of every four developers are already using AI for code-related activities. And these could be things like code writing or summarizing, code explanation, problem solving, anything toilsome that they do. That's also good. And a lot of those developers are saying that they are actually seeing meaningful productivity gains. So when you combine those three numbers, there's a good amount of sort of top-down forces that are saying, hey, it's imperative for our business to infuse AI into our applications, but also bottoms-up, where developers are saying, hey, this thing actually works. It's helping me. And very rarely do you get a technology where you get both top-down and bottoms-up forces, and I think that's where good sort of cultural shifts happen. If we look at specifically around coding improvements, again, we saw some very positive numbers. So 67% of the developers said that they saw marked improvements in code-related activities. Again, check. Good adoption, good trust. We also saw this idea of developer flow, developers staying in flow a lot longer when they use tools like that. So we started seeing, you know, AI sort of transforming how we do software development. We also saw some negative effects, and this was a very good sort of key point, a key sort of focus for us. We saw this negative correlation to reduced stability. All that means is the more you use AI for software development, we're pushing more errors into production or we're declining the code quality, right, which makes sense because you're just pushing more code out there, so you need to kind of have checks and balances throughout the software development lifecycle. We also saw that there's still a lot of distrust. So 39% of the people say that they have zero or very little trust. So on the one hand, you have these sort of positive momentum forces of this idea of adopting AI, but then you have these sort of negative forces of declining quality, reduced stability, as well as reduced trust. And then this is the problem that we're really trying to solve is how do we not just focus on developer productivity? It's great. That is a means to an end, and that end is the entire sort of software delivery performance. So EnterCodeAssist, this is a few words that just kind of explains maybe the vision of what the product is. So it is an AI-powered, built for enterprises, so, you know, the operational controls, the security, the privacy, and the indemnity that comes with it. So SaaS portfolio of products, it's multiple products, and it's completely, you can consume it. You don't need to know anything about AI. That enhances the entire software development lifecycle. So we're not just focused on what's going on inside the IDE. By providing you with contextual code assistance, so tailored to your enterprises, every enterprises have, you know, different coding styles, different standards, different, you know, libraries, and those types of things, focusing in on quality and performance. So you notice that productivity, again, becomes a leading indicator into performance, into software delivery performance, which is maybe in the world of software development, that's the end goal. A few weeks ago when we created this slide, you know, Gemini 2.0 was sort of the latest and greatest model. Fret not, Gemini 2.5 is in private preview. We can get you access to it. But currently, if you use the product, it's Gemini 2.0. So it's a pretty good model with one million tokens. And we're consistently improving models. You know, it's a model that kind of becomes a leapfrog sort of a thing. You know, every two weeks, you know, two months, six months, we get a new model, and we keep improving it. So the new benchmarks for Gemini 2.5 are very promising and very up to speed with some of our other vendors that are coming out with the models. When I look at this product, I see it as these two sort of semicircles that are mushed together. On the left-hand side, you have the model. And the model is trained on the world's data. And, you know, it's going to give you, statistically speaking, sort of, you know, middle-of-the-line code. And that's sort of that left side, the blue side of the semicircle. And it's going to continuously improve and improve and improve. On the right-hand side is where the context piece comes in. That's where the one million token comes in. And really, the quality comes from infusing the context with the prompts to give you much more meaningful and contextual responses. If you look at the left-hand side, there's a lot of variability in how users use this tool, right? Some people are just great at prompting. Some people are not. Some people can be trained. Some people trust the product. Some people don't. So you'll see a lot of variability on this side on how they interact. On the right-hand side is where you get, you know, the meaningful thing, the quality piece of it. You know, we talked about this idea of declining stability. How do we attack that? We think of these contexts as three different categories. At the very top, you have project context. So think of that as the project that you're currently working on. So that's everything in the developer's workspace. So the repos you're working on, the applications that you're working on. With local code-based awareness, we can determine your intent and automatically pick the files and folders that are needed to, again, augment the prompt. Or you can explicitly define files and folders or even entire repos. That's where the big, long context window becomes very important. Enterprise context is anything that is not inside of the developer's workspace. So think of them as your golden standards of writing code. So you have your own libraries, your own SDKs, your own patterns, your own policies, anything that you need to make sure that everybody is writing according to those standards that are very specific to you. We index those repositories. It doesn't matter where those repositories are. It could be in any source code management solution. And then we use that as a RAG database, again, to enrich the prompt to give you much more meaningful, high-quality responses. And then at the very end, you have the engineering context. Then that's anything that is not code. So developers use tons and tons of different common tools. So you may have some design docs or some PRDs in Google Docs. You may have some tasks in a project management system like JIRA or some GitHub issues or GitLab issues that you're trying to solve. And, you know, sky's the limit. It's not just what's happening inside the IDE. How do you break out of the IDE? Get that context in time without breaking flow. And again, augment that prompt to give you better responses. Now that you have all this context, you want to have operational control over it. What gets indexed? Who has access to which parts of the index? What are people allowed to send to the model? And also, what about just the deep insights and verbose logging? You know, so you can get acceptance rates. You can get adoption metrics. You can slice and dice them down to the developer level, the IDEs that they're using, the language that they're using. And it's not to just, you know, pinpoint on who's doing better or not, but it's to kind of figure out where are rooms for improvement? Where do you need to put more pressure, put more training towards, right? So we give you all of those types of things. And like I mentioned, that there's a lot of variability on that left-hand side as you're prompting. Some people are just inherently good at prompting. And so Andrew's going to talk about agents where we can take some of that variability away by pairing with these agents that can take some of that burden away from you and give you better quality responses right off the bat. So this is kind of how I see the product as it stands. I'm going to show you a very quick demo, and then I'll invite one of our headliners because I'm just the opening act. I call this demo the plan, build, push demo, right? So let's say that I'm just a backend engineer and I'm building an API. And in order for me to do that, I've got to first plan it. I have to make sure that I have, you know, maybe some design doc or PRD where it tells me exactly what I'm going to build. Then I'm going to actually build something, and I want to make sure that that is contextually relevant to my organization, to my project, to my enterprise, whatever you want to call it. And then I'm going to push that into GitHub and have it review my code. So for getting the requirements, I'm going to use Code Assist tools. I'm going to, you know, assume that this PRD is in a Google Doc, so I can, from within the IDE, I can get that PRD, and I can understand what exactly is happening. Then I'm going to create an API spec using Code Assist for Apigee, which is aware of all of the APIs in my organization, and it gives me, again, contextually relevant responses based on those APIs. Then I'm going to actually code the backend. And again, I want to make sure that I have, you know, maybe in this particular case, every time somebody creates a backend service, there is a set of standard backend routes that every service must have. And I want all my engineers to write those. So I want to use that code customization feature to make sure that I'm veering in the right lane. And then lastly, pushing it to GitHub, we're going to use the code review agent to give me this sort of extra line of defense for, you know, testing. Maybe there's something I missed, and it's giving me sort of that type of higher quality thing. So here I am inside of my IDE, and I'm going to type at Google Docs. So this is invoking the tool, in this particular case, Google Doc. And I can just ask a natural language prompt. I can say, you know, in this case, list my doc. Now, I can go into my PRDs, but it's very long. But I can also ask other questions. I can ask it to summarize that for me, right? And maybe I just want to quickly just get an idea of what this application does. I'm only interested in this APIs and dependency section. So I can even say, explain just that one section for me, so then I can see exactly what I need to create. So as I'm having a conversation with it, everything becomes context. So here it says, I need to create an API spec to manage packages. Okay, so let's do that. So I'm going to say, create an API spec to manage packages. Now, note that it's a very generic prompt. I didn't say what a package is. By the way, this is a shipping API, so this is literally a physical package. And as it's giving me this spec, if we scroll down to where that package component is defined at the very bottom, you'll see things like it has height and width and depth and weight and special handling instructions. Where is it coming that from? That's not just coming from the model, because the model has no idea how I do it. If we go to the Cloud Code extension, which is another plugin, there's an Apigee section. And inside Apigee, we have API Hub. And API Hub has all of my existing APIs registered. There's an API called Tracking API. Let's click on that. And if we open that up and look at some of the components defined in that, there's already a package component that somebody else in my organization has defined with the right schema. So as I said, hey, I need an API to manage packages, it sifts through the APIs inside the API Hub and tries to reuse components. So again, it's consistent code. Okay, so now that I have the API, now I have the API spec open, I can just create a prototype. So here I'll say, using this spec, which is open here in the side, just give me a Flask app that gets some package information given a package ID. But this time, I want to use a local file. So I want to use a local JSON file called packages.json. So I'm explicitly defining what context I want to use. So it's given me some code, and if we scroll down, it tells me what context is being used. So of course, it's using the shipping API, but it's also using that packages.json file. So that's local code-based awareness where I can explicitly define which files to use. Now, this is great. This is just a boilerplate, sort of a scaffolding of an app. As I mentioned, that every time somebody writes a Flask app, let's say in this organization, I need a set of standard handlers, and they need to be defined somewhere. And I don't need them for every single, you know, with every single developer inside of their local environment. They're defined somewhere else. So let's call this, you know, this shipping repo, the golden standards repo. And that has been indexed. And you can define what gets indexed within a repo. So we have this file called .ai exclude. It works very similar to a .git ignore. It has the same patterns that you can define and say don't index these files. Maybe these are sensitive information and do index the other ones. So here I have this file called backend handlers. And this is the file where I've defined those set of standard handlers that I want everybody to use. And so, for example, I have this discovery slash discovery route that I want, and it has a very specific schema. So name and version and owner and those types of things. And maybe this is a microservice, so I have this slash liveness endpoint. Again, very specific schema with status, code, and timestamp, and then readiness one. These are the ones that I want to use, and I want all my developers to write these things. So let's go back into the IDE. We've already indexed those. I'm going to clean up my Flask app. And just for the sake of posterity, we'll just, you know, pull that PRD again. So we're asking, hey, explain that API section to me again, just so I know, like, what are those standard routes? Maybe I don't even know these, right? So I'm just asked, like, hey, just explain the API's independency section, and it's going to say, you know, you need the discovery route, you need the liveness and the readiness route. Now, I can implement this, or I can even ask questions. I can just say, well, explain what this route does. So I just said explain the discovery route, and it even says, like, I'm pulling that from that backend routes.py file, and it's explaining what that particular thing does. Okay, this looks great. Let's go ahead and implement it. So I'm going to say implement the discovery API backend route. Again, I have not defined the context. It's pulling the relevant snippets of code from that RAG database, and it's giving me contextually relevant information, upping the acceptance rate, right? So it's matching that. I don't have to use my brain. I don't have to do context switching, go to some repo or some wiki page to figure this out. I can pull that in. It shows me a diff view. I can accept this. This is, of course, showing you inside of the chat, but I can do the exact same thing inside the file. So I've already written the discovery route. Maybe for the liveness one, I'm going to just start writing my code. So at app route slash liveness, and as I'm writing code, it's finishing that thought for me. So it defines this function, and it needs to return that same JSON. So if I type return, it's giving me that status code timestamp, which if we go back to the repo, you can see that it's the same one. So let's just say that I've worked on this app, and I've been doing some other things too. So I'm going to now commit this into my test branch, and I'll create a pull request. So here I'm creating a pull request from test domain. I'll give it a title. Hey, I just created some standard backend handlers, but I was also working on some unit test files that I was working on before. I'll give it a description, and that unit test file, maybe, you know, I was kind of in a rush, so it might have some errors. As soon as I create the pull request, a reviewer gets added, a Gemini Codacist review agent gets added. It gives me a summary of all of my changes, file by file, what happened, and also does a review of, you know, what are things I can do to improve. So in this case, it's seeing that there's a lot of sort of string literal errors. I was missing single quotes. I think there were even some spelling mistakes. Again, I was in a rush, and I did not use Codacist for this particular thing because I thought I knew better. And if we scroll down, you can see that it's, you know, main is spelled wrong. And I can now accept these suggestions in line here as part of this code review. I can batch them up. I can ask questions around them. I can say, hey, what is this particular thing doing? And again, this becomes a pair programmer as an agent for me where I can tell exactly, you know, how to improve the quality of my code. So yet another thing there. So hopefully that just kind of give you a general sort of a narrative in terms of how do you plan things using Codacist tools to pull the relevant pieces of information. Then when you're actually writing code, how the tool is aware of your enterprise context, whether it's APIs or whether it's your coding standards, and it's giving you suggestions based on this so you can stay in flow longer. And then we have this extra sort of layer of protection built in with the code review agent. It kind of follows you around where you are. So with that, I'm going to invite Andrew, who's going to talk about some of the product themes and some of the cool new features you can test. Awesome. Thanks, Amir. So I am super excited to be here to talk about kind of our focus for the coming year for Gemini Codacist. A couple of big themes. So first, focus on the developer, the developer experience. We want to make this fast and easy for everyone. A focus on the enterprise. Being able to make Codacist useful for very large organizations with tons of developers. And focus on the entire software development lifecycle so that you are not just looking at the coding task, but everything that makes you productive. So let's start with the developer. Codacist for individuals and Codacist in cloud IDEs. I'm going to touch on both of these. Codacist for individuals. This is available for free today to all of you. This is specifically for students, hobbyists, freelancers, and startup devs, but a lot of you are probably here with a company, representing a company. You can still go home and use this on your personal time. Anybody with a Google Cloud address, just with a Gmail account, you can sign up. You can get this for free. You get the IDE plugins inside Visual Studio Code and the JetBrains IDEs. You get all the great Gemini features that you expect, like access to inline code completions in chat, generating unit tests, and asking for help debugging. We give you generous limits, practically unlimited code completions, 180,000 requests per month. That way you're not going to be hitting some limits saying, hey, by the way, you've run out of your free tier. You need to upgrade. Unlimited usage. Chat. Previously, 128K tokens for chat context. Now, announced here at Next, up to a million tokens of context for your chat usage so that you can put in those big files and make those big requests. Whether you are learning or playing, Gemini Code Assist for individuals has what you need to get started. Now let's talk about Code Assist in the cloud IDEs. This is even faster. If you want to get up and running today with zero installation, this is a complete Code Assist experience that's running inside a browser window. It's exactly the same experience that you would get inside of an IDE. We now have two potential options for you to choose from, depending on what you're most familiar with. Cloud Shell Editor. Just go to ide.cloud.google.com, and you can connect this to your cloud run instances so that you can easily run and debug without manual setup tasks like port forwarding or language-specific debug settings. And if you want to have something that is a beautiful, customized, step-by-step editing experience, well, I really encourage you to check out Firebase Studio. This is the product formerly known as IDX. We're relaunching it as Firebase Studio with a rebrand here at Next. It's an incredible experience, and I really encourage you to give that a try. Again, both of these are free for individuals, so these are just some of the ways that we're trying to make the developer experience faster and easier. Okay, so now we've got all this great free content. Why would you upgrade? Well, let's talk about the upgrades for standard and enterprise editions. With an upgrade for Code Assist, we have one of the best... Well, I don't know. I need to say... I'm not allowed to say that legally. It is an incredible indemnification policy that allows you to allow your legal team to sleep better at night. If you are challenged on copyright grounds for code generated by Code Assist, Google will assume responsibility for the potential legal risks involved. This is very, very powerful for our big enterprises who are concerned about their legal risk. In addition, with the standard package, you get amazing mobile experiences with Gemini for Android Studio. You get Gemini in Firebase to build and run multi-platform apps, and Gemini in databases for database development and management, and all of these tools are available in one package with a Gemini Code Assist standard license. But for the enterprises, something even more. Our enterprise customers get Cloud Assist with all the help for cloud infrastructure. They get all the power of Code Assist plus code customization, observability, and organizational policy controls. I'm going to be talking about the first two in a moment. If you want to learn more about organizational policy controls, come back and hear me talk again on Friday. There's also advanced features with Gemini in BigQuery and advanced workflows for Gemini in app integration. Gemini in app integration is actually really, really cool because you get a visual flowchart UI that you can connect together all the different pieces and tie together different apps that are already existing inside your enterprise environment, like Salesforce or Jira. And finally, as Amir showed us, you can create and manage your enterprise APIs with Gemini in Apigee. All of this, again, at the enterprise level. Let's dig into two of those, enterprise code customization and observability. Code customization is an exclusive for our enterprise customers. With code customization, you get the tailored code suggestions that include references to your private APIs and libraries, help conform to your coding styles, and, frankly, just help it get the right answer more accurately the first time. With code customization, we are securely indexing and referencing your private code repositories. All of these are stored server-side, so it doesn't matter whether the repo is cloned to the local developer machine or not. This code is only for you. It is stored in an isolated, private tenant environment in a database that is just for you so that it is not seen by Google engineers, it is never shared, it is never used for model training, completely private to your organization. And we meet you where you are. Whether you're using GitLab or Bitbucket or GitHub, whether you've got cloud repos or on-prem repos, we're able to index any of them or all of them. So if you have a mixture of code storage places, we can index and give suggestions across the whole breadth of your source code repository. And we keep that index fresh, re-indexing every 24 hours so that as your developers make changes, they continue to get updated suggestions. And all of this runs on top of the base Gemini model. So as we release new models into production, those automatically get upgraded along with your code index. So now your devs can get answers at their fingertips and spend a lot less time searching for examples and API signatures and just stay in flow. For observability, we recently launched our observability dashboard, which allows you to see the usage and impact of Gemini inside your organization. And there's a couple ways we think about this. First of all, adoption. Adoption is how many people are actually signing in and using Code Assist? Are people actually using the tools? What about trust? Are your developers actually accepting those code suggestions? Do they trust the output from the tool? How many lines of code are being suggested? This is available in the dashboard. And then you can tie those to demonstrate acceleration and impact for your enterprise. So if you think about taking the basic data points for how much your developers are using Code Assist and then tie that to your Dora metrics or maybe tie that to Jira story points or maybe start to tie that to revenue or time to market. And that helps you tell that story to your executive leadership about the power of Code Assist and how it's improving your developer team. Now, for those who want to dig deeper, we also have a robust logging infrastructure, which allows you to capture logs from Gemini, includes all of the prompt data and the response data and the context that was sent to the model. All these logs, again, are completely under your control, never seen by Google, but they're inside a data bucket that you own that you can look at manually. You can use our reporting structure to generate log reports, or you can even access it by API and do your own log reporting. For customers that really want to go deep, we want to give you the option to do that. So now let's talk about the software development lifecycle. Code Assist agents and tools, this is maybe the hot topic for this year, super, super exciting, and as Amir showed, Code Assist in GitHub. Now, we know that there are multiple agents that are needed across the software development lifecycle because software development is not point A to point B. It's a loop, and it goes around and around. It goes from the beginning to the end and back to the beginning again. And so you need not just a coding agent, and frankly, as a product manager, I care a lot about the coding agent because this is where I spend a lot of my time, but we know that this is just a piece of the entire productivity workflow. We want to recognize the bigger picture where we're looking at not just a coding agent, but also documentation agent, testing, security, monitoring, and deployment. It's about a comprehensive support for AI across your entire workflow. With agents and tools, this is how we improve that software development lifecycle. We're providing more self-service workflows so that now your developers can actually just trigger workflows and are independent and don't need to ask for help and are able to keep themselves unblocked. Now, let's talk about the difference briefly between tools and agents. What's the difference between a tool and an agent? A tool is any plug-in code that modifies the prompt or the response and is triggered inside the chat window. We saw Amir demonstrate that at Google Docs. Tell me about my product requirements document. Or maybe at GitLab, tell me what pull requests I have open. An agent is a semi-autonomous piece of code that executes complex or long-running task chains and is defined in natural language. If you saw the keynote this morning, you saw we talked about using a mixture of agents framework that allows us to take very specific prompts for each individual task and chain them together, we find that this gives a much better result than just a single one-shot prompt. So you might have one piece of that subtask that's your developer prompt and is going to go write some code. And then you might hand that off to the next piece of the agent, which is the tester prompt, and make sure that it was actually generated correctly. And then maybe a third piece that's going to be coordinating all of that flow together. This helps remove the toil and the repetitive work, like creating unit tests or writing documentation, and we are excited to see all the ways this is going to improve all of your experience with GitHub or with Code Assist and GitHub. So Google is launching a bunch of these integrations as part of our framework here at Next. And I want to give a quick plug, break out supercharger development workflow with Gemini Code Assist tools. If you want to go deep on tools and agents and learn how they work and all the different things we're offering, that session is tomorrow, Thursday. Go check that out. And finally, Code Assist for GitHub. I'm jumping a little bit ahead of myself there. As Amir showed us, Code Assist for GitHub allows you to have a code review agent that gives you automated pull request summaries so that you can easily see what's changed. This helps free up your senior engineers so they're not spending as much time looking at the first review pass for the junior developers. We have in-depth code reviews with potential issues and suggested fixes that you can commit right there from the interface. And you even have that severity assessment that shows, well, this is a low, medium, high, or critical issue so that you can quickly skip to the things that are most important when you're doing a human review. And finally, we have integration with style guides. If you have a bunch of arguments inside your company about which is better, do we use snake case or do we use camel case or forbid you want to use kebab case, hey, you can put that right inside a style guide markdown and put it right in your Gemini root folder and the Code Assist for GitHub plugin will help respect that style guide in the code review comments. So looking ahead, 2025, focusing on that developer experience with lots of token support, prompt editing, context support, search integration, and then focus on the enterprise for the really large customers in the room with support for additional source code management providers, support for third-party ID and login, support for dashboards and some of the verbose logging that I talked about earlier, and finally, that focus on the entire software development lifecycle with agents and tools and integration with tools and docs and everything that you need for your entire workflow. Now, before I hand it off, I want to leave one final plug, and that's here to talk about the Google Developer Program. We have two bits of a developer program here, one premium for individuals and enterprise for teams. I want to talk about the premium for individuals just briefly. This gives you a set of Google tools, and you can get it up and running immediately with Google Code Assist Standard, expanded Firebase Workplaces, $50 in Gen.AI developer credit, Google One AI Premium, up to $1,000 in Google Cloud credit, unlimited access to our Skills Boost library, and some other goodies as well. This is a great way to do prototyping and see the best of what Google has to offer. So if you want to try out all the tools and play with all the toys, I encourage you to give the Google Developer Program a try. Go ahead and subscribe today. Because we're here at Next 2025, hey, how about a 25% discount for everybody in the room or everybody watching online with that very hard-to-remember secret discount code, Next25? And so with that, I now want to hand it off to Cody to hear about how Sentry is using Code Assist in the real world. Thank you, sir. Hello, everyone. As Andrew mentioned, I'm Cody. I work at Sentry. I look after our developer experience program. Really excited, and what I really enjoyed about the presentation is thinking about all the different ways that Code Assist is bringing context into what you're building inside of code. And at Sentry, we focus a lot on how we can bring better context into your errors and traces and all the information that's going wrong inside of the applications you build. So I wanted to spend a few minutes today and talk about how we look at using Code Assist inside of Sentry. So how many Sentry users do we have in the room? Is anyone a Sentry fan? Okay, I see hands going up. That makes me feel better. So Sentry is a monitoring platform, but what we focus on is really getting close to how developers are building software. A lot of monitoring platforms focus on the operations teams. We want to be the platform that developers are using as they build out applications. And so we focus a lot on giving you good context about what's going wrong inside of your code as you're building it out. And so we focused initially on errors and events. So as errors pop up, being able to give you good stack traces, information around what's going wrong, good messages, good understanding about what's breaking in your app. We moved on and started bringing in things like traces and spans into there to understand how application communication is moving throughout your app. And then looking at things like replays. So all of these things, though, are really meant to bring better context into what's going wrong inside of your application. So when Andrew and Amir were talking about all of the things that Code Assist is doing, it really resonates with me because I think about all of the different contexts that we bring when things are going wrong inside of an app. And we have this tagline on our website, and I'm not bad by 4 million developers, because really, like, we focus on making sure that developers are having a good experience as they're building out their applications and being able to monitor and understand what might be going wrong and really making it actionable to fix it. One more forward. And so thinking about what we showed around the software delivery lifecycle, when we saw all of those different agents on there, I think about the ways that Sentry plugs into that and the ways as you're building an application and things start to break, how you can start bringing these into those different stages to make them much more informed. And so when we look at things like being in an IDE and building out your app, being able to understand what broke and exactly how many layers it broke, whether it's a slow performance issue inside of a trace, whether it's an error message that's popping up, whether it's things showing up inside of a session replay, whether it's monitors firing off, all of those are things that we want to be able to bring into all of those different stages that they were talking about as part of their presentation as well. And so when we think about this in the light of like a Codesist scenario, being able to take all of that context and not have to jump into Sentry to actually pull that out, being able to actually have it as a tool that you can use right along with Codesist and bring it into your application is super valuable. We're talking about this and how we can make better connections so that tools like Codesist can pull more context in and give you a better, more informed answer. Because I think we all know the more you use tools like Codesist, the more context you're able to provide around what you want it to actually do, the better results you're going to get out of it. And so we can come in and say, here's your error message, here's the user that was impacted, here's the systems that they were coming in from, the OS, the version, the SDK version, whether we can see a trace and we can say, it was slower at the API tier, slower on this specific call, when we can bring in profile data and be able to say, this function was performing slower. All of that can be things that we bring in to Gemini Codesist so that Codesist can give us a better solution to the problem. There we go. Maybe. So I'm not going to narrate the whole video. I'll talk through what's happening, but I want to talk more about what you're actually getting out of this. So in this case, when you have the Sentry tool for Codesist set up, you're able to actually talk to your Sentry environment. So you can at mention Sentry, you can ask it for information about your organization. So this organization is all of the different projects that you have inside of your environment. You can go in and say, what are the most user impacting issues inside of a given project? It'll list out all of the issues that are happening and it'll order them by how many people were impacted in each one of those because we get down to the individual user level. We can then expand each one of those out and say, hey, give me more detail on this specific issue. And you saw we had these unique identifiers around what was actually going wrong. Like Cody ShipTrack 4, for example, the 404 not found. We can go in and have it pull those down and see all the detail around why that was showing up, what page were they on before, where in the stack trace did this happen. All of these things come together to give you a really good understanding of what's breaking inside of the application. And all of this context is stuff that we can take alongside the context CodeAssist already has and pull it in to solve the problem also. And so when you're inside of Sentry, you can go in and actually see all of these things visually and go in and interact with replays, play them, watch what the user was experiencing. You can dive into these traces. But ultimately, if you're trying to fix that problem, viewing them inside of the console is less helpful. You want to be able to bring them into your actual code and be able to use it as part of the solution to fix what was broken. And so that was the demo of walking through it using Sentry. We're really excited for CodeAssist. We're excited for tools like this that make it easier to bring all of that context in a really centralized way. We're building out more tools like Autofix inside of Sentry that'll provide more context down to CodeAssist. But ultimately, we'd love for everyone to try out Sentry. We have our own trial. It's not as cool as the Next25 code, but we do have a trial. Go check out Sentry.io and we'd love to talk to you over here afterwards. Lastly, we have a survey. We'd love for everyone to jump on and check it out and let us know how we did and let us know what you'd like to see more of. I think Amir, Andrew, and I are going to hang out for a little bit if anybody wants to stop by and chat. But thanks for your time today. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.