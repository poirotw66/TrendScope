 Sam Lugani Welcome, everyone. Thank you for making the choice to attend this session. I know this is the last session for the day, so we really appreciate your time. My name is Sam Lugani. I look after product management for confidential computing at Google Cloud. I'm very happy to be joined by Vint Cerf, Google's chief internet evangelist, Daniel heading product security at NVIDIA, and Joanna, who looks up at Google Cloud. Imagine a world without email, without the web, without instant communication. That world existed before the groundbreaking work of our next speaker. He co-designed the TCP IP protocol, the very language of the internet. This is what he had to say about confidential computing. This is a game changer, just like the advent of email was 50 years back. This is a game changer, just like the advent of email was 50 years back. This is a game changer. But this snippet is from five years back when he launched Google Cloud confidential computing to the world. We are honored to have him join us in person. Please welcome a true internet legend, Dr. Vint Cerf. There we go. We are honored to have him join us in person. Please welcome a true internet legend, Dr. Vint Cerf. There we go. Thank you for not introducing me as the distinguished Dr. Cerf or the talking dinosaur, which is actually getting pretty close. Well look, let's talk a little bit about AI. It seems to be what is on everyone's mind. The internet is at the core of some of this. Keep in mind that the internet itself keeps reinventing itself. Think about it. We added fiber in the 1980s. There's mobile in 2007 with smartphones. There's the cloud. There's the low earth orbiting satellites and now there's AI. So the internet is just an infrastructure. And what's happening is that it has facilitated an extraordinary number of new developments and applications in addition to consuming new technology and trying to turn them into useful components. The most important thing about internet, from my experience anyway, is that it started out in openness. Some of you may remember or maybe have heard about the first paper. It was published in 1974 called A Protocol for Packet Network Intercommunication. It described the details of the TCP protocol even before the IP had been split off. The reason I bring it up is that our work, Bob Kahn and I, were trying to develop a technology to support the command and control system for the United States military. And people say, how did you get away with publishing that openly? And we said, well, we just did it and nobody noticed. So we're not sure that that would work in these days. But the openness part extended in many dimensions. One of them is everything was supposed to be able to connect with everything else on the net. Now today we might decide maybe that wasn't the safest idea. But at the time we didn't know exactly what needed to connect to what. And so we said we better not have any restrictions at all. And we also understood though that privacy would be important. And of course today we know privacy is even more important than ever given the amount of information that is promulgated around the network. So if you think a little bit about openness, it has been a boon to us and others that the AI applications have been openly available. It allows people to try things out. It gives people the freedom to create. And that's been beneficial for us too because we learn from all of the customers who are discovering the power of artificial intelligence. So we want people to have the right to explore and to invent. So we are hoping that we'll be able to drive new AI applications not only from our own experiences, but from the experiences of our customers. But we also feel a certain responsibility. I don't know how many of you have tried out some of the AI applications, but I've tried a few, especially the large language models. And I remember deciding that I wanted to have one of them write an obituary for me. And I know that sounds terrible. But I thought, well, they had been trained probably with enough information off the web that they would find the obituary format common because people die every day and their obituaries are on the net. And there's a little stuff about me here and there like in Wikipedia. So I thought, that's a reasonable request. So I asked the chatbot to produce a 700 word obituary. And it started out, it said, we're sorry to report Dr. Cerf passed away. And then it gave a date that I thought was way too soon. I didn't like that part at all. This is terrible. And then it got to my career. And it mixed up stuff I did with stuff other people did. So I got credit for things I didn't do. And other people got credit for the stuff I did. Then it got to my family. And as far as I can tell, it made up family members that I don't have. So I remember scratching my head trying to figure out why would that happen, especially if it was trained on factual material. And I came away with the conclusion. Now, look, I'm not an expert here. Don't misunderstand. It's my little cartoon model. But one thing that we thought was that maybe you needed to have more context about what you ingested. Like, you know, this paragraph is a bio of that person. And this paragraph is a bio of that person. And you shouldn't mix the two together. So one of the things we are learning, and I'm sure you must be as well, that you need to figure out how to help these large language models formulate adequate context as they generate their output. In any case, that's just one level of responsibility that we have. We want to have reliable artificial intelligence, agents and applications for our customers to use. We want them and you and we to be able to trust them. I will tell you my most recent experience though really surprised me. I asked one of these bots to take notes at a conference call and it produced an extremely well written and accurate summary of the discussion. I don't know whether if we had just asked it the question it would have produced all of that and we didn't have to bother meeting at all. It would have just done it for us. But the accuracy was very impressive and very helpful. So there's a lot to come. So here we are. One of the things that the Internet allowed is for people to discover each other that had common interests. It also allowed them to discover information that they were interested in. You remember that in 1991, it was Tim Berners-Lee that released the World Wide Web. You probably don't know that nobody noticed at the time until a couple of guys named Marc Andreessen and Eric Bina did the Mosaic Browser at the National Center for Supercomputer Applications. And they released that freely just like the TCP IP protocols were released. And everybody noticed that. They all downloaded Mosaic. And one guy noticed and thought it was really a BFD. And he brought Marc Andreessen and Eric Bina and others to the West Coast to start Netscape Communications. And that got started in 1994 and they went public in 1995 and the stock went through the roof. And that was the beginning of the dot boom. And what's important about all this is that the World Wide Web showed us how people could learn from each other. Because if you remember the browsers of the day and the browsers today still let you see the source code of a website. So this thing called Webmaster, which didn't exist, was created by people sharing their information, seeing each other's websites and discovering how they had done it. So the same argument can be made for a lot of the AI applications. People get a chance to see how other people's models respond to prompts. They learn about what prompts are capable of doing or not doing. I love the idea that we're reliving this learning from each other idea. So we used to have webmasters. Now we have prompt engineers. And there's probably going to be another job that gets invented another ten years from now or less that we aren't anticipating. But it's all because we're allowing each other to learn from each other. So the application space is seemingly endless. That's true of software in general. And I think that one of the big difficult challenges they had with regard to governance is fingering out what kind of governance principles and what kinds of responsibilities the people who offer applications based on AI should undertake. And I'm a big fan of what's called multi-stakeholder policy making so that multiple parties who might be affected by policy get a chance to sit at the table and develop something that everybody can agree to. So I think just like the internet grew by six or seven orders of magnitude from its beginnings to the present day, I think AI is also going to grow in terms of application space. So now comes this other interesting question about privacy. I think that it is likely that a lot of people developing technology based on AI will want to preserve proprietary interest in what they've done. And so the confidential computing capability, now that it has moved not only from the conventional CPUs but to GPUs and TPUs, is a key element in allowing people to privately develop their ideas and then make those parts available. That they want to. But this is a very important tool to help people trust the cloud based system for doing that kind of development. People should have the ability to trust to preserve their intellectual property. So you should be in control of that and nobody else. And I'm very proud of Google's development of this technology because we offer exactly that. We don't need to know what you're doing. We just need to know that you need access to this computing capability and you can do that in private. So now let's figure out how we elevate trust in artificial intelligence. One thing for sure is being able to understand when we can rely on the generative output of the AI models. My little experiment with the obituary taught me that there was still work to be done. My sense right now is that we are deeper and deeper into understanding how these systems actually work. I don't know if you've heard the term sparse encoding, but in conversations at the bar you can use that to sound like you know more about AI than you really do. It turns out that's a mechanism for figuring out which neurons are being fired as the model responds to a prompt. We're actually literally trying to disarticulate what it was that caused a particular group of neurons to fire against a particular prompt. The more we know about that, the less likely the model is going to be to generate unexpected and incorrect output. So we'd like to be able to understand that deeply enough to guarantee you that the output is reliable. I really think that that is going to be key to anyone trusting the AI based applications. As we move from the generative AI of today producing imagery or sound or video or text to the agents that are based on AI, that trust factor is going to be extremely important. I can imagine a scenario where an AI agent knows that you want to buy a new car and it wants to help you do that. So you say, well I need a new car and the agent goes out and buys you a Ferrari. Now maybe you wanted a Ferrari, but if you didn't want a Ferrari, maybe you needed the agent to understand a little bit more about what it was you intended. So having the agent show that it knows what you want in the dialogue with you is going to be important. But then I got to thinking about this. If we communicate with the agents using natural language, what are the agents going to communicate with each other by? And if they use natural language, does that mean that the agents will have the same ability to misunderstand us as we do each other? And that suggests to me that maybe we should be thinking about formal languages for agent to agent interaction with well-defined semantics and context. I don't know how far we've gotten in that space, but I can tell you that if I were going to use an agent to do something or to use multiple agents to do something, I'd like some reassurance that we have confined their understanding to something that is verifiable. So this whole idea of trusting agents on the dialogue side is important, but it's also important to know that the agents can secure their interactions with us and with other agents and with other databases. And that's why AI and security is so intertwined and that's why we're so interested in technology to hold the agent accountable or hold the AI accountable for what it's actually doing. So that, if we succeed at that, of course, will increase everybody's trust in these things. So I've already mentioned confidential computing and I can tell you it was really, you could tell, I was really excited about it when it was announced five years ago. At this point, we are at a much more refined state where we can apply those same technologies over multiple computing engines. And I'm starting to think a little bit about what it might mean for agents of one cloud to interact with agents of other clouds and whether or not we can secure those exchanges and do so in a way that is verifiable. It's certainly important for us to be assured that the agent we're talking to is the one that we expect. It's not too different than what happens when you go to a website. You'd like to be sure you got to the right website so a digital signature is your friend there. Similar kinds of exchanges between agents start to make sense, which leads you to one other interesting problem. I mean, we talk a lot about our own IDs and verifying our IDs and making sure that no one else can pretend to be us. Well, what about agents? Do they have to have IDs and do we have to be able to verify them? Will agents have to be registered somewhere? I don't know the answer to that right now, but I'm sitting here thinking maybe we will have to have a vocabulary for verifying the agent is the one that we expect, just like we want to make sure the website is as well. So there's some room here for development. The thing I like about the way we've developed the confidential computing is that you get to do what you want to do in privacy. The only thing that knows what you're doing is the computer that you're running on. So Google can't see that and no one else in the cloud can see it either. Just you. Very important. So we've invested heavily in this capability. We're continuing to invest it in other computing engines and platforms as we go along. And we think that these practices are going to be fundamental to your ability to trust the results of your work using these artificial intelligence machines. So you're part of this interesting AI revolution. You're helping to create it along with us and others. The openness and the privacy and the confidentiality and the reliability will add up to things that you and others will be able to rely on. So I'm hoping that what we'll hear next from Sam is a little bit more about the detail of how we actually get to our final objective, which is to make sure that all of us can trust the AI agents and applications that we build on top of these amazing cloud platforms. So Sam, let me ask you to come up and tell us some more. There you go. Thank you. Thank you, Vint. It's an honor. We've come a long way. Think back to 2019 when Google and dozens of partners came together and founded the Confidential Computing Consortium. We all took a leap of faith. In 2020, Vint launched us to the world and we became the first major cloud provider to offer a truly simple way to use confidential computing. Single checkbox, no code changes required, truly lift and shift. And the progress we've made since then is quite remarkable. Today, GCP offers the best-in-class region availability for confidential computing so that you can use confidential workloads no matter where you are. We've gone from what was a vision back then to really a vast portfolio of confidential computing capabilities. Let's try to visualize confidential computing and what that means for a bit. Imagine I'm in the 60s. It was a beautiful decade. And I'm wearing my banker's hat. Maybe that's the way people used to be dressed back then. And this is everything I hold dear. My memories, you know, a few gold coins as well because we're in Vegas. So I close this and the keys are in my pocket. That's encryption at rest. Everything here is stored. It's locked in. I have the keys. Now I have to go to a bank. So I start with this box and I start my journey towards going to a bank. That's encryption in transit. The box is still closed. And now I'm at the bank. And I'm in one of these teller lanes which are out there. So I stand in one lane and there's people adjoining where I'm standing. And as I make my way to the teller, I have to actually open this box and transact on it. I have to use what's in this box. So as I'm transacting on this data, if someone besides me is so well inclined to take a peek at what I'm doing, they can. So as I'm using data, my data is at risk. Now I'm creating this confidential environment. There's an umbrella in lieu and it has a beautiful cloud interior. That's my confidential cloud. And now I'm doing the same thing but I'm doing it in this enclosed environment. So I got everything done in a jiffy. And so the environment I'm in is confidential. There's isolation around the workloads and other workloads in that same physical box. And there is verifiability through attestation which gives me the assurance that this environment has not been tampered or altered in any way. And that's what confidential computing is all about. And we're seeing tremendous impact across every major vertical. And today we're here to show you how people are actually using confidential computing to open up new possibilities. So there's companies like Swift and they're powering their money laundering detection models using confidential computing. They have a lot of data from different banks coming in and they want to make sure this data stays safe as they figure out insights from this data. There's a company called AI Genomics and they're doing something really unique. They are providing solutions for pathogen detection and AMR or antimicrobial resistance. And that's a big problem in hospitals. It's the idea that viruses and bacteria develop resistance towards the medicines we have over a period of time. So in their space they're dealing with very sensitive data that's coming in from different entities, hospitals, healthcare centers. And this is genomics data. So they have to make sure that that data stays safe. They're building AI models across this data and they want to make sure their IP stays protected. But the outcome is beautiful. They want to provide insights around disease outbreaks and treatment options. So it's an example of how confidential computing can actually make a difference in all of our lives. There's Google Ads as well and they introduced something called confidential matching, which is a way for publishers and customers to match their first party data with Google data. And all this is happening inside a TE or a trusted execution environment. So there's guarantees and limits around how this data is used, who can access it, and there's transparency through cryptographic attestation. And then, you know, in the Web3 space, Flashbots has a really interesting use case. So in the world of blockchain, there's different participants and they try to get their transactions included in the most efficient way. But if someone saw your transaction or had some idea as to what that transaction was all about, they might try to jump ahead of you. And that's called front running. So what Flashbots is doing is they're running this auction inside of a TE such that it's very hard for someone to peek in and unfairly manipulate the data in any way. So all this is finalized in a block which is running inside a TE. They're also doing a great job bridging the gap between Web2 and Web3 through an initiative called OAuth3. So imagine an AI agent and that AI agent is going to help you, you know, plan your financial journey in a much better way. One option is to give that AI agent all the information you've ever had about, you know, every transaction you've done in the last couple of years. Or if you just want to get better at, you know, doing more sort of planning around limiting expenses on travel, you give that agent a slice of that data, what's relevant to that agent. And so Flashbots is working to make that happen. And there's Monotago and they have a really interesting use case around duplicate fraud financing. So imagine I have an asset, it could be an apartment, a piece of land, and I go to a bank and I say that, look, here's my asset, give me a loan against this asset. And the bank says, great, I'll give you a loan. And then I go to another bank and I say, look, I have the same exact asset which I've already got a loan on, but the bank, too, doesn't know that. And I say, give me another loan and the bank will maybe give me another loan. And this happens depending on the geographical location you're in. Now, banks are not going to share that data with each other because it's customer data and it's competitive. So what Monotago is doing is they're creating this environment inside a TE where banks can collaborate with the data, yet retain the data ownership, and they get insights as to where the duplicate fraud financing is happening. And in a lot of these examples, the idea is that organizations want to share data, they want to collaborate on data, without truly sharing data. And that's kind of a weird concept. I mean, you're sharing data, but the idea is you're retaining ownership of that data and you're making sure that that data in its entirety is not going to another individual. And we have a solution called confidential space, which provides these benefits. It's built on the bedrock of confidential computing. So there is confidentiality, there's isolation, there's verifiability, but there's the additional advantage that only the authorized workload can access your data. And the concept we term it as is the operator or the workload owner being outside of the trust boundary. And this allows multiple parties to collaborate without having to really trust each other blindly. So imagine you're trying to create a radiology model across all the different radiology images you're getting from different hospitals. How do you ensure that that stays private and yet you're able to build that model in a privacy-preserving way? That's something which confidential space can enable. Now, one of the big things we come across is, look, we want to do more in this space, but it's complicated and that's understandable. And it is our mission from a team perspective to make it very easy for everyone to be able to embrace confidential computing. And Gemini is a great way to do that. So I'm very happy to announce that we have Gemini Cloud Assist in private preview with support of confidential computing. So let's see a demo of what that looks like. Here's a Gemini Cloud Assist portal. And I can just say, look, help me create a confidential space environment. And it's going to provide me a couple of options. So it says, look, you can be the workload author, you can be the operator, or you can be the data collaborator. So choose your persona. So for now, I'm just going to say I'll be the workload author. And so it's going to now check all the necessary API permissions that are needed for this. So it does a check on the API permissions, and it does a check on the artifact registry, and it looks like I have all the permissions. And then I say, yes, go ahead and tell me what I need to do next. And as soon as I say that, it's going to give me a bunch of things which will help me set up my Docker repository as well as my Docker image. And boom, I'm done. This is it. I enter those commands, and you have an environment set up which you can modify to your use. And that was the first persona. There's other personas as well. So there's the operator and the data collaborator. And you could do all of these with Gemini Cloud Assist. So it's really, really powerful. I kid you not. Two years back when we launched the solution, it took me a day and a half with the best of engineering talent available to get this environment spun up. This would take me less than an hour. And that's the power that Gemini and AI agents can bring into this whole equation. We're really in this new chapter of confidential computing. And the inflection point has really been our embrace of AI. Because at the end of the day, we need to use AI in a privacy-preserving way. So the first aspect of securing the data, this financial data, this medical data associated with it. And all that has to be secured against manipulation, unauthorized access, or poisoning. And then there are the models and the weights themselves which need to be secured. So if you think about it, we need to use AI. We need to use AI with confidentiality, with verifiability, with isolation. What better way than confidential computing to secure our AI future? And that's why this destiny around AI and confidential computing is completely intertwined. And we're seeing that in the marketplace as well. So let's see a demo of this. But let me just first add some context as to what we're trying to accomplish here. You may have heard about RAG or Retrieval Augmented Generation. Essentially, it's a technique to enhance the accuracy of AI models. So most companies are not going to build, you know, large language models from scratch. They're going to take an off-the-shelf model and they're going to customize it to their advantage. And RAG is a perfect technique to do that. So let's say I take a large language model. But at the end of the day, I want to make sure that that large language model is running inside a TE. Because I want to make sure my prompts and my data is protected against the AI model provider as well as the cloud operator. And then more so, I have, let's say, a lot of my own data. Imagine a hospital in this case. And I have a lot of my proprietary data, my patient data, which also I want to make sure that it's secured. So I'm running that also inside a second TE so that I get the guarantees around making sure that data stays private. And there's additional guarantees around insider misuse as well. So that's really the environment. And all this is powered by confidential space, which we're going to see right here. So this is the private GPT we've built, which is powered by confidential space. Right now, we're sort of in this basic mode, which is that the RAG element is not playing a part in the first set of demos we do. Here's the attestation token. It shows that we're running inside a valid confidential space image. The debugs are disabled. It can show you the image digest as well. And now, I'm going to ask a very basic question about a patient. And I say, look, Jane Doe hasn't been doing very well on her recent treatments and has lung problems and joint pain. What is one option her doctors could look into? And it gives me, you know, a certain reasonable answer. It says that, you know, doctors should look at alternative therapies such as acupuncture and massage therapies. It's a reasonable answer. But as a hospital, I have a lot of data on Jane's condition. I also have a ton of stuff around, you know, Jane's patient history, as well as case studies, which the hospital has done, the trials, which may not be public right now, and some guidelines, which we need to adhere to. And all this is in a separate vector DB. And I'm going to sort of integrate it with confidential space. So I have processed the context and the attestation is verified, which means that, look, it's running inside a TE. And now I'm going to ask the exact same question again. And now it's giving me much more relevant information. It's saying that I should consider investigating something called biologics, which is a new class of drugs, because Jane has inflammatory conditions like rheumatoid arthritis and pulmonary fibrosis. And so that might be a good option for me. And this is really powerful. This doesn't take a long time to figure out. But if I have family and I'm going to a doctor, and the doctor can get better outcome in a short span of time, that's a good use of a technology and that's a good use of confidential computing. So I encourage you to think of such use cases as well. And we have a pretty vast portfolio of confidential computing now. There's a foundation that we've laid with confidential VMs on Intel, on AMD, and we now have support for GPUs as well. And we have services like confidential GK nodes, Dataproc, and Dataflow, where you can enable confidential compute. And then there's the applications on top, which is confidential space. And this journey is continuing. We're on this mission to achieve confidentiality everywhere. And that's our goal. So I'm very happy to make a few announcements. We have confidential GK nodes on C3. And this includes support for Intel AMX, which is a CPU-based accelerator that Intel has designed to enhance ML applications running on CPUs. There's confidential VMs on C4D, which is globally available on the AMD platform. There's confidential VMs on A3 in preview soon and GA later in the year. And then confidential space is also supported on C3 and A3. So no matter what your hardware preference, you can use confidential space today. But there's one more thing. I often get asked this question from customers, where is confidential computing on Vertex AI? It's, yeah, confidential computing support for Vertex Workbench is now in preview. And as with most things, it just requires one single checkbox to enable. I'll leave you with one last thing. Our vision is to make confidential computing available and accessible to everyone. Whether you're a large company or a small company, no matter where you reside in the world, whether you have the technological resources at your disposal or not, we want you to be able to use CC and get the true benefit of it and unblock use cases which you did not think were possible. And in this journey, we have an incredible partner. Please welcome Daniel from NVIDIA on stage. Thank you. Thank you. Thank you, everyone, for having me back here again this year. We did this last year. Got to speak about confidential computing. And it's been a minute. Like, 12 months have gone by and a lot has happened. I feel like I've been set up really well by Vint and team in telling this story and perhaps telling it in maybe a very unique NVIDIA way. So we started our journey for AI all the way back in about 2012. AlexNet came out. It was a convolutional neural network for image and image processing, state of the art at the time. And a lot of people went, wow, that's really cool. Image processing and classification. But we looked at that model and had a very different observation, maybe uniquely NVIDIA who did parallel computing. We said, hey, this is a dense compute demanding parallel algorithm that is generalizable to almost any workload. We saw in that moment not just solving image processing, but almost any problem could be recast in this AI mode and need dense parallel computing. And we said, ah, we have a problem. We don't have this much scale across the planet to service everyone's workload on every application that could ever be built in the future. And we predicted every application would have AI in it. And back in 2012, that probably seems rather crazy, right? But today it seems sort of self-evident. And we doubled down. We redirected where our company was going. We took our CUDA parallel programming paradigm, the most accessible parallel paradigm on the planet at the time, and built the most flexible AI software ecosystem to run on GPUs from there forward. And recognizing that the universe and the ecosystem would need scale from us. How do we get scale out into the market? And then we go forward a few more years. Now we're 2018. GPT-1 comes out. And I think the rest of the world sort of caught the idea that, hey, this applies to me. I could solve my problems with this. And, you know, we looked at GANs. We looked at VAEs. We looked at transformers. A lot of different models there. But all of them, lots of data, doing a lot of processing. And this is right on the heels of GDPR, where data processing become intensely focused. Hey, how do we rendezvous our most precious asset, all of this data, into a place where I have this scale and this compute? It turned out that was a problem. And in around 2018, ourselves had this aha moment. Like, hey, confidential computing was a real thing that we would need. And we partnered with Google and others to bring this into fruition with Hopper. We released the platform in 2022. Shortly thereafter, in 2023, released Hopper CC and A3 with Google, delivering that into market. To really address this question of how do we rendezvous data and trust and scale in the same place at the same time to realize this vision of what AI was. Now, that was the generative AI place, right? You know, we had sort of simple RAG LLMs. It generally fit in a single H100 at the time. Works really well. Solved a lot of the problems. But we were already looking ahead at the next set of problems, right? Scale was a problem. And now, sort of fast forward again. Here we are today, Agentic. We heard about Agentic building out the next generation. These models are very different than what they were before. It's not sort of an input and output, one-shot sort of situation. We have much more complicated, long-thinking models, much more intelligent and robust rule following. We've got mixtures of experts, chain of thought, reflective reasoning, decomposing problems into complex subtasks, make tool calling, accessing data potentially from trusted sources, potentially untrusted sources, simultaneously collecting it together and then trying to verify what's going on and produce a reliable, strong token stream to go achieve our business objectives, whatever that may be. The challenge, of course, there is we need more scale. These models that used to be sort of input-output are now much larger, much more capable, multimodal. We need more scale. About 10x to keep the latency on track for these new generation models. Oh, yeah, and the long-thinking means we're doing about 10x more tokens than we did. What used to be done in 1,000 tokens one-shot is now 10,000 tokens in a long-thinking context. So we've got this huge scale problem, fantastic for NVIDIA. We've got the next generation of scale coming with Blackwell. But also at the same time, we've talked, you know, scale, trust, scale. Hey, we need more trust. This world of agentic is now doing multiple models, maybe from multiple providers, accessing multiple data sources in multiple locations. How do we construct trust amongst all these collaborating entities to not just secure the workloads, maintain their privacy, delegate authorities? It's a very complicated problem. Fantastic that we have CC, who has these foundational primitives to scaffold up the trust that we need to envision this future of agentic AI. Still running on our most trusted, most sensitive data across the planet. Really exciting times for all of us. And some of which we didn't anticipate because you brought these problems to us, right, with the solutions that you were pursuing using AI as we predicted back in 2012. So how are we dealing with scale? Fantastic timing. We just had our GTC event a couple weeks ago. We launched a bunch of products and we've got products here, too, with our Google team. We've got performance and key improvements, certainly in the Blackwell architecture. We launched this last year. FP4 performance, fantastic improvements there. We've got scale up and scale out with full encryption across CC nodes, building into the Blackwell. They announced at the keynote they had an A4 that we're actively building out as long as A3 already deployed for your use today. Super exciting. And then the announcement from Jensen and Thomas about having Gemini with confidential computing on-prem in your own environment. Which is yet another advancement. And I think really underscores, I think, what we're all about, Google and NVIDIA, of realizing this vision of AI accessible by everyone, everywhere. Making it safe, secure, private. And all of that is wrapped up in this mission that we are all collectively for CC. Building trust is hard. And making it accessible is hard. You know, how do we take these complex agentic workflows on the left, map them into this world on the right? We do that through a lot of efforts. Attestation services is at the core of what NVIDIA does to deliver that. We have live cloud services for doing verification online. We are investing, doubling down for this agentic problem of scaffolding trust with more different and varied language bindings in the attestation SDKs to make sure that we can access and do attestation across the variety of workloads that are being brought to us through this agentic motion. What's going on, we've got OPA rego policies that are very consistent with other cloud environments to make that easier and accessible to write the policies that you saw, like Sam demonstrated there within his work, as well as partnering with folks like Google who are simplifying it yet more as you go out and build out these agentic workflows within environments like Vertex and others. So we anticipate this getting simpler, but we also see everyone raising a head to make yet more complex interactions going. So it's a race amongst all of us. It's super exciting to be partnered with Google in this endeavor. There's a lot to do. We're excited to see what you all bring to us in terms of your challenges. I didn't know that there's called out just the foundational transformation that's going on across the industry today. And I hope to hear what that is from you all in 12 months and see what's happened since then. And Joanna's going to tell us a bunch more. I'm going to. Thank you. Thank you. All right. Thank you so much, Daniel. Now that you've heard about NVIDIA's amazing GPUs, it's now time to see it in action. So here we have, well, I want to click play. It's a create a Kubernetes cluster UUP page. It's the UI for GKE standard. After you pick a supported region, you select one NVIDIA H100 GPU, and then you go into the security tab and select Intel TDX as a UI. And then you go into the confidential computing type. It's genuinely that simple, very similar to what Sam showed with Vertex AI. One check box and you're done. Oops, that was laser pointer. So now you've seen how easy it is to create. What about performance? That's pretty much the first question that I get after every kind of CC overview customer meeting. Great news. We tested our confidential GPUs for you. So this benchmark we used is the LAMA 13B inference benchmark, and it measures performance in terms of tokens per second. So what that signifies is the speed of text generation. So higher is better. And when we compared the confidential VM numbers to the non-confidential equivalents, we found that it was pretty comparable. So pretty exciting stuff. And so we do recommend confidential GPUs for inference tasks like this. We also tested our confidential GPUs on the TensorRT Vision Transformer Inference Benchmark, and that one measures performance in terms of images per second. And we found that the confidential VMs as well here were quite comparable to the non-confidential counterparts. So this is really exciting that confidential computing can deliver, just as what Daniel said, it delivers high throughput, low latency for large-scale inference tasks. But I must always caveat that, you know, performance is very workload dependent, so please test out your workloads today. They are available on Google Cloud, and you can complain to me if the numbers are too high. And, yeah, we're in preview, and we'll hope to be GA shortly. All right, so what do we use these for? Sam kind of touched on these already. But kind of like our customers fell into two major buckets. The first bucket was really customers who wanted to protect their end users, whether it was from them as the AI workload operator or from Google, right? So if I'm the owner of an LLM, and, you know, my end customers are putting in their financial data into my LLM, I want to make sure I, as the LLM owner, can't access the end users' data, and then Google can't access it either. And then the second bucket was ensuring that your AI ML workload is compliant. And so we hear a lot of customers, especially in the European sector, using confidential computing for these reasons. An example is OPPO. So OPPO is a very prestigious electronics manufacturer, and they're really excited adopting confidential computing to produce a privacy-first AI experience for their customers. And here they call out two specific threats, exactly as we said. One, they want to make sure that the OPPO user is protected from OPPO, and also from Google. All right, jump to another demo. So this demo, just to preface, is a confidential VM with an H100 attached, just the ones we talked about with the performance. And we're going to show the UI of the private inference chatbot. And click play. So this UI client is connected to a confidential backend. It's a confidential VM that's running a Gemma 9B model. And the UI has a policy set you can see highlighted in the corner. It said that it can only connect to a confidential backend that's running specifically Intel TDX. And you can see the way it verifies that it's connected to a confidential backend is through attestation. And at the top, you'll see that the UI has already verified this attestation. It's pulled bits and pieces of the attestation part here. And it also verified the claims of the NVIDIA H100 GPU. So you can see the GPU claims here as well. Okay, so now we're ready to type in our secret prompt. And it'll be about financial data since this is our handy financial assistant. And so when you submit this prompt, it's an encrypted prompt. And when the chatbot sends back the response, it's also encrypted as well. So you can see that's highlighted there. And so it's really important, you know, especially with financial data or any sensitive data, that your chatbot is encrypted end-to-end and attested. So, you know, feel free to try this out today. Again, this is running on confidential VMs with the H100 GPU attached and available in preview. Here's another couple customers we want to showcase, which is Cosmium. And they built their key management system on confidential VMs. So you can see key materials are very sensitive. So they run on confidential VMs. Our partner and customer, Talus, they created a product called End-to-End Data Encryption, which is integrated with confidential VMs. Also very common and applicable for financial services. And, yeah, I'm out of time. We hope you enjoyed the session. We really wanted to showcase as many use cases and demos as we could and to show you the flexibility and all the cool ways you can use confidential computing. We hope you enjoyed your session. And please do rate us in the app. We appreciate it greatly.