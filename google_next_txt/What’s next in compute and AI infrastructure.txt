 Welcome, everyone, and thanks for joining today. I'm really excited to share more with all of you about how we're delivering the best of Google's infrastructure to help you innovate and unlock that value for your business. Our approach is centered around the idea of a workload-optimized infrastructure. Top analyst reports are recognizing us for our approach. For example, we recently made a big leap in the latest Gartner Magic Quadrant for cloud platforms, surpassing AWS in completeness of vision and surpassing Azure in ability to execute. AI workloads are fundamentally different and bring new and really, really unique demands on the infrastructure. Our approach to this stands out in the market because we're the only cloud that's focused on delivering an end-to-end integrated system across AI-optimized hardware, software, and consumption models. We call this the AI hypercomputer. Last year, we announced Trillium, which was our sixth-generation industry-leading TPU platform. The momentum with Trillium is great, but, of course, we're not stopping there. Hopefully, as you all heard from Sundar yesterday, we're really excited to announce Ironwood, which is our seventh-generation TPU. We believe that Ironwood will be a transformative platform not only for Google internally, but for the entire AI industry. Of course, it starts with the chip. Ironwood is the most powerful and energy-efficient TPU chip to date with an optimized stack for both PyTorch and Jaxx. But perhaps more important than the chip is actually how we network them together into pods. With each pod containing 256, the small one, or an amazing 9,216 chips operating together in unison, with a single Ironwood pod, we can scale up to 9,216 chips in a single interconnected network fabric. That's 144 times bigger than AWS Trillium 2. Our strategy here with GPUs is to be first and best with NVIDIA GPUs. Last year, we announced A3 Ultra. This is based on the NVIDIA H200 platform. And then soon after, we were the first cloud to launch NVIDIA B200-based A4 VMs. And we're also the first hyperscaler to make both B200 and GB200 available to customers. Over the past year, we've deployed the equivalent of dozens of football fields of accelerator capacity stacked with racks and racks of GPUs and TPUs with plans for much more this year. Training workloads are unique in that they need to run as highly synchronized jobs across thousands of nodes in tightly coupled clusters, where unfortunately, a single degraded node in that large-scale cluster can potentially disrupt an entire job, delaying time to market and delaying innovation. To address these challenges, we're excited to announce Cluster Director. Cluster Director is software that makes it easy to deploy and manage a group of accelerators. And it works under the hood of products such as Google Kubernetes Engine, and it also supports Slurm out of the box. Here in red, you can see that we still have two bad nodes, and we need to replace them. So we'll go ahead and replace them. And then now we have a happy, healthy cluster. Seemed pretty easy. So we're going from proactively informing the customer of the issues so that we can address those in advance before they actually even create any challenges. 100%. And now that we have our job running, we need to keep those resources utilized. So let's take a look at how we're able to do that with a powerful feature called straggler detection. I'm really excited to announce the preview of HyperDisk EXA pools. These are built for the largest and most demanding AI training workloads. We think that 2025 is going to be the year of inference. At the same time, inference is becoming much more diverse and complex and is evolving rapidly. Therefore, we believe the real measure for customers to focus on here is intelligence per dollar, not just tokens per dollar. By this measure, Gemini 2.0 Flash is 24 times the intelligence per dollar of GPT-40 and five times better intelligence per dollar than DeepSeq R1. And we didn't have the chance to update this slide, but as you heard yesterday, Gemini 2.5 Pro just came out and this raises the intelligence bar even further than what we're showing here. First, we're launching powerful new inference capabilities with GKE Inference Gateway. This can reduce serving costs by 30%, reduce tail latency by 60%, and increase throughput by 40% all at the same time. In 2022 at Google, we created Pathways. This is a distributed runtime meant for novel AI research. And today, Pathways is powering all of Google's large-scale training and inference infrastructure. And now we're making that same core capability available to you. We're also really excited to announce VLLM on TPU to make it easier for customers to use TPUs with PyTorch for inference. You're likely responsible for more than just AI. You know, this means that you have to balance your company's time and investments across a wide range of workloads. Let's start with how we're optimizing those compute and storage resources. It starts with our Gen 4 compute platform. We have the latest processors from Intel, AMD, and ARM, each of which helped improve performance by 30% to 40% over prior generations. AppLovin, who's a leader in mobile advertising, saw a 40% performance improvement with C4D VMs. NetEase, in their latest game launch, saw a 50%-plus performance increase when migrating from C2 to C4 VMs. Our C4A VMs, powered by Google's Axion processor, are setting the standard for price performance. Spotify modernized their infrastructure by modernizing to C4A and saw 250% better performance. In fact, our C4A instances were recently tested by a third party and found to perform 10% faster than the latest ARM-based instances from other leading cloud providers. We also have many customers that are looking to rapidly migrate enterprise workloads from on-prem to Google Cloud. HD Supply was able to use this to optimize their cost by closing data centers and migrating to GCVE. By doing this, they saw an 80% increase in the speed of deployment and 66% increase in IT staff productivity compared to their on-prem environment. Hudson River Trading utilized hyperdisk storage pools to enable configuration of highly performant, persistent storage that can dynamically flex to meet their workload needs. Ultimately reducing their storage cost in half. Today, we're really excited to announce a further expansion of these titanium capabilities. The titanium ML adapter securely integrates NVIDIA Nix to provide faster GPU-to-GPU communication. Working in tandem with our titanium offload processors, which integrate those GPU clusters into our Jupyter data center fabric to provide even greater scale. I'm really excited to be joined on stage today by Vitter from Salesforce. My team has been using Google Cloud for about six years, and it's really been a great experience. On a typical day, we spin up close to 300,000 instances and provision about 15 petabytes of storage. And, you know, we've experienced rock-solid reliability and performance from Google Cloud over years of this really demanding workload. Going forward, and this is something I'm really excited about, we're bringing all of our customer-facing apps onto Google Cloud. We wanted to leverage Google's leadership and AI and data to build more AI into our products. But as we added those testing workloads, the performance and reliability really shone through, especially over alternatives. And, you know, I don't want to jinx it, but uptime has been nearly perfect over the last couple of years. We're on the cusp of being able to unlock the next stage of scientific discovery. Isomorphic is using these scientific models in combination with our AI hypercomputer that I talked about before to accelerate drug discovery. First, we need to ingest, process, and manage this massive data for machine learning. We leverage Google Cloud Storage to do that and separate the data according to governance needs. We then index all of this data in Google BigQuery, so we leverage Bigtable and BigQuery to serve data to the models. And we leverage Google Cloud's AI hypercomputer to manage our fleet of TPUs and GPUs. Researchers like the ones at Isomorphic need massive high-performance computing to handle the massive data volumes. So today, I'm pleased to announce the general availability of H4D, our first high-performance computing VM offering 200 gigabits per second of RDMA. In December, Google Quantum AI announced our latest quantum chip, Willow. And I'm really pleased to have it on stage with me here. So we can check it out here. If you want to see it up close in person afterwards, we can share it with anyone that wants to take a closer look. From mainframe to general purpose to accelerated computing, and now quantum, each builds on the infrastructure foundations that came before. The computing infrastructure we're building today will power the breakthroughs of the future.ENNEW señor they wanna train? You can share the curve. LOOK HOW L smileando And next Спасибо Police Ao To abajo Elco comme .えっЫ pl engines ago