 IAN BALENTINE- My name is Ian Ballentine. I'm a developer advocate for Google DeepMind. IAN BALENTINE- My name is Wei. I'm also a developer advocate at Google. So today, we are super excited to talk about the hybrid use case for Edge applications in this session. So in the past few years, as you all know, the field of machine learning, especially LLMs, has been moving so fast. It feels like every other day, there's a new model that's being released. The LLMs are becoming smarter, ubiquitous, more helpful to us. At Google, we have been hard at work to build the cutting edge LLMs to shape the future of artificial intelligence. We released the initial Gemini models back in 2023. And now we have the even more powerful Gemini 2 model series. Many developers love the Gemini models, but some want more control and really own the models. So we released the Gemini model series to suit their needs and enable them to build amazing LLM-powered apps. The Gemini models are Google's cutting edge LLMs. It has native multi-model supports. Beyond text, you can upload images, videos, and audios. And the Gemini models will use those smoothly. It supports up to 2 million tokens in the context window, which is really, really large if you think about it. If you have complete questions to ask, you can use the Gemini thinking model to reason about your question and give detailed analysis. One of the best features I love about the Gemini models is the built-in search tool. As you probably know, all LLMs today have a training data cutoff date. So the model itself doesn't have the latest information. But using selected Gemini models, you can ask something that requires accessing the internet or the most up-to-date information online. And the Gemini model will leverage Google search to give you an accurate, factual, and up-to-date answer. There are many additional great features in the Gemini models, such as bi-directional streaming, function calling, and so on. So if you still haven't used the Gemini models, you're missing out a lot for sure. Meanwhile, the Gemini models are a family of lightweight, state-of-the-art open models. These models are built from the same research and technology used to create the Gemini models. They are available on Kaggle and Hugging Face. Anybody can download them and use them. We released the initial Gemini models last February, and we keep iterating and improving the models since then. The developer communities love the Gemini models, and we are seeing incredible adoption. Over 100 million downloads and a vibrant community that has created more than 60,000 Gemini wearings. Just last month, we released the Gemini 3 model, which is now a state-of-the-art, multi-model model. There are many Gemini wearings as well. For example, CodeGemma for coding tasks, RecurrentGemma for LLM research, TXGemma for therapeutics development, and so on. A common question from our developer community is that Gemini models and Gemini models, which one should I use? Well, they each have their own advantages. Gemini models have the best LLM performance at a very affordable price. It can access real-time information via search or use test-time compute to deeply reason about your question. Meanwhile, Gemini models have open weights, which means you have great control over the models. You can download the weights, fine-tune the models however you like, and deploy them on your own devices, in data center, on your laptop, or even on your mobile phone. One of the major reasons for using Gemini is to run a very powerful, intelligence-packed model on edge devices. You might be asking, why would I do that? Edge devices such as pc or mobile phones are naturally weaker than dedicated accelerators. So isn't the performance going to suffer because of this? Well, that's true, but there are several important advantages you should consider. First, latency. Latency and user experience are important. If you run on device model, then you can basically save the round trip of sending and receiving data because everything happens locally. Second, keeping everything on device means no sensitive data is ever sent off. This protects user privacy really well. Third, I'm a big hiking fan, and when I go hiking in some national park, it's actually quite common to have no cellular network, and that's just in the US. If you go to a remote place, say, Africa, that's even more common. And on-device LLMs don't really have this problem because no internet is ever needed. And lastly, running on-device costs zero, basically. So there's going to be a cut down in LLM billing or data center cost, depending on whether you are calling somebody's LLM API or deploying LLMs by yourself. Of course, running LLMs on device is no easy task. We need both a smart model with strong capabilities and sufficient hardware compute to do a good job of actually running LLMs. ML research in particular is pushing the boundaries for smaller yet more capable models. And hardware vendors, especially the chip vendors, are also doing their best to pack more compute power into their chips. In the meanwhile, we need software. Software stack needs to evolve as well, which is why Google's open source media pipe framework is perfect for this purpose. You can leverage media pipe to run LLMs fully on-device. Media pipe is cross-platform, and you can deploy across web, iOS, and Android. It can run on different hardware, CPU and GPUs. Here's a high-level overview of Google's edge AI stack. At the bottom are the hardwares. On top of them is Light RT runtime that executes the AI model on-device. The media pipe framework wraps Light RT under the hood and helps you build highly efficient machine learning pipelines by taking care of all the input-output tensor, conversion, synchronization, and many technical details. On top of the media pipe framework, there are a number of pre-built media pipe tasks. These tasks are high-level solutions targeting the most common machine learning use cases, including computer vision, natural language processing, and audio processing. Now, as far as we are concerned, two tasks are relevant. They are natural language classification and LLM inference. We will talk about them in a little bit. Using media pipe task is very easy. So here's the example code for running LLM inference on web. You import the library first, create an LLM inference option to specify some common parameters like model path, temperature, top K, and so on. Then you write out your prompts, and finally, you can generate the response using Jema model. It's literally four lines of code. Media pipe takes care of all the tokenization, detokenization, streaming, and everything for you. Now, coming back to the Jema models. Instead of choosing one over the other, is there a better way of taking advantage of both so that both models can shine for their most suitable use cases? Of course, that would be hybrid LLM. By hybrid, I mean using both local and remote models at the same time. There are several different ways of building hybrid LLM applications. Let me share some common patterns. First, there's the sequential pattern. Sometimes also called cascading architecture. So in this example, a user query comes in. We first run it on Jema on device. Then we decide whether we should directly return the Jema's response to the user. Or if we feel the response is not good enough, then we send the query to the Jemina model on the cloud for a better answer. To determine if we send the query to the Jema model, one way is to look at the token logics or log properties and set up some kind of confidence threshold. Another way is to use a trend reward model to evaluate the quality. A second architecture is to send the query to both a Jemina model and a Jema model at the same time. And use a response merger to decide which one to return to the user. The merger could be based on latency, like first come, first return, or just based on quality. And lastly, the third pattern, a routing architecture. Here we have a dynamic router which uses a BERT model, actually a smaller version of BERT model specifically designed for edge devices called mobile BERT to dynamically classify if a given user query should be sent to the own device Jema model or the own cloud Jemina model. And, of course, we use media pipe to run the router model first and then we can run the Jemina model on the device as well. We actually build a demo for this architecture and Ian will show you in a minute. The router needs to be fast but smart enough to make the routing decision. We fine-tune the mobile BERT model using data generated by Jema 3 27b model. Here's how it looks in Google AI studio. We basically write a prompt that describes our requirements and instruct a model to generate a list of examples of user queries and the labels. The labels are just local or remote. Very easy in AI studio. After downloading the data, you will need to manually clean up the labels, though. Then we take this bootstrap data and fine-tune the mobile BERT model based on the media pipe text classification tutorial pretty straightforward in Colab. Then we have our router model ready and can deploy it in an app or on a laptop. Next, I'll pass it over to Ian for demo. Thank you, Wei. So, as Ray mentioned, we have our routing model. And we're going to show you the hello world of applications to demonstrate this, the simple weather app. So, the first thing we need to do is we need to understand the capabilities of our different LLMs, our local LLM and our remote LLM. So, in this case, we're going to use the Jema 3 1B model. And we're not going to provide any system instructions or give it any access to external data. We're just going to see what the capability of this model is. So, this is about a 500-megabyte model in memory once it's quantized to int 4. And on a high-end phone, you could output perhaps about 50 tokens a second. So, it's actually pretty quick. So, if we have a look at this example right here, I'm going to send it a simple query, which is, what's the weather in Tokyo? And we can see that it's able to give us some temperature, some humidity. It's able to also give us some links to sites to find sources. So, it's a pretty comprehensive response for such a small model. Obviously, this is based on its trained data. So, it's limited in terms of the timing at which it had this information for. So, it's not going to be accurate. But it's a good start. Next, we can give it some system instructions. And this will help refine the model to do specifically the tasks that we're after, which in this case is going to be being a weather assistant. We're also going to tell it to be concise, because the more concise it is, the quicker we can output. The response for the model as well. So, you can see in this case, it's able to partially follow the instructions. It's pretty good at actually doing the two sentences. And it's pretty good at understanding that it's supposed to not give additional detail. But it still doesn't have all the information to give us live statistics. So, to do that, we would need a rag-based system or we'd need to have it connected to an API in order to do it. So, let's give it a query that's a little bit more reasonable. Something we would expect an on-device model to be able to do. So, in this query, we ask what the weather is in March in Tokyo. So, it's seasonal information. It's something that we should be able to have a model tell us even without internet connectivity. And you can see here that it's answer, about 46 to 61 degrees, is plus or minus 3 degrees compared to some live information. So, it's actually pretty good without any additional information. But at this stage, what we might do is we might actually attach it to a rag system to make sure that it is incredibly accurate in its response. And that wouldn't necessarily need to go online or to have internet connectivity to do. So, let's look at the remote models. So, we've got two examples here. We've got the Gemma 27B, which we could use, for instance, on a high-end desktop or a server. And in this model, we're going to give it the same system instruction and we're going to see what it comes up with. And actually, you can see here in this example, it's pretty good at following the instruction. It doesn't give a specific response. It very much respects the, if you don't have real-time information, don't give it clause. And it's pretty general, gives us a good idea about what the weather might be at that particular time of year. Another option, as Ray mentioned, is to use Gemini with search grounding. Now, what this allows Gemini to do is it allows it to go and look up information from Google Search and use that in its response. So, it gives an answer here about 51 degrees. And if you look at the Google page at the time when the query was made, it's 50 degrees. So, it's even able to cite the sources which you got it from. But you can imagine here you could tie it with tool use over to an API so it could get precise data. So, this is what our architecture would look like for our app. Anything below this line here is going to be on-device. So, the query, Gemma, and all of the local data. And anything above it is going to be in cloud or whatever distribution mechanism you choose. So, we follow the journey. First, we're going to ask what the temperature is in Tokyo. It will go to the BERT model which will assess that particular query. And it will make a decision as to whether it's going to be local or remote. Let's just say it goes down the local route. Gemma is going to take that query and it's going to use tools or a RAG system to go and find that information from a local database. Now, the assumption here is that the data that you could store for your application, you could have some amount of it that's going to be suitable for a small mobile. So, if we take the seasonal data example, it's pretty feasible for us to have this for every city around the world that would fit on a single device. Versus, if we had a version in the cloud, we could have terabytes or even petabytes of real-time weather information that Gemini or another remote device could process. So, if we go remote instead, the query would go up to the Gemini model, hosted on Vertex, GCP or your preferred platform provider. And it could use RAG, API or Google search to augment that to get the right response. One thing that we can also get from Gemini is we've got code execution. So, Gemini is able to generate code and execute it in order to give you the most accurate answer. So, let's just say that the data you have, there's no averages in it. You could use Gemini and Gemini would make a decision. It would write some code and it could average out the data that returns in response to give you that one average value. So, that's pretty much how the architecture would work. Another example is you could use two Gemma models. So, you could have a smaller 1B model here and you could have a 27B more capable model in the cloud. Again, similar system. You give it access to a remote database or an API and it's able to give you a more comprehensive answer at a fraction of the speed. So, we're going to move over to a quick demo now and I'm going to just talk through what's happening in this demo. So, we have a query at the top which is what is the weather going to be like today? So, this is the query that we're going to send initially to the router and then onto the local remote services. So, we have a local response and a remote response. And when we press this button, the router is going to decide which one to send it to. So, we wait a couple seconds and then we're going to get an answer from either our local or remote service. So, in this example, it picked to use the remote service. Probably because it involved the word today and needed real-time information. And you can see at the bottom, we have the decision it made and then the router confidence which was 97%. So, it was pretty certain it needed to use the online service to be able to do this. And that's probably because as of when I recorded this, it needs live information from that time. So, it was 65 degrees in Las Vegas when I did it. So, pretty early in the morning. So, why don't we find an example that we think would work really well with the local LLM? So, for instance, the seasonal information. So, if we run this one, hopefully the router will decide that this query is suitable for the local LLM. And we're going to get an answer just back from the on-device model. And sure enough, we should have a response. There we go. So, it gives a reasonably general response. So, it says it's going to be mild, wet, and gives a temperature and says that there might be showery days. And again, if we compare that to actual data, we can see that it's pretty similar. So, even with the base knowledge of the model, you might decide that that's acceptable for most user queries. Say I was just walking onto a plane or in the middle of a flight and I didn't have access to the internet, I could use that. So, what if we want to compare the two examples? So, this is an important step to help you with the labeling. But if you, for instance, to run the same query for both models, you could get an assessment about how good the response is and how you might choose to label it. So, we're going to ask it which months are hurricane season in the Atlantic. And actually, you can see that the 1B model knows the answer to that. This is pretty standard knowledge. The Gemini model, it gives you a much more comprehensive example. But in the case of just needing to know that information, maybe the local version is suitable. So, there you go. That's how you would use these two models together. One thing to point out, it's worth measuring the response latency for both of these. And I'll explain a little bit afterwards why this is important. There we go. So, some considerations. Wade talked a little bit about bootstrapping the data. So, you could, for instance, train this on a load of general queries. Like, these are the kind of things you would expect to go to the cloud. These are the kind of things you'd expect to go to on device. Things like changing application pages. Maybe that's a query that you want to keep locally because there's no reason for it to necessarily go remotely. And one thing you can do is you can start collecting data from your users as they use queries in your application. And use that to better fine-tune the router to be more and more accurate. You want to make sure that you don't overfit the text classification. So, make sure you keep some queries back to use for the testing phase. Next is the actual cloud LLM decision. So, I've mentioned a couple of examples you could use. You could use a Gemini model. You could use a Gemini model. And it really comes down to what you want your cloud model to be able to do. Maybe you want to use the best model possible, our Gemini 2.5, to do a complex reasoning task. But you know for most queries it's not going to be suitable, in which case you might want to use the local model for that. So, you have a couple of decisions to make there. Next is the LLM acceleration. So, the demo I just showed you was running on a CPU of a standard mid-range laptop. But if you use acceleration with either GPU and NPU, for instance, on a mobile phone, you can get much better performance. What you do have to keep into consideration is you do have one energy constraint, which is the battery. So, that's your one cost on phones. You want to make sure that you're not using that all the time, because otherwise you'll degrade the power for the battery overall. So, again, it's still a balance you have to think about. And then the next one is the offline user experience. So, let's just say you know you have no internet connectivity, but you still want to give an answer. The router itself is going to give you a confidence score as to the labeling. If it, for instance, thinks the high percentage, maybe 80% plus, that it should be a remote routing, then maybe you want to use a threshold to decide whether the local LLM is even going to answer that. In which case, you could have a fallback. Something like, sorry, we can't answer that right now, because, you know, it needs some real-time information, or we need to connect to the internet. But in most cases, you might find that around about 50%, it would be acceptable to show a local response. So, again, it's kind of a threshold that you need to be able to manage. And finally is what you actually use for the routing decision. We talked about some really obvious ones, like connectivity, but it could be based on cost, it could be based on quality, it could be based on whatever heuristic you need. One example, for instance, related to some of the local things is maybe you might have, you might know that you want to, like, keep some data on device, in which case you could say that could be your routing decision. This is user data that we're going to keep personal to the device, we're not going to send that one across, and we're going to just process that locally. So, again, this is just one architecture. Wei mentioned a couple of others, but you can see how combining these two LLMs together, you can get a pretty powerful solution. So, I'm briefly going to touch on the latency. So, we're making assumption here that we have, this is the time here for the router to execute, and then this is the time for the LLM to do its inference. And you can see the difference between the local and the remote, very simplified, is going to be that for the local one, you have the routing time plus the LLM inference time. For the remote one, you have the same routing time, but then you have the LLM inference plus the transport. And the general assumption, if we're talking about getting the fastest possible response, is that this section here has to be shorter than these two sections combined. So, the transport is obviously going to be affected by connectivity. So, you're on like a low bandwidth mobile network, or you're struggling to get signal on Wi-Fi. So, that's going to have an impact, even if the LLM inference is actually quite quick, like we might find with the Gemini Flash model, for instance. So, yes. So, it's something that you need to benchmark and measure when you're thinking about that. But again, this is just for the case where speed is your key criteria. If, for instance, it's quality, maybe you're prepared to wait a little bit longer for a remote service to be able to give you a more comprehensive answer. So, again, you need to kind of factor that into your decision making. So, where would we go from here? So, we talked a little bit about having capability evaluation, trying to figure out what it is that you need the LLMs to be able to do and to kind of where they sit on this bandwidth. We also mentioned that Gemma has a number of different models. So, we have a 1B, a 4B, a 12B, and a 27B. You might find that the 4B model is what you need for the device that you have. Or maybe you need a minimum of a 12B. So, what we encourage you to do is to look at the different capabilities of the models and figure out whether they're able to satisfy your use case. Next is the alternative routing decisions. So, we mentioned a couple that you could use there. But, obviously, you could use a different router, such as an LLM, directly to make the decision. And Wayne mentioned a mechanism in which you could do that. So, you could use the router, see whether the router's own query is good enough, and then have that decide. And what we'll find is that as local LLMs get more and more powerful and quicker, you might find that they're able to do it both in the time that you want to, but also to be able to make that decision better because they're able to comprehend specifically the query that's being asked. Next, and we briefly touched on this, is fine tuning. So, at the moment, these are all base models that we've trained for you to use. But, say you wanted to have the model do something very specific. First of all, try and understand what you can achieve with the long context and with the model's system instructions itself. But if you find you have a very specific purpose, you could fine tune a Gemini or a Gemma model using Vertex AI or your preferred fine tuning method. And then, finally, multimodal hybrid applications. So, we've talked about the text interface, but, for instance, the Gemma 3 series is a multimodal model. So, the 4B model upwards supports vision input. So, for instance, you could have it process images on your device. You could have your phone take a picture of an image, and then you could process it using the local Gemma model. Or if it's more complicated or you have more data, you could process it in the cloud. So, we mentioned why you might want to choose a hybrid solution. We mentioned the different technologies involved in that. We also mentioned the different architectures. What we've demoed today is just one of them. But there are definitely different approaches you could take. And then we talked about how you might optimize your application, both for the capabilities of the models, but for the capabilities of the network and the performance of the device. And, finally, and this is our biggest recommendation, evaluation. Set up your evaluation cases to really understand what's acceptable. And especially when it comes to upgrading your local models and your remote models to make sure your applications still behave as expected. So, as with all these sessions, your feedback is greatly appreciated. We'd love to hear from you. And I'd like to thank you all for coming and listening. Unfortunately, I don't think we have a huge amount of time left in the expo to see the demos live. But if you have been to the AI Vertex model garden, we have some demos of this running. There might be just enough time before you have to go and catch your flight to go and check it out. But we're going to move over to some Q&A. And thank you very much for listening. Thank you.