 Good afternoon, and welcome to Cloud Next. Day one of Cloud Next and right after lunch, which is a pretty busy time, but seeing you all makes me believe that the popularity of this session, second time, has really soared. So happy to note that. My name is Himan Shumeira. I'll be joined by two great speakers, Anup, who's partner in crime on this topic, and Wilson, who's our esteemed customer. We're happy to share the stage with him. So far, a very happy customer. We announced cross-cloud network two years ago. We have many, many customers who've adopted the deployment models that we have with the cross-cloud network architecture. In this specific session, we will focus on the use case one here, building distributed applications. We will primarily focus on how our customers are using this architecture to build and deploy their distributed applications across multiple clouds and hybrid clouds environments. We think that these deployment patterns will continue to accelerate as customers modernize their deployments with SaaS, with AI, and other things. We'll introduce a number of exciting innovations as we talk through the different deployment models here. We've seen two primary deployment models. On the left, we see customers who have large on-premise states, and they continue to move things to cloud. The push has been Gen.AI as customers build Gen.AI applications and leverage SaaS and other tools to bring applications to cloud. These customers need a very strong global, robust connectivity. That's the number one thing they ask for. As they continue to accelerate their deployments on Google Cloud, they want an architecture that can scale. They want an architecture that is future-proofed, as they adopt best of reach SaaS and other things. And, of course, security is top of mind for everyone. Customers want to build a secure and compliant infrastructure. On the right, you see customers who were predominantly born in the cloud. They started out with potentially one cloud environment. And as their dependency on partner services, as their dependency on other SaaS has increased, we see them potentially in multiple clouds. So some of the key careabouts for these customers are similar. They want robust cross-cloud connectivity as they adopt different applications. They want to move data, et cetera. And, of course, security is, again, top of mind, whether that's private security or it's public internet-facing security. In this session, we'll talk about design patterns that address such use cases. We've divided the session into two parts. I will talk about connecting and securing cross-cloud distributed applications. We'll look at some common design patterns, how customers are integrating service connectivity, how customers are integrating security. Wilson will talk a little bit about how at Electronic Arts they're using some of these innovations. And Anup will talk about how you can expand this architecture into a more native service-centric, for service-centric native applications. All right. So we published various design guides to talk through various pillars of the deployment model. I think there are three pillars there that I'm going to cover. One is how do you develop robust connectivity with products like cloud interconnect, cross-cloud interconnect, et cetera, and some of the innovations that we're adding there. Number two, equally important, how do we build a scalable architecture that is future-proofed, right? And number three, how do we integrate security? One of the key things for us has been how these deployment models are future-proofed with the rush towards building Gen.AI applications, et cetera. With AI, we are seeing a huge build-out of network infrastructure. We have never seen that before. The whole industry is seeing this. To address this, we are introducing a huge improvement on our interconnect with the introduction of 400-gig interconnects. This essentially gives customers a fast lane to migrate their data to Google Cloud for various applications. This has been a big constraint for customers, and we are solving this now with 400-gig. 400-gig obviously gives 4x the dedicated capacity of 100-gig sort of thing. That's obvious. But then it also makes it simpler to manage with the reduction of footprint for customers as they build out 400-gig in front of their sites. This, over time, results in lower cost, which is very important, lower cost of bandwidth. 400-gig interconnect has the same powerful capabilities of the interconnect today that customers are used to. Things like security, MacSec, and application awareness that I'm going to talk to you about as well. Last year, late last year, we introduced application awareness in Preview. A lot of our customers are using this. Application awareness is very important as customers bring more and more applications to cloud. Various workloads, whether they are AI workloads or others, often don't have the same levels of priorities that traffic flows. Customers have different workflows. For instance, it could be a backup job that has lower priority versus a user-facing application that has a much higher priority. So how do customers prioritize their applications such that the bandwidth is utilized most efficiently? We launched application awareness on interconnect that enables very granular QoS, quality of service policies, for either strict priority, where you have traffic classes, six traffic classes that you can have strict priority over, or enabling bandwidth sharing across those traffic classes with the ability to specify and control the max bandwidth per traffic class. This is SLA-backed, obviously, on the interconnect and enables very granular control of the interconnect bandwidth such that customers can control their application flows, look at top talkers, optimize their infrastructure, even during traffic surges, etc. A few of our customers are using this capability to bring critical workloads into Google Cloud. We talked about interconnect. Interconnect enables dedicated connectivity from on-prem to cloud. We have launched cross-cloud interconnect that gives you the same dedicated connectivity across clouds. A lot of our customers are using this today. Now we have at least 30 locations each with both AWS and Azure, where we are directly connected with them across the globe, where you can build that private connectivity across regions. A lot of those details are available on our website. We have made a number of advancements there. While cross-cloud interconnect enables dedicated connectivity at 10 gig and 100 gig speeds, we have pre-provision capacity with Oracle Cloud, where customers can go lower than 10 gig speeds for some applications if they need to. We have connectivity with Oracle Cloud in various locations, and we now also support integration with their GovCloud. Customers rely on interconnect to build a highly available, resilient architecture. There's three new things here that I'm going to announce. One is now we have support for BGP best path selection with BGP attributes like ASPath and MED attributes. This allows you the ability to influence incoming and outgoing traffic paths very granularly, right from your on-prem or other cloud location all the way to your VPC infrastructure. For customers that have global footprints, for customers that want to influence the routing on where the traffic ingresses and egresses from Google, this is a very important capability. This is in G&R, and a lot of our customers are already using it. On popular demand, we made significant improvements in interconnect maintenance. Interconnect maintenance dashboards are now available with Cloud Hub. Cloud Hub, it basically provides you full visibility into maintenance, insights, etc. ongoing basis. It's not just available on UI, but it's also integrated with unified maintenance APIs that you can import some of these metrics into your own dashboards. Similarly, we made improvements in our loss and latency metrics. A lot of that is available with VPC flow logs for hybrid traffic. And customers can look at that again to look at their top traffic flows, etc., and also look to optimize how their applications are leveraging the hybrid infrastructure. All right, that was sort of pillar one where we covered robust connectivity and some of the new innovations on Cloud Interconnect. Number two, how do we build a scalable VPC architecture? One of the main things here is this architecture needs to be optimized for simplicity, scale, and high performance. And finally, it needs to be future-proofed as your organization grows, as your organization adopts more SaaS, as your organization, whether it's customer-managed SaaS, whether you manage it yourself, or whether that's Google-managed SaaS. We announced Network Connectivity Center and VPC spokes for Network Connectivity Center last year. This is in GA with all of the features that we've talked about shipping now. This really simplifies VPC connectivity, gives you the topology flexibility. It enables support for both IPv4, and lots of customers are leveraging IPv6 subnets. With this, it gives you 10x scale on our VPC. We have around 1,000 customers already using NCC and taking advantage of a lot of these capabilities, including 10x scale. With NCC policies, you can enable very granular subnet and route policies across your VPC infrastructure for export filter and things like that. Support for lots of customers have asked us this. Does it support across multiple projects, multiple orgs? All of that is supported by default on NCC. Dynamic Route Exchange that allows you to bring all your on-prem routes into the VPC infrastructure is supported with NCC. NCC is a very high-performant gateway-less design that leverages the Google SDN data plane to enable line rate, very high-performance performance across all your VPCs at global scale across all your regions where you might be. NCC is ready for deployment, and lots of customers are using it. One of the things we also added with NCC is enabling policy control for inter-VPC NAT. As customers expand their footprint, they might have to reuse IP addresses across subnets. There are subnets that span across not just VPC boundaries, in some cases even across cloud and on-prem boundaries, often leading to overlapping IPs. In this example, look at how some of these subnets span across different VPCs. This can often lead to complex configurations. With NCC, we enabled inter-VPC NAT between the VPCs, again in a gateway-less fashion, such that customers can deal with IP address exhaustion and simplify their architecture, leveraging this. We will continue to improve NCC architecture and build more capabilities on it. But let's look at a few case studies. The major e-commerce vendor that's deployed NCC today, one of the main things for them is they have peak demand during Black Friday, Cyber Monday time, and they need to continue to optimize the infrastructure to do this. Their adoption of NCC has enabled them to really simplify their design. They have a lot of VPCs in production, and they continue to grow their footprint. The dynamic route exchange was a key feature for them that they have leveraged. Also, which we talked about, we haven't talked about Private Service Connect, that integration of Private Service Connect propagation with the NCC architecture was a key thing for them. They've deployed a lot of SaaS leveraging this architecture on Google, leveraging Vertex and other memory store, et cetera, applications leveraging this architecture, and they're using this in production today. Another example is a large semiconductor company. A few things from Network Connectivity Center, NCC stood out for them. The VPC scale, they wanted their VPCs to be deployed by different developer teams, where they had sharded the different developer teams into VPCs. And customers did that using NCC. They were able to give each developer team their own sort of VPC to sort of build out. The granular controls that we talked about of export filtering and other things were very important for them. They're also leveraging the NAT capabilities today to allow the overlapping IPs across the different VPC teams, such that full control can be given to the DevOps teams, developer teams to manage their own VPCs. These flexible connectivity options with NCC, whether that's Interconnect and other things, have been very useful for this customer. This is one of our customers who gave a very glowing reference of NCC. What they basically said, this NCC integration across all their VPC environments was very necessary for the control they needed as they control their routes across the VPCs as they build out and future-proof their deployments on Google Cloud. Now, NCC has continued to become the gold standard for VPC for peered networks. Similarly, for non-peered network, private service connectivity enables seamless connectivity with this architecture across any SaaS, whether that's Google Managed SaaS, whether that's partner Managed SaaS, that partners who host SaaS on Google, or whether that's SaaS that you manage yourselves. PSC is, again, very reliable, high-performance. It's a proxy-less architecture that enables very high-performance capabilities across non-peered networks. With service connection, automation, et cetera, we have made it very simple to deploy across different VPCs as it enables. We give you the flexibility of choice to use PSC endpoints, PSC interfaces that enable bidirectional communication, producer-initiated bidirectional communication across different services through the service landing VPC shown here. One of the key elements of this, which I touched upon a little bit, was PSC propagation over the NCC architecture. You can develop PSC on one VPC, but then expand it using NCC to enable the endpoint, PSC endpoint, into any VPC that you need access to. NCC supports this policy today. It's in GA, and a lot of our customers are using this, and it tremendously simplifies the PSC design as customers begin to scale their deployments in Google Cloud. I really love this case study. It brings to light a lot of the things that customers use Google for. If you look at the standard AI pipeline, it goes through data preparation, data ingestion, training, and then inferencing workloads. In a lot of these workloads and similar such use cases, one thing is very common. Customers are leveraging various SaaS, and it requires massive data movements across these different SaaSes. And PSC is becoming pivotal to such deployments as customers build these Gen AI applications. In this particular example, you're first ingesting and preparing data. So PSC for data flow enables you to stream data into Google Cloud Storage. It's a very simple model to stream data into it. Vertex Notebooks, a lot of customers are using Vertex Notebooks to develop models. This requires small amounts of data into Vertex Notebooks as customers are fine-tuning models. Again, connectivity over PSC. As customers fully train their models, they leverage Ray on Vertex. Ray on Vertex requires large-scale data into Ray such that models can be trained. Finally, for inferencing, Vertex Online Predictions enables... PSC enables connectivity to Vertex AI Online Predictions for inferencing. And PSC also is connected to Cloud SQL if customers need RAG to fine-tune the results of the inferencing. So if you look through the AI pipeline, PSC is becoming very pivotal. The deployment models we talked about are very pivotal. And I'm explaining this deployment model more in the context of AI. But such a deployment model where you have different SaaS available can be converged with the PSC model. And I would consider using that for a lot of different things. All right. So how can we do a session without a demo on AI? Such deployment models, even though simple, can run into issues. So let's play a demo and see how Google Cloud Assist can solve some of these things. Okay. I think there is an audio issue here. And I can't connect. So something is wrong here. But we're going to use AI to troubleshoot our AI platform. So this is Gemini Cloud Assist Investigations. And we're describing our issue here when we say I can't connect to my PSC endpoint. And next, we're going to add the resource so it knows what we're talking about. And now we've started our Gemini Cloud Assist investigation, which is using Gemini to analyze the information from our project. And so this is pulling in the topology of our resources and the data sources connected to them, analyzing that information so they can come up with a root cause hypothesis for why we can't connect to our PSC endpoint. And already, it's come up with some observations. Such as the fact that our PSC endpoint is pending status and hasn't been accepted by the producer side. And so all the observations are then summarized in our root cause hypothesis, which tells us some suggested next steps, too. All right. So that was AI trying to debug an AI deployment. Fairly common. You set up a PSC endpoint. It's not been authenticated. And leveraging Gemini Cloud Assist that was launched as well. We are leveraging that to debug various problems in Google Cloud, including problems that you might encounter on networking. We have a large ecosystem of PSC services. Google services, partner managed services, et cetera, as well. Finally, coming to the last pillar here, security. Super, super critical. We won't get into a lot of details on this. There's a lot of other security sessions that will do more justice. My goal here is to just show you how this fits into the cross-cloud deployment models. Cloud NGFW, we launched this in GA last year. It's a distributed approach to a firewall where granular firewall policies can be enforced at the host VM level. It does two things. It leverages the threat detection engine from Palo Alto Network, so it gives you the best of that. And it leverages the Google data plane, so it integrates that, such that without any changes to routing policies, you can build firewall rules at that host VM level. This is in GA. A lot of customers are using this. And one of the key things here is building hierarchical sort of policy rules that can be enabled at the org level, such that they apply to all of the different VMs. Similarly, we have secure web proxy that gives you protection for any sort of web traffic. Fully managed. It enables all your compliance goals that you might need. It's a proxy architecture that enables compliance goals, et cetera, and gives you protection against web egress and other things. This can be deployed as an explicit proxy, but then also as a PSE, private service connect endpoint. And we'll show you how that sort of design comes together. Integrating these two things in the deployment architecture that we've been showing is super simple. Cloud NGFW is enabled as a policy on the host VM level. At the host VM level, it protects against any sort of east-west traffic protection. You can build granular rules at the org level, hierarchical rules at the org level, and granularly enforce them at the VM level. The web proxy shows up as a PSE service attachment. It's an isolated VPC that you might use for internet egress. And enabling a PSE service attachment for the secure web proxy makes sure that there's centralized control possible for any VPC. More detailed design guides are obviously published to walk you through all of these architectures. And all of this is supported with the cross-cloud network sort of deployment architectures. Now, without further ado, I'll invite Wilson to speak about how his team at EA is leveraging some of these innovations. Thank you, Valsan. All right. Hi. My name is Wilson Chen. I'm a Cine Principal Architect at Electronic Arts. For those of you who don't know, EA is a video game publisher and developer. We push our products across all the gaming platforms you can imagine. One of the products that we make is called EA Sports FC. It is a soccer simulation game. It's also the biggest sports video game in the world. A lot of the infrastructure that supports this product lives inside GCP and uses some of the networking services and innovation that you heard about just now. But before I tell you about all those, let me tell you a quick story. Once upon a time, if this thing plays, I have to go click on this. Okay. Once upon a time, there was an island, lots of different islands making up a particular kingdom. Every time a new island gets added to this kingdom, there will be a bridge from the new island onto the capital city of the kingdom. And so to get from one island to the other, you travel through the capital city. There is a tradition in this kingdom. Every year, there's a big parade that goes on. Lots of people join in on the parade. The parade goes around all the islands of the kingdom. And it's a really big deal for the kingdom. So last year was no exception. However, five days before the big event, one of the king advisors said, we have a problem. If we want a bigger and grander parade that the king wanted this year, we are going to saturate some of the bridges that we have within our capital city, and the parade can't get through. So they went to the engineer and they said, hey, can we just, I don't know, widen the bridge? The engineer said, well, these bridges were built some long time ago, so finding materials for them is going to be challenging. And they weren't really built to be widened, so probably a month. So that was a no-go. So one of the architects said, I know a guy. He is the resident wizard in one of the islands. He will know what to do. So they brought him to the water chamber, and he said, yeah, I can help you with this problem. All we have to do is build a new bridge, you know, connecting two of the islands and, you know, diverting some traffic. Then we'll be done. And I need a few things, though. You know, those automatic signposts that I help you put into your kingdom around. All you have to do is stick another sign onto that thing, and it will point people to the right direction, including the bridge we're going to build. And with some coordination with other wisters of your other islands and your capital city, we will recite a spell. And voila, a new bridge is built. And so it was. Three days later, a bridge was built. And so the parade went on. Everyone was happy. No one got left out. Nothing was good. And obviously, everyone lived happily ever after. So what happened in that story isn't that far from the truth of what actually happened in our product launch last year. Literally five days before the launch, we discovered a capacity problem in our on-premise network. And we went to the Great Wizard. And by the way, if you look around the room, you will catch a glimpse of the Great Wizard right there. And we said, hey, we need to do something. And so we built a cross-cloud interconnect. And we put it something that connects GCP and JWS. We, just like in the story, we stick a spoke into our NCC. And voila, we have a new cross-cloud interconnect. And it was literally done in three days. That was awesome. We avoided a major disaster in our launch. So we already had NCC set up previously between our on-premise network and GCP. And that's how we route traffic. And so it was really easy to stick another spoke, say, this block in AWS, get advertised to the NCC, go over at the cross-cloud interconnect. And that all worked pretty seamlessly. We also take advantage of some of the PSC stuff that you saw earlier. We use it a lot to do internal service hosting. So it's not third-party. But, you know, for example, in one of our cases, we actually have a third-party vendor coming into our infrastructure and operate a time-series database for us. In that case, we put them into the private network. We put a PSC endpoint in front of it. We propagate it for NCC. And all of a sudden, the rest of our network can have access to it without the two sides getting into each other. So that's great. And that's how we are wanting to build our networks for the future. So all these things that we have, all these patterns and all these architectures, are helping us to look at how do we build more stuff. So as an example, obviously, everyone wants to build Gen AI application with Vortex AI, right? Everyone just keeps talking about it all day. So, yes, if we want to build one of those, well, we're going to build that, put up a PSC endpoint, propagate it for NCC, and all of a sudden, everywhere else, we have it. And that, to me, is the big benefit in my mind. I am not a network engineer. I call myself a pretend network engineer. And so I don't want to look at low-level networking details. I really don't. I could, but I don't want to. And this allows me to put in networking connectivity. This allows me to put in this network architecture without me thinking too, too much about it. And as the story pointed out, it can actually be done really, really quickly without too much fuss. If you know the right people to involve, if you know what you want, that's really good and awesome for us. But that's probably enough of my rambling and my crazy stories. I will pass it to Anoop to tell you more about server-centric architecture. Thank you. Thank you, Wilson. That was an extremely engaging narrative. I don't think I have seen such a narrative at Cloud Next before. Okay. So I'm trying to bring it home. I'm going to start off from where Hemanchu stopped and build upon the blueprints that Hemanchu spoke about and show how you can start from there and evolve into service-centric architectures. So before we go there, why? Why service-centric architectures? So with 75% of the enterprises prioritizing Gen AI applications in the next two years, it's clear that this technology is going to shape how applications are going to be developed. Gen AI developers, alongside the developers who are modernizing their existing applications, are increasingly adopting service-oriented architectures to increase the speed of development. With 65% of them leveraging Kubernetes and serverless platforms to build these modern applications. With this architectural shift, service owners are looking to rapidly deploy, iterate, and secure their applications, abstracting away the complexities of the underlying network and the security architectures that are in place. More than 60% of the developers cite environment provisioning and or ticketing processes as major friction points when it comes to developing net new applications. Adoption of service-oriented architectures comes with its own set of challenges. Adoption of service-oriented architectures, as we spoke in the previous slide, application development velocity is hampered because of the manual ticketing processes that are in place, creating bottlenecks between the different admin personas, SecOps, NetOps, and the developer themselves. Establishing bespoke connectivity models between services across a heterogeneous environment because not everybody has the luxury of starting with a clean slate. The heterogeneous environments could span VMs, serverless, Kubernetes, SaaS applications, and these could be spread out across your multi-cloud and on-prem environments. This has resulted in a complex networking architecture that is difficult to manage and scale. And last but not the least, the inconsistent security architecture that is in place, that is built on top of this complex network, leads to critical gaps in the security posture of the enterprise or the organization, exposing these organizations to vulnerabilities and potential data breaches. So, I'm going to flash the same blueprint that Hemanchu spoke to you about. It's not by chance. This is exactly where I wanted to start, but I'm going to rewind and recap what we introduced last year at Cloud Next. We introduced something called service-centric cross-cloud network. So, with service-centric cross-cloud network, Google's strategy is to empower the developers and enhance the security posture of the organization through simplified connectivity and providing full autonomy to all the admin personas. So, service owners can start their journey in isolated VPCs or non-routable VPCs where they are free to use or rather reuse any IP subnet. The security teams can start using hierarchical security policies on these VPCs that can be applied at the org level, folder level, or even at the project level to ensure that these isolated VPCs comply with the enterprise security posture requirements. Kemanchu already spoke about the uniform framework for publishing applications into the service VPCs, so I'm not going to go there. And he also spoke about how NCC can provide a scalable hub-and-spoke architecture to consume these services and provide connectivity between the workloads that are trying to access these services. The one variation that I would like to add to this blueprint is a load balancer in the service landing VPC. The load balancer in the service landing VPC with PSE as backends provides a couple of very interesting patterns. You can now have granular traffic patterns between the workload and the PSE. By using Cloud Armor, which is tightly integrated to the internal load balancer, you can get service rate limiting and web protections. And lastly, because of the rich ecosystem that we have built with service extensions, now you can bring in a security partner of choice like data loss protection and use that as a value-added service on that load balancer to protect the PSE servers that you're trying to access. So what's next? So what's missing? PSE still requires close coordination between the producer and the consumer. Some common questions that we see as we speak to customers is, can the consumer get a catalog of all the services that they can access? How does the consumer get all the attributes of the service that they're trying to access? Which port is the service published on? Which region is the service hosted on? I'll give you details of why this is important in a bit. Thirdly, if the service developer is using Kubernetes, highly likely that they would have tens, if not hundreds of services that they are managing in the same Kubernetes cluster. Do I need to instantiate one PSE per service? Or is there a more scalable way of achieving this? And last but not the least, you heard about Cloud NGFW. Lots of customers have started adopting it. The question that comes up most frequently is, can that Cloud NGFW also protect the PSE? So let's start with discovery of services. App Hub effectively dismantles the producer-consumer divide by providing an automated discovery and cataloging of all the services, services, together with the policy-based visibility of the services across the organization. So now, it makes it extremely easy for the consumer to look up all the services that they have access to. And that basically starts the journey in terms of how the consumers can establish connectivity to the service. So let's now explore what else does App Hub provide? App Hub also enables network automation. These auto-discovered services that we spoke about in the previous slide, they use a consistent data model, also termed as service binding. So let's start with the service binding. With service binding, you have a uniform service access across all cloud networking products. What does that mean? Simply said, it just means that you can use service binding now as a back into your load balancer. And very soon, you'll be able to use service binding as a DNS service endpoint as well. You can do this without any additional information regarding the service, including but not limited to port, protocol, region, and zone. Service binding also provides real-time updates for the attributes of the service, like service health and service topology, service quality, which enables the consumers to build globally resilient services. How? Since the service binding is a back into the load balancer, the health that is propagated from producer to consumer over service binding can be used by the load balancer to automatically fail over cross regionally and provide the five lines availability that your service may require. Okay. Now let's shift focus a little bit and speak about GKE environments. We spoke about producer VPC, consumer VPC. The same paradigm applies to GKE environments as well. We see broad-based demand from platform engineering teams to provision isolated GKE clusters for each service team. These service teams, as I spoke about earlier, are responsible for tens, if not hundreds of services. Let's start with a simple case here where you have two GKE clusters, workload in the consumer GKE cluster, trying to access multiple, it has dependency with multiple services in the producer GKE cluster. Instantiating one service, one PSE per service is good if you need that kind of isolation between the services. But is there a simpler way of achieving the same? So to enable performant, secure, direct service to service communication with native GKE multi-cluster service experience, we are now introducing multi-cluster PSE services. This delivers direct service to service communication between non-peer GKE clusters with source-based load balancing. This new connectivity pattern that you see on the slide is enabled by enhancements to GKE data plane V2. This pattern is supported across non-peered GKE clusters. And so now you can have overlapping IP subnet space between the two GKE clusters. And this also gives you a very elegant way of solving the IP proliferation issue with large GKE clusters. The recently introduced cloud service mesh capability with Cloud Run extends the same pattern to Cloud Run services as well. So now with a single PSE, you can expose multiple Cloud Run services into the consumer space. So how about VMs? So we do allow the same pattern to be extended to VM-based workloads as well. A VM workload on the consumer side, now we're going back to the pattern that we spoke about earlier. A VM workload connected to the service VPC over NCC can also access multiple services, which may be hosted in a GKE cluster that is not peered over a single PSE. This connectivity pattern is enabled by a couple of enhancements to the load balancer. So we're going to be able to do a future roadmap item, which will be available soon, which is SNI-based routing, as well as all port support on the internal proxy load balancer, which when used as the service attachment on the producer side, provides the capability to access multiple services over a single PSE. Switching gears, let's speak about security. I'm also announcing the availability of cloud and GFW support with the internal proxy load balancer. This simplifies the provisioning of enterprise compliant security posture, providing autonomy to the security operations team to enforce consistent policy for all the PSE services in the service landing VPC. With 24x threat efficacy compared to any other cloud provider, cloud and GFW integration with the load balancer offers simplified configuration and operations with no change to the routing for the traffic. So in this architecture, you see that the cloud and GFW is integrated with the load balancer, with the PSE as the backend to the load balancer. So in this pattern, we do not recommend that you use PSE propagation with NCC, because it gives a backdoor entry for the workloads to directly access the PSE and create a security hole. You could also choose to use VPC service control or VPC SC as an added protection layer, you could also choose to use VPC service to the PSE. So in this case, we have a lot of data that we have to use in the PSE to prevent the access to the PSE from any other entity other than the load balancer. This comes in handy when you have workloads or other services also hosted in the service VPC. We have been working very closely with a number of core development partners to bring many of the capabilities that I spoke to you about today. Goldman Sachs is one of them. And this quote is a testament of what they were able to achieve with the innovations that we just spoke about. Streamlining service discovery, as well as accelerating service delivery. So we're coming to the close of the session here. In summary, we provided you an opinionated, scalable and secure blueprint or network architecture that future-proofs your network for emerging needs of your organization. We also spoke about how the same architecture also allows you to evolve to a service-oriented architecture or networking as you build your Gen AI applications or modernize your existing applications. So with that, we're coming to the close of the session. Your feedback is highly welcome. And we'll be here for a couple of minutes if you have any questions. Thank you. Thank you.