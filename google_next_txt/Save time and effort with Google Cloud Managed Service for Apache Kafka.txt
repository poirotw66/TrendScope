 Well, must have been a good party yesterday. Lots of empty seats today. I'm glad to see that. Now, before we get started, I wanted to address a very difficult topic. It's affected me personally. It's affected many of my colleagues. It's even affected my children. And by the looks in your eyes, I can see that it's affected you as well. That, of course, is AI. Now, just a year ago, I couldn't imagine that in polite company, with children present, no less, we would throw around words like agentic, willy-nilly, like it's nothing. And yet, here we are doing this. These are difficult times. So, I wanted to let you know that there's no shame. It's happening to all of us. There are support groups running all around this building. If you need help, this is not one of them. We're here to talk about infrastructure. We're here to talk about Kafka. My name is Kier. I work on Kafka at Google. And when I say I work on Kafka, I talk about Kafka. With me is Neil. He's an engineer, meaning he doesn't just talk. He builds things with Kafka. He'll tell you what they did at MadHive. So, here's how we're going to handle this. First, I'm going to give you the prototypical timeshare pitch. I'm the talker. I'm going to tell you why we got into the business of building managed services for this technology. What we're trying to accomplish with this, then Neil is going to come up and tell you what they did with Kafka. And we're going to close out with a sneak peek for what's getting launched in the next few weeks. That's the plan. Now, it's not quite what the talk says. There will be some hints about how you save money and time. Absolutely. Just so you know, we had to submit our title before we knew what we were talking about. So, with that, why or why did we build another way to manage Kafka? The answer is microservices. I talked to an engineer I know who's been running a very complicated insurance reconciliation logic for one of the largest insurance companies in the United States. For decades and decades and decades, they ran this with not so much as a unit test. And I said, how is it possible that you needed to do this? And we have all these crazy frameworks and distributed services and all of this. The answer was really simple. Our ambitions have just gotten much greater. Insurance reconciliation compared to what we're building now in e-commerce is just trivial. It's just data processing. So, organization after organization finds itself in this sort of red fire ant nest of quadratically scaling integrations. So, this is not the first time our ambitions have hit the boundary of what the technology was able to offer. We've been here before. We've been here before. And we've come up with this architecture. Right? The field of clovers. The event bus. Some people have made fun of me for using this expression. Parenthese out of fashion. Parenthese out of fashion. I don't know. You can pick your choice. Pick your words. But event platform maybe. Integration platform. Middleware. Whatever you call it. Right? Field of clovers. Point is that there's no N2. N2. Sorry. N squared integration scaling here. Right? Everyone just contributes what events are happening in their service to the bus. And from the bus. If you need to know something, you pick it up. If you're building a unified data platform, which is often what's bringing people into this journey of unifying everything. Right? Maybe you're putting data in BigQuery. You're building a lake house. Whatever it is. Right? It's a big happy dandelion in this field of clovers. But it's just another microservice. There's no difference really. And here you start seeing the reason why Kafka was such a successful technology. Right? If you're going to push everything that happens in your company through this one bus, it has to have the throughput and really the features that is the superset of all the services in the company. If you run it in a single box. Right? And you connect all of your services to that single box. It's not going to work. Kafka solved that problem. It gave us horizontal scaling for the bus. Which you couldn't do in Rabbit. You couldn't do it in IBM MQ. You could do this in Kafka. Right? So all of a sudden, this became possible. And that's why it's been such a success to my mind. So all of that being true, what I see day after day as I talk to customers in GCP, or those folks who are just engineers, who are not yet with us, I see this. There are a few services that are plugged in. They're in the field of clovers. And then there's 80% of the services that are still the fire ants. And to steal a word from an engineer I spoke to a couple of days ago, this person said, you know, I'm a professional slow walker. I manage this infrastructure, right? And I take all the new use cases that people bring to me and I slow walk them so they don't plug into this thing. Why is that happening? We'll take that question in a second. But really, that's the reason we decided to get into this business. Because we still see this picture. And to solve that, what you need is an accessible event bus, right? You need it to be easy. You need it to be safe. You need it to work so that you wouldn't need professional slow walkers. So a couple words on what that means, how we think about what an accessible event bus is. And then we'll talk about what you can do with it. So, when I think about the set of problems you need to solve to have an event bus that works, I think about four things. There's multi-tenancy, metadata integrations, and security. So multi-tenancy is just making the thing scale, right? You need a box big enough that scales for all the use cases, right? What we've done in our managed service already is to say, well, we'll help you set up and run the thing. Well, we'll make sure that it's easy for you to get the right size. So we got that going. On the metadata side, you need to start specifying the structure of the data. So you're not just looking at raw bytes. Fortunately, in the open source community and even with some managed offerings, there's schema-aware clients. So you can use Aver or Protobuf. That's starting to help us. On the integrations, like how do you actually plug in a given service into the field of dandelions, or sorry, field of clovers, we have Kafka Connect and all the plugins that go with it. Debezium, yeah, BigQuery connectors, all sorts of things that are going there. So that open source technology has been quite helpful. And of course, there's security. We've been able to solve some basic things. We've managed to make sure that our clusters are secure by default. They run on private networks. You have a solid authentication option with OAuth and managed identity. And you can manage your authorization as code with Terraform or other scripts. These auth and auth Z boxes are not quite as green as the others because we haven't done all the, we haven't done everything in that space yet. You know, we've picked our favorites. Now, this gets us to my kind of last point on this, which is the reason you need slow walkers is all the empty space in this columns. And that's kind of what our roadmap is. That's what we're trying to do ultimately. So in multi-tenancy, the biggest thing that's happening is scaling in isolation. Right? When you bring it in your use case, you're risking that the person who built it doesn't really know what they're doing. Or they really know what they're doing and they're building an uber successful service. That's going to blow up and stomp over the priority use cases. Right? So how do you make that work by default? How do you deal with hot brokers and hot partitions automatically? If you look beyond that, there is, and you expand to the old use cases, you start looking at ultra-high availability global deployments that are multi-regional or global truly. Or you start looking at ultra-high performance things that maybe look to sacrifice some availability for maybe extra latency. All right? So there's work to do there. On the metadata, you need a place to store the schema. You need a schema registry service, which we kind of know how to do. That's a state that's already out there. But we haven't provided a solution. What's beyond what's available in open source is a way to model the relationship between schema and topics. If you think about how we're managing this today, the relationship between schema and topics that tells you, this is what I have on disk today in the Kafka universe is managed through a convention. On the client side, no less. Right? The client says, you know what? We're going to call this, we're going to call the schema topic name underscore Java class name. It's absolutely ungovernable. Right? And therefore, it's not easy. And therefore, it's not safe. And then if you look beyond that, what we really want is to have some metadata on the fields. Right? We want to understand what's sensitive, what's not sensitive, who is allowed to see this or not, who is not allowed to see that. And so if you look on the security side, on the, you're right. What that prevents us from doing is what we've already done in databases and BigQuery, which is row and column access controls. Right? You can't get there. You can't have security without having the metadata. So, and in the middle you have the connectivity. Right? Let's say you had the core storage working, but you still need to plug those services in. Right? You need it to be, you know, I called it serverless, but really you need to solve all the multi-tenancy problems for Kafka Connect and the integrations. And then you need to start looking at the idea of, are you copying data that doesn't need to be copied? Right? Do you need to sometimes have it be federated? Do you need to be able to access data directly through Kafka or does Kafka access data that's stored elsewhere? You need to be able to access data as well. You need to make sure that you're taking data that's stored for the private sector and you're having to access data here. Right? So this is the, kind of, the thing we're trying to accomplish here. Right? We think that if we can get those yellow boxes to turn green, we will have an accessible service bus, or an event bus, or an event platform. platform, that will get us all to the field of clovers. Luckily, the green boxes have been enough for thousands and thousands of companies to build amazing things with Kafka, and Neil is going to tell you one such story. Thank you, Kier. Hi, everybody. I'm Neil. I'm a principal engineer at MadHive, and today I'm going to tell you a bit about how we've used GCP's Managed Kafka to re-architect some of our event processing pipelines. So a bit of background, what does MadHive do? We are an advertising company, specifically a demand-side platform, so we have clients who want to run their ad campaigns. We participate in real-time auctions to purchase ads, execute their ad campaigns, and provide reporting functionality on top of that. And so kind of at its core, there's a few players here. We have the operator, our clients, they define ad campaigns. We have our bidder who participates in these real-time auctions, sends out bids. And then we have a handful of peripheral services that receive playback events that tell us that ads have been played, that we've won auctions. And we aggregate all of that data in order to give real-time information to our bidder to determine how to proceed with an individual ad campaign. And so, you know, what we do at its core is kind of a lot of high-stakes counting. And we have some pretty strict requirements about what our counting system needs to do. Specifically, we essentially can't tolerate any mistakes. And this means that we cannot duplicate events. We cannot undercount events, right? We pay for these ad slots. We charge our clients for these ad slots. And so we need perfect counting. We also need good durability. Our bidder is going to send out a bid with some information that it knows at that time. And we need that to be durably persisted before we can send our bid out. The risk of failing to do this is that we deliver an impression that we can't answer certain questions about after the fact, like what was the bid price that our bidder sent out? So these are kind of the functional requirements. And then we have a handful of scale requirements as well. So we produce about 100 megabytes per second of raw data that we need to be able to ingest and process. We have about 100 milliseconds to handle an individual ad request in the real-time bidding ecosystem. We obviously can't spend all of that just writing data to Kafka. And so we strive for something like 10 milliseconds to durably persist our data. And so this right here is kind of what our current architecture looks like. On the left-hand side, we've got the kind of players here. We have our bidder that is receiving requests, sending out bidder responses, and persisting that data. We have our supply partners who are sending us information about whether we won or lost a bid. And we have ad players who are playing ads on an end user's device and are notifying us that they have started the ad, that they've finished the ad, things like that. And so in this architecture, we write all of this data to Bigtable, which is kind of an ephemeral data store. We then have this kind of dotted line process, which are all effectively batch processes right now. And so we take that data from Bigtable, we periodically aggregate it and merge it into BigQuery. From there, we aggregate that data at the campaign level so we know for a given campaign how much, you know, how many impressions has it delivered across a number of dimensions, different publishers, device types. And that gets fed back into our bidder, and we use that for our real-time decisioning. And so this architecture has served us well for quite a while. We get high throughput and low latency from Bigtable. We have item potency in our sort of ETL pipelines that merge all this data in, so we get that exactly once semantics that we need. And then, of course, we can aggregate that data to make decisions. But in, you know, the years that we've used this architecture, we've definitely seen a few key drawbacks that we'll talk about. And so the first one of these is that we have pretty bad tail latency, right? Any sort of batch process happens periodically and is going to mean that, you know, the period with which you execute this is going to give you some tail latency. And so right here we have median and 95th percentile latency. The median latency is acceptable, not great, sort of on the order of a minute or so between an event happening and it being fed all the way back into our bidder to be used for decisioning. The 95th percentile latency is even worse, right? We have anywhere between 10 minutes and an hour for these sort of long tail late arriving events to be ingested and used for decisioning. You also see the sort of sawtooth pattern in the 95th percentile because you are periodically executing these ETL pipelines. And so you get, you know, it executes, you get sort of lower latency than average, and then you have this period of time where no ETL happens and your latency rises and you execute your batch pipeline again. So the second drawback here is that there's kind of an impedance mismatch between the way our system is architected and the domain that we operate in. And that's because at its core, auctions are effectively a state machine. So our bidder sends out a bid and we enter this sort of new bid state that you see on the left. And from there, a number of different things can happen, right? We can have an SSP tell us that we won the auction or we lost the auction. And so that sort of transitions our state, right? We won, we lost. We can receive a playback event from the ad player that says, I've actually played your ad. And this sort of renders our bid a delivered impression, right, that our client will pay for and we will pay for. And we could also receive an error, right? The ad player fails to play the ad entirely, and so our bid is in this sort of playback error state. And so this is kind of the domain model that we operate in. And the problem with this and our current architecture is that SQL is stateless, right? We kind of have a state machine, but the thing that we're doing is not managing that state deliberately. And so we've got this sort of funky-looking table on the left, right, which is how you can think about what our data model is after we have aggregated all of the state and events for individual bids. So we have our bids on the left and then sort of an array of events, right? We were told we won, we lost. We were told that the ad played, it finished, or it failed to play. And what we do with that is we run some SQL against it, and you can see sort of an example on the right where we take all that data and we sort of determine what is the current state of every one of those bids and how does that contribute to the aggregate state of a campaign. And so being that SQL is stateless, the problem is that you recompute this every single time you want to answer these questions, right? The state is not explicit here. It's sort of implicit based on all of this data. The other problem with this is that it's kind of cumbersome and error-prone, right? If you look at this SQL, it's a little difficult to follow, right? The state machine is not terribly clear. It's easy to, you know, unintentionally have overlap between your states where you write conditions such that, you know, one state doesn't completely exclude a different state. So sort of difficult to reason about and a little inefficient in the way that you need to recompute all of this state every time. And so given all of this, we decided it was time to rewrite our event processing architecture and why Kafka, right? We sort of explored a number of alternatives here, and Kafka is a great fit for this problem as a technology. And if we go back to the requirements that we had earlier, we can see we need no duplication and no undercounting. And we get this from transactional semantics in Kafka, which is that if we want to consume a message and produce a message and ensure that that operation either completely succeeds or completely fails, transactions allow us to do that. We need durability before we send our response. Obviously, Kafka implements durable storage, and it also gives you a number of knobs to tune how much durability you want, right? How many brokers do you want to have replicated your data before you receive an acknowledgement and you proceed with whatever operation you were doing? So those are sort of the functional requirements, and as far as scale goes, this is kind of the level of scale that barely breaks a sweat for Kafka, right? 100 megabytes a second, 10 milliseconds to, you know, read data. Kafka has absolutely no problem with this. And so this is kind of our V2 architecture, and from a kind of topology perspective, it looks very similar, right? We've got all of our key players on the left. We have, instead of Bigtable now, we have this sort of raw bit events topic. So we ingest all the raw data that comes from our bidders, from SSPs, and from ad players. You can see that BigQuery is still in the picture here, except it's no longer in the hot path, right? We can leverage something like Kafka Connect to take this raw data stream and persist it to BigQuery, kind of out of band. And then for the real-time use cases, we have a couple of services. One is the event handler that sort of consumes this raw firehose of data, and it manages the state of all of our individual transactions. And so it gets an event. It looks at the state of that transaction currently, the event data, and it determines how does that change the state of the transaction, and do any downstream consumers care about this state transaction, or sorry, this state change. And from there, we have impression events, right? So these are state changes that can be aggregated by another service that produces these kind of final campaign-level counts that feed back to our bidder and allow us to make real-time decisions about this aggregate data. And so Kafka is a great technology for this problem, but also a log is kind of the perfect data structure as well for an event-driven state machine. And so what you kind of end up with is, in our case, a stateful Go app, where you have all of the state that you need for a particular partition in memory. You can consume an event. You can look at the current state of that transaction, the event data, and you can instantly produce state changes based on that. You can notify downstream consumers of those state changes, so they can process them, you know, for these campaign-level aggregates. And you also get durability where you can persist your entire event log, your state checkpoints as well. And so when a service comes down and back up, all it has to do is go to Kafka, consume this state topic from the beginning, and it rehydrates the state of all of these transactions, you know, from scratch. And so here's a slide with some code to convince all of you and myself that I am an engineer, and I occasionally get to write some code. And I won't go into great detail here, but this kind of shows at a high level the framework that we use, which is that you receive an event, your service goes, and it reads the current state of your transaction. And sorry, when I say transaction, I mean auction. This is sort of the term of art here. So you read your auction state, and you say, okay, here's my current state. Here's the event that I've received, and how does that change the state of my auction? And so based on that, what you're going to do is one of two things, or potentially both of these things, you're going to update the state of that auction. So I started in state X, I received an event, and I'm going to transition to state Y. And then you also might want to produce some event for downstream consumers, right, the service that wants to aggregate at a campaign level. So we had a new bid that got a playback event. This now counts as an impression, and so go and increment your counters by one. And so you get this kind of tidy framework. And more importantly, you can execute all of this within Kafka transactions. And so you can produce these output messages, and you don't necessarily have to commit these every time you read a message, right? You can kind of batch these, commit them at a cadence that gives you the right tradeoff between overall throughput as well as latency. And so kind of looking at how this performs at scale, one of the great things of this is that the state management itself is pretty lightweight. So we have at the top here a chart that shows the kind of raw data volume, which is here about 30 megabytes a second. I said 100 megabytes a second in the beginning. That is uncompressed data, and so we end up with about 30 megabytes compressed of raw data that we need to consume. And then on the bottom, you see the outputs to our state and impression topics, so the topics that are responsible for persisting individual auction state and notifying downstream consumers of state transitions. And you can see that on the bottom, we have somewhere between 0.5 and 1 megabytes per second of state management. So we take this 30 megabytes a second of raw data, and we condense it down into, you know, 1 30th of that for our state management and transition management. And so the other really important thing here is what does end-to-end latency look like for this entire system, right? We started with about a minute in the median case to receive an event, process it, produce some aggregation, and feed that back into our bidder for real-time decisioning. And so a kind of simple way to reason about, you know, what our end-to-end latency is is to look at the volume of events that get produced, which you see on the top here, and this is somewhere between, you know, 5,000 and 7,000 events per second in this case. And then we look at the bottom where we have consumer lag, so how far behind are all of our individual consumers in consuming these events? And in this case, it's about 0.5 to 1,000 messages. And so we're about 200 milliseconds behind because we are lagging by about one-fifth of the messages produced each second, right? This is kind of a huge improvement over that one-minute median case. And so kind of, you know, after completing this re-architecture, we have a much more simple and faster critical path. So we no longer have BigTable and BigQuery storing this data. We have just a single place where we durably persist our data, and that's Kafka for both our auction states and our campaign aggregations. We've taken what was an implicit state machine that needs to be recomputed every time we want to answer questions about campaign's aggregate state, and we've turned that into an explicit state machine where the state is the thing that we manage directly. We can also get now instant state updates, right? We don't need to ingest an event, persist it, and then wait for this sort of micro-batch pipeline to figure out how does this affect my aggregate state. We receive an event. We figure out how that changes the state and these aggregates, and we can immediately produce them for downstream consumers. And so, you know, we get rid of this tail latency that we had with our micro-batch approach. And lastly, we've decoupled ops from analytics here by taking BigQuery out of the critical path. So like I mentioned earlier, we can leverage something like Kafka Connect to stream all this raw data into BigQuery. It lives there for analytical use cases where we need higher fidelity data, but we have less strict requirements about the latency on that data. And the rest of our real-time use cases remain completely in Kafka where we don't have any of this batch latency. So the kind of lessons that we've learned going through this. The first is that, you know, Google Cloud's managed Kafka doesn't make this free. You're still kind of responsible for understanding your client configuration and tuning your applications and your infrastructure for your workloads. That said, you get a lot of things out of the box that make this much easier than it would be otherwise. You get monitoring for your managed Kafka instances in Stackdriver, and so this makes it easy to tune your application, look at your metrics, see how this affects the, you know, performance indicators that you care about. You get Terraform support. So not only do you not have to manage the infrastructure yourself, you can also put this into Terraform or other infrastructure as code, so you don't have to go to GCP and manage all of this by clicking buttons, which is error-prone, difficult to duplicate across environments. And so big shout-out to managed Kafka for making it really easy for us to validate, scale, and productionize our new architecture. With that, I'll hand it back to Kier to talk about what's next. All right, finishing stretch. All right, sneak peek into what's coming. We just went to the application layer. I'm sorry, I'm going to take you back down into infrastructure, but that's what Kafka is. So I left you with this picture when I came down from the stage, and so the idea is that we're starting to make upward progress along these tracks, particularly in the ecosystem side. So the first thing is in the integrations, the connect service. There's a person there who worked on it sitting in the audience. He's a modest guy, so he won't stand up for claps. But if you look in the managed Kafka UI or our service, Kafka service UI, you will see this today. It's available right now. You can create a Kafka Connect cluster, and it comes to be built with several connectors. So first of all, there's MirrorMaker, and that's for replication between two clusters. So you can use it for migration. You can use it for proofs of concept, right, if you just need to sample some of your data from existing clusters, or you can use it for a disaster recovery kind of strategy, right? Active, active, active, passive, sort of a classic thing. The second thing is BigQuery. We're part of the data platform, and so we wanted to make sure that that worked out of the box. Right alongside that is cloud storage for open format storage. And actually, interestingly, we can think of BigQuery as both the way to get data into structured native storage in BigQuery, but also as a gateway to open formats. So BigQuery can get you the outputs in Iceberg, can get you outputs in Avro, and so forth. So these things are related. What is not on the slide is the connectivity to PubSub, which is your gateway to all the existing cloud platform integrations. So if you want to get to your cloud storage notifications into Kafka, if you want to export logs out of cloud logging into Kafka, or if you want to send G events to Cloud Functions to trigger them, integrate with all the serverless things we have, that's available. It's a preview, so there's plenty of work to do, but this is ready for testing and feedback. So that's the integration. We've turned green. Moving on to security. We wanted to make sure that those boxes on auth and authorization were really green, which is why we're using this color, also because we didn't have the design. So first of all, on the authentication, we're really big fans of OAuth. It's really lovely with application default credentials and Kubernetes. It spares you from having to manage credentials. But if you must, MTLS is coming. We expect another few weeks. So check the release notes. And on authorization, we wanted to close a gap that was left in the service. So we used Google Cloud IAM and Google APIs, and therefore clients like Terraform, to help you manage access to the cluster and the administrative API. But once you were in the cluster, access to topics, consumer groups, et cetera, right, that was not handled by IAM. That was handled within Kafka by Kafka ACLs. And so what we're doing is we're bringing Kafka ACLs and wiring them up to our API so that now you'll be able to express your full access policy in Terraform, for example, down to the topic and consumer group level. So that's also something that's on track. It's not quite ready, but we think that within a few weeks, you should see it in the release notes. So that's security. Moving on, metadata. Not surprisingly, our next step here is a schema registry. So this is a snapshot from our staging environment. So this thing is real. I'm pretty excited about this. The real story here is not that you get the fancy UI and you can look at that, you can diff with color. That's really fun. The real story here is that underneath this is the implementation of the sort of standard open source REST API for schema management. So if you see your, you know, average serializers and deserializers, they'll just work, right, for this API. You can just point them at this thing and they'll register their schema and keep going. So that's the big step in this direction. We're at the start. We're seeing plenty of folks have success with what we have today. But again, there's work left to do. There's work left to get us to that field of dandelions. But I think we're making pretty good progress. With that, I will say, check this out for yourself. It's available. You can test this. If you're interested, there's a talk happening right after this on running Kafka clients in an out-of-scaled way on Cloud Run. So that should be an interesting and relevant sort of thing to do. And we'll stick around to talk to you all. I am much more grateful to all of you who came here and to those people who didn't occupy the empty chairs. No respect to them. But to you, many thanks. Thank you. Thank you.Baik Lacht We'll be right back.