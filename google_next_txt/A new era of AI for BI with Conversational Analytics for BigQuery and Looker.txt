 Over the last decade, we have seen a massive explosion in data, both in data types and in volume. We've also seen growth in data being used in more places, in AI ML systems, in Gen AI applications, in business apps, in streaming and batch processing. This creates a lot of disparate data, disparate data that gives you challenges getting value from it in BI and AI tools. It takes too long to unify data. Insights are sometimes based on out-of-date information. And sometimes you have inconsistent metrics because they're calculated differently in different systems. Today, we're going to show how customers are solving these problems with BigQuery and Looker together. My name is Adam Wilson, and I'm a group product manager on the Looker team at Google Cloud. I'll be joined by three other co-presenters today that I'll introduce in a second. Today's session will be in three parts. First, I'll give an overview of how BigQuery and Looker work great together. Second, I'm so pleased to be joined by one of our customers, a joint customer of BigQuery and Looker, Adiel, that will talk about their journey with the platform to transform their business. And third, I'll invite up my colleague, Greg, who's a product manager on the Looker team, to talk about how we're bringing generative AI into the core of the Looker platform, specifically conversational analytics that lets anyone chat with their data. Let's get into the details. So with BigQuery, you've worked on eliminating data silos. And with Looker, you can also reduce the silos that exist in your BI and analytics landscape. This is because Looker is a complete business intelligence platform for all of your analytics needs. It includes modern BI experiences, self-service tools so that anyone can find answers to questions themselves, custom application development with rich APIs, and insights wherever users work, whether that's in spreadsheets or presentations in workspace and beyond. But Looker is also about ensuring trust. I bet we've all seen examples of where two different dashboards have been presented that have inconsistent numbers. And it leaves users asking, can I trust this? And the time that our teams have spent debugging why the numbers are different and fixing it. As we reach more people with data through Gen.AI experiences and through custom apps, the opportunity for this problem only multiplies. This is why at the core of Looker is our semantic model that ensures a foundation of trust. With Gen.AI, the semantic model is going to be even more critical because it's what lets you deliver superior accuracy by grounding in your enterprise business data. With Looker's semantic model, you define the metrics that are important to your business, whether that's revenue, customer lifetime value, conversion, and you relate that to other important entities in your business too, like orders, customers, or events. With this model defined, users, whether they're in dashboards or in Gen.AI applications or in custom applications can use those metrics to drive insights, knowing that it's built on a foundation of trust. Looker's semantic model is even better when you pair it with BigQuery. And there are a number of advantages. First, Looker's in-database architecture uses all of the power and scale of your data warehouse. Second, Looker provides a number of capabilities that let you manage performance and cost with tools and metrics. And finally, Looker lets you reuse your metrics everywhere across all of your workloads. And we'll go into each of these three on the next slides. What is Looker's in-database architecture? When a user goes to view a dashboard in Looker, we generate a SQL query based on the semantic model and send that directly to your data warehouse. The advantage of this is that we take advantage of the power of the data warehouse, the capacity that you have in BigQuery, the ability to elastically scale, the security rules that you've defined there. Additionally, this is a different architecture than many other BI tools that create extracts and copies of data. With Looker, we don't create any copies. This means there's no latency. When new data lands in your data warehouse, it's immediately visible to users and dashboards. It reduces the likelihood of errors by not needing to copy and keep large volumes of extracts up to date. And it also reduces cost, less moving of data, and less storage. Looker works for over 50 different databases in the cloud and on-premises. But we've also done a lot of work to optimize it specifically for BigQuery. This includes support for BigQuery features, such as nested fields. So with one efficient query, we can get summary-level data and details. We've also optimized our query generation for BigQuery features like partitions, time zones, and pivots. This also allows us to take advantage of all of the innovation that is happening inside of BigQuery. There are many examples of this, but let me give you just one. This is an example of semantic search that allows you to define a vector index and then use that from within BigQuery. Let's take an example. Let's assume you're an e-commerce company and you want to run a target promotion, but you have tens of thousands of different SKUs. How do you decide which ones to focus on for the promotion? Well, you could use a rigid filter just based on product category, but that's not very nuanced. With vector indexes, you could define an index in BigQuery that takes all the attributes of your SKUs and then allows you to do a search against that to figure out which ones are the most semantically relevant to what you want to run your promotion on. So for example, if you were selling housewares, you could put in country chic and find the most relevant items there. With Looker, you can access this vector index directly from the dashboard. So a buyer could type that in to a dashboard and instantly see the most semantically relevant items driven directly from that index defined in your data warehouse. Looker also helps you to control performance and manage your costs inside of the data warehouse too. In many cases, dashboards operate on summarized data. For example, sales by month or sales by product category. Looker can use your semantic model to determine whether there's a pre-created summary that could be stored and then used to answer that question. We call this PDTs. With PDTs, you get faster dashboard load times and reduced query cost because we use the pre-created aggregate summaries rather than going against all your raw data in BigQuery to answer a question. We take this a step further with a feature called aggregate awareness where a user in Looker can be exploring data, moving from high-level questions to very detailed ones, and Looker will automatically determine whether we can use the pre-created summaries or whether we need to use raw data. Beyond this, we have robust query logging and history information so that you can look at how real performance is in the system and make decisions about which features to use when. Performance is something that we're continuing to invest in. And here's just an example of some of the improvements we've made over the last year in terms of improving performance. So I mentioned those PDT and aggregate awareness summary tables. We recently made it so that all of those are now available in BigQuery's tables too, so that you can reuse them all the other places you write SQL. This helps you to have consistent data and reduce duplication of ETL across your data warehouse. We've improved workload management so that you can control where PDTs get executed and which reservation they go against in BigQuery. We've simplified authentication between Looker and BigQuery so it's easier to get started. And finally, we've taken audit logs from Looker and we've brought those into cloud audit logging so you get one unified view of your data warehouse, business intelligence, and application logs in one place. The semantic model is useful not just in Looker, but everywhere. We have a rich API so that you can query the semantic layer from your applications. And we've also pre-integrated that into common analytical tools. So if you have a group of users that wants to use one particular tool or workflow, you can support them there with trusted data. We have connectors for Power BI and Tableau so that they can use the semantic model, and also for spreadsheets like Sheets and coming soon Excel. Spreadsheets are a very interesting case because they're actually the most widely used analytical tool in the world. And it's extremely powerful, but it also creates governance challenges as you have multiple copies, in spreadsheets people can slightly change calculations, and the extracts of data aren't subject to your role-level security in your data warehouse. If you're a BigQuery customer and you also use Sheets, you're probably familiar with Connected Sheets, which is a feature of Sheets that lets you create a pivot table that is live connected to your BigQuery data. So users can explore different fields from your BigQuery tables, can refresh that, and share it with others. Looker makes Connected Sheets even more streamlined. By using the semantic model, Connected Sheets allows users to explore across multiple different BigQuery tables, and it allows them to use the complex metrics that have been defined in the Looker semantic model, all without the users writing a single snippet of SQL. Looker ensures that the calculations are correct, even as people drill down inside the pivot table. Another place you can use the semantic model are Looker Reports, a new feature that we brought to the Looker platform. These are new, visually compelling reports with a free-form report canvas, an entirely new visual stack, and advanced formatting options. With Looker Reports, your creativity really has no limits, and it's easy for anyone to create reports on their own. It's available in the Looker platform, and it works with data from Looker semantic model. Plus, it also allows users to directly connect to data that's in spreadsheets, in BigQuery, or in other databases. You can even combine data from your semantic model with other data from these other sources. For example, let's say you had sales forecast in a spreadsheet, and your actual sales data in BigQuery. You could combine those to look at sales attainment. And again, a user can do all of this with no code, no SQL. Easy for all users across the organization. Because the semantic model is at the core of Looker, we have a robust lifecycle solution to manage it. Multiple analysts can define metrics. We have an environment in IDE to develop and test those metrics, and collaboration tools to help you review changes with built-in source control. This is all built into the Looker product, but it's also extensible. If your team uses Git or has your own deployment systems, Looker can integrate with those. And we've also introduced something new called Looker Continuous Integration, which are new tools to help you move forward with confidence. You can define tests that verify your business logic or the integrity of your dashboards. And these test suites can be run before you push changes to end users, ensuring your team catches issues before your users do. So these are some examples of how Looker and BigQuery really work great together. We have customers all over the world that are using these platforms. And now I'd like to introduce up Quarantin and Bastien from Adio. Adio is a global home improvement retailer operating in over 20 countries, and they use Looker and BigQuery together to transform their business. The couple things I'm most excited about about their story is the scale at which they operate, the number of users they've impacted with BI, and also their vision for the future and innovation, and how they want to use Gen AI to make that even better. Quarantin and Bastien, come on up. Thank you, Adam, for this amazing presentation. Thank you. There you go. Okay. Hello, everyone. We are very happy with Bastien to be there for this presentation. Before starting, we will introduce very quickly ourselves. I'm Quarantin Engler. I'm DataViz Product Manager for Adio. And I'm with Bastien. Hi, everyone. I'm Bastien Hersou. I'm the technical leader of the global DataViz in Adio. AI will be a revolution for analytics. Some of the features you have seen during this summit seems to be magic. Unfortunately, I have a bad news for you. From a data point of view, it's not. In Adio, we want to take all the advantages of the AI. For that, we are building a strong data foundation. On this presentation, we will explain how our company, Adio, is shifting from a data-driven organization to an AI-driven one. I'm not sure everybody knows Adio in this room, so very quickly, I will explain a little bit our business. So we work in the third biggest DIY company, which is called Adio, or Le Roi Merlin Group. So we are selling DIY stuff. We are proposing different services about DIY, but like Google is doing with all the products, we want to impact positively the lives of our customers. So our slogan is like that. We want to make home a positive place to live. To give you some figures about our business, in Adio, we are 100,000 employees. We are saving half a billion users worldwide. We are present in 21 countries, mainly in Europe, but also in Brazil and South Africa. We are making 30 billion euros in volume with around 8% online. Let's go deeper in our transformation. Thank you, Grantin, for introducing the context. Adio has begun transitioning from data-driven strategy to an AI strategy one year ago that indicates a desire to greater maturity and technology. This transition will be possible thanks to three domains inside of Adio. The first of them is data engineering solution. In this domain, you will find how to expose your data, to structure data, how to improve the quality of your data. Then, you have analytic solutions. In this domain, you will find how to explore, how to visualize your data. And the last one, the AI solutions. In this domain, you will find every new AI initiatives inside of the group Adio. And the puzzle is very important to highlight that AI cannot replace data engineering and analytic solutions, but complement them for an optimized decision making. Now, let's focus on our three pillars we will see in this short breakout of Adio. First of all, we'll talk about our legacy sunset to show you why it's important to rationalize our dashboards. Then, we'll talk about more deeper on our common data models and AI data viz. So, first pillar of our transformation, the rationalization of our dashboards. To highlight the need to rationalize our content, this figure, 163,000 is the number of reports we had two years ago in our company. We had more reports than employees. So, Adio, we are data-driven, which is cool. People really like dashboard, maybe too much, and we had to improve this topic. To have a decomposition of this big amount of reports, we had, at that moment, so two years ago, five different tools. We were finalizing the decommissioning of ClickView. We were starting our onboarding on Looker with the first hundred reports on the platform. We had a big amount of content in Power BI for 5,000, a big amount as well in Looker Studio, and the main part of the dashboard was on our old BI tool, SAP Business Object, a BI tool we had for maybe 20 or 30 years, and we had so much dashboard to work on. So, the main part of this rationalization was to decommission Business Object. So, the strategy we had on Adio was to reduce a number of analytic solutions. We wanted to stop to maintain five different BI tools. We wanted to identify two main tools that can fit with 100% of our needs. So, our target, actually, is to have a mix between Looker, Looker Studio, and Power BI. So, the main part of our rationalization was to shift all the business object report inside our Looker platform. This job will be finalized by the end of the year, and we have divided by three the number of reports in our company. This rationalization work is not finished at all. At the end of the year, we want to open the extension Looker Studio Pro inside Looker, and we will start to work on the globalization and the rationalization of our Looker Studio, shifting all the Looker Studio reports inside our governed Looker platform. Thank you, Quentin. Now, let's dive into our common data models inside of Adio. The iceberg metaphor is cleverly used to represent the visible and hidden part of the data management. The above part of the iceberg represents Looker, the data visualization and insights that a customer interact with. And the below part is BigQuery and represents the massive amount of data storage, the processing and optimization of our data analytics inside Adio. Let's dive into more KPIs for BigQuery. In Adio, we have more than six petabytes of data. In comparison, six petabytes represent 1.5 million movies in 4K resolutions. We have around 35,000 datasets and 2.8 million of tables. And all of this data is well organized thanks to our homemade tool called Data Finder that facilitates the governance. For example, every Finder team is owner of a digital product inside of Adio and this product produces its own data. And above all of that, we have Looker. As you can see, we have more than 23,000 users on one instance. That indicates a strong user base and a very trust in the tool thanks to the strength of the semantic layer. We have more than 200 LookML projects automatically created thanks to our provider Terraform. Like the Finder, we have one LookML project for one product inside of Adio. We have around 3,000 dashboards. The number of dashboards is not so big compared to the number of users. This is our volunteer to rationalize our dashboard usage. And the last but not least, the cache usage. Here, I'm going to only talk about the Looker cache, not BI engine, not BigQuery. But this amount of cache suggests a very efficient platform and a lot of cost saving. But now, let's dive into how we can improve our BigQuery usage. Here is the current state of the data inside of Adio. This is a disorganized data structure. For example, we have a lot of isolated data sets for every digital product. We have a lot of data fragmentation because of the local digital product based on Italia, Spain, Portugal, and so on. And we have legacy data warehouse, which is not scalable and not very efficient. So, the target of Adio is to move on data component strategy. data component is aggregation of data from every digital product, local digital product, on unique source of truth for every business perimeter. For example, we find business perimeter for marketplace, for sales, for supply chain, and so on. And thanks to this strong data structure, we will be able to build a robust semantic layer and create business game changer dashboard based on data we can call AI ready. So, last pillar of our transformation, AI database solution. So, we have seen that we are building a strong data foundation with our data components where we have all our technical data, we are building as well a strong semantic layer with Lucker that enable our business to use this data with a business way of displaying this information, what this opportunity it will bring for analytics. For me, it will be clearly a game changer for analytics. It will demultiplicate the number of opportunities, and we use the analogy of the iceberg. Now, we are shifting to a tree, a tree of possibility with many branches. We still have our strong foundation with Looker, with BigQuery, but we will have a new way of addressing analytics. We will have additional point of contact to address analytics and to let our users use our analytics content inside different point of contact. First of all, the extension of Looker Studio Pro inside Looker. We will have a better experience, an easy to use experience for our users using Looker as a self BI. With AI, we will have also a new way of using analytics, conversational analytics. Greg will introduce it, I think, with a demo later. It will be as well a game changer. You will be able with your natural language to find your KPI. With slide generator, you will be able to transfer all this information directly into your slide index with a seamless experience. With LookML Assistant, you will be able to generate a beautiful LookML code to have a strong and robust LookML code for your semantic layer. And last but not least, API Explorer and AI agent, I think it will be a revolution for us to be able to put your analytics directly embarked in your digital product and let your user interact with the data they are using directly in your digital product. So, to make a quick synthesis, the challenges are numerous, but the added value for the business will be really higher. So, we need to be proactive to embark and to take the wave of analytics, of AI, sorry. Thanks, everyone, and I will let Greg continue the presentation. Thank you. Hi, everyone. My name is Greg Michnikov. I'm a product manager on the Looker team, working on Looker conversational analytics, and I'm very excited to talk to you today about what conversational analytics is, dig into some detail about exactly how it works using our new agentic architecture, and also to talk about why the Looker semantic layer that we just heard about is such an important part of getting consistent and accurate results when you chat with your data. And with any luck, we'll also do a live demo so we can see some of this in action. Conversational analytics is our chat with your data feature within Gemini and Looker. The natural language interface of conversational analytics makes it much easier for all users to get answers to their ad hoc questions, and with the natural language interface, it removes the need for familiarity, not just with SQL but even with the BI tool. Users can ask questions and get the data that they need to take action. Something that used to take maybe months or days now can be achieved in minutes or even seconds with this on demand insight generation tool. And users aren't bound by the charts that were created by others in dashboards. Before you'd have to look at a dashboard and see if the answer that you needed was there. Now you can ask a question, but very importantly, even though users are not bound by dashboards, they are still bound by all of the data governance that you've put in place in Looker. Queries are always run as the user asking them, which means that all of the access controls that you have will still apply. Another important piece of conversational analytics that we'll take a look at in the demo is the reusable agent context. A data owner or a data analyst who's very familiar with the data has the ability to create this reusable context that contains things like field descriptions, business jargon, or other instructions that Gemini should keep in mind when answering the questions. And this can be created once in an agent one place, then all the users in your organization can benefit from this when they use the tool. And we'll take a look at that as well. So, how does conversational analytics work? It's powered by our new agentic architecture, and at the center of this agentic architecture is the reasoning engine. Users ask, the reasoning engine takes the input from users, which consists of the question the user asked and additional context that's been provided, and it provides output in the form of data tables or charts that answer the user's questions. And how does it do that? Well, the reasoning engine has access to a suite of AI services that it can use to answer a question. For example, a typical data question probably starts by fetching the context behind the data source. In the case of Looker, this means fetching the explorer metadata, understanding about all the fields. In the case of BigQuery, this is fetching the schema. Again, so you can see the fields, their names, types, and descriptions. After the context tool, the next step that we're going to dive into in more detail is the data query step. In the case of BigQuery, this means generating and running a Google SQL query. In the case of Looker, it means making a Looker API call. And once the reasoning engine runs that query and gets the data back, at that point, it can use a host of other services to get the user the final answer. This includes the visualization generation tool to create a chart that the user can use to understand the data. It also includes the insights tool, which can be used to take a look at that resulting data and look for anomalies, look for trends and other insights that might not be evident to the user right away. And it can also use our new code interpreter that you may have heard about in the morning session today and that we'll be seeing in the demo. This is a way to use Python to do advanced analytics that aren't possible right now. in any BI tool. So we're going to take a deeper look. We're going to zoom in a little bit on the data query step of the reasoning engine. Conversational analytics works with both BigQuery and Looker, but the steps that it needs to take are quite different in one case versus the other. In the case of BigQuery, the reasoning engine needs to generate, or the NL to SQL tool to be specific, needs to generate Google SQL. It needs to not only select the fields, but also to decide on all of the joins, to write up the formulas for all of the calculated metrics, to define all of the filters. In the case of Looker, it's a reduced problem. And that Looker semantic layer that we heard about earlier today really reduces the scope of the problem that the LLM needs to solve in order to answer the query correctly. So if you have Looker and if you've implemented your Looker semantic layer, all of the work that's gone in there has provided so much valuable context that your users take advantage of every day when they use, when they look at dashboards or when they use the Explorer UI to do data analysis. Right? They benefit from the fact that, whoa, sorry about that. They benefit from the fact that you've already defined business, that the LookML developer has already defined business metrics, a one-time place, a source of truth for how you want calculations to be done in your organization. They've already defined joins across all of the different tables in the project so that they're done consistently every time. And they've already specified what date concepts they want to be used so that you can group and filter data consistently. And so all this effort, all this work that's gone in to providing context to the business users in the regular Looker UI also benefits you and also benefits all users when they ask questions of their data in conversational analytics. Let's dig a little deeper into a specific example. So we have a question here. How much revenue was generated by traffic source for accessories in the last 30 days? And in the case when you're querying a Looker Explorer, again, I want you to think about how much knowledge is already contained in that Looker Explorer that will not need to be generated or created on the fly by Gemini. When you're creating a Looker, again, when you're creating a Looker query, all that the LLM needs to do is list out the fields to be used. Maybe you have a specific definition of total revenue in your Looker model. Maybe something that says we're only going to count revenue when the status is complete or when the is returned is set to no. Now, this is something that in the case of Looker, when the LLM generates the query to answer the user's question, all it needs to do is list out the field. And you can rest easy knowing that all the work that's gone onto your Looker model is going to deterministically be applied to translate that field into the appropriate calculation when the query is run against your data source. Similarly, all of the fields across all of the tables in your Looker Explorer are connected by a series of joins that are already defined in the Look ML, which means that, again, the task for the LLM is significantly simplified. It doesn't need to consider the joins. It just needs to list the fields. You can see in this example, users and order items are likely two different tables in the data source. But to Gemini, it's just a single flattened schema. It just lists the fields. And Looker deterministically will join them the way you've defined ahead of time. It's the same thing every time. And so you can feel comfortable that your users will get accurate and consistent answers based on all the context in the Looker model. If you compare that to the SQL that Looker generates, and if you use Looker, you might recognize the SQL tab from the Explorer UI here, it's just a much more complex problem. There's a lot more that goes into generating SQL than into creating the body of a Looker query API call. So we're going to now see this live in a live demo. With any luck, we're going to take a look at some of these things in a real live example. And conversational analytics, by the way, as been said in some of the previous sessions, is available. You can all try this out. So if we could switch over, please, to my laptop. I'm going to take us through a couple of demo questions live using the feature. And again, with an emphasis on what is the reasoning engine doing? How is it really helping us to get to accurate, consistent answers? And the role that the Looker semantic model is playing in really reducing the scope of the problem that the LLM needs to solve. So let's start with, I'm going to be asking questions of a Looker Explorer here. And pardon me for copying and pasting, but I don't want to mess up my typing too much. What was the total margin for each product category in Berlin last week? So as I ask this question, we're going to look up here and see the different steps that the reasoning engine is taking. First, it's getting that context retrieval, understanding the data to use, what fields are present. Then it's moving on to write the query, in this case, a Looker query. And before we even look at the answer, I want to just go back up to the question for a second and think about, what was the total margin? How does it know what total margin means, right? If we're generating SQL every time, we have to make that decision. Just like a human does, it's the same thing for the LLM. Every time, if we're generating SQL, it has to decide what total margin means. But here, because we're using Looker, all the LLM needs to do is tell Looker, I want to reference the total margin field and feel confident that it's going to do that in Looker itself. So, and similarly, and I'll show this in Looker in a moment, not just the total margin, but also last week. What does last week mean? Did last week start on Monday? Did it start on Sunday? Right? Are we going to decide that in SQL every time? Are we going to do that the same way every time? Right? That's something that people struggle with, and it's something that LLMs struggle with, too, when they're writing SQL. Again, Looker solves this problem, because if you use the filter last week in Looker, you can feel comfortable that Looker is going to deterministically translate that to SQL the same way every time. So, we ran this query. We have this feature within Conversationalytics called How Is This Calculated? We can pop this open and see a text sort of bullet point summary of what happened. So, you can see in this case, it selected the columns, the category, and the total margin that I asked about. It filtered the data on the address city field for Berlin. And it's actually interesting to think about. If you've tried generating SQL with an LLM, perhaps you've noticed that it's actually not trivial when you ask about something happening in Berlin to get that right, because I didn't say that the Berlin value was contained in the address city field. So, the LLM needs access to sample values from the columns in order to have the best chance of getting an accurate and consistent answer. And last week. And so, we also have this button, Open and Explore. And what this lets you do is, for someone who's familiar with the data, to open up the Looker UI and really double-check. Does this look like what I expected? This is to build that trust and that confidence. That there's no hallucination going on. That this is really relying on the metrics and the definitions that you've defined in your Looker model. Notice that the query actually contains the filter is previous week, right? It's not writing some date between that might be the same or might be different. Who knows what time zone it's in? Who knows if it started on the right day of the week? Right? By telling Looker previous week, you know that Looker is going to go in here and do the same thing every time. Right? And you don't need to necessarily look at the details of the SQL, but I think it's enough to understand the concept that it's hard to get the SQL consistent every single time. Right? It's much easier to get this Looker query consistent. And I'm just going to paste this into a JSON thing just so it's a little bit easier to read. But you can see like this is the entirety of what the LLM needed to do. Right? The model and Explorer were already known. It chose two fields. It chose the field to filter on, which again is not trivial, but it had the sample value so it identified the right way to spell Berlin, the right casing. And then it literally passed the Looker filter expression last week into the query. And Looker is doing the hard, heavy lifting of translating that into SQL. Let's take a look at another example of the reasoning engine at work. So I'm going to ask another question of the same data set. Show me the relationship between total order count and average margin. So two different Looker measures for each product category. So first again it got the context. It's running the query. And now with any luck it's going to generate a scatterplot. Okay, excellent. So again one of the one of the tools that it has access to is the VisGen feature. And so we got this scatterplot and maybe I'm curious to see. Okay, I see that salads have low order count by high margin and T has a higher count by low margin. So maybe I can ask a follow-up question. What's the average margin for each T? And we've really built this to try to be as conversational as we can. With the new agentic architecture that we just introduced a month or two ago in our preview, we do a much better job of using the conversation history, of being flexible, of sort of using the, of using all these different tools to get to the answer. So here again you can open up, you can see that it filtered on the T column and it gave us that as just margin. So on demand we can start to really dig into our data and understand, okay, certain types of T have lower margin and higher margin. We can try to figure out why so that we can take steps to maybe focus on the T's that are going to make us a higher margin. Alrighty. Here I want to showcase a question that again speaks to the idea that the reasoning engine can do multiple steps. We used to, in a previous version of the feature, always do one shot. It would always be one SQL query every time without that flexibility. But here's a question. What was our best selling product category last quarter and which product in that category did we sell the most of? So hopefully what we'll see here is that the reasoning engine is going to recognize that this is really something that requires two different queries. First, it's going to understand what was that top selling category, which was coffee. And then it's going to ask, let me scroll up so you can see what it just did there. What product in the coffee category had the most sales last quarter? So the reasoning engine successfully broke this question down into two parts. It executed one query. It observed the results. And then it carried that through into the second query where it needed to filter on coffee. And of course, every time I'm querying Looker, I can continue to open and explore and make sure that we are getting the, you know, make sure as somebody who is familiar with the data, I can make sure that it's doing what I'm expecting. All righty. Two more. I want to showcase quickly the reusable agent context. So let's say that I ask this question. What division is doing the best and which is the worst? All right. So what is best and worst? That's a judgment call for a person, right? That's a judgment call for Gemini also. So let's see what approach it chooses to take. It wrote, and notice that, by the way, the reasoning layer is rewriting the question. The first stage is rewriting the question for the query generation layer. So I asked what's the best and the worst. It said, what is the total sales by division? So it decided to interpret this question in terms of total sales. But maybe I actually have a preference, and I want all of my users to have a preference, that if a user asks to compare divisions, which are doing well or not, I want to use total order count as opposed to sales or revenue. So this is where I can create that reusable agent context. I go here. I create my next demo agent. I'm going to select the same explore. And here are my agent instructions that I just mentioned that we're going to use total order count. And I'm going to go ahead and start a conversation with this agent. I'm going to ask the same question that I asked before. And hopefully what we'll see is that this agent context, and this is a really simple example, of course, of just defining one thing in the agent context. But you can imagine that for your business, right? There's a lot of things that you know, right, that Gemini doesn't know about how your business operates, about how you want questions to be answered, what filters you might want to be applied, what people mean when they say certain things. And you can see here that even in this simple example, you know, it went to asking what's the total order count, right? So that reusable context is not just for me. I can share this agent with everyone in my organization. They still need access to the data. Like I said before, the questions will still run as them. Even if you share the agent with them, that doesn't change anything about the data access story. But now everyone who asks a question like this, if they use this agent, will sort of get the question answered the same way that I did here. The last thing that I want to show is a feature that is going to be available in preview very soon if it's not already. And this is our code interpreter within conversational analytics. You may have heard about this in the session this morning about Gemini and Looker. This feature allows us, after we do the context fetching, after we run the data query, if the question is something that would not normally be answerable through SQL, through a traditional BI tool, it can use advanced analytics using Python to get to a place that BI tools have really never been able to get to before. And I'm going to start with two very admittedly simple questions here. But just to get, I want to put the idea in your head that you can go do this right now and this will work. It doesn't need to be a crazy example. It's something that you really can try. So I turned on the advanced analytics. What's the correlation between total order count and average margin for each product category? So again, fetching the context, still using Looker as that source of truth to write the data query. But now hopefully what we're going to see is that the reasoning engine is going to choose to use Python to get the margin. And sure enough, I'll calculate the correlation and let's see what happened here. Sorry. Wrong one. Here we go. So again, admittedly a simple example, but it passed in the data. It generated Python code to calculate the correlation. It's using pandas here. You can see with the data frame. And it got the result here. And it's telling me that the correlation between order count and margin is approximately negative 0.63. That sort of matches what we saw earlier in that scatter plot where there's a little bit of a negative relationship there. Okay. I think in the interest of time, I'm going to wrap up the demo. I want to just jump back to the slides to tell you very quickly about some things that are coming up soon in conversational analytics that we're really excited about. Perfect. Thank you. So later this year, conversational analytics will be available in the Looker dashboard and Looker report interfaces that you've probably heard about earlier during this session and in previous sessions. This means that it's not just going to be a standalone interface. You can actually talk to your dashboard, right? Summarize the data in the dashboard and not just the data that's there, but you can actually dig in, ask a follow-up question, have it go and execute another query using the dashboard as context to give users answers in the context that they're already familiar with. So that's probably, that's on a roadmap as it says here coming later in the year. And then our conversational analytics API. All this functionality is not just available or won't just be available inside of Looker tools. You can also build your own experiences anywhere. And again, you heard about this maybe this morning in the Gemini and Looker session. There's another session coming up tomorrow morning that I'd really encourage you to check out about this topic specifically, about how all this reasoning and the power of the Looker semantic layer is going to be available not just in Looker, but wherever your users are. This could be in chat. This could be in custom apps. It'll be in agent space. So that said, that wraps up what we were going to say about conversational analytics. I think we're almost out of time, but I think Adam's going to come up here and wrap it up. But thank you very much, and I appreciate your time today. Thanks, Greg, for showing us a bit about conversational analytics and Looker, and it truly is a new era of AI in BI. If you want to learn more, there's lots more that you can learn. We have four sessions tomorrow covering more details about conversational analytics and more details about Looker. Also, if you are interested in shaping the future of what conversational analytics experiences look like, there's a special offer from our user research team. They're looking for people to help shape what AI looks like. You can take a picture of that QR code there. And with that, I'd like to thank you all for joining us today, and enjoy the rest of your next. Thanks. Thanks. Thanks. Thanks. Thanks. Thanks. Thanks. Thanks. Thanks. Thanks. Thanks. Thanks. Thanks. Thanks. Thanks. Thanks. Thanks. Thanks. Thanks. Thanks.