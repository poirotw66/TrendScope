 Sebastian Mugazambi Please welcome Sebastian Mugazambi. My name is Sebastian Mugazambi, and as you all know, I will soon be joined on stage by Jeff Dean, Google's chief scientist, for a fireside chat on Google Cloud TPUs and what's next for specialized AI hardware. When we developed TPUs at Google, we had three main problems we had to solve within AI infrastructure. We designed them to deliver the best in-class performance per dollar and performance per watt for all key AI workloads while maintaining high absolute performance at very large scales. And since 2015, when Google first announced TPU v1, up until today, we have been making giant lips between every generation of TPU chips with improvements to make them faster, more scalable, and efficient. Well, speaking of improvements, on Wednesday, Sundar and Amin announced our seventh generation TPU, Ironwood. Let's take a close look at some of the giant lips in advancements we've made to this TPU. Behold! Music Behold, the future of AI compute in the palm of my hand. We can't wait to put the power of Ironwood in the hands of our customers later this year. TPUs power many of Google's AI-driven services, including search, translation, image recognition, innovations coming out of DeepMind, and more. By making them available via Google Cloud Platform, we're able to pass on the same value proposition to our external cloud customers. Year over year, we have seen cloud TPU chip our usage grow by nearly 8x. Some of the most exciting innovations you've been seeing this week are powered by TPUs. Our Gen AI models, video, image, and Gemini multi-model coming out of DeepMind have been trained and served on TPUs. Google has been focusing on specialization at data center scale for accelerated computing since 2015. There are seven really cool things I'll highlight. Let's first start by taking a look at where a lot of the magic happens, the TPU ASIC. A core innovation of TPUs is their use of systolic arrays, which minimizes memory access, a common bottleneck in traditional processors. When it comes to delivering power to the ASIC, while there might be efficient power delivery from the power facility to the data center racks, we need to consider power efficiency within the racks as well. Driving power efficiency to deliver the most flops per watt takes innovating at the chip level. In IEEE APEC 2024, we published the industry's first 1kA mass-deployed vertical power solution, delivering 1,000 amps to ASICs in the Z dimension, with almost 70% reduction in distribution power loss. To have these high-powered chips communicate coherently during a single job, they're connected to each other through our proprietary ICI-Torres topology network within a single ICI domain. We at Google call it a pod. In our seventh-generation TPU, Ironwood, which has 9,216 chips per pod, the ICI topology supports dense models, which are the focus of the vast majority of the ML community. The whole model is activated for each input example or token. See, as great as they are at achieving lots of great things, sparse computation is going to be an important trend in the future. Now, building diverse topologies at this scale requires making the links reconfigurable without electrical to optical conversions. Google has innovated around optical circuit switching, or OCS, since its introduction with TPU V4. Now, imagine you want a 16 by 8 by 4 slice configuration of the Ironwood pod. With OCS, we can connect these racks from anywhere on the data center floor. All these innovations have enabled us to build larger and larger clusters. With Ironwood, pods are connected on ultra-high-speed ICI fabric. Six of these fabrics connected on a non-blocking 16 petabits per second DCN fabric, further scalable to more than 400,000 Ironwood TPUs in a single job. This delivers a staggering 1.8 zetaflops of FP8 computes. Now, after hearing about all these investments in driving hardware innovations for TPUs, you might still be wondering, why specialized AI hardware? Why TPUs? I could cite many more, but here are the top six takeaways from my end. First, they enable massive scale. Customers like Lightrix leverage this size to scale their text-to-video models on our TPUs today. Now, imagine what more they could do with Ironwood. A single interconnected pod of Ironwood chips will deliver over 1.7 petabytes of HBM capacity at 7.2 terabyte per second per chip. This is an 800x improvement in shared memory of our first TPU pod. The third reason for specialized hardware is cost and power efficiency. Google has been on an impressive trajectory of efficiency since the first TPU chip we designed. TPUs have become nearly 30 times more power efficient over the past seven years and nearly six times more cost efficient over the past three years compared to our first generation TPUs, contemporary CPUs, and GPUs. Enabling AI acceleration at scale is not enough in itself. We need to scale reliably. OCS drives much more reliable provisioning of larger slices than hardwired designs. Fifth, beside multi-slice configurations in a single ICI domain, TPUs have the ability to route around failures so the training workload can continue to make progress. Large enterprises like Salesforce are training foundational models reliably at scale using the flexibility of TPUs. Finally, specialized hardware is essential for performance. With Ironwood, a single pod will be able to deliver a staggering 42.5 exoflops of computers. For context, this is over 3,600 times better compute performance than our first generation TPU. Now, it is my distinct honor to introduce the men of the hour. A colleague, Google's chief scientist and core tech lead for Gemini. As a pioneer in the field of AI and a driving force behind Google's groundbreaking AI infrastructure, he works across Google Research and Google DeepMind on AI applications that are solving problems for the benefit of billions of people around the world. Please join me in giving a warm welcome to Jeff Dean. So, before you joined us, we just played a short video clip of the Ironwood TPU. And as you know, Sundar and Amin announced, you know, this will be available for external customers later this year. Let's actually start there. I'm curious to hear about your thoughts on this announcement and, you know, what it means for the near future of AI workload acceleration, given you have pretty much been at the center of AI acceleration for the past decade. Yeah, I mean, I think, obviously, the new chip has, like, got really impressive specifications. It's going to be really exciting. We're looking forward to using it for our internal uses for training Gemini models and so on. But, really, the whole progression of progress over the whole series of generations of TPUs is really what we care about is that you can sort of take, you know, the recipe that has worked extremely well in deep learning is, can you train larger scale models with more compute and more data? And that makes the models more and more capable of, you know, impressive things. And so, you know, Ironwood will help, you know, enable us to take that scaling further. And it does so really efficiently with, like, low energy use. So I'm pretty excited about the chip. It's a nice-looking chip. Currently, as one of the overall tech leads of Google's Gemini AI model effort, as well as chief scientist at Google DeepMind and Google Research, your focus on AI advancements has, you know, your focus is on AI advancements for societal benefit. Can you elaborate on some of the most promising applications for AI that you're currently excited about and how advancements in AI hardware play a crucial role in realizing those AI applications? Yeah, I mean, I think, obviously, AI is becoming more and more capable. And that means there's all kinds of applications in the world that are now unlocked and able to sort of be tackled that, you know, five years ago, ten years ago were sort of only, you know, in our imaginations. You know, a couple I'm really excited about are the ability to have AI models that really help inform healthcare decision-making and education. You know, I think we have taking education. You know, there's now this close-to-realizable dream of having a personalized tutor for any sort of piece of information someone wants to learn that can sort of transform that raw material into whatever style works best for that person's learning style. You know, some people like reading a, you know, a linear chapter of a biology textbook. Some people might like to hear a conversation about, you know, the cell structure between two AI-generated voices. Some people might want a little educational game that helps inform, you know, their understanding of that. And I think, you know, we're close to being able to do that. So that's one that I'm particularly excited about. I and some colleagues and co-authors got together and wrote a paper called Shaping AI's Impact on Billions of People recently. And I think that has a nice treatment of a bunch of different areas where we see excitement, but also some areas where, you know, we see, you know, potential misuse of AI and how can we prevent, you know, some of those downsides while maximizing the upsides. Yeah. And sorry, and one more thing is the role that AI hardware plays in all this is making AI training and inference much more efficient, enables you to bring that capability to more and more people with lower cost for everyone. And I think that's really part of the reason we really are pushing on how do we build the highest quality models with the lowest absolute cost so that everyone can sort of benefit. Your work has been integral to much of Google's infrastructure and developer and machine learning tools. So now looking ahead, what are some of the most significant unsolved challenges in AI infrastructure and hardware that you believe will be critical to address in the coming years to unlock the full potential of artificial intelligence for the benefit of billions of people? Yeah. I mean, I think obviously inference is going to be a really important aspect because as we get more and more capable models trained, there's going to be ML algorithms work that's going to be needed to sort of how do you take a very large but kind of unwieldy model, make that turn into a much smaller model with techniques like distillation and like much better quantization techniques and so on, so that you can run it in very low compute environments in data centers, also on your mobile phone, other mobile devices. And that will then bring the capabilities of these currently quite large models into much smaller form factors and make people able to use them much more throughout their daily life. Jeff, thank you so much for sharing your vision with us. This has been incredibly illuminating. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.