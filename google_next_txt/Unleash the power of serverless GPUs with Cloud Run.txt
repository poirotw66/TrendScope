 Hi, I'm Frank, Tech Lead of Cloud Run. In today's section, I'm going to showcase the power of serverless GPUs with Cloud Run. First, I'm really excited to share with you that Cloud Run GPU is generally available today. It means Cloud Run GPU is ready for your AI workloads in production at scale. Cloud Run GPU is an amazing way to build your AI applications. You can deploy in any containerized applications with GPUs attached to your Cloud Run instance with ease and velocity. It offers fast cold start, deployment speeds, and low surfing latency. Cloud Run GPUs have a global coverage with supported regions in US, Europe, and Asia. The new zonal redundancy option offers higher SLA, so your applications can be built to be more resilient in the event of a zonal failure. We also have reduced the price of L4 GPU by 20% since preview. And last but not least, GPUs will be supported with Cloud Run job soon. You will soon be able to do batch processing or fine tuning with Cloud Run GPU jobs. Cloud Run GPUs has three key advantages to help you build high performance and cost effective AI in production. First, it's on demand. Cloud Run automatically scales up GPU instances when traffic grows or even spikes. It also automatically scales down to zero to optimize your costs. Second, it's fast cold start. Cloud Run can start a new GPU instance with driver installed within seconds, which is critical for the online AI inference workloads with on demand GPUs. Third, it's highly scalable. Cloud Run can scale up to 100 GPUs in just a couple minutes to support large scale production AI workloads. With the technical innovations of Cloud Run GPUs, we measured the time to first token with cold start of less than 20 seconds for popular open models, including newly released JAMA 3 and other popular models like DeepSeq R1 and NAMA 3.1. If we remove framework and model loading from the cold start time, the raw GPU instance startup time with drivers per install is optimized to less than five seconds, something that typically takes multiple minutes on virtual machines. Next, I will have a quick demo. I will start a load generator to send tons of prompts to a stable diffusion service hosted on Cloud Run with GPUs. We will see how Cloud Run Auto Scale GPU instance in action. Here, I firstly configure the load generators to start 100 clients with two seconds of RAM-up intervals. Then, let's start the load. Now, as more clients are up and running, we can see the powerful building Cloud Run Auto Scaler detects the increase in traffic and on-demand spin-up GPU instances. In just five minutes, Cloud Run Auto scales up 100 GPU instances to serve the load. Cloud Run also auto scales down GPU instances when traffic drops. I capture a nice diagram to demonstrate that after my load test traffic drops, Cloud Run Auto scales the number of GPU instances to zero so that I don't need to worry about paying the idle resources. Lastly, I want to thank our customers, Wayfair, Vivo, L'Oreal, Chapter, and many more who are already using Cloud Run GPUs in production and loving it. And Cloud Run GPUs wouldn't be possible without our partners. Special thanks to Olama, Nvidia, and Hugging Face. We can't wait to see how you build and run your AI applications with Cloud Run GPUs. Start using it today by following this link or QR code. Thank you.