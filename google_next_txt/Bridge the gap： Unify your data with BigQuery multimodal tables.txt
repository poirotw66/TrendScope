 . Hey, everybody. Welcome to today's talk on bridging the gap with multimodal tables in BigQuery. I'm Jeff. I'm a developer advocate at Google Cloud, and I focus primarily on BigQuery. Today, we'll begin with a background on kind of getting the historical context about how BigQuery has tried to solve bridging that gap between structured and unstructured data in the past. We'll talk about what's new. We'll introduce a concept called Object Ref, which is really the focus of today's talk, and we'll see how it enables you to use structured and unstructured data together. We'll also have a bunch of demos throughout to make things a little bit more concrete. I think it's easier to see things live, so we'll show that, and then have a recap with some takeaways, some things for you to take home. So just a background. Kicking off with something that I think we all probably recognize. We have this kind of data world that's a little bit fragmented. We have highly organized structured data that sits in neatly, you know, neat tables. We also have semi-structured data, your JSON, your logs, and then we have this, like, vast, wild world of unstructured data that continues to grow. Your images, your audio, your documents, your videos, whatever it may be. Historically, these all required different ways to analyze that data, and they lived in different locations. And the real challenge wasn't analyzing each of those types of data, but it was analyzing them in the same context. So we could analyze them in isolation, but thinking about actually within the same context, that was always a little bit difficult. For example, could we think about customer sentiment or tonality from a call, and then marry that with purchase history for the individual? That integration or bridging that gap has always been slow and complex. We could do this in a silo, and then we'd have to somehow merge these results together. So today we're going to discuss a new concept in BigQuery called Object Ref. Let's just get right into it and get a quick preview. I do want to show this illustrative example, though. Imagine that we're tasked with finding electronics products that have high return rates. These are like attributes in a structured table. But we also have customer photos or videos of those returns, and we want to find out whether they were damaged upon arrival. So that's the machine learning on those images. It's really difficult right now. It can be difficult to get all the...to kind of answer this question in a simple SQL query or writing a simple Python script to do so. We could definitely get these answers in isolation, but then joining them together is a whole different step. So we're making this easier and easier to do, and we'll talk about how BigQuery is addressing this problem. You can see how we can solve this pretty elegantly in just a few lines of SQL here. We could also do this in a very straightforward way with Python. So before we get into these types of queries, I'd like to first give an overview of how we got here from the BigQuery perspective. Like, how did we get to this whole unstructured data and marrying it with structured data in the first place? If you're familiar with it, BigQuery started to bridge this gap around 2023 with the introduction of object tables. And object tables are basically a read-only structured metadata layer over your objects in Google Cloud Storage. So the idea wasn't to move your unstructured data to different places, but rather to put this SQL accessible layer on top of your unstructured objects in Cloud Storage. And you see here this example where there are a bunch of images in a Cloud Storage bucket at the top, and below it is an object table that sits on top of those objects in Cloud Storage. So we see fields like URI, content type that just show their images when they were last updated, and these kind of things. So we can query, we can start to query these objects in Cloud Storage from a SQL interface. And object, yeah, object tables really just present this metadata layer of your GCS objects so that you can query them. And they include this schema on the right side. So you see these fields, you can look through them. But they enabled a bunch of different use cases, like being able to query things like what is the oldest file in my bucket or what is the largest file in my bucket without having to hit the GCS API directly. Access delegation was another use case. So you could query, you know, you didn't have to give access to Google Cloud Storage to all of your data analysts. Instead, you could use a service account that sat in the BigQuery layer to access those Cloud Storage objects and query them without having to, like, give access to those raw assets to the rest of your company. And importantly, this URI field was able to be passed, and is currently able to be passed into BigQuery machine learning functions which enable inference over these unstructured objects. Which, this is just one example of that. So here we have, let's see if we can get the, this, I don't know if, oh, sweet. Oh, okay, sweet. We got this little laser pointer now. So here we have a query where we're running an ml.generateText function over an object table that includes images. And we're calling this Gemini model, and we're saying, okay, for each of these images, return a, a description in a single sentence of that image. And we'll get those results back. We'll get the result with a URI in a structured format. And we can use that for, you know, ad hoc analysis. We could join that to other tables, et cetera. And this is a pretty powerful capability that allows us to look at our unstructured data and figure out what's going on there. It's still a perfectly capable pattern within BigQuery, but as the unstructured data within people's GCS buckets began to grow, we heard from developers that there were more and more limitations that they found. And I'll go over them here. Number one is that with object tables, because if we go, if we just look back again, we were, we were doing this over an object table. So all of the items in this object table were analyzed. If we were using object tables, one of the limitations was that they're read-only. They have a fixed schema, and you can't add additional columns to them. They're really useful, but there's this limitation that it's like, if I want to get the one-sentence description for each of my product images, I can't append that as another column on the same table. Second, there's one object to one row. So every row is mapped strictly to one object in cloud storage. It makes it hard to link related files that way. An example being thinking about those product images, if we have a product image, we might take five or ten photos of the same image from different angles. But that means there's five or ten records in the same object. But we might want to associate them all with one row or one product ID. Third is access control. With object tables, we have one access, we have one cloud resource connection that's associated with the entire table. So if you have differential permissions based on your objects, you need to create more than one table. And next is scalability. We only started to run into this with some very large customers that had like millions and millions or hundreds of millions of objects that they were trying to put into their, or catalog in their object tables. But it was capped at 65 million records. We're now up to 300 million records as of this week or so. So this will continue to grow. But there is still a scalability challenge for some really large companies. So this, all of this points to the need for a more flexible, integrated way of dealing with unstructured data directly within BigQuery. And that brings us to the core evolution. And really the what's new here is object ref. So what is object ref? Well, it's a column in BigQuery. And it's of the struct data type. It's defined with the fields that you see here. And the key elements of this include a URI, which points to an object in cloud storage, an authorizer, which is, allows BigQuery to securely access those objects in cloud storage, a version. And a version is interesting because it's a specific generation ID for that object in cloud storage. And this locks a reference to preserve a precise version of the object so you can reproduce, like, pipelines that you might need to run more than once over time if you have more than one version of an object. This is particularly useful for machine learning workloads. And details include some details around, like, the content type or the size and things like that. So this is just GCS metadata. And we can just look at a quick example of it here. We see in this example a video that's stored in my bucket. That's the URI. There's an authorizer that's a cloud resource connection that allows BigQuery to access that file. There's a version. And then there's some details or some metadata around that object in cloud storage. So let's start with a really simple example. Where does object ref fit into our BigQuery tables? We have this table that looks at, like, an audiobook example. We have some standard fields, like the ID, a release date, and a rating with standard data types, like integers, dates, floating points, whatever. But how do we add unstructured data to this? Well, object refs live next to or in conjunction with all of our structured data. That's how we're starting to bridge that gap. What's really interesting about the object ref column is its composability. You can define these columns directly within your standard BigQuery tables. And you can move them across tables. You can add more than one column to these tables, more than one object ref column to tables. In this example, the object ref is called audio. And it has elements like URI, authorizer, version, and details, as we just looked at a second ago. So these live side by side with the rest of your structured data and allows you to run analysis across your structured and unstructured fields all in the same query. And because object ref is of the struct data type, it supports nesting within arrays. And this is great for scenarios where you have one primary record that relates to multiple unstructured files. So the audio book example, I think, is a nice one because we could logically segment an audio book into things like chapters or different segments. We have this primary record. It's an audio book. And we have the full audio as this column here, this object ref. But we also have all of the chapters that are other object refs. And this is an array of object refs. This is kind of a pseudocode representation. But you could imagine this has all of the other fields as well. Authorizer, version details, et cetera. So storing chapters in an array allows us to preserve order and relationships, enabling analysis like maybe I want to figure out what is the overall emotional tone of a story arc over the course of this book. But I also want to have the full audio referenced as a column two. And, of course, we can continue to add more and more object ref columns as we'd like as well. So here I just added another one that is the cover art for this book. So that might display in my audio book app. There are a couple of ways to create object tables. Or, sorry, object refs. Number one is with object tables. So currently when you create an object table, you don't get this ref column. But very soon, and in this demo we'll show in just a second, the ref column will be automatically created. So you'll automatically get that if you create object tables very soon. The second way is programmatically. So we'll be using functions in the obj namespace, which is a new namespace. And you have direct control here. You can pass in a URI and an authorizer, which is just like a cloud resource connection, and get an object ref that way. So with that, I think we can go over to a demo just to show how this works. So we'll switch over to the live demo. And we'll go through both scenarios here. I guess we'll start with the second one. But the idea here is we have an object in cloud storage. We have like an MP3 file, an audio file. And I'm going to use the make ref function with that file, or with that object, and a cloud resource connection that I've already defined. I'll also wrap it with the fetch metadata function, which allows us to fetch the GCS metadata. So this will just return an object ref that we can check out. And it has all of the elements that you'd expect. So we called it audio ref here. We can call it anything we want. I just chose audio ref. And we have URI, version, authorizer, and details. So that's one way that we can create these. A second way is using object tables. So this is the very simple, the most, basically the most simple syntax to create an object table. And we're creating this object table over a bucket and a prefix within that contains a bunch of phone calls. So we create this object table. And when we check it out, we should now have a ref column that's automatically created. So if you've created object tables in the past, you might be familiar with these fields that you see here. But on the right side now we see a ref column. And ref, of course, includes all of the elements that we just discussed. So a couple of ways to create this. Another way, which is really the first way that we were looking at, is to apply it to an existing BigQuery table. So let's just check this out. We have a table that has a bunch of phone calls for some sort of maybe a couple of different tech companies. And people are calling in and there's the URI column that has a bunch of different calls. There's a little bit of information here about why people called, the product name they're calling about, and some other info. So this is all of our structured data. But then what I want to do is create a new table. I'll call it calls mm, standing for multimodal. We can call it whatever we want. And I'll select everything from the original table. Plus I'll create this object ref column. So we can create this. It should just take a second. And then I'll check it out. So we have all the structured data that we just saw a second ago. And now we have this new column appended called audio ref. So we now have our structured data and our unstructured data all together in a similar table. And here's my cue to go back to the slides. So we'll go back to the slides. And what does this allow us to do? Well, object ref drastically simplifies the way we can do AI and ML on our unstructured data in BigQuery. So this slide just shows a little bit of pseudocode. This won't actually run as is, but it's kind of illustrative. And we'll get into actual examples in just a minute. But BigQuery machine learning functions like ml.generate text, ml.generate embedding, a bunch of these new AI.generate functions now allow us to input object refs. All we need to do is pass the object ref column. And BigQuery securely handles sending that data over to your model, maybe like Gemini or other models, and returning the results back to us. Plus these new functions like AI.generate bool provide strongly typed outputs from large language models like true false, which is really interesting too. There's other talks about that here at Next, so I won't go too deep into that. Now, another interesting thing here is that we can actually pass multiple object refs into a single prompt. So for example, I might want to ask a model to compare two product images or analyze a document based on some sort of accompanying diagram or schematic. We can also pass arrays of object refs into the functions we just saw. So for example, we might pass an array of calls or audio interactions for a customer support use case to see, to understand like what was the conversation, conversation flow over time and understand was the, was the problem resolved at the end. You can pass all of these, you can also pass, you know, these multiple object refs or arrays of object refs into these functions just by continuing to add them kind of at the, at the end. So what does this actually enable? Well, here's just some, some illustrative use cases where we're able to ask sophisticated questions that blend our structured context with a deep understanding of unstructured content. So for example, we might want to know which furniture items were returned with a specific code, where the image or photo said that there was actually no structural damage. So people say that it was damaged, but we don't detect any damage via the, the images or the video being input. Sales calls where a competitor was mentioned that we lost. You know, we can, we can see what's structured and what's unstructured in this annotation. So just this example here, this example from the last slide, you can see how powerful this becomes with just a few lines of SQL code. We pull from our sales table and we have some standard filters like item is furniture, code is defective or damaged. And then we also add this AI generate bool at the end that returns a strongly typed output true, true, false. So for instance, we only return results where there's no sign of damage. And we are passing in images and videos if they exist for each, each record or row. So this is really powerful to see in one query. Next, yeah, we'll move over to the demo. We'll move back to the demo screen. And I just want to show how this works. We're going to use the same example with those calls. So I'm first just going to create a model in BigQuery. We'll be referencing the, just a Gemini model that's hosted in Vertex AI. We're not actually creating a new model. We're not tuning a new model. We're just pointing to a model that already exists in Vertex AI. So we can call it from BigQuery. Now here's an example is we have, we have some, we have a bunch of call info as we saw earlier in this table. And we want to look for high value customers. I just chose a number. Customers with company revenue over $15 million last year. And we get two results back. There's only five results, there's only five records in the table. So we look at these two results. And so we get, we get these two results and we can see some info for them. This is, you know, these are all of the columns. And what we can do is then add something like, the customer wants to buy something. And we're passing this audio ref. So we're passing the actual call through and saying, okay, these are people, these are companies that are deemed high value by some metric. And they want to buy something. So in this case, when we run this, we're passing all of those audio, audio refs or the audio, the MP3 files to Gemini. And it's going to figure out, do they want to buy something? And it looks like David Martin from Synergy Corp does. So we could, we could check out this call and we could actually listen to it just to, just to check it out. Let's try this and see if it works. Yes. Hello. I'm with Martin and Sons. And we're interested in Synergy Corp's cloud storage solutions. I saw an advertisement and wanted to discuss pricing plans for enterprise level storage. We need a reliable solution for our growing data needs. Can you connect me with a sales representative? All right. Can you connect me with a sales representative? It said nobody. But, but yeah, it's, it's really interesting because we can put the, we can put video, audio, images, documents, whatever we want into this. And it's, it's really interesting to write this all in one query. Now I also want to show an example of an array of object refs. Here we're creating an array of these audio refs and we're grouping them by company name. So for every company name, let's get all of these audio refs together and we can just check them out. So there's two companies, there's five records. We can see global tech enterprises has three elements within and Synergy Corp has two. So there's two rows in this output, but there are, you know, there, this array contains three elements and this array contains two. And the interesting thing is we can now pass that, we could pass that to a large language model to figure out like what are the themes that customers are talking about. So within these audio files, oops, within these audio files, what are people saying? I'm sorry, I'll actually show the query. Again, we're calling the Gemini model. We're saying give me two to three key words with an array of audio refs grouped by company name and return an array of themes. We can check out those results that I just ran and people are asking for, you know, these, these themes within those calls. So we passed all those calls through and we get those results back. All right. So let's move back to the slides. So we're not just limited to SQL. I know we've been talking a lot about SQL, but we want to ensure that there's parity with Python as well. So if you love using the pandas API, you love using BigQuery data frames, well, we're enabling all of this there as well. You can create multimodal data frames that have these object refs in them as well as your standard, your standard structured elements or columns in those data frames. As well as we're allowing Python UDFs to use our new object functions under the hood. That's actually what happens is when you run these Python functions, this kind of SQL is generated behind the scenes for us and runs on the BigQuery execution engine. And in this case, when we're using Python UDFs, you're able to use any library that you'd like to, even if, you know, we don't support it otherwise. You can define your own Python UDFs, get your, you know, use any libraries that you'd like to access your unstructured data. And this is really interesting because the get access URL function under the hood, what it does is it takes the reference to an object in cloud storage, gets assigned URL, allows the library or your function to use that, and then write back to cloud storage. So there's a little bit of magic that happens under the scenes, but we try to abstract it as much as possible for you. So you can, you know, use your own libraries and write things back to cloud storage using these functions. So this, we're really trying to ensure interoperability or parity, I guess it's really parity between SQL and Python. So you can use the best tool or the most comfortable tool for you for the job. Now, governance is also important. I don't think we can go through anything without talking about governance. So separate data silos or managing unstructured and structured data is always a pain, but object ref helps, or object ref inherits all of the big query governance that you're used to already, governance principles that you might already be used to. So things like column level security, row level security, data masking, it'll all apply here too. So in this example, it's still the audio book example. We have our, our structured columns. We have the object ref column. And here we restrict the object ref column so that a user or a group of users cannot access it. Just as we do with any of our traditional columns like audio book ID or release date and things like that. So this is column level access. And you, you'd apply it just as you would with any other big query column. Row level access is there too. In this example, what I do is I, I say, hey, no one's able to access something like where the URI in the object ref has my bucket, but they're allowed to access rows that have other bucket. Or any other bucket, I guess. And then lastly, you can note that there's this authorizer that has two different connections here. Maybe we have one of the connections, we allow one of the connections to access GCS and the other connection doesn't. So this is going more at the object level rather than, rather than at the table level. So there's this kind of single consistent governance framework across all of your data types that you can access in BigQuery. Now, next thing we'll do is go through a demo that I think is really illustrative to show you how we can bring that power of structured and unstructured data together using object ref. And we'll use a common civic service called 311. If you're not familiar with 311, many cities around the world offer this service. It's a non-emergency phone line or mobile app or online app that allows people to report issues that they see around where they live. So these are things like there's a pothole on the street, there's a broken street light, there's a tree that fell down, there's a car that's abandoned, these kind of things that allow local governments or municipalities to fix items that might need attention. Historically, these systems relied on text that people input and maybe a photo and they were generally manually processed. So a dispatcher would look at the text and be like, okay, we need to fix this. I'll give it, assign it some sort of priority. But the context can be limited and we might miss related items as well. So today we'll just look at an illustrative example of how we can enhance this type of system using object ref. Beginning with structured data and we'll go more and more in depth with additional details or additional layers of unstructured data as we move forward. So let's move over to the live demo. And I have a couple of tables here. Number one is this city 311 reports table that I created. And you'll see a bunch of structured info here. A ticket ID, when it was reported, a geography point, and some other info like there's a leaking fire hydrant. Here's the description that the person actually input. You can see a huge sinkhole, it's going to eat a car, fix soon. People will input all sorts of stuff and I have too. I live in Los Angeles, I have a 311 app on my phone and I put in all sorts of stuff. So people will put like one word descriptions, people will put all sorts of crazy stuff. And it's not always illustrative of what's actually happening. So I just want to show you this is some of our structured data that we have here for this example. We also have media. And media in this case refers to for each ticket ID we have audio, images, and video. You know, people might not have all three of these but just for this example pretend that, you know, for each of these tickets we have audio, images, and video. So what I'll do next is join these two tables together. And I'll take everything from our reports table. I think it's called, yeah reports. And I'll also create three object refs. One for audio, one for images, and one for video. So I'm going to create a new table called reports MM, which refers to reports multimodal. And we can call it whatever we want. I just chose this. And we can also call these object refs anything we want too. Now what I want to do is look at, like let's, let's look at the, what is this? The image, you know, this image example. So I'm going to pass an image through. And I'm going to say, okay, this is the category that was actually reported. Like when I input this into an app, I say the fire department needs to come fix this or something like that. Or the roads department needs to come fix this. That's, that might be like a primary category or issue. But I'm looking for any secondary issues that might be present in an image that I uploaded. Because although I say that the fire hydrant is leaking, there might be other things in the background as well. So we're just asking the Gemini model to look through the image object ref for anything that might also be present. So we can check this out. We'll run this. Again, it's going to pass through those images and we'll get some results. So the category reported was sinkhole, pothole, leaking fire hydrant. And we see the secondary category that was generated by looking, Gemini in this case, looking at these images. So here we see like abandoned vehicle, but there's a broken tree branch behind it. And I think we could probably take a look at this. If I could make this a little bit bigger. So here we'll go over to Cloud Storage. Check out the images. And check, check out this image. And in doing so, this is a kind of a messy scene. But, uh, there's a car, uh, shouldn't be there. And that was a primary issue. But we see these tree that's also fallen down. And I think, I don't, what did it say? Broken tree branch. It's not, not exactly perfect, but it's pretty close. Um, for this illegal dumping trash pile, um, for which the description was, uh, let's see. There's a huge pile of trash in the alley. I can't walk through it. It gets bigger every day. Okay. Someone reported that. Let's, we could check out that image as well. Uh, and here it is. So, it is getting bigger every day, probably. But, but, uh, the model also found that there was this graffiti on the walls. So that's like a thing that we're all of a sudden able to expose is there are other issues apart from what the person actually submitted. So, pretty cool that we're able to get these secondary categories, or whatever we want to call them, from our unstructured, uh, data. Now, moving on to, like, audio. Here's an interesting example where we can classify the urgency of audio based on people calling in. Um, in this case, we might want to analyze the tonality. We could make this, with this prompt, obviously, a lot more, uh, a lot more detailed. But in this case, it's probably going to just analyze, like, the tonality and how, how excited or angry people are or what they're saying in it. So we're going to, uh, output this urgency string. Uh, and that'll be derived from the audio, uh, inputs. So we'll run this and check out. Some of them are urgent. Some of them calm, agitated. And, uh, like this leaking fire hydrant is, is classified as urgent. Uh, it's a leaking fire hydrant. It's creating a puddle. But that doesn't sound, to me at least, it doesn't sound like a huge issue. So I'm kind of curious what the call actually, what the call sounds like. Let's just check it out. See if we can play it. And... Hi. I'm calling to report a leak from a fire hydrant on Highland Avenue near the movie theater. It's been running for hours. And it's starting to cause a puddle that's getting bigger and bigger. So the person, I don't know if it sounded super urgent, but we could obviously, uh, the, it might be taking some information like the location or what's around it, uh, what's around this leaking fire hydrant. And that might be why it's urgent. If the person was absolutely yelling and furious, it would probably be urgent as well. And, and obviously this prompt is not very detailed. Now next we can add video. So we're doing this one by one right now and we'll add them all in just a second. But video is an interesting use case too, because we could ask what is the estimated safety impact in about 10 words. And we're going to come up with an impact field. We're passing the video ref through to this function. And we're going to, and I think the, the thing about video is it shows what happens over time, which is really unique because if we saw that fire hydrant and it was like just dribbling water, that might not be so much of an issue. If it's spraying water all over, if it's like creating a huge inconvenience or, or something like that, that's a problem. And that's where video is really interesting. So let's run this and just check out what the results are. What is the impact, uh, or estimated safety impact in just a few words. So the impact here, uh, of the, of the leaking fire hydrant, we can see that it's minor water waste, no safety hazard. That's what the model thinks. Um, some of the others are like the, the trash pile that we're looking at, significant fire risk, um, which is interesting. Uh, so I kinda wanna check that out too. And, uh, let's go over to the videos. You can check this out and just look, I mean, we, we obviously saw there's a fire risk. This is, this is like a massive pile of, of trash here. Um, but interestingly, there, there happens to be, uh, a fire right behind it. Uh, of course this is just illustrative for, for this demo. But it's interesting that, that in an, in a video, we're able to start to see what's going on behind the scenes, uh, over time. And Gemini can start to infer this for us. Um, now what I wanna do is go over, uh, combining all of these modalities for like a new report or a new way to prioritize this for this kind of local city government. Um, in this case, I'm going to ask for a, a lot of things actually. And I'm going to ask, give me the primary issue in just a few words. If there's any secondary issues, describe them as we saw. Uh, report the urgency, give me the urgency score from like one to ten. Do I need to solve this now or not? And give me an AI generated description, incorporating the text, images, audio, and video. So we can compare that to what the user input. Uh, give me one sentence description. And what city government function should actually address this issue. In doing so, I'm passing the text description that the citizen input, as well as their image, audio, and video. So for every record, we're combining all of this data together, passing it over to Gemini and asking for the answer. And lastly, we're ordering everything by the urgency score that we're generating. So this is hopefully going to give us something that'll, that'll allow us to understand what's urgent versus what can wait a little bit longer. Um, so we passed all of this information in. Again, we have some of our, our regular, uh, structured data. And we also have, uh, a lot of the derived columns that we just created. So interestingly, we look at like, this is the trash pile example. And the secondary issue is fire. Um, the original description was there's a huge trash pile. It gets bigger every day. The AI summary description is, is a little bit more detailed, um, because it takes into account not only the text that the person input, but also the images, the audio, and the video. We get a recommended action. So remove the trash. We should probably put out the fire too. And the urgency score is 10. Uh, the sanitation department should take care of this. I think the fire department might want to take care of it too. But we could look through, you know, we could look through each of these. Uh, and, and now we can see that what is number six? The water department should take care of the leaking fire hydrant. Okay, great. So, so we're able to prioritize based on urgency. And this just helps us better, um, you know, come up with a 311 system that is maybe a little bit more, uh, I don't know, uh, comprehensive than what we had in the past. But what's really interesting here, again, I think is that we're passing in all of these different modalities of data. Into, uh, into one, um, into one call. And we're incorporating, and we, I mean, we could filter by date. We could, you know, use these geography functions to map things. There's a lot of stuff we can do with the structured data that we have on this table as well. So let's move back to the slides. Um, yeah, just to recap, I, I think we, we just, we, we just recapped that actually. But, uh, other applications you could think of, um, I mean, when we think about tickets or, or like support tickets that we might submit to a city government, we might also think about, uh, tech support tickets. Um, people might say, hey, I'm having a problem turning on my computer. Here's a photo. Here's a video. Something like that. Or I'm having trouble with my code. Here's a photo. Here's a video. Um, and, and all of this could be, could be input. Uh, structured, uh, data from insurance claims like policy info, date, location, what is the coverage type, um, could be combined with photos from the, from the claimant. It could be combined with recorded statements, uh, repair estimates for like if my car got in an accident, um, could, could also be put into this. And we could let the AI cross verify the, the damage consistency on the car, um, with the estimate, with the photos, with my statement, uh, to detect any potential fraud, uh, or, or, or anything like that for insurance adjusters. All right. So just to recap real quick, object ref is not available just yet. We're working on bringing it as soon as we can, uh, and we're realistically looking at early May as a rollout, uh, as preview with allow list. We like to bring it to, uh, public preview with no allow list this summer and generally available in the fall. Um, I'll have a link in just a second so you can sign up. As we move toward general availability, there are a non, a number of other things that we're thinking about. So we'd like to make, uh, object refs even easier to work with day to day, looking at things like connection management and how we can create, uh, connections for you automatically. Um, driving performance and scalability is obviously of utmost importance in the big query world. We want to do things at big query scale. Uh, data lineage is, is also something that we're hearing right now. Um, it's really important for production ML systems so we can track how object refs are being moved across the life cycle and how they can be integrated with vertex AI or other platforms. And lastly, we want to make object refs available for data sharing through systems like analytics hub so that you can share your unstructured data to other organizations. If you have other ideas, please let us know. We're happy to take all of that into account as we continue to build. Um, just to end some takeaways, uh, the idea here is that we want to natively have unstructured references alongside structured data so that we can analyze it all together. Um, we want to leverage big query machine learning functions and Python UDFs directly on top of these object refs for integrated AI ML and other custom transformations. And of course, uh, applying fine grain access controls or other security across all of our data types. Um, we'd also want to bring parity for SQL and Python, uh, so that you can use the tool that you're actually comfortable with. And, uh, I think that's, that's it. Uh, for next steps, I just have a few things. Please take these down if you'd like. Um, we can give you preview access. There's a link here. Uh, if you run into any issues, please let me know and I will do my best to get you signed up. There's a survey for our, uh, UX research team who's trying to figure out how we're using multimodal data. If you'd like to fill that out, there are a couple of additional sessions going on. The one on the top, uh, deep dive into multimodal embeddings and vector search is happening in ballroom E at 11 o'clock. It's going to be good. And the second one, uh, data science with BigQuery data frames will, uh, was yesterday, but it was recorded. So please check it out if you're interested. That also, uh, has heavy integration with object ref. Uh, that's it. Thank you for attending. I'm looking forward to seeing how you use object ref. Thank you for having me on the object ref.