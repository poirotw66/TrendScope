 Hello, everyone. Welcome to today's session. Excited to see such an engaged audience for a late afternoon one. Quick show of hands, how many of you are going to the next afterparty tonight? Not so many, huh? Are you all excited? It's going to be a great show. So, getting back to this exciting session first. So, I'm Vidya Shanmugam. I'm here to talk about several of the AI capabilities in BigQuery's AI and data platform and how it can help you innovate your data workloads. I'm Vidya Shanmugam. I'm a product manager in BigQuery. I have with me my colleague, Tim Bizzold, also a product manager at BigQuery, and our value customer, Pooja Panchagnola, who's the director of enterprise data management at GAF. You'll hear from both of them later in today's session. The goal of today's session is twofold. First, we'll start with going deeper into several new launches we have had in BigQuery's data platform, including data preparation, our BigQuery notebooks, and BigQuery Studio. You'll see a lot of advancements and how it can help simplify your data platform. Second, we'll hear about some of the best practices to accelerate your data journey from our customer, as well as we'll go do a deep dive into several features. So, there's going to be a demo at the end that Tim is going to do. So, stay tuned. Don't go away anywhere. With that, let's jump in. When we've talked to several of our customers, these three challenges race to the real top, right? One is the time sink. Imagine you're spending weeks and weeks wrangling your data only to get bogged down in code. You want to go from your data to insights in minutes. You want to get that as soon as you can. The second one is the inefficient data analysis problem. In this ever-changing, rapidly-changing world of Gen AI, how many times have you heard Gen AI in this session and all of these sessions you've heard at Next? Next, as much as this technology is evolving, organizations are not embracing the full power of their data and their analytics. Tools are fragmented across their organization. Data and AI is not available at scale for your orgs, which means the insights, the apps, they're all limited to a small set of experts. This results in inefficient and unproductive workflows. The third challenge is missed opportunity. Data is incredibly valuable. It holds immense value, but not when it's untapped. Data gets tangled in silos. You don't have metadata. You have complex workflows, which means your insights remain hidden. Let's change that. With BigQuery, we have a portfolio of tools at your disposal to address several of these data challenges and make your data teams more proactive, more efficient. First, we'll start with Gemini-enabled data preparation. It's a first-of-a-kind AI tool that helps you clean, prep your data with ease, run ETL pipelines with no code. Second, we have a very fully integrated notebook experience, which goes from data to code in just minutes. Lastly, we'll talk about our unified data platform that helps you break down your data silos, that helps you unlock your metadata and go to insights with ease. The power of data plus Gen AI is immense. Imagine a scenario where you have a data engineer on your team who has tons of raw data, there's so many inconsistencies, and they're having a hard time preparing this data. What if it's automatically cleaned for you? What if the pipeline is automatically created and scheduled for you? Wouldn't that be really wonderful? And so I have Tim here on stage to talk about one such scenario and how data preparation as a tool can help you. Go ahead, Tim. Fantastic. Thank you, Vidya. All right. Welcome, everyone. Let's see how we can tackle the time sync problem. First of all, as a data practitioner, you may be familiar with the amount of time spent preparing data and getting it ready for analysis. Studies have also shown that it can take up to 80% of a data project's time. And in addition, a fragmented tooling landscape can also contribute to the time sync. However, with Gen AI, we've seen promising results in tackling these difficult tasks. So today, we are announcing the general availability of BigQuery data preparation. A Gemini feature that allows users of all technical backgrounds to efficiently prepare data for analysis. It leverages Gemini to help you automate these difficult tasks. Data preparation helps you build visual data pipelines natively in BigQuery Studio. It generates SQL that runs on BigQuery and therefore has the complete and comprehensive set of data transformation capabilities. Built-in schema handling helps you manage schema drift and prevents your production pipelines from failing. Data drift can be addressed by defining validation rules or data quality assertions that automatically route invalid rows to a designated error table and therefore help you ensure data quality and integrity. And the best part, this Gemini in BigQuery feature is now available to all of our BigQuery customers without any additional cost. not sure if you've seen the other sessions today on around BigQuery, but you may have seen an experimental preview for our data engineering agents. What they do is they allow you to specify your high-level intent, capture that intent, and break it into pipeline code. And if you have low-code users, instead of looking at a wall of code, BigQuery data preparation really helps you break this pipeline code down into manageable pipeline steps and see a preview of the data as you're developing your pipeline. During the preview and learning from customers like GAF, we've observed that our customers find a lot of value for data preparation for their power users, data analysts that are able to self-serve when building their own data pipelines and data preparation. But there's also a desire to seamlessly integrate with data engineering teams and engineering processes. And we've built for that by allowing you to view data preparations and pipe query syntax and check that code into a Git repository for version control. All right. Tying this all together is yet another new feature that we are announcing to GA this week called BigQuery data pipelines. That is a new feature that also allows you to create these tasks for data preparations, SQL queries, or BigQuery notebooks in a single click. You can think of these as your data models that are being chained together to one single cohesive data pipeline. It is also the home for our data engineering agents. Backed by Dataform, these software development features allow you to collaborate on these code assets and publish them to Git repositories. All right. Let me hand it back over to Vidya to discuss our BigQuery notebooks. Thank you so much, Tim. We talked about the time sink. Now let's talk about the second challenge, the inefficient data analysis. There are several blockers that stop users from doing the next level data analysis. Tool-centric workflows, they cause disjointed processes. Poor collaboration, it hampers knowledge sharing within your users. If you rely on advanced analytics, if you rely on coding skills, creates barriers, and it hampers from knowledge going to several key contributors and stakeholders in your organization. All of this results in manual effort, it results in high costs, delayed insights, and most importantly, insights that go unnoticed and not available to you. So with BigQuery notebooks, we're able to solve several of these data issues. BigQuery notebooks is a serverless workspace. It's directly integrated into the BigQuery console. If several of you are familiar with the SQL editor kind of interface, it's available in the same BigQuery Studio integrated experience. It's an embedded notebook built on top of CoLab Enterprise. It supports multiple engines, BigQuery, Apache Spark, local pandas. It supports both BigQuery data frames and Spark data frames, which is a familiar pandas interface as several of you are aware, for preparing, training, and scoring your data. What this gives is for all the ML developers and for the data scientists in the house, the ability to work with your familiar tools, but with the power, the scale, and flexibility of BigQuery. That's super powerful. It's also integrated directly into DataForm. So Tim mentioned where data preparation and pipelines that was integrated with code management and Git repo. So DataForm allows that integration of these notebooks so you can manage all of your code assets as well. And no talk can go without all of the Gemini integrations. So in notebooks, you have embedded in-context Gen AI experiences where you have code generation, code completion, as well as debugging, making your data science workflows very efficient and simple. We also support Apache Spark in BigQuery for all of those who use open source. This enables both batch or serverless Spark development directly within your BigQuery notebook. There is no setup. There's no infrastructure or clusters for you to manage. It provides auto-scaling for any workload of any size, and you only pay for what you use, right, the consumption model here. So that's the second challenge. We talked about the time sink, we talked about the inefficient data analysis, and now going to the last challenge. There's the missed opportunity with untapped data insights. And with BigQuery's seamless and unified data platform, we are able to unlock the data insights. Imagine a workspace where data analysts, data scientists, and data engineers can seamlessly work together, regardless of the preferred tool of your choice, regardless of your skill set or your expertise. Regardless of one's role, right, like whether they're an analyst, they're a developer, they're a data scientist, engineer, or a product manager like me, they're able to all work in a single, cohesive workspace. That's the future of BigQuery Studio. You're able to do all the tasks, whether it's data analysis, creating a pipeline, creating a production workflow, sharing your code or data assets all in a single space. This is where productivity and collaboration truly takes center stage. That's the power of BigQuery Studio, which we're going to show later on in the demo. What is BigQuery Studio? It's a seamless, single, unified workspace for all the users in your organization. So no more switching between applications. Whether you're writing SQL queries, whether you're developing Python scripts, building ML models, all can be done in a single environment, right? That's so powerful to have it all together. No more language barriers. Whether you're familiar with SQL, Python, Spark, JavaScript, or even natural language, all of that is available in BigQuery, which means data is accessible and not logged to all the users in your organization. It also provides collaboration, centralized version control, revision history, so you can track changes, you can collaborate, you can use all of the software development best practices that you're familiar with for your data and for your code assets. Some of the coolest features are integration with the Gemini Chat agent, SQL query, and code completion and code assistant. It's like having an AI partner that works alongside you for writing SQL queries, for writing your code, for answering questions, for getting insights at no quick minute for you. This is the future of data analysis and BigQuery is leading with it. Since the launch of BigQuery Studios GA, we've had hundreds of thousands of users embrace it. There's rapid adoption and positive feedback. It's one of the most popular workspaces where we run over a million queries every single day. To bring it all together, BigQuery is a differentiated data and AI platform that empowers every single user in your organization to become a data person. It's a platform that fosters collaboration, embraces innovation, and unlocks the true potential of data. So we sincerely hope you'll look at all of these features and try it in your workflows. With that, I'd like to invite Pooja, our customer, to talk about how GAF is using BigQuery and embracing the data platform. Thank you, Pooja. Thanks, Vidya. Hello, everyone. I know it's late in the day. As Vidya and Tim alluded, we're closing in on day two of two really long sessions. So thank you so much for sticking around. I know some of us are definitely thinking about that after-conference mixers. I'm definitely doing that. Margarita or two, right? So I promise to keep it fun, light, and shockingly relevant to data. My name is Pooja Panchagnola, and I lead the Enterprise Data Management team at GIF. We are responsible for data engineering, data transformation, data product creation, and data observability using Monte Carlo and metadata management. We use Google Cloud Platform as our enterprise data lake and Google BigQuery as our cloud data warehouse. Let's start with a funny story, like I mentioned, something that you'll never really hear on a Gartner white paper. This week, thanks to spring break, I came across a headline. TSA stops a man at Newark Airport passing through security with a live turtle in his pants. Live turtle in his pants, right? And TSA's comment? It wasn't even the weirdest thing they saw that day. And you can't make this stuff up, right? Now, besides being funny, slightly bizarre, and deeply concerning, and no offense to any animal lovers here, it made me think that turtle is a perfect analogy for so many data problems we have with data. So stick around. Because sometimes, the value is there. The risk is real. But it's hidden in plain sight. Until something breaks, or worse, we end up in Monday morning all hands. So let me bring that back to something real. At GAF, we're the leaders in roofing and building materials. Part of what sets us apart is our customer engagement, and how we reward our customers through incentive programs, which are tied to loyalty, engagement, and spend. This is a critical investment for us. Sounds simple, right? Send a reward, make a customer happy, repeat. But under the hood, the process is pretty complicated. There is a lot of data fragmentation. Data is received in various formats, unstructured, emails, Google Drives, et cetera. So the data has to be processed manually as well. Data lives scattered in emails and files named final, underscore final, underscore really final, version 3, use me. Sound familiar? Right? So, and the reconciliation and closing of all of this data is done manually by the smartest humans armed with least scalable tools. So what are the risks? I'm sure most of you intelligent people here have guessed, right? There is a potential of overpaying, duplicate payments slipping through the cracks. There is no easy way to know if there's a common address that exists on all these multiple files, and no way to connect the dots across our multiple reward partner programs because we have several of them. Somehow, it worked or we made it work because our business is incredible and they held it together with a lot of duct tape, coffee, muscle memory, and sheer willpower. But the reality, there's always that one missed file, that one whoops, and it all unravels. So then came the moment. Our business team came to us with a seemingly simple ask. They asked, hey, can we quickly stitch this data together? And you all know that's basically code for, hey, can you untangle this mess? Years of manual logic, mismatched formats, and data that's in multiple sheets in about 15 minutes, right? But we gave it a shot. And in trying to solve for it, we uncovered something bigger. If your business spent most of their time requesting, finding, and not being able to prepare the data, then what real value were we bringing to them? That was our aha moment. We weren't just lacking tools, or the problem wasn't about messy files. We were actually lacking trust, visibility, and scale. So this realization pushed us to explore new solutions. Since our existing data preparation was very manual, custom scripts, and required a lot of engineering support. So we partnered with Google and jumped into the private preview of Data Prep last year. Our teams collaborated together, and this helped in the evolution of the product that it is today. So I guess we can call ourselves product influencers. Yay! So what are the benefits that Data Prep brought for us from a technical standpoint? We had three primary benefits. Sampling and proving option made profiling and exploration of the data large data sets very simple so we could spot issues very quickly. Second, using the AI-generated assertions, which provided automated validations in a very simple and visual manner. Remember Clippy without sounding dated? Anybody? Clippy from the Microsoft days? Think of this as Clippy. Clippy was finally useful. Right? And we were able to build repeatable workflows without writing code at every step of the way. So we went essentially from control, find, plus panic to click, fix, plus deploy. And the real game changer was the ability the not-so-tech-savvy people, the data analysts, the business analysts, not just engineers, to prepare and explore the data on their own using the low-code, AI-driven insights Data Prep provided. To sum it up, what we wanted to build was a connected platform, not a maze of folders, intelligent enough to be able to spot issues proactively, scalable, something that can grow with our business, not break under it. For this particular use case, it was providing the right reward data to the right people at the right time with transparency and trust. So let's take a step back. About seven years ago, this was our journey with Google Data, Google Cloud Platform. GAF began its GCP journey aiming to centralize as much data as possible into our enterprise data lake, which was hosted on GCP. Our motto was simple, bring it all in. And it became the backbone for our analytics and data science and our internal applications that required us to distribute data. But as data landscape evolved, we recognized a crucial shift. With AI, automation, and real-time demand, everything accelerated. We realized it was not about how much data we collect. It was about how fast, smart, you can activate it. So we re-architected. We put BigQuery at its core, built a product-centric framework, and moved from collect, store, transform to analyze and act. We're building standardized, centralized, curated data sets with lineage, data observability using Monte Carlo to catch patterns and outliers automatically, and reusable data products that can plug and play like Lego bricks. So our core architecture principles lie on us being modular, where new data sources can plug right in, repeatable, where every use case follows the same trusted playbook, scalable, where performance grows effortlessly with our business demand. This wasn't just modernization for us. It was foundation for continuous intelligence. So what's next? So for our next phase of data journey, we want to build data solutions and data platform where scale isn't something we react to, but we build it by design. We're not solving for today's needs. We want to create a platform, or we have embarked on a journey to create a platform that's adaptable, that can grow, and evolve with the business needs with minimal friction and maximum trust. How are we achieving that? First, we're building standardized and reusable plug-and-play type of pipelines that can handle both internal and external data sets. These pipelines will be templated and modular such that it doesn't require us to reinvent the wheel every time we want to build an ingestion pattern. Second, using the AI-powered data quality checks, we want to put, we want to be able to catch outliers, address issues, before they become and cause damage to us in production. Think of it as your data's own immune system, right? So every time there is a new data set that's onboarded, it meets a baseline level of quality. Third, we will be creating a data product marketplace with standards, ownership, and metadata baked in. Think of it as Amazon.com for data products, which are self-serviceable, consumable, and secure. Finally, governance. As more teams are onboarded and more data products go live, we need a framework that is structured to be built without bottlenecks. We want to build a governance model that allows SLA-backed ownership, role-based access, and clear security boundaries. With this future vision in mind, someday in the near future, we'll be in a place where teams are not emailing back and forth and questioning a version 38 of a spreadsheet. People are not second-guessing their numbers. A data platform that is future-proof, modular, intelligent, governed, and built for growth and becomes part of our business in a manner that it is the most boring part of our business because it just works. So we want to get to a part where we're not just managing data, but we're not just managing data, we want to be able to drive insights from it. So remember the turtle in the pan story, people? Right? So let's bring that back home. Right? That story was hilarious, but also painfully accurate because just like that turtle, our data and our most valuable insights are tucked away in outdated logic, hidden in downloaded folders, and surrounded by processes that can barely hold them together. So the next time someone says, our data platform works just fine, just remember, someone also thought a turtle in their pants was a good idea. Maybe we need to rethink that. Thank you. I'll hand it back to Tim. Fantastic. That was great. Thank you so much, Pooja. Can I ask you to stay on stage? Maybe you can chime in during the demo, which is the next part. All right. Let's have Gemini create some code for us. The scenario, Pooja, that you introduced a little bit was where builders share data with GAF, and this arrives in some file and some folder, let's say on GCS, and that needs to be standardized, cleaned, and moved into a common table using data preparation, and then we'll use some BigQuery notebooks to analyze that data. Absolutely. Shall we jump in? All right. Let's swap places. All right. Let's move over to the demo. Great. So, this is one of the tables where a partner of GAF has shared data, and this is all data that has been generically or has been synthetically generated just to give you a flavor of some of the challenges that you're working with. So, let me open this up in BigQuery data preparation. And right away, we can see that Gemini has looked at the data and the schema and has come up with a couple of suggestions here. And something like this without this feature would take us weeks if not months. So, one column here is the string date when the project was approved. So, these are building projects that the builder has shared. And they have addresses and they also have a date here that's in different formats. And we're bringing in this as the most common denominator and very basic data types here. So, many of these columns are strings. So, what Gemini actually detected here is, oh, there's two different formats and it should also not be a string. It should be a date. So, why don't we take a look at this SQL expression that was generated and preview the data to see if it actually works. And now we can see it's a standard format and it's also converted to date. So, that looks good. Let me go ahead and apply this. So, now we're building a SQL data pipeline step-by-step. There's another example here that I'd like to show you with a JSON array where there is some data buried in here. There's multiple JSON elements and we're mostly interested in the roof size. Yep, and this is a very common scenario for us and data in these formats comes often, structured, unstructured, semi-structured, so this would be a perfect scenario to demonstrate the semi-structured format of the column. So, I can scan through the pre-made suggestions here, but I can also generate my own code for this using natural language. There's various ways in data preparation where you can create these SQL expressions, and this is one instruction here where it's saying, only give me the numerical piece from this sub element in this JSON structure, and also, by the way, because it's now just numerical, we don't have that descriptor also converted to a float so I can use it for math or filtering. So, there's a multi-step process that just happened here in a matter of seconds. That's fantastic. And then, so what happens next with this type of home builder data? Do you maybe need to do some joints? Yes, we absolutely do, because we get, just in a builder scenario, our files are in hundreds, if not more. So, yes, absolutely, we get home builder data from multiple builders at a given point in time. Awesome. So, let me do that and define, this is, again, yeah, the data that was shared with us, and now we can enrich this data with our own data sets that we have about the product information. There's some product description in this data, and we're going to use this product description to actually generate lookup keys. So, Gemini figured out these are the keys I can use on the right-hand side, and it also gave me some insights about how this join will look like. So, if there's any filters that are happening alongside this join, but it looks like everything looks good, so let me go ahead and apply this join, and now we're regenerating the preview that will now also include the additional product information here. And this is where I believe you don't have to be a data engineer. A data analyst can simply do this with the click of a button as long as they know they're using the right data. Right. That's from the preview, right? You can see if it looks good, you can assess that, and oftentimes the data analyst can assess that best. Yep. Very powerful. All right. Then let's just load this into a unified table. This is my builder partner dataset. I'm going to load this into this unified table, and now we've constructed a SQL data pipeline as a model represented in pipe query syntax, and we can take a look at the full DAG here, and I can also specify things like the write mode, append these rows in because there's other data preparations from other partners loading into the same table, and oh, one more thing. This problem, this solves our problem of cross functional, cross program, visibility, where you're able to bring this all together and stitch it in a unified manner. And Pooja, you mentioned data quality assertions as well. Yep. Multiple quality issues in flat files and Excel sheets. So we'll be using this address in a minute, you know, and if aspects of these addresses are really critical, we can see what filters or data quality assertions are being proposed here, for example, if it follows a certain format. And I can go ahead and also send any invalid rows to a dedicated error table here. So this is how easy it is to define these assertion rules inside data preparation. All right. Next. And just so you all know, when we did the data prep preview, that feature was not available. It's great that it is available now. We're continuously updating, and that's the beauty of an AI product, as we're changing our foundation models that you may have learned yesterday in the keynote. We just released 2.5, so this is becoming smarter over time. That is fantastic. Another cool Gemini feature that I want to show you. So we've built the ingestion flow for our partner symbol homes. We have other preparations loading into this unified schema. Now it's about analyzing. And let's take a look at some notebook code here. Before we open this in the notebook, there's a cool insights feature that allows me to get started with analysis, which is generating a bunch of analytical questions for me that I can further dig into. For example, find me projects where the particular shingle needed by date is earlier than the permit approval date. So that means projects that might have delayed shipments to the project on site. So the tool was able to uncover insights that we did not even know existed. All right. So let me open up this common unified table in a Python notebook and initiate the runtime. So these Python notebooks, as Vidya was saying earlier, very powerful unified feature in BigQuery Studio. What we're doing here is we're querying BigQuery, and that allows us to leverage the result set of that query in a Pandas data frame. So you have all of the familiars Pandas environment here available in notebooks. And let me get the first insights here. So once my runtime is initiated, I'll go ahead and fetch the data from this unified table. And I think one of the first insights we wanted to find out was can we analyze any projects that were delayed possibly? And are there any patterns? So I'm now in this data analyst, data scientist role, and I can go ahead and ask different type of questions in natural language because, as Vidya, as you're saying, we have Gemini built in here into BigQuery notebooks. So would this be a good handoff between a data analyst to a data scientist, perhaps? Exactly, yeah. So once the data wrangling has been done, we can shift focus over into finding insights in the data. Fantastic. All right, so here's our table with all of the building projects, and then on the duplication side, what's going on there? Why is that a problem? That's a problem because home builders update their addresses in multiple different ways with creating duplicacy in the data. So address stitching is a big problem for us because it could be XYZ street, it could be XYZ street with ST, there are multiple formats of data, and there is really no easy way to reconcile that information together to see if, is it the same builder in multiple addresses or multiple builders in multiple addresses. So stitching, address is a very critical data element for us, and stitching the data using address quality is a challenge. All right, let's get back to the duplicates in a second. Just one more analysis that we seen earlier. So these are all of our projects by region, and we can see that each chart overall shows the number of projects that GEF has supplied, and this is, again, all generative data. But there is a trend in the northeast region where we see a higher ratio of delayed shipment. So this is now something that we can follow up on. It's a wonderful insight to share with leadership. Exactly. So is it a supply chain issue or are there any other type of things we need to look into? Now coming back to the duplication side, this is something we can also investigate further here in this notebook. And by the way, since we're looking at this next example here, it's kind of fun to see the different data frame options that you have. So one is Colab Enterprise and Pandas data frames. But if you handle data that is so large that it doesn't even fit into memory for this notebook, you can actually also run big frames, which is pushing down the queries. All of the work that you do in this notebook will be pushed down into BigQuery itself. lots of options for you here. All right, let's take a look at this table. And you can see we identified a couple of duplicates for our home builders. And that is something we need to take action on? Absolutely. If we could take action on this directly and be able to export this, that would be a big win. So there's multiple ways we can do this, Pooja. We can share a notebook directly with users that have access to BigQuery Studio, but these are often business users, I suppose. Yep, absolutely. Most of them are business users. And they may not have access to BigQuery. So what we're going to do in this last step of the demo is we're going to move this data back into BigQuery and make it available this way to business users. So the last step is, okay, take this data frame that we're currently working with and generate a table in the data set that we've been working with called partner duplicates. Not sure if it's large enough for you. Yeah. You should be able to see it. So this basically loads this data frame back to BigQuery and makes it accessible to business users. Okay, let me find this data set and refresh the content. So here's our duplicates table. Yay. And with the duplicate entries and then we can go ahead and open this in Sheets or in Looker Studio. And you can link this to Looker or any other type of visualization that business users are used to and they can slice and dice the data on the fly. Exactly. Fantastic. All right. Fantastic. Thank you, Pooja. I appreciate it. So in this last step of the demo, let me show you how BigQuery data pipelines ties all of this together. So in this feature that, again, as I mentioned earlier, is also GA as of this week, you can create all of, you can add all of these tasks or models into a data pipeline. You can add existing data preparations, existing notebooks, as I've done here. You know, one data preparation for the first partner that we've worked with called Apex Foundations, then the data preparation that we've just built in this demo from Symbol Home Builders, and the notebook that we've just built to create those duplicates and push it back into BigQuery tables. And I can go ahead and schedule this pipeline to run on a schedule. We also have ingest connectors, so we're slowly really allowing you to build an end-to-end ingestion transformation pipeline here natively in BigQuery Studio. So this falls right back into what we would like to design in the future, where we are able to use repeatable, reusable patterns with very minimal ETL, or ETL that can be repurposed, and you don't have to write code every single time. Exactly. And check this all into BigQuery repositories for review with your data engineering team, which is now also embedded as a preview here right into BigQuery Studio. All right, that concludes the demo. Last but not least, let me call out next slide, please. Oh, yeah, that's me. Let me call out two more sessions tomorrow for you that might be interesting, unifying data with BigQuery multimodal tables and democratizing AI in ML with our amazing Vertex AI integration natively into BigQuery. We really appreciate you being here. Thank you for joining us relatively late on day two. Appreciate your time and attention. If you think this was a five-star performance, we'd love to have your review for this. And last but not least, some of the amazing Gemini features, some of them are in preview. So this is a link that takes you to sign up, signing up for some of the Gemini and BigQuery previews that we have currently going on and enable those right into your project. All right. Thank you so much.