 Hello, everyone. My name is Sachin Gupta, and I'm super excited to be here in Las Vegas to talk to you about 10 infrastructure innovations to accelerate your AI solutions. Let's get right into it. Let's first talk about what I hear from you, our customers, about what you're experiencing in cloud today. Now, we all know everyone is looking at how to get the most out of generative AI, how to make sure that they can improve productivity, increase revenue, and innovate. At the same time, how do they manage their data? How do they secure everything? They're also concerned about costs. How do I make sure that I'm utilizing my infrastructure effectively? How do I make sure I get the best performance, but also at the best cost? And then finally, it's a hybrid cloud world. There are going to be reasons why some customers have data that must remain on-prem, and how do we enable AI to run where you need it? I'm going to talk to you today about how we're helping you securely connect users, data, and models. How we'll help you optimize for both performance and costs, and how you can run AI anywhere. Now, at Google, we have a unique advantage. We've been working on AI for over a decade. We've been working on cutting-edge infrastructure for over two decades. That starts with our data centers. It's all about our global network, about TPUs, liquid cooling, and now our 42 regions. Let's take a look at a video highlighting this Google advantage. Let's roll the video. For more than two decades, Google has been building infrastructure that now supports billions of users every day globally. And now, the AI services pulsing through these data centers empower the unprecedented innovations you read about in the near and distant future. With Google infrastructure at your fingertips, you are at the forefront of these very innovations. Google's data centers are designed for performance, scale, and efficiency. We power businesses, connect people, and drive innovation worldwide, wherever and whenever you need it. We're helping transform industries that have unique security and sovereignty needs, like financial services. Our network spans continents linked by over 3.2 million kilometers of fiber, enough to reach the moon and back four times. Our fiber isn't just on land, but also in the water, where we've spearheaded the construction of 33 subsea cables. We enable secure, and seamless connectivity across cloud and on-premises, and bring our cloud and AI capabilities closer to where you need it. Google's global infrastructure isn't just about reliability, scale, sovereignty, and security. It's about helping organizations of all sizes build on a foundation of modern, intelligent infrastructure. Google's global infrastructure is about building infrastructure is about building, intelligent, and all of the things that we're doing. All right. Are you ready to get right into it? How we bring the power of Google infrastructure to you? I'm going to talk about 10 innovations across networking, storage, and Google distributed cloud, and we have a great lineup of guest speakers as well who will come on stage later. Let's get to number one. Let's get to number one. Building specialized infrastructure. There was a brief mention of this in the video, and we announced a partnership with CME, Chicago Merchandise Exchange, last year, on how we are transforming capital markets. We're working with them in migrating a financial exchange into the cloud. This has required us to innovate in delivering low latency, extremely low jitter, deliver multicast with low variance, so that the different receivers receive the multicast feed at approximately the same time. This is where nanoseconds matter. Reliability is key to make sure that trading continues uninterrupted, and we can offer extended trading hours. With this partnership with CME, we are bringing to market an ultra low latency and high performance computing environment that transforms capital markets. And we're excited to bring this technology to other customers globally. That's number one. Now let's talk about in the age of AI, how can we help you secure global connectivity? Let's take a look at just a high level example. Let's say there's a high level example. Let's say there's a high level example. Let's say there's a customer that has their own data centers, is a multi-cloud user, has multiple branch locations, spanning continents, as well as is a heavy SaaS user. And this customer is looking at, you know, I have my data in one place, I'd like to use this model in the cloud in another place, I have my users coming in from somewhere else. They end up creating this mishmash topology sometimes, having to manage complex topologies, but also complex stacks that they may need to go into a colo facility and procure and then direct traffic to that stack. We want to make this completely simple and bring to you a fully managed and reliable secure backbone. What we're doing here is opening up Google's private backbone to service all enterprise WAN connectivity needs with CloudWAN. CloudWAN. CloudWAN is now in preview and helps you securely connect your users, your applications, your data, and your models. Let's go through what CloudWAN is all about. CloudWAN. So, of course, you have your VPCs in the cloud that you may be using with Google Cloud. You can obviously connect into this over the internet. You can come in through various peering partners. We now have thousands of those. And if you want a dedicated pipe, you can use Interconnect. We then worked with our customers who said, help me connect with a managed SLA to other cloud providers between Google and other cloud providers. For that, we introduced cross-cloud interconnect. The new thing we're adding here is a technology called cross-site interconnect. Now, you can take traffic from two of your sites and transit over Google's backbone as seamlessly as any of the other interconnect technologies. We also add in here the ability to host your SD-WAN head end of choice from your third party in Google Cloud, run your partner security services, also, in our backbone that we'll talk more about and consume native services from Google Cloud like the Next Generation Firewall. All of this connectivity orchestrated through Network Connectivity Center. One more thing. We enable premium tier networking so that when your traffic enters our backbone, it stays on our backbone to get you the lowest latency and the best performance and the most cost effectiveness before it exits to its destination. When you pull all this together, you get cloud-WAN. What does this mean for you? 40% better performance at 40% lower total cost of ownership. This is a no-brainer. It should be something that every enterprise customer is looking at on how to transform their WAN with cloud WAN to support the AI era. I'm very excited to invite on stage Chris D from Citadel, Director of Platform Engineering, to come and share his thoughts and journey on Cloud WAN. Chris, welcome. Chris, thanks for spending some time with us today. How about we get started by telling the audience a little bit about Citadel and about your role there. Sure. So I run the cloud platform infrastructure at Citadel Securities. And at Citadel Securities, our business is providing liquidity and making markets in exchanges all over the world. In various different asset classes and a fully global set of exchanges. So in order to be able to do that efficiently, we have to be able to constantly and in real time be able to accurately price all of those securities that we're making markets for. And so in order to do this, and that's where my role comes in, we need a very reliable hybrid network between our research applications that are running in Google Cloud and all of the exchanges in the global network of colos that we have on-prem. So, you know, as we started working together, what were some of the problems you were looking to solve for? Yeah, so initially we moved into Google Cloud to take advantage of the super high-scale storage and compute that we could get access to. And we very quickly realized that one of the key critical problems was going to be how do we move all of this data from all around the world, from all of these colos, from data sources and other cloud providers efficiently and reliably into Google Cloud so that we could run at the scale we needed to to turn our research platform into a real strategic advantage. And so we've spent a lot of time working with Google to figure out exactly how we want to architect that network and working with them to deliver features that have significantly improved the reliability and scalability of our networks. Okay, so lots of data needed for research, lots of different locations need to move it reliably, securely. You've already been using some of the Cloud WAN capabilities. What's been your experience so far? Yeah, so the experience has generally been very good across the board. We've made use, obviously, of, you know, the bread and butter, the global VPCs, the interconnects, you know, cloud routing in general. We've made use of the cross-cloud interconnects to ingest data sources from various sources that are running in other cloud providers. And, you know, we're actually, we're piloting the cross-site interconnect right now to connect our on-prem data centers together using Google's network fabric. And in general, yeah, we've seen significant improvements in the stability of our networks and in our ability to get data into Google Cloud by making use of all the Cloud WAN suite of features. Amazing. Now, where do you see this going from here? Yeah, so our research is going to keep growing bigger. Our business is going to keep expanding to other regions in the world. And so our network is just going to keep growing more and more complex and more and more critical to our business operations. You know, as hardware gets denser, we run much more research. We're going to need more data and we're going to need it from more places. And so, you know, this is where we look forward to partnering with Google Cloud to come up with the solutions that we need to be able to scale our research to the point that it needs to in the future. More and more and more scale. More and more and more scale. More and more and more data. Chris, thanks for the partnership. Thanks for spending time with us today. Of course. Thank you. All right. Take care. All right. So with Cloud WAN, connectivity is, of course, super important. But equally important is how we help you secure against network threats. Now, too often you've had to make the choice of using a first party service from a cloud provider that's just not best of breed. Or bringing your own and having a complicated way of trying to integrate and route traffic to your own service. We want to get rid of all of that. So let's go back to the topology that we talked about earlier. And in the center, let's talk about our first party services. And I'll talk about a little bit later about partnerships that we have so that you can run your own best of breed network service of choice natively in our backbone. We provide our own first party services with Cloud NGFW, Cloud Armor, and something new that I'll talk about, which is DNS Armor. And you can connect in partner services just as easily. Let's talk about Cloud NGFW first. We've partnered with Palo Alto Networks to bring the best cloud firewall natively as a first party service in Google Cloud. We're now enhancing this to provide you web filtering capability and the ability to apply tags and hierarchical policies that makes it much simpler to be compliant and to administer security and reducing faults and errors. Cloud NGFW offers up to 24 times the threat efficacy of cloud firewalls from other cloud providers. Simply incredible. Now, I'm sure a lot of you know about Cloud Armor. We've taken some of the similar capabilities in terms of hierarchical policies, org-wide and project-wide policies, to make it simpler for you to consume Cloud Armor and brought that as new capabilities to Cloud Armor. Just, I think, I think a couple of years ago, and almost all the time, Cloud Armor ends up blocking very, very large DDoS attacks. In fact, we have stopped an attack of 389 million requests per second. A great capability available globally on Google Cloud that now we've made simple through the hierarchical org and project policies. Now, a lot of the newer attacks, things like Log4j, end up happening through DNS exfiltration. We decided to work with Infoblox, a leader in the space, to create a first-party solution to give you DNS protection. We call that DNS Armor. Now, what makes this so powerful? We can detect errors and issues, exfiltration exploits, much, much more quickly. And the reason we can do that is because Infoblox is able to see up to 70 billion DNS events analyzed a day in partnership with Google Cloud. The partnerships here are really important. I talked about Infoblox and Palo Alto networks to create our first-party services. In addition to that, when I talked about Cloud WAN, BT and Lumen can provide in-region or first- or last-mile connectivity into Cloud WAN as well. And then we have partnerships such as Fortinet and Checkpoint, which is about bringing your own stack of choice into this environment. And then finally, we have partnerships such as Accenture and HCL Tech and Wipro to help you with your Cloud WAN transformation. So, we will continue to extend this partner ecosystems to help you get the most out of Cloud WAN with all of our security capabilities. UKG is able to leverage our security capabilities to improve protection while also simplifying how they manage all of this. All right. Let's talk about something different in networking. How are we going to help you optimize AI workloads through network innovation? When you start thinking about inferencing, serving generative AI, and doing it efficiently, it can be very challenging. And here's why. You can have GPU and TPU infrastructure that is constrained, expensive, constrained, and maybe in a few different locations. How do you effectively route traffic to all of this infrastructure? And how do you make sure you're allocating in a smart way so that you can save on cost, increase utilization, and get the best possible performance? In order to solve these problems, we've introduced GKE Inference Gateway. This is model-aware, metrics-aware, optimized, intelligent routing and load balancing for AI inferencing. Let's take a look at how this works. So, in traditional serving, you would have a traditional gateway. You have a bank of GPUs and TPUs. At the bottom, you can see that there's request queues, and there's something called the KVCache with the number of tokens it has to process. And you can see a traditional gateway may simply do round-robin or some other mechanism and pick a GPU that's overloaded. Now, quite simply, we can add custom metrics like looking at the request queue, constantly monitoring that, monitoring the KVCache. And with GKE Inference Gateway, we can find the right GPU to offer the best performance. It is also aware of the type of query. Is this a chatbot or is this a reasoning task? Because a chatbot may need a lower latency outcome where a reasoning task may be able to afford higher latency. It also doesn't just support load balancing within one region. If you have pools of GPU and TPU capacity, some sitting in one region, some sitting in another region, or maybe even some sitting in another cloud or on-prem, you can use our networking capabilities with AI inferencing to automatically move the traffic to the right pool and more efficiently get the best performance and the best reliability. So, tremendous networking innovation to help you get the best performance at the best cost for AI inferencing. What can you get out of this as a result? You can get up to 40% higher throughput at up to 60% lower latency, which for so many inferencing applications becomes so important, and all of this at up to 30% lower cost. Snap is already leveraging some of these capabilities to deliver extremely high performance for their users while also optimizing costs. Lots of great innovations in networking. Let's now switch to storage. Storage is equally as important to help you ensure you're getting the best performance and cost for all your AI training and inferencing. Let's take an example here where you have a multi-region cloud storage bucket, and you've got GPUs or TPUs that you're using in multiple different regions, and you now need to move that data into those different regions if you want lower latency, higher throughput. Or if it's sitting in that multi-region bucket, you're probably going to experience much higher latency and have less throughput. So if you move that data, it can be costly. It's, you know, operationally burdensome. How can we make that easy for you? Very easy. We have created a new capability called Anywhere Cache, Cloud Storage Anywhere Cache. Something that you can simply turn on and reduce latency as well maximize good put. So let's take a look at what this means. From your multi-region cloud storage bucket, we will automatically create a cache where you need it. This is no small cache. This is a cache per zone that can give you 2.5 terabytes per second of increased throughput. It can also automatically cache one petabyte of data per zone. This results in up to 70% improvement in read storage latency. Anthropic, one of our large partners, is already using Anywhere Cache to make sure that their data sits where their TPUs sit, and they can get throughput of up to 2.5 terabytes per second. Now, if you need even better performance, both for reads and for writes, and you're looking for sub-millisecond, you need a zonal bucket. And to talk about this, I'm very proud to introduce Dennis Serenyi, Distinguished Software Engineer for Google Cloud Storage. Dennis, go on stage. Thank you. Take it away. All right. Thanks, Sachin. It's great to be here. People love object storage because of its unlimited scale, ease of use, and price point. Sachin was just telling us about Anywhere Cache, which is a big step forward for object storage performance with its zonal co-location, high throughput, and low latency. But with AI, every millisecond matters, and write performance matters. So sometimes you need zonal primary storage. That's why today I'm thrilled to be unveiling something game-changing for object storage performance, rapid storage. Rapid storage combines the low latency of a block storage system, the high throughput of a parallel file system, and the ease of use and scale of object storage, all brought together under the familiar cloud storage API. And rapid storage is available as a cloud storage zonal bucket. So all your reads and writes are zonally co-located. Rapid storage is the first object storage system that offers sub-millisecond read and write latency. And it scales incredibly well, up to 20 million queries per second in a single bucket, and up to 6 terabytes per second of throughput in a single bucket. So let's take a peek under the hood and see how we provide this performance. The secret ingredient is Google's distributed file system, Colossus. Colossus provides sub-millisecond latency and tens of terabytes of throughput in a single cluster because, like many parallel file systems, it offers a stateful protocol that enables direct access between Colossus clients and the disks. Colossus is battle-tested and incredibly flexible. In fact, it supports pretty much all of Google's storage needs today. Everything from being the storage underneath BigQuery and YouTube serving workloads to providing storage for all persistent disks. And it's the storage engine underneath Gemini training and model serving. With rapid storage, we're exposing Colossus directly to the cloud, seamlessly integrated into cloud storage. We're making the cloud storage API available via the gRPC protocol and leveraging gRPC streaming. This enables us to front-load all interactions with the cloud storage control plane to when that stream is first established. So afterwards, reads and writes go directly to Colossus without any extra overhead. And we've integrated rapid storage into cloud storage fuse. So simple local file support that you probably already have in your applications can take advantage of this performance. Now, to see how fast rapid storage can go in real-world AI ML applications, we ran a pre-training workload over a one trillion parameter model. We focused on storage-intensive parts of the workload that can sometimes cause GPUs to block and sit idle. Thanks to rapid storage's sub-millisecond latency and high throughput, check point restores ran two times faster. Checkpoint writes ran four times faster. And random read data loading ran 20 times faster than with regional cloud storage. And this really is just the beginning. We're really excited to see what rapid storage can do for workloads beyond AI ML, like analytics or database workloads. Rapid storage is available in preview right now. Thanks, everyone. Now I'll kick things back over to Sachin. Thank you, Dennis. Dennis is one of the co-creators of rapid storage. It's really amazing to hear directly from him. We're about halfway through. How are you doing? All right. All right. All right. Let's keep going. We talked about how we help you optimize for your AI workloads. But how can we take AI to help you manage your Google Cloud Storage estate? Let's talk about that. Now, what are you trying to do? What do we hear? It's about security and compliance. Do I have the right data retention policies? Am I using the right data for training, for example? How do I manage for cost? Am I using AutoClass where I can? Can I do bucket moves and other operations in a simpler way? For this, we introduced Storage Intelligence. The Storage Intelligence helps you manage and optimize a massive Google Cloud Storage estate that can be tens of billions of objects. Let's take a quick look at what the product can do. So very simply, when you get onto the console, you can enable Storage Intelligence. It will first go ahead and take stock of where are your buckets, how many objects do you have, what's your average object size, lots of great information. If you've applied custom metadata, there's more manual classification of objects, you can get information on that. You can see how many have AutoClass enabled versus not where there might be an opportunity to save money by turning on AutoClass. You can also click more into this and say, okay, you know what? AI checkpoints, maybe they can move to colder storage. I want to do that automatically as the checkpoints get older and older. I can then look at 81.2% of these don't have AutoClass turned on. Great opportunity to save cost. Just by getting all of these insights and leveraging Storage Intelligence, Spotify has been able to reduce their storage costs by 37%. Let's take a sneak peek at where we're going from here. Sorry, talk about scale. Anthropic is doing this at 85-plus billion objects. This is amazing scale. So this is not for a small environment. You can do this at any scale you like. Let's also take a look at where we're going with the product. I just want to give you a sneak peek. Maybe hard to see at the bottom here, but there's a little toggle that says object auto annotation. Let's turn that on. You turn that on, we can go through the object and classify automatically for you that we see, you know, people in some of them, nature, toys. What do we see in these objects? We're then going to make it very simple where you can just tell it what you want to do. Help me find all images with human faces and make sure they're not used for AI training. It will then create the steps it will take. I'll identify some of the images. You can verify the samples. If that's correct, I'll go put those in a separate bucket and make sure those are restricted and cannot be used for training purposes. You can say click execute plan. It will go ahead, give you the sample. You can confirm that it's operating correctly, and it'll go create the bucket and make that happen. So, bringing agentic interaction with your Google Cloud Storage with storage intelligence. Now, the best place to run AI is in our public cloud regions. We provide sovereign control so you can keep your data in your region of choice, manage your own encryption keys, make sure only the right authorized users have access to the data. But sometimes, there are reasons why you cannot use the public cloud. So, how do we bring AI capabilities on premise? There's various use cases of different industries trying to do different things with AI. In manufacturing, using vision detection to detect issues or safety issues inside the manufacturing floor or opportunities to optimize for performance, reduce error rates. You can find in retail, how do you do loss prevention? In public sector, it can be about public safety. Lots and lots of use cases. Now, there are going to be industries where it's so highly regulated that it needs to be an air-gapped environment. Like, the cloud and the private cloud cannot connect to anything. No internet, no Google. It might be a latency requirement. You can't have in that manufacturing floor, perhaps, all of that data moving into the cloud, generating a response and coming back. You need to react more quickly. So, there are multiple reasons where maybe the internet connectivity is flaky, where you need these AI capabilities at the edge in your data center on that manufacturing floor on your premise, basically. And so, we're now proud to introduce Gemini on Google Distributed Cloud. Google Distributed Cloud is a fully managed cloud offer that runs on your premise in your data center or at the edge and is available in both air-gapped and connected modes. Let's talk about the new capability here. Gemini now runs on Google Distributed Cloud in a confidential compute environment enabled on top of NVIDIA, Blackwell, DGX, and HGX hardware platforms. We're also partnering with Dell with specific SKUs. And in fact, if you go to the hardware verse, you can see all of these and see some of the great work that the team has done to bring Gemini into this environment. Now, it's not just about Gemini. We're here to help you build your AI application, your agent, your assistant. And so, it's about how we help you manage the data lifecycle, bringing more of the Vertex AI capabilities, a full Kubernetes platform into this environment, making sure it's not just Gemini. You can load your model of choice into this environment. Very, very excited about enabling the best AI models even on premise. And so, it's not just about leveraging open source only, which has been what you have available so far on prem, but getting the best that Google has to offer through Gemini. Now, Jensen, actually, there was a video in the keynote, I don't know if you saw it, where Jensen talked about this being utterly gigantic. This is so great to partner with NVIDIA to make sure the solution is fully secure and highly performant for enterprise use cases. We're also working very closely with ServiceNow so that they can make their agentic AI even more powerful through Gemini. But customer trust and data protection is extremely important for them. And then, we're super excited to announce a new partnership with KDDI to bring the power of Gemini to Japan, to businesses and customers, while ensuring that we have data locality and we meet all the local regulatory needs. I talked about agents a little bit. One of the agents we hear a lot about is around enterprise search. Everyone loves the enterprise search capabilities, the power of Google search. But how do I do this in a fully air-gapped environment? How do I do it on premise with my data so that it never leaves? In order to talk more about this, we're going to show it to you through a demo. I'd like to invite my colleague, Camilia Aryafar, who is Senior Director of Engineering for Vertex AI Search and AgentSpace. Camilia, welcome. Camilia, take it away. Thank you, Satin. Let's take a look. So, AgentSpace Search is a state-of-the-art AI application that brings Google-quality search on-prem with the highest levels of security, compliance and data protections. AgentSpace Search helps on-prem employees across roles and industries to get their job done. So, imagine a financial analyst researching an investment plan or a lawyer researching a case or an engineer troubleshooting a system. They can all save hours a day doing their job. So, let's actually take a look and see how it works. Okay. The first step is for the administrator to set up agent space in the console admin. So, let's say I'm Camilia. That is the GDC console admin. Correct. Yes. So, I log into the GDC admin console and create an agent space app, enter the right credentials, and then configure all the details. So, the biggest challenge that enterprises have with data on-prem is that data is siloed in various systems. So, agent space searching GDC comes with pre-configured and pre-built enterprise data connectors that can help bridge that problem. Here, for example, I will select SharePoint on-prem as an example and then configure the data for ingestion. So, you're talking about SharePoint, but can you talk about like what are the other types of sources here? Yeah, absolutely. So, we have many connectors available, for example, like ServiceNow, Jira, and you will see some of the examples. Okay. So, multiple on-prem type of data connectors. Exactly. And you can repeat this process as many times as you need to actually get the information like set up and process. Okay. So, once complete here, you can see that this is active and the data is ingested into the search service. So, the console UI allows me to manage and monitor the ingestion process and the completion and all of that stuff over time. Mm-hmm. So, like I said before, and to your question, Sachin, you can repeat this as many times as you need with as many data connectors as you need to get all your data ingested. Great. So, your data stays on-prem, but this is seamlessly ingested and indexed into the search service that we have available on GDC. So, okay, you've connected the data. Is it ready now? Can I use it? Yeah, absolutely. Okay. Let's take a look and let me show you. So, agent-based search comes with an out-of-box, pre-built, nice, intuitive UI. So, Sachin, let's assume you're a financial analyst at a bank. Yep. You need to put together a corporate report on top emerging credit risks for your client. So, let's go ahead and ask that question. When is the credit risk report due as the first step? Okay. And you can see here that agent-based searches through relevant information from your email, SharePoint, Confluence, and tells you when the report is due, who the audience for the report is, and other type of information that you may need. Okay. So, it just simply just gives me the answer when the report is due. Yeah, absolutely. Okay. So, let's now search for other information that you need to actually go ahead and create the report. So, let's go ahead and ask another question. What are the top emerging credit risks for a corporate lending portfolio in the last quarter? Okay. Okay. And then you can see that agent-based search returns both a summarized AI answer and traditional search results. All of this data is coming from and is grounded in your enterprise information on-prem. Okay. So, these are like the data. All the sources are linked. Yep. Yep. Absolutely. They're on the right-hand side. So, you know, when you do this on public cloud, it's multi-turn, it's context-aware. I can then dig in and ask more questions. Can I do that here? Yeah, absolutely. You guessed right. So, that's the next step. Let's go ahead and ask a follow-up question. Exactly as you said, it's multi-turn. Let's go ahead and ask, show me details on market volatility risk affecting corporate clients now. Okay. And you can see agent-based response now with additional information and you can click a link icon here to show what exact sources are used to generate that specific point that you're showing here. That's amazing. And, you know, I couldn't help but notice the nice table at the bottom. That's just such a cool output. Yeah, absolutely. That's nicely showcasing some of Gemini's multimodal capabilities. So, let's keep going, Sachin. Let's go down and let's ask one of the suggested follow-up questions. So, you want to understand if one of these have an impact on your client. So, let's go ahead and suggest and select this suggested summarize support tickets related to these risks. And then you can again see that agent-based search is able to retrieve the information from ServiceNow, your ticketing platform, to get this information all together. So, you can see that it provides a summary broken down by all the various risk categories. So, look, this looks really powerful but one question that's going to be top of mind is, I mean, you've got ticketing systems, you've got client data. So, I should only have access to search results if I have access to the source data. Yeah. How do we ensure that? Yeah. That's a great question, Sachin. So, we know that access control enforcement is critical. So, which is why agent-based search is designed from the ground up to respect access control set up by the customer. And it's a permission-adverse system. So, during the ingestion process, we actually index and ingest the access control information. And when we are doing a check at the query time, we actually do a pre-check and pre-retrieval filtering to make sure only the information that the user has access to enters our pre-retrieval pipeline before we actually feed the information into Gemini or the LLM for response generation. Yes. I mean, so this seems so powerful but yet so simple. It's not just for the super users. Any enterprise worker can use this. Yes, absolutely, indeed. Given that it's a fully managed service on GDC, it's easy to deploy and scale. Enterprises can now power every knowledge worker with this powerful intuitive search tool on GDC. So, with that, we have concluded the demo. Camilia, that was fantastic. Thank you for showing us that. Thank you so much. Camilia. Camilia. Wow. Just go back to the previous slide, please. I just want to recap that. So, because you might have seen a lot of AgentSpace demos, AgentSpace search demos. Remember, this is happening on your data fully air-gapped, never leaving, and giving you this powerful search capability, connecting to your data sources, multi-turn, multi-modal, permissions aware. Very simple, natural language interface. Extremely powerful capability, available soon on-prem. Okay. Okay. Let's now move further to the edge and talk a little bit about how we're bringing AI to the tactical edge. So, sometimes you need this in a much smaller form factor. You need it in an environment where there's low or no connectivity. You have to process data locally. Could be a logistics use case. It could be some other use cases where latency becomes super important. And you have data sovereignty needs that say this also needs to be fully air-gapped. So, a small appliance that's ruggedized for this purpose. For this, we have introduced Google distributed cloud air-gapped appliance. This is cloud and AI delivered in a box. This is an appliance about this big. It weighs about 100 pounds or around 45 to 50 kilograms. And has three servers built in. It has a top of rack. And provides AI capabilities like translation services, OCR, speech-to-text inside the appliance. It is ruggedized. It meets all the other capabilities where it can be deployed in rough environments as a forward deployment. And it is IL-5 accredited by the U.S. government with IL-6 accreditation in progress. So, taking AI to more and more locations out there in the field. We're working with SAIC both for public and mission-critical use cases. In the area of logistics by leveraging this air-gapped appliance from GDC to deliver AI to support their logistics requirements. Let's now talk about a connected edge. This is now focused on thousands of stores, branches connecting into Google Cloud and providing cloud services in each of those locations. Again, in the keynote, you probably saw the video from Brian from McDonald's talking about how they've taken GDC connected. Three servers that can be deployed in every single store and support store operations, improve customer experiences, improve staff experiences as well. We have now updated these appliances to include GPUs. You can see these as well in the hardware verse. We can now bring, through partners, additional capability into all of these locations. Things like automated checkout, store analytics, loss prevention, and things we can do with computer vision. Partnerships with Eversine, with Intenseye, with Standard AI, with Toshiba, with others. With Toshiba, you're able to get real-time insights. You're able to provide a personalized shopping experience. If you go down to the hardware verse and to the expo and you look at some of the demos, you'll also see this demo from Eversine. Where there's a camera, there's somebody doing automated checkout, and it can do vision AI to detect loss prevention. Where if you don't scan it and take it across, it will automatically detect that for you. So a lot of real use cases that can have tremendous immediate value for all kinds of retailers enabled through Google Distributed Cloud and our great partnerships with companies like Eversine. All right. There you have it. Ten infrastructure innovations to accelerate your AI solutions. If you want a cheat sheet of the ten that we talked about, you can see the list. Everything from networking to storage to distributed cloud. If you want another cheat sheet of the new products that were announced and where they're at, are they generally available in preview or coming soon? You can take a snapshot of this. And I really encourage you to continue your learning journey. Lots of great sessions. Some have happened. Some are yet to happen. And I really encourage you to go to the show floor. I visited all of those demos yesterday. Simply Fantastic really brings out these innovations to life. Thank you so much and enjoy the rest of Google Cloud Next.