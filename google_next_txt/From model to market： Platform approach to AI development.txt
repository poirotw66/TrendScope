 . Hey everyone, good evening. Welcome to Cloud Next on the very first day. And given the fact that it's the evening, we appreciate you being here. We're really excited to talk to you more about a platform approach to AI development. My name is Colby Hawker. I am the lead product manager for AI Studio on Vertex. And I have a couple of colleagues here. They'll introduce themselves here in a bit. And yeah, we should go ahead and get started. So I want to go through the agenda real quick. Today we'll highlight rapid prototyping on Vertex. And then we'll get into iteration and how to quickly get to production. And we'll hand over the balance of the time to George and Elham from Dialpad, who will discuss a very interesting and practical example of going from model to market. So the idea of a prototype is to test a goal and to get people aligned. And on Vertex AI Studio, you can craft and mold AI. And it can easily get you that prototype. It's now easier than ever to start with a simple idea. And this could be a high level concept or maybe some specific requirements. And as models have become more capable, a lot of the heavy lifting can be done behind the scenes. And we hope to inspire novel applications that will help take your ideas to prototype and then eventually get to market as quick as possible. And in this example, I'm telling the studio that I want to develop an app to help create images for a blog post. I added that I want to give users two different inputs, blog post, text, and background color. And once you submit your idea or requirements, we'll turn all of this into a prompt. And we'll pair the prompt with a capable model. In fact, you'll see that we'll even add a couple of placeholders here in the prompt. And these inputs are exactly what I asked for. And they can be replaced dynamically by user content in the application when you're making those API requests. And that means that you can easily process your own data with this prompt. In the studio, you can essentially simulate these inputs and the outputs. So we've up-leveled the design in Vertex A Studio, if you haven't noticed yet. So it's now easier to interact with multimedia and handle large context windows. And with an updated model picker, it's a lot easier to go and make these trade-offs with different models, evaluating the price and the latency and core strengths. In fact, you can even check out third-party models and open source models and then compare them side by side. And most notably, the studio is the home to Google's most powerful and capable AI models, including Google Gemini 2.5 Pro with Flash, which is right around the corner. It is coming soon, so keep your eyes peeled. This unlocks brand new possibilities for rapid prototyping, enabling you to do a lot more with a lot less energy. The large context window and native tools of Gemini can really be used as a foundation for most applications that you're looking to create. And with the right tools, you can push AI models to their limits. With one click, you can use Google search data to enhance your responses, or you can append your own data repository to go and process. Now, imagine prototyping this app that generates help for blog posts. You can use Google search data to identify trending topics. And with function calling and code execution, you can flag potential errors. You can look up information from a database. And you can even schedule the posts, creating a fully functional prototype. So let's assume you just want to get to an MVP. And now you can actually package your prompt and all of your settings and your model all together in a shareable Gemini-powered app. One of the big benefits of being on Google Cloud is we have a very large ecosystem and a quiver of resources that you can use to help you deploy your apps. And Cloud Run is one of those resources to help you deploy. And it's a simple way to run your web applications online. And you don't need to manage all these complicated setups. And the big one here is that you can automatically handle the traffic, and it will scale as you scale. So this Cloud Run integration, this is brand new. This is a public preview on Vertex AI. And now you can very easily deploy an app prototype. And I'm going to show you how. So earlier we created a prompt to create companion images for a blog post. And so next you can see here that we can click on Build with Code. And now we're going to generate, we're going to go and we're going to create the app. And this is going to kick off the deployment process. And the resulting app, it's going to utilize Gradio by default to create a front end for the app. So full disclosure, I took out about a minute of this clip, but it took about two minutes of process. And so now I can open my web application. And this is a publicly shareable URL. I chose to make it public. And so you can see here I'm testing the app. And I'm adding the blog post text. And this is a blog about Kyoto, Japan. And I put in the background color. I'm making the background orange. And you can see it's generating the images for me. And I'm testing that variable, which is the background color. And so I can repeat this process. And the benefit here is now I can share this prototype with even non-technical stakeholders. I can get some feedback. I'm up and running in just a couple of minutes without having to go and fiddle with code and settings. And if I wanted to, I can go and edit the source code. And I can abstract away different elements that maybe I don't want. I can remove the Google Cloud warnings. I can change the template. Or I can customize my code. And I can actually sync this with my own repository, which I think is really great. And so now I have this MVP. And I can send it out for different types of feedback. So now you have the prototype. And you edit. And you're going to revise it. And you can revise the code however you want. You can redeploy it. And you can take this to the next stage. A major accelerator in development in Vertex is the Vertex SDK. And you can get to production and make updates really quickly. As you can see from just these code snippets here, you can, for example, use the SDK to assemble different prompts, the components and prompts. You can manage the versions. You can manage tags and notes. And you can also manage and generate assets programmatically. And this is really helpful to iterate and eventually get to production. So the next steps with that iteration, my colleague is going to come talk about. So I'll turn the time over to Evit. Thanks, Colby. Hey, everyone. I'm Advait Bapardikar. I'm an outbound product manager for Vertex AI. As you start to iterate to production, you're going to realize that a lot of what needs to happen is about hill climbing, hill climbing on quality. Specifically in Gen AI, you're going to go through an iterative loop that is scientific in nature to understand how do you make the model behave the way that you want and improve the quality over and over again, given that these systems are non-deterministic. So for those of you who have a machine learning background, perhaps you've seen some type of loop, not necessarily like this one. But you're aware of the fact that you're going to do some amount of testing, some amount of evaluation. Then you're going to do some amount of adjustment and redo that loop. For those of you who are new into the Gen AI or ML space, perhaps this is a new concept. But this is one that's, I think, key for you to all remember. So we have this loop specific for Gen AI. The first part of it is around testing. What are you going to be testing? You're going to be testing the prompt. And the user interface in Vertex AI Studio helps you with that. So this is the interface. You're going to first start with configuring your prompt. The first step of the prompt is a system instruction. A system instruction tells the model how to behave, the role that it's supposed to take, and perhaps the style that you want it to follow. The next part is the prompt itself. This is the part that you're going to spend majority of your time iterating on. So there are a number of tools on Vertex today that can help you with this. You can type in a prompt on your own. You can use the Help Me Write functionality to automatically write your prompts. Another tip, Gen AI models like Gemini 2.5 Pro, really good at writing prompts. Ask it to help you with that process, and it can write an extremely elaborate prompt for you. The next piece is around model parameters. So these are parameters like temperature, top K, the number of output tokens. These all impact the way that the model behaves and the way that you're going to get reliability in your outputs. Another configuration you may do is do you want the model to output JSON formats? Do you want the model to be grounded with Google Search? All of that is configurable directly in this user interface. After that, you need to now compare these prompts. So inside of Vertex AI Studio, you can do comparisons of prompts. In this case, you can either compare the same prompt with multiple models, multiple prompts with the same model, or multiple prompts and multiple different parameters, or any sort of permutation and combination of those. So here we have three different prompts that I'm comparing. I pick the one that I like, and then I can save it. Now, why do you want to save prompts? There's this whole life cycle when it comes to versioning that you need to be aware of. Prompts will improve over time as you make adjustments, and that is part of the process of hill climbing as you iterate over time. But when you do have a brand new prompt and you deploy into production, sometimes you may later on find out that this model actually did not perform the way you wanted. You want to move back to an older version. So now I can see all the prior versions. I can share those versions with others. I can document details about them, and people can discover what are best practices to use for different use cases. Beyond that, now you also need to do integration-level testing. So the SDK directly inside of the console studio gives you a button. It shows you exactly what you need to embed into your application. Now you can call the model from inside of your application. You can see, is there anything breaking in terms of integration testing? How does the text render? Perhaps you're finding that it's actually too long or it's too wide and need to do something on the user interface? Getting that connection directly into your application is a key piece of the story. So that's the first part. You're testing. You're doing that testing end-to-end. The next piece is around evaluation. And I would say if there's any one thing you're going to take from this talk, evaluation is the thing that you really want to work on developing a skill set for. This is something that I think a lot of people are chronically under-investing in. You want to have high-quality data sets, and you really want to view evaluation as a core part of this process. So last year for the evaluation service on Vertex AI, we released capabilities that allowed you to use statistical-based evals, tools like Blue Scores, for those of you who are aware of that. We also have Auto Raters. Auto Raters use an LLM as a judge to score the outputs of the model as if it was mimicking a human. But there's still a couple challenges that customers have. First of all, what do you do with agents? Agents are multi-turn interactions. They integrate with things. Since we introduced a number of agent-based tools at Next today, we're also introducing evaluations that work with all these agentic frameworks. The other piece is around automatically generating rubrics. A challenge of Auto Raters is that they're not necessarily consistent, and they lack transparency. You don't know why a model has made a decision on which prompt is better than the other. So with our rubric-based approach, it generates a rubric for how should you evaluate a specific type of prompt or request. It then will score and explain why it provided that score. Because that explanation is provided, it tends to be more consistent from eval to eval. And then finally, for you as the developer, you have a bit more explainability in terms of why did the model make that decision. Beyond that, you're now going to go into the rewriting process. Perhaps you've identified a couple issues. You want to make some adjustments. How do you do this? Gen.ai today, the art of writing a prompt is very much that. It's an art. It's not necessarily a science. Every model is different. Every version of a model is different. And sometimes it's hard to know what the best practices are. The Vertex Prompt Optimizer tries to take that art and move it into a bit more of a science. You can take that same data set that you had for evaluations, pass it into the Prompt Optimizer. It'll iteratively rewrite these prompts and then evaluate how well that does. So moving from something that would have been very much random guesses and moving it into a bit more of a data-driven statistical approach. So you're going to be hearing a lot more about this from the Dialpad team that's going to be coming up. But we want to talk about a couple more things before that. Next is fine-tuning. What happens if prompt optimization or prompt engineering doesn't work? In that case, then you're going to move into fine-tuning. On Vertex AI, we have extremely cost-effective multimodal fine-tuning. You can fine-tune with text, images, video, audio for a specific use case. This will create a task-adapted version of that model that is unique to you, unique to your data set that you can then deploy. But maybe that is also not suitable. So one of the things that we're announcing today is our new model optimizer endpoint. So the model optimizer, instead of forcing you to make a decision on which model is appropriate for you, will route between Gemini Flash and Pro for various different tasks, given the way that you've established that optimizer. So you set a budget. You specify that for that budget, you either want to optimize for cost, optimize for quality, or balance in between. And depending on the query, the model optimizer will route between Flash and Pro dynamically to keep within that budget and maximize the quality that you get for the dollars that you spend. So in certain circumstances, the best solution might be to actually combine models, and the model optimizer provides that for you. And then finally, when it comes to getting quality and getting to production, the last piece is around scale. This is where you're trying to drive massive volumes of usage. You want consistency. You want an SLA to know that your business-critical applications are going to be available for you. And Provision Throughput provides that for you. With Provision Throughput, you're getting a set amount of capacity that is dedicated to you, coming with an SLA. And in the event that you under-provision, it automatically rolls over to the pay-as-you-go endpoint. So you don't have to worry about knowing exactly what your peak is. You try to get somewhere around your upper end of your average, and we can flex up and down for you. So that kind of goes through the whole process of hill-climbing quality and eventually scaling to production. So to now understand a little bit more about the story of how a real customer did this today, we're going to pass that over to George and Elham. So give it up for them. Thank you. All right. Welcome to Next. What better way to illustrate how to take a model to market than through a customer story? My name is George Lee, product manager in Google Cloud AI Research. Our team brings new innovative capabilities that are research-backed for Google Cloud products, like Vertex AI Prompt Optimizer. We're now moving on to the panel with a customer that I've been delighted to engage with in recent months. Dialpad is a customer intelligence and communication platform that brings real-time AI insights into traditional collaboration tools like chat messages, phone calls, and video conferences, as well as more sophisticated sales center and contact center solutions. Joining us is Elham Mouamadi, a senior applied scientist at Dialpad. She is instrumental in bringing the most important generative AI capabilities to Dialpad. Her role spans from robust model evaluation to building out evaluation frameworks, as well as continuous refinement of prompts for Dialpad's core use cases and products. Elham, welcome to Next, and thank you for joining us. Thanks so much, George, for the kind welcome. I'm excited to be here and to be part of such an incredible event. Awesome. I thought we'd begin by understanding Dialpad's approach to AI and how you guys think about it. Obviously, Dialpad is an AI-first company, but what value has your business seen through AI models? Dialpad is indeed an AI-first company. We're not just using AI as an add-on feature, but we're using it to transform the way teams collaborate and communicate. The real value we've been seeing is in boosting productivity and helping teams with their decision-making processes. For instance, Dialpad AI can automatically summarize calls and pull out key action items so teams don't have to focus on manual note-taking and can instead focus on the conversation that is taking place. We also use AI to provide coaching insights to customers so sales and support teams can improve by analyzing conversations at a scale. From a business perspective, AI has helped us stand out by reducing manual work and by offering deeper insights into customer needs. Ultimately, it's about making conversations more efficient and more actionable for everyone involved. So clearly, at Dialpad, AI is not just an add-on. It's foundational. Let's quickly discuss the Dialpad's evaluation process and model deployment. So before you adopted Vertex AI, what did the process look like and what specific bottlenecks and pain points did you have? Sure. At Dialpad, we follow a pretty structured evaluation process to make sure that our models are high-quality and are reliable. We start by collecting a manually annotated dataset which reflects a real-world use case. Then we run automated evaluations against a set of key metrics that we have defined for the task. If a model doesn't meet our performance requirements, we look into its errors, tweak our approach, and then iterate. With large language models, a lot of the work involves creating and adjusting prompts. And what would work with one model does not necessarily work with another model, which makes it tricky. Before Vertex AI, our prompt refinement process was really time-consuming and resource-intensive. A lot of work and effort went into manually creating and adjusting prompts. And this was not always reusable effort across models. After adopting Vertex AI, we streamlined this step, which effectively cut down our iteration time and sped up our deployment process and without a compromise on model performance. Yeah. So, I guess for model evaluation, it is really vital. We all know that. But at the same time, it is time-consuming and laborious. Before we dive deeper into how Vertex AI helped you solve that, let's quickly touch upon use cases. So, you work on Dialpad's most critical use cases. But today, we're here to discuss the AI Playbooks use case, specifically on topic detection for customer call interaction. Could you briefly describe that use case for the audience? Yeah. I would love to share more about our AI Playbooks feature. AI Playbooks essentially uses a large language model to analyze call transcripts between agents and customers. The idea is to make customer interactions more efficient and more focused by checking if certain topics are being discussed in a call. Here's how it works. First, the model detects certain topics related to the call, whether it's support, sales, or recruiting related. Then, it also identifies the speaker that triggered that topic. For example, was it the agent or the customer that mentioned something about that topic? It also summarizes key conversation points related to the topic in the form of questions and statements. It's a way to structure the dialogue so the agent can follow up with the customer on key areas. Finally, the model provides a brief explanation on why it flagged a certain topic or classified information a certain way. You can think of it as insights into the conversation so the agent can provide more effective answers to the customer and they can stay on track. You could also think of it as a smart note-taking tool for both agents and supervisors. That's a powerful use case, empowering human agents, which is one of your true important customer segments. So for this use case, how did you formalize the process of model evaluation and what are some of the key metrics that you had defined? Right. So AI Playbox is composed of different tasks and each task needs to be evaluated separately with its own set of metrics. First and foremost, we have the topic detection task. Imagine you have the budget topic in a sales call. Our top priority is to correctly mark its presence or absence during a conversation. Then we have the speaker identification task. For example, was it the agent or the customer that said something about that topic? We also have the utterance classification task, which helps organize information in the shape of helpful Q&A pairs that can be provided to the user. Maybe it was a question posed about budget by the customer or some information shared by any of the call participants. All of these are classification tests and we track the model's performance on them using the standard metrics of precision recall and F-score. Finally, we have the explanation generation test. In the case of the budget example, the model explanation can be something like the customer's budget is $20 per license. This, of course, is a language generation test and has its own set of key metrics. To answer your question about formalizing this process and to make it more consistent, we developed an in-house evaluation framework which is called LLM Evaluate. We actually shared and open-sourced it during the recent Calling 2025 conference. It helps us quickly and efficiently compare different models, prompts, and hyperparameters and ensures that we're not introducing any sort of model regressions but are instead improving performance across the world. And I think this is an important note that the audience can take away is to have some sort of structure around your model evaluation process. Let's quickly move on to model migration now. It was one of your mandates to move from our older Google POM2-based TechSpison model to the newer Gemini 1.5 Flash model. What are the reasons for that mandate and what challenges did you initially face? Yeah, to answer your question about reasons for migration, our initial model, which was based on TechSpison, was actually working well according to both manual and automated evaluations. However, we wanted to make sure to stay ahead and take advantage of recent model releases. Once we saw Gemini's improved performance on key tests, we decided to make the switch. But that was not all. Beyond model quality, there were also a couple of other factors that we considered. Cost, latency, and scalability were among them. And Gemini offered competitive advantages in all of those areas. It was faster, more scalable, and more cost-efficient, which made it a really strong candidate for our needs. So model migration is actually a key challenge that many customers have because new models get released every few months or so. It's really important to take away that, you know, having a structured evaluation process plus leveraging some of the tooling can really help this. So please help explain how Vertex AI, specifically Prompt Optimizer, helps solve this migration challenge for you. Right. Yeah, so first of all, I would like to explain a bit about the challenge that we were facing. Although Gemini offered many advantages in many aspects, we quickly discovered that the prompt that we had originally developed for TechSpison wasn't working that well with Gemini. In fact, there was a considerable gap in performance, although Gemini met all of our other requirements. So eventually we had to ask ourselves, how much time could we dedicate to manually creating and adjusting the prompt at each migration cycle? And given that time frame, would it be possible to manually create a prompt that satisfied our performance standards? As you can imagine, this was a very long and iterative process, and the result was not always predictable. So once the prompt optimizer became available, we used it to redefine this process. What we did was, instead of manually iterating on the prompt, used an annotated data set, so the prompt optimizer could automatically adjust the prompt to the new model. It quickly created several versions of the prompt and iterated and evaluated the performance and optimized the prompt based on a variety of default and customized metrics that we were allowed to add. And the result, it was a significant boost in precision from 68% to 74%, and the best part of it was that it saved lots of time and effort in the process. That's a significant improvement and a compelling reason for automation. So even when there are obvious reasons where newer models provide benefits, such as cost, quality, and latency, ultimately, you know, having to meet a certain performance bar and having to readapt your prompts requires a lot of time and manual effort. So what was the actual time savings that you had by using the prompt optimizer tool? That's a good question. Without the prompt optimizer and doing this process manually, it would have taken us at least two weeks to get to a model with a prompt that met our performance standards if it is indeed possible to manually create such a prompt. With the prompt optimizer tool, that time has been cut down to just three days. And as I mentioned, the best part of it is that we're no longer required to do manual prompt tuning, which is really long and unpredictable. So two weeks down to three days, that's nearly 70 to 80% time savings overall. Now that you have successfully deployed your model into production, how did Vertex AI and Gemini help with scaling? So first and foremost, the prompt optimizer tool allowed us to scale our migration process. This is essential as we're constantly updating our models to make sure that they perform at their best. And it just made that process much quicker. Aside from that, the prompt optimizer helped us migrate to Gemini Flash, which is a faster and more scalable model. Having that model as the backbone of our AI playbooks feature means that the feature now operates in real time. It detects topics and summarizes conversation points moments after they're discussed in a call. Also, compared to our previous setup, our new setup, which uses Gemini Flash, has big advantages in terms of cost, scalability, and latency. And all of these factors are essential as we support more and more customers with AI playbooks. Great. We're now at the end. What last piece of advice would you leave with the audience in this room and developers who are tuning in today, either on model evaluation and deployment or adopting Vertex AI? My biggest piece of advice would be to focus on setting up pipelines and processes that give reusable results, rather than focusing on manual work that needs to be repeated with each model release. I believe it's essential to have a solid evaluation pipeline in place. Investing in tools like automated evaluation pipelines and automated prompt design tools is very important. And it's going to pay down the road. It ensures that you can thoroughly test each model as it gets released, and you don't have to start from scratch at each point. Awesome. That's a valuable piece of advice. Thank you so much for sharing Dialpad's journey today, and thank you so much for tuning in. Thanks so much for having me. All right. Let's pray. Let's pray. Thank you. Thank you. Let's do it. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. problem. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye.