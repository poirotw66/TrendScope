 Good morning, everybody. Thanks for being here up bright and early 8 a.m. to talk about migrations and modernization. We have a really great session lined up with two distinguished speakers. One is Mark from Ford. The other is Rizal. Oh, I've switched the names. Mark is from Quest. He's the chief data officer of Quest. Rizal is from Ford. So what we'll do is we will go through what we have in terms of BigQuery migrations, the whole platform. So I'm going to cover that. I'm going to hand it over to Steve Walker, who's a distinguished colleague of mine. He's going to show you a couple of demos. And right after that, we'll get Rizal, and then we'll get Mark. Sounds good? What we'll also do is this is a tight crowd. So what we can do is we can answer any questions that you have after the session or if you have time within the session. All right? Let's get going. Four key things that we're going to talk about. Why modernize now? I'm assuming if you're in this session, you have some interest or a lot of interest. So we'll try to briefly touch upon that. Second is the how. Migrations made easy. So this is where we'll cover the product announcements that are happening, things that are going live now. All right? And then I'll hand it over to Steve for the demos, and then our distinguished guests will cover what they think about BigQuery and what they think about moving to BigQuery. All right? So if you've been at Next since yesterday, you might have seen this slide a few times. I will just summarize it in one sentence. BigQuery gets you everything, data, AI, and all kinds of data in one platform. So that is your why you should modernize. Now, the how part or the why now part is the BigQuery migration services. That's the product area we own, I own, and this is what we're going to deep dive into today. This is an area we've invested a lot and continue to, especially within the last year. By way of summary, we have a suite of services that are completely free to use. If you're a partner, we have API access. You can use it, white label it. And if you're a customer, we have CLI, UI, all support. But it gets better. We have created special incentive programs, both for our customers and for our partners. I'll show them in the later slides as well. And the other thing is, with all things agentic, we've been trying to leverage Gemini and AI as much as possible to the goal of reducing human toil and reducing human error. And again, as I said, we are in it with you. So we want to make the how easier. Migration incentive programs. We have a great consulting arm, Google Cloud Consulting. We have migration black belts. We'll have one of them come up on stage today to demo. And we have a robust partner ecosystem. We have some partners in the audience, familiar faces. All right. Let's get to why. So the why is, you know, okay, what will I get for it? And what will I not get for it? What am I risking it? So the now piece in the decision, I think hinges on a few things, right? If you are heading a large company, the data or IT or, you know, all of data AI, for a company, for you, the risk part of it, the price performance part of it, the complexity part of it, that's tactical. That's like, should I invest this CapEx this year or should I hold off? And we'll get to those questions. The second thing is, okay, even if I have those things mapped out, how does the actual process look like? How does it pan out? So we're trying to address both of those. If you're deciding the direction for your company or your organization, we want to be able to answer that. If you are figuring out, how do I get the move going? We want to be able to answer that as well. BigQuery Migration Services is a suite of services that we've been continuously investing in over four years. And we have brought it to a level and we continue to bring it up to a level where we address all parts of a migration lifecycle. The key thing is, again, it's completely free and we are trying to make it more and more comprehensive. So again, if you're a partner, you can use one or more of these services. You don't have to use all of them. If you're a customer, same thing for you. Pick and choose. Whatever works best. This picture, and I wouldn't want you to go and start reading, but the boxes in yellow are different parts of a migration, how we conceptualize a migration. And similarly, the parts in red and the parts in the pink color. The boxes in blue are the investments we've made in automating the migrations, in making them more intelligent, more smart, and also getting the human toil out of the way. So just remember that the boxes in blue, there's a box in blue corresponding to a box in yellow or orange or pink. The idea is we want to cover all the pain points. A couple of quick call-outs. This is not even from this year. These are numbers from last year. Rates of automation. So BigQuery migration services runs an automated assessment. You can run an automated assessment. And the automatic parsing ability of just scanning your database, just scanning your source system, and getting coverage of the workload is in the high 90s. So that's saying something. The details tell you that even in the assessment phase, through automation, within one to two days or at the most a few days, we're able to scan millions of queries, terabytes of data. Now, this is not your data will be petabytes if you are a large-scale data platform. This is just the data that the tool is scanning. And this is code translation statistics. You can see consistently hitting 99%. So we are not even saying 95%. And I'm happy to talk to you more on what this means. But we are pushing the limits. We are pushing the limits of automation. And how we are doing that is through innovation. So these are the announcements today. This is what's new in 2025. I'm just going to walk through them. We are announcing the general availability of Gemini-enhanced batch and API translations. So more than 15 sources, Gemini embedded. We had this capability for what we call as interactive translation, which is basically a Google Translate like left or right. So you put in code on the left side, you get the translated version on the right. Guess what? To get to large scale, we have enhanced that functionality for large batch translation mode where you input files. Now, it's hooked up with Gemini and it's generally available. The second is, we're also very excited to announce and preview Gemini-enabled translations. What's the difference? The difference is that for the Gemini-enabled translations, you can now invoke Gemini before the translation workloads get fed into our compiler engine. And what that means is, you no longer have to submit clean input. You can do preprocessing with Gemini and you can now start submitting code that is a mix of, for example, XML and SQL or any form that you want. We're also excited to announce the preview of source lineage for all the sources that we cover. So that is going to be a dependency graph of what your current systems are and it's going to help you figure out what dependencies will stay and what will break as you move your workloads over. We're also announcing a new category of assessments. These are light assessments. Each of these with total cost of ownership. So that will be the projected cost, your landed cost on BigQuery, on data analytics. And we're announcing those for Cloudera or Hive, Snowflake, Redshift and Oracle. A lot of assessments already exist in a much more comprehensive form and I have those in a slide later. But the idea here is that these assessments will not require any input from your source. The idea is that we export out a signature to your database. There's a comparison that happens and then viola. You get total cost of ownership. You get a projected view. A lot of investment has gone in. Clouds are getting connected and data is moving across and we want to make that easy. So our next set of investments is in the data migration space where we are very excited to announce data migration for Cloudera, Snowflake and incremental compress for Teradata. We already have Teradata and Redshift generally available. What we've done for Teradata here is we've gone really smart and done something known as Delta Compression which basically will let you keep your updates from source Teradata to destination data analytics in sync with minimal amount of data transfer. And finally, we are also excited to announce launch of metadata and governance for Cloudera. So we are targeting on-prem Hadoop systems. Double-clicking on a couple of these announcements, the Gemini enhanced batch and API translations support for very large-scale migrations completely free of cost with Gemini. We are doing the tuning, we are doing the input batching, the sequencing, all the heavy lifting for you. All right? The other thing is easy integration for partners. So especially for partners, this is either a complete suite of services that you can use or you can experiment or you can try out with your efforts with all the agentic efforts that you're putting in in building your tool set. And then the biggest thing is that as workloads migrate, you can customize and we do the calibration behind the scenes. Gemini enabled translations, I already talked about it. The idea is that now you can submit almost arbitrary code and get the best expected output for data analytics. And very shortly, what we'll also be doing is we will be backing this up with a full end-to-end validation. So today, whenever your code is translated, we do what is called a dry run, which means that the semantics and the syntax of the query is checked, but it's actually not run on any data. Now, going forward, we will run it on data and we will do an end-to-end validation so you're not only getting, hey, my queries or my code is running correctly, you're also getting a view on the performance. So I will just summarize this quickly. This is a busy slide. This recording is available afterwards, but the items in blue are current, which means everything from assessments with TCO, configurable batch interactive translations, items in blue and black. They are currently available. The green is what we've announced today and the red is upcoming. So let me give a little bit of teaser on what's upcoming. We will be doing the integrated into invalidation, as I mentioned. We are also partnering with a small set of our partners to develop automated ETL migration for informatic and data stage and that is also going to be available very, very shortly. finally, we are also working and going to release very soon the same automated assessment for Databricks and the ability to migrate data from Databricks as well as be able to do any configuration changes automatically for you when you move over your Python or Spark workloads. With that, here is a couple of resources. resources. There is one which is a landing page where you get all the details of what I talked about today. On the right are the incentives both for customers and partners. And with that, I'm going to pass it over to my colleague, Steve Walker. Hello, hello, good morning. Everybody hear me okay? And you can see my screen, fantastic. Hey, everybody, Steve Walker from Google. Man, those lights are super bright in my face. I'm going to do two demos. We're going to do a lineage demo first, and then we're going to demo some of the Gemini enhanced translation stuff that Mohit just mentioned. What you're looking at here, let me do a refresh, is our data flow from our lineage tool. Why is lineage important? Lineage is important because it shows us how our data gets from A to B and what happened to the data along the way. Super important in a migration. Also, a key point about that, for those of you with data governance requirements, it's critical and foundational that you'd be able to identify data at this point, the next point, et cetera, so you can comply with your governance requirements. Those are two of the reasons lineage is important. I'm going to jump into this UI. This is part of our earlier acquisition, and we're going to release this coming up shortly. What you're looking at is the data flow portion. All these objects on the screen are tables, artifacts, and our sample database. If I click on one of these, for instance, clients, you get a lot of good information. For instance, you can look at the access, a script access this, the last time it was written to, the pipelines that it belongs to as part of, and also the users that accessed it. That's super helpful. You can do that for every single one of these. If I go down the line here. Now, those of you who spent some time in the Kimball methodology, you may recognize this approach. We have our dimensions on the left. I feel like this microphone is on. I'm getting a lot of back feed. Anybody else? Okay. So, we're going left to right dimensions to facts. If you want to click on any one of these connectors here, you get additional information. It's helpful. The script, all that information you will go on as far as lineage. Now, if I go to the far right and look at our final fact tables, for instance, timesheets fact, I can click on this. I'll get a full picture of everything, every single thing that happened to the data that makes up our final fact table. So, should you be the accounting auditable type, it should give you everything you could ever want. Okay. Let's go back here. Now, I can also, I might be wondering, how did this table get created? So, I can click on my table here, I can then go to code, and I can expand these, and it'll actually tell me the code that created that table, right there. That's highlighted in green, and you can quickly tell that this was created with a common table expression from this table called int timesheet projects. And you can do that for every one of your tables quite easily. Just going to go back here. Okay. So, that is data lineage. I'm going to jump over to translation. I'm going to jump over to our console, which everyone I'm quite sure is familiar with. So, what is a translation in BigQuery? So, this is the main BigQuery screen. if I go down here. It's a little awkward with the trackpad. Then I go to SQL translation. Yes. You can see your translation, translations that you set up. You could start a new translation. We did all this last year, but not a ton new right here. I'll just mention a couple of quick things. We'll just go US dialect. We'll go from our good friends over at Teradata. And that's going to be one option, Google SQL. So, you fill the rest of this out. You're going to click create. You're going to start your new job. I already ran a job for us. Let's go take a look at it. Okay. So, the job I'm about to show you, this is the input source for that. I actually have exactly one piece of SQL I'm translating. And I have a file here called demo input underscore Gemini. This is our AI configuration. This is how you tell the system that you want to apply some Gemini goodness to your translation. In case you're wondering, this metadata zip file is what we use to understand and make sure that the SQL is paired with its schema. So, let's just look at this really quickly. So, a very straightforward query. All right. We're doing the select distinct user ID. So, just kind of pretend you work for online retail. You're looking to understand how many users were active. Right? Pretty straightforward. Okay. We're going to translate that from Teradata to BigQuery. Now. I'm just going to readjust your microphone. Uh-oh. Uh-oh. 400 witnesses here saw you do that. I just want to mention he put it back in the exact same place it was 36 seconds ago. Okay. So, we looked at our query. Fantastic. Now, let's go back to where I was. All right. So, I have already processed this, already ran this translation. Now, what if I have been a Teradata user since the beginning of time? I'm just moving to BigQuery. I don't understand all the goodness of BigQuery. how can I be helped? How can we help that user? So, with our Gemini support, I can do something like this. I can set up my config file here, and I'm telling it, do a query customization and target the target SQL versus the source SQL, and you give it an instruction, also known as a prompt. I'm just going to do a carriage return here, just so you can read this a little easier. All right. So, I'm asking Gemini to rewrite and optimize the query for BigQuery from Teradata. For instance, if there's something like a count distinct, perhaps suggest an approximate count distinct, and then add comments to the code explaining the approach, and maybe a brief overview of the functions in BigQuery. That would be pretty cool. Let's see how good of a job it did. So, I'm going to go over here to our translation, I'm going to click on code, and this is my one file. I'm going to click on this, our file. So, here is the original. Here's actually the basic translation without any of the fancy Gemini goodness, but I click on my Gemini suggestion option here. It's going to take me over there and show me the suggestion. the color formatting. Okay. Okay. Okay. Okay. Okay. Without the color formatting, one second. Man, this trackpad. All right. So, we got a nice, I'm just going to see if I can copy this without crashing my MacBook. Let's go over here. Okay. So, this is the Gemini suggested output. I actually really like this. So, it suggested to go from a count distinct to an approximate count distinct. Those of you not familiar with that option in BigQuery, it's an approximation versus an absolute empirical number. Fun fact, if you use an approximate function in BigQuery, any of them, they're generally within 2% of the actual empirical number. So, it's much less expensive than doing a count distinct, but almost as accurate. And there's a lot of good use cases where you don't have to be 100% accurate. All right. So, it did an approximate count distinct. It gave us some great comments there. Use upper and trim. Use approximate count distinct for faster approximate count. Ah, there's my explanation. Fantastic. This function provides an approximate count of distinct values, which is often faster, dot, dot, dot. It's a good choice. Gave us a good explanation. And then the final line, this optimized query is more efficient for BigQuery, especially for large data sets. I really like this option because when you're going from a Teradata or from an Oracle that spent decades optimizing their systems, either on indexes or like an MPP approach, this helps get the users adjusted to and accommodated to BigQuery and how our system does things. Okay. Normally, I would take questions here, but we're not taking questions. That's my entire demo. Thank you, everybody. Thank you, everybody. All right. Hope you all can hear me. Yes. All right. I'll introduce myself. Hi, everyone. Thanks for being here at 8 a.m. I wasn't quite sure I'd make it here myself, so glad we made it. So I'm going to give you an unvarnished and pretty authentic view of what it takes to move to BigQuery. I work at Ford. My name is Razal. And Ford has this unique relationship with Google where we are fully, fully committed to the Google Cloud. As in, we don't do hybrid clouds. We don't do cross-cloud compatibility. We go Google Cloud native first. That's really where we're at. So it's a pretty close partnership with Google. It's a cloud provider of choice. Now, Ford Credit, by way of business, is a pretty exciting place, to be frankly. We have the manufacturing arm, which is Ford itself. And then we have Ford Credit, which is the finance. So it's the manufacturing finance come together. Really exciting for us. We're undergoing this massive transformation. So in simple terms, what Ford Credit does is, if you ever want to buy a car, which I hope will always be a Ford, and you go to a Ford dealership, you have the option of getting a loan from any bank, any credit union. Well, one of them will be Ford Credit. And Ford Credit finances about $100 billion or so. That's the size of our portfolio. So a significant chunk. And now it's on on BigQuery. So BigQuery is pretty important to us. And the reason we're there is because BigQuery is a consistent source of investment from Google. We've studied this. It's in the middle of just about every roadmap that Google has. So if you're on Vertex AI and you're using the feature store, well, it's backed by BigQuery. If you are using Data Canvas, new product, it's on BigQuery. If you're dashboarding using Looker, well, that's also on BigQuery. The next phase is going to be conversational data. A lot of innovation happening there. That's also BigQuery. So we feel we're very comfortably, strategically situated right now, basically, with BigQuery. That's why we're on it. Now, what I'll do is talk about primarily, I guess that's why you're here, what does it take to go to BigQuery? And what are the lessons learned? And that was the title of the presentation. So I really want to get there, what are the lessons learned, as soon as I can. So when you move to BigQuery, there are two primary things that you're going to do, right? Number one is going to be you're going to transfer data from whichever warehouse you're on. You're going to go from on-prem or whichever your data sources into BigQuery. That transfer has to happen over and over and over again. It's not a one-time thing because you'll have thousands of tables, I don't know how much, terapetabytes of data. You'll have to do it incrementally. So that's one activity you'll have to do, which means you will need software engineers, as we did, to come ahead, containerize this tool called DTS, which was actually in one of the slides that Mohit covered, pretty cool tool. The reason we like it, it's API-driven. Now you get to containerize it, you get to spin it up, you get to transfer the data, you have the logs, you need observability, and the good thing is it's integrated into the cloud consoles. So that's why we used it. It's been pretty effective for us. So that's the first activity that you're going to do. The second activity you're going to do is you're going to validate the data once it's in the cloud. And the way it works is you have, I don't know whichever database you might be on. I mean, for us it was Teradata. I mean, I wish we had all those features that Mohit was talking about. But let's say you're on Teradata, you're going to take one row in one table in Teradata, you're going to hash it using SHA-256 or whatever. And then you take the same row in BigQuery and you'll hash that, let's say SHA-256. That's how you validate the data on-prem versus the cloud. And that validation is super important to us because we have financial data on it, we have treasury data on it, we have customer, product, PII, everything. I mean, data query is a pillar for us. And now, when you do those hash comparisons, 90% of the time it's a match. So, you know, hallelujah, knock on wood. 10% of the time, well, it's not so much hallelujah, I guess, because it didn't match. And that's where you then have to take each row and you have to break it down into its columns and start comparing those between on-prem and cloud for validation. This could be really important data. It could be data that's going into our financial statements, for example. Right? So, what you will again have to do is containerize this tool called DBT, which was on that slide previously. That's provided by Google. It's open source. You'll containerize it. You'll spin it up. It's going to run multiple times. And it's a software engineering project. There will be log consoles. There needs to be observability, so on and so forth. So, you have to account for that, basically, in the work that you're going to do. And here's the kicker of it all, right? So, you transfer the data. You validate the data. And what we saw, personally, in our transformation is that every time you do a data extraction from source and you run this validation on your source data warehouse, the legacy, you're going to incur a compute cost, right? And you're going to have the same compute cost on BigQuery, basically, because that's where the data is landing, and that's also where your validation compute is happening. And what we've seen is the cost comparison between on-prem and BigQuery is about 5.22 to 1, which means if your on-prem cost is $5.22, your BigQuery cost is 1, which was a great, I mean, it was a pleasant surprise for us. So, another reason that we actually are glad we picked BigQuery. Okay, so let's move into this, the lessons learned part of it, because this is really important. The way these data migrations go is these are not projects. These are entire programs. It's like moving a nervous system from one location, from one body to another, right? We have all these tools and everything, but I want to share a big picture with you all. And I say it's a nervous system because there could be 40 applications that are feeding data to your on-prem data warehouse, right? And now those 40 legacy applications have to connect to BigQuery in the cloud. So, you have to test it out. And this means the data impact is really wide, right? If you have a person who is your chief financial officer, they need to know this is happening because they have to go and validate data in BigQuery. If you have a chief operating officer, they need to know. If you have a sales team, revenue officer, they need. So, anybody who has a C in your company and all their teams are affected by it, which is why these tend to be really large programs. So, number one thing you'll want to do, and that's a lesson learned for us, was you really have to communicate the heck out of this thing. Left and right, up and down, across the whole company. I mean, all of Ford Credit knows Teradata is there. It's a priority one. I mean, sorry, this migration is there. It's a priority one for us. The other thing is inventorize, which I referred to earlier. You're going to have 40 source applications. Take a deep inventory of all your applications, connect to your current system, legacy warehouse, and do a test connection to BigQuery. Same on the destination side, right? So, on the destination side, when data comes out of it, if it's going to Moody's or any other system that is treasury financial accounting or anything based, they now need to change connectivity and connect to the cloud. So, inventorize, and we took about six to eight months to do this, and it was a serious effort. It wasn't just one Excel. We were testing out systems. What works? What connection doesn't work? What tools are there? What are not there? And every organization is going to have its own unique fingerprint on this, right? So, that's what I mean by inventorize. I mean, I won't be able to cover all of these. I think someone's going to blow a foghorn to get off the stage, so I'll pick a few. Another thing to look at is freeze periods. So, for us, when we did such a big investment, made such a massive move to BigQuery, the planning on this is massive, right? Some of the data you can validate weekly while you're lucky. Some data you can only validate every month, and sometimes data just comes in quarterly. So, how are you going to validate that on a timeline for 40-odd applications that are connected to your systems, right? And then there are freeze periods mandated by the SEC that you're not allowed. You strongly prefer you not make any changes to any application during a three-month period, for example. So, account for that in your planning as well. Another thing I want to talk about is parallel runs. At some point in time, as we did, you're going to have two data warehouses running in parallel, right? If you have a serious business, and you have an on-prem data warehouse that's been there for 40 years, it's trusted. It's a trusted system. They're trusted to two, three, four decimal places, right? And when you launch in on BigQuery, they want to compare what is the result on-prem and what's the result on BigQuery. And to show that, you are going to have to run two warehouses in parallel for a certain amount of time. So, you have to do the finances, then run the numbers on the cost of both of those. So, make sure to do that. Another one is reports. We have thousands. So, the legacy is baked in. We then had to go in and rewire thousands of reports to connect to BigQuery and run accurately. And then you discover, well, all these reporting systems that we had that connect to legacy, well, those old versions don't work with BigQuery, and they have cloud versions, so now you're entering into new license negotiations for those reporting systems. So, that's what I mean. It's really an entire ecosystem that you're sitting on, and it's an entire nervous system that's moving to the cloud. The last thing I'll move before I head over to talk about, before I head over to Mark, is decommissioning. When we do these programs, and this was really, I got road rash from this one, decommissioning in itself can take up to two months. As software folk that most of us tend to be, we think decommissioning is, hey, you have a software, you shut it down. Well, it's not. Because what will end up happening, when you have two systems running in parallel, you will actually end up with some code that is touching both of them. And that code, and we had a lot of it, will have to be gradually commented out line by line to disconnect from on-prem, and still function appropriately. So that's one. So code decommissioning itself can take you a month. Look into that. And then there's a hardware decommission, which we don't tend to think about, but equally important. That means to decommission, you have to account for possibly a month or up to a month, because you have to wipe your disk three times, because it had PII on it, and then you have to uncable, put it in a shipping box, and send that server back to where it came from, which, if you're lucky, is across town, if you're lucky, is across an ocean. And that is the legal definition of decommission. Otherwise, you may have a million, multimillion dollar bill to pay, because you did not account for that shipping. So a lot of learned values, lessons learned for us. I'd be delighted to answer more questions for you guys. And I'll hand it over to Mark. Come over. Let's see if this mic's working. I love the decommissioning ending, because that's what very few companies do well. And Mohit, I got a call a year or two ago from one of my former heads of engineering. I was a big query partner about seven to eight years ago at a large global bank. And he called me. He said, I finally turned off all my on-prem warehouses and lakes, and big query scales beyond my imagination. And that was, and we're talking, what, about nine years ago, that bank went into early conversations with Google in terms of a cloud partnership. I wish all that capability was here nine years ago. It's great to see it now. I'm going to talk a little bit, I'm going to raise this up a little bit more along Quest and why we've recently chose Google as our strategic partner in the stack with BigQuery as the main engine, really powering the whole stack. So just a little bit about Quest who do not know us. About one-third of the adults in this room will come into one of our labs, one of our patient service centers, and have lab work this year alone. About 50% of hospitals and physicians we annually serve. Almost 220 million requisitions in 24. I don't want to drain the slide. 80 billion patient data points. So one of two very large labs in the U.S. We also have a global footprint through other acquisitions. But a little bit about that, and then just kind of a little bit about the market broken out between the physician channel, which I believe is about 64%, 65%, and the hospital channel generating lab work. And so in that is a lot of data which actually analyzed correctly can change healthcare. It's one of the reasons I joined. I spent early part of my career in healthcare, left for financial services when we created this role called the chief data officer. And I came back to healthcare a few years ago because I believe now we have the technology to make a real difference. And so also the reason I joined Quest is we're sitting on, I've worked on the biopharma side, I've worked on the health plan side. I'd never had the data that comes in through our labs and the pathology data and the genomics data to truly make an impact in precision medicine. Why did we pick Google as a partner? If you met me in 2010 when I was at J.P. Morgan as their chief data officer, I talked to a symposium once about the fact that our technology hadn't really changed much in 15 years. We still use either Enbin or Kimball, left or right architectures. We got creative in doing ELT versus ETL, but the architectures, the tech really hadn't changed. And along came something called big data around 2012. And I call all these the disruptions that any chief data officer's had to live with, strategize around and apply since. But if I look at all the disruptions I've had to actually deal with in the last 12 plus years, they're dwarfed by Gen AI and agentic AI. That's the biggest disruptor we've seen. It's bigger than all the others aggregated, but it's also an opportunity to really solution differently. And we sat down with Google and we talked about how could we think about self-serving insights in a different way. I was very lucky, believe it or not, in 2012 I had a Wall Street technologist come interview with me and he presented a vision of a self-service data fabric, 2012. Now the technology wasn't ready, but the concept was my users wait months to get insights, but they want it in minutes. How can we start looking at technology differently and bringing it together? We sat down with Google last year, we walked through the vision of a video I'm about to show you, and Mohit's boss, Mohit told us we were thinking too short-sighted, that they're building more AI capability, more automation, more self-service than we were thinking. So in the video, which is just a short version of a video, which really goes through our whole data strategy, think about that. I mean, our goal is to really let our business users have conversations with the data. Our goal is to turn months into minutes. In my career, I'd say probably 10% of the insights I've seen across healthcare and financial services were ever acted on. 10%. How can we change this so we can start driving more actionable insights, even if we double the number of insights that are acted on? The growth companies can have, the operational efficiencies we can drive out, the risk reductions we can do are just substantial. So let's see. Oops. Oops. Oops. Oops. Oops. Accelerating self-service to data and analytics with Quest Data Fabric, a strategic partnership between Quest Diagnostics and Google Cloud. Meet Akira, an analyst at Quest Diagnostics. Her current technology is holding her back. She knows there is a better way, but innovation at this scale demands more than brilliant minds. It requires the right technology. Akira accesses the new Quest Data Product Catalog. She can see the common data products on the landing page and selects orders and results. Please generate a histogram of order counts by order status of the family health clients. Sure. Here is the graph. I want to schedule a new data extract from the enterprise orders and results data product. Which specific data points would you like to extract? Here are the data points I need to be extracted. I need you to answer a few more questions. Do you require the PHI fields to be de-identified? Yes. Here is a link to the extract location. With a new data fabric user experience, Akira isn't just an analyst. She's a catalyst for change, empowering Quest Diagnostics to deliver better patient care and achieve greater business success. Unlocking your best solutions at the best value together. So that first data fabric example I gave you that I saw in 2012, it was a 24-hour SLA turnaround. And it just gave the users a sandbox to actually go explore. So to me, we actually created these vignettes with the concept of using more of a Gen AI approach and then conversations with Google over the last six months of where GenTik was going. And this is going to be pivoted much more toward GenTik. The one caveat I'd make is that, you know, last year I went to a conference in my hometown of San Francisco, a big tech conference. Every software company I talked to was taking a little bit different approach with AI. I had two dozen softwares in our stack. That's two dozen different Gen AI approaches, different ways to ground, too much complexity to manage. I run best of breed in large organizations where I had 1,500 engineers. I actually think a single stack partner, and that's one of the reasons we partner with Google, single stack partner right now, will run faster and further and enable our businesses to a level that trying to put two dozen different softwares together, each with a different agentic approach, each with different RAG approaches for Gen AI would just be too complex. But the one piece I'll add is, I mean, we're going to see more change in the next one to three years than we've seen in the last 30. So get ready because it's coming. applause You'll be happy to take questions outside, I think we have had time, but just some housekeeping, please provide your feedback. Really appreciate you joining us, and thank you very much. Looking forward to working with you and partnering with you. Thank you.