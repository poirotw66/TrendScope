 Music Music Music Music Music Music Hello, welcome to our talk. Thank you very much for coming. We're going to be talking about what you see on screen, which is building a serverless toolkit for developers at Shopify. So my name is Nathan. I'm a developer at Shopify. I'm one of the guys that helped build this serverless toolkit that we'll be talking about. I'm joined by Dan, who is a staff dev at Shopify and a user of this serverless toolkit. Archie, a container specialist at Google who's helped us solve lots of problems along the way. And Owen, a staff infrastructure security engineer who helped ensure what we built was secure. So with the exception of Archie, who's from Google, we are from Shopify. We make commerce better for everyone. That's the tagline. We're responsible for some very big numbers, you know, $8.9 billion in revenue, trillions in global commerce, millions and millions of merchants, big and small across the globe. So that's who we are. Now for why we built this serverless toolkit. Let's start from the beginning, right? So Shopify loves GKE and Kubernetes. We've been using it for a while, and I like to think that we've gotten pretty good at it. Over the past six-plus years, we've built a lot of tooling on top of GKE and Kubernetes, mostly to support our most sophisticated applications, right? So the big business movers. This tooling acts as a sort of platform in and of itself, and it's grown more and more sophisticated over time. And for some of our smaller and our less sophisticated apps, we were curious kind of, well, is this a little bit overkill, all of this platform tooling, right? So we thought, you know, Cloud Run had just sort of launched at the time, and we were thinking, could we run some of these smaller, some of these less sophisticated apps on Cloud Run? So how do we go about doing that, right? We could just point devs at Cloud Run and say, have fun, go try it. Well, it's not exactly that simple. There's still a significant amount of kind of setup and toil involved, right? Things like your database and load balancing and caching, et cetera, making sure things are networked, can talk to one another, and making sure everything's secure. We didn't want to offload all this toil onto devs. We're used to automating as much as we can, so we wanted to automate this, too. So we got to building, and we built a thing called ProdKit. This is the serverless toolkit that's mentioned in the kind of main slide. It's a kit for production, hence the ProdKit. ProdKit does two main things. It is a way to set up and manage infrastructure, and it is a continuous deployment solution. There are a few main things that sort of shook out of this process, a few main things that we built. We built the ProdCLI for developers to interact with, a GitHub Action workflow, that's our sort of continuous deployment, and a set of Terraform modules. They all interacted a little bit like this. The user would interact, for example, if you're starting kind of left to right down at the bottom sort of section. The user would interact with the ProdCLI. That would result in Terraform changes, which would result in changes to their infrastructure. Similar story, when they go to deploy a change or make changes to their code up near the top, they would push something with Git. That would trigger CI, CD, again, resulting in changes to Terraform and changes to their infrastructure, newly deployed apps, that sort of thing. So what does this look like in Action? I'm going to do a demo, but it's not a real demo. It's just slides because real demos never work. So we're going to start with just a basic Rails repo. If you're not familiar with Rails, it's a pretty standard kind of model view controller web framework. This is already a Git repo. It's pushed up into GitHub. From here, we can run prod init. This is going to initialize this sort of tooling. So we get a number of files. I really want to focus on the top one and mostly just that bottom one, that prod YAML. So I'll dig into what those kind of do in a minute, but for now, let's just get to making some infrastructure. And it says that we can do that down near the bottom after we've run this prod init. It says we can do that with a prod up. And this is going to action that prod YAML file, which sort of defines what infrastructure that we want. So prod up is going to do some stuff, again, actioning whatever's in that prod YAML file. And it ends up at a Terraform plan. This lists all of the resources that are going to be created and modified, et cetera. We're going to look it up and down, make sure it all looks good. And if it does, we'll say yes, please do this. You know, kind of apply these changes. And it creates infrastructure for us. Great. So how do we deploy our app to this infrastructure that we have somewhere off in the cloud? If you remember, we have that GitHub action now in our repo. That GitHub workflow is able to kind of build and deploy our app image to our newly created infrastructure. We can make that happen with a git push or a merge to main via APR. Or you could run prod deploy. This will kick off that kind of build and deploy workflow. It'll deploy whatever's on main in your GitHub repo, and it'll open up the browser to show you what's happening. And it looks like this. So it's kicked off the deploy for us. Okay, we can watch it go. There's just two steps. Like I mentioned, it's going to build our app image, and then it's going to deploy to our infrastructure. So let's say it deploys. We've got our app somewhere. How can we see our now deployed app? Well, we can come back to the terminal. We can run prod open. That's going to open the browser with the right URL and everything, and there we go. There's our now brand-new running application in production. Ta-da. Great. So what just happened? We can do that, specifically on that prod up step where we're actioning prod YAML. That's really what I want to talk about. So we start with this custom YAML format. I've been talking about prod YAML. It's relatively small, relatively simple. We take this prod YAML, and we translate it into Terraform. It's not really more complicated than that. So here's what that looks like. As you can imagine, our web process here gets translated into a Cloud Run instance. Same with database into a Cloud SQL instance, et cetera. Similar for caching. So how did we end up at this solution, right, instead of just using Terraform or something else, whatever? Well, we tried a few other things first. We started, actually, with just the gcloud CLI and ClickOps. So if you're not familiar with gcloud, it's a CLI tool made for managing Google Cloud infrastructure. So initially, prod up, instead of actioning a prod YAML file, it would just run through a long list of gcloud commands. They were baked right into the tool. There was no infrastructure as code anywhere. Our thinking here was that we wanted to expose devs to as much of the kind of raw GCP platform as possible. At the time, a big goal of ours was to get devs to kind of explore GCP, explore what was possible, and use the platform. This was great for doing that, learning about GCP. Devs were able to explore and try new things, all sorts of incredible and wacky things. And it was also super easy to kind of maintain, right? It's, again, just a long list of kind of command invocations. But there were two main issues for this. It was not repeatable, right? So if the gcloud CLI ever got an update or if the code that we've written to wrap that ever changed, you'd get a slightly different result on every prod up. We also noticed that we were sort of just building a worse Terraform. A lot of our code was just checking the current state and the desired state and trying to reconcile those, which is something Terraform is already sort of good at. So we said, okay, let's just try Terraform then, if that's what we're building anyway. So we offered devs a prod.tf Terraform file right in their repo. We created a bunch of Terraform modules so that this file in their repo could be small and they could just make references to all of the modules that we wrote, right? For example, a cloud run, a reference to one of our cloud run modules would look like this, right? We've got, let me try the laser. Oh, check that out. Yeah, so like it's pointing off to our actual module often like the prod kit repo and it's all sort of relatively terse and cute, whatever. This was really nice because it let us improve and change the underlying implementation of the infrastructure. So long as we kept this interface the same, so long as this file didn't need to change. But that was a pipe dream because this file did need to change and it needed to change relatively often because, you know, Terraform is relatively rigid, Codron was new at the time, there was lots of changes happening. So we were constantly bugging devs to go and update this prod.tf file for changes that they didn't really need to care about. So from here, we ended up at this prod.yaml that I've been talking about. This allowed us to completely change the underlying implementation of their infrastructure so long as we still gave the devs what they asked for in prod.yaml, right? So long as they still got a web app and a database and a cache, et cetera. And as a sort of plus, the devs get a much smaller and easier to understand interface, right? You could go to any repo and go, oh, this app has a database, it has a cache, et cetera. So now our setup looks a little bit like this, kind of coming back to this flow. Everything's the same except for that prod.yaml down near the bottom. We've added that in. Now the prod.cli looks to that prod.yaml as a sort of source of truth for what infrastructure to make and maintain and deal with. Okay, so we have this whole setup. Nice. But is it secure? Right? And for that, I'm going to hand it over to my friend and colleague Owen Cummings, who's going to talk about how he secured the whole thing. So, you've gone and made this sleek, elegant solution for allowing developers to deploy things to Cloud Run. Caveat. You've gone and gave your developers a bunch of agency over infrastructure. And with that comes a lot of security challenges. So, a bit of context. At Shopify, historically, a lot of our core applications run on Kubernetes. And as a result, we've built all of these systems and processes in place to safely deploy applications on Kubernetes. But we've gone and brought this new tool in that is a simple, sleek approach to deploying Cloud Run applications that minimizes all of the overhead that comes with deploying through this rigid pipeline. So, now we have to go and reassess how we're doing security. And so, basically, we had to modify a lot of existing controls and add new ones to enable developers to ship code safely without the need to worry about security. So, broadly speaking, when I think about securing Cloud Runtime, you know, this is sort of what comes to mind. You know, who built the container? What dependencies does it have? Who can access the instance? What permissions does the services identity have? How are we passing secrets? You know, all of these questions fall under a few areas of security which you see on the screen. So, we're going to talk about those. So, software supply chain security, at least in the context here, is we're really thinking about the risks that arise when Procket users are building and deploying containers. So, to resolve some of the risks around this, at Shopify, we build software, build materials for all of the containers we build. And it tells you just things like what's in the image. And some tools for this do vulnerability scanning when it's done. So, combining this with image signing and binary authorization creates this powerful combination where you have a mechanism to define and enforce policies for what is allowed to deploy in our infrastructure. Fun fact, binary authorization was originally made in collaboration with Shopify back in the day. So, a software building material, here's just a snippet from one. You know, I don't want to just dump like a thousand lines of random JSON on your screen. So, here's an example. It sort of tells you information like what is a package, what is a version, here's a hash, here's where it was introduced in this container. And this is really useful information, especially if you index it. And security teams love this because when, you know, you want to answer where is this package running at our company, you just go look at the system that has all these indexed SBOMs. So, as I mentioned earlier, binary authorization. It is a relatively complicated system. So, we're just going to look at it from a simple lens, which is basically, you build an image. Like, you're a developer. You build an image, you push it to artifact registry and try to deploy your application. In the background, binary authorization is at work. It's basically, you define checks within a tester such as vouchers to say things like, does this image have a software building materials? Is it free of any particular vulnerability we care about? Or, was this built by Shopify? And these checks then, you know, sign the image and then when binary authorization actually goes to, or when you go to deploy your application with something supported by binary authorization, it'll actually check the policy and say, hey, what checks do we care about to deploy in this environment? And so, we can say things like, in this environment, we only deploy images built by Shopify or we don't deploy images with the CVE and so on and so forth. The vulnerability stuff is a little bit more complicated, especially with the volumes that exist at scan time. So, I'm happy to talk about this later if you want to. Alternatively, you also have dry run mode, which is great for telling you when things are violating your policies but aren't going to block developers from shipping. So, next we have identity and access management. It's kind of an evergreen problem in security. You know, great. We know what's in our prod kit images and can cryptographically attest where it came from, but we want to make sure with identity and access management now that we're using identities backed by, like, highly trusted sources like Google. We want to authenticate with short-lived tokens and generally practice the principle of least privilege for all parties involved, machine identities and human identities alike. So, we want developers to have the ability to modify their infrastructure, but we want them to be able to do it safely minimizing the risk of human error. So, we need to create secure green paths so developers are less likely to need to make security impacting changes. By default, developers shouldn't need permissions to modify security impacting settings. There should be clear defined paths. So, we build these explicitly defined, well-documented gates for risky actions. So, let's talk about some of the things we did for prod kit. So, we've gone and changed this whole paradigm now where we're no longer running in Kubernetes. We no longer have all of these cool things we built for Kubernetes to make it run and expose things effectively and safely. So, you know, it's all moot. So, how we approached this was we used Google Cloud Load Balancers and Identity Aware Proxy. So, broadly speaking, it kind of looks like this. We have a Google Cloud Load Balancer, you know, and all these different endpoints. Their backends mapped to with URL maps. And we use these to sort of support the majority of access use cases for Cloud Run applications at Shopify. So, let's talk about some of them. Human authentication. This one's very simple. You know, you have your load balancer. We check, did the request come from someone with a Shopify email? Great, cool. IAP says, yes, let's let that through. And then IAM will check, does this person have the Cloud Run Invoker permission on the instance? Great, cool. You know, you're the developer who built the prod cadet. Go have a look at it. Now, other things that are more important is like, how do people expose these applications publicly? Because we want it to require friction, so you can't just go and do like, prod expose or click like, allow anyone to touch this thing, right? So, we want to have this tool that was built called prod publish. And so, it clearly warns developers that, hey, you're going to expose your application to the internet and it allows you to customize the paths you are going to expose to the internet. And so, on the back end, this is all managed by an internal infrastructure management tool. So, they don't actually have the ability to go make this change themselves, but what they define with prod publish gets submitted to this infrastructure management tool and it will update the load balancer, configuration, URL maps, all this stuff to make sure that you now have a public route into your application. And while there are options to do this in Cloud Run to just expose your application publicly more simply, we use the GCLB because like, custom domains aren't supported through a lot of these flows. So, this allows us to have our cool like, my app, Shopify.com. Now, robot authentication, this was another area where we had a lot of work. So, basically, when a request comes in, but it contains an authorization header, the URL map bypasses IAP, sending it straight to Cloud Run for validation because Cloud Run is capable of validating these tokens provided they're signed by Google. And this creates sort of an ideal green path for service-to-service authentication because we rely on short-lived ID tokens rather than having to implement something like basic authentication and do the whole secrets management process. Google does provide support for external identities if you do use something other than Google for an IDP or whatever backs your identities using something called workload identity federation. So, secrets management. Cool. Now you've got all your applications set up. It's exposed to the internet in the way you want. It's all done safely. Now, how do we get developers putting secrets in their application without putting their secrets at risk? We had to sort of create a new path for this at Shopify because historically we used a tool called ejson. And what that does is, you know, you have your cool JSON file containing your secrets on your developer laptop. You encrypt them with ejson. It gets baked into your container image and then we seed secrets at runtime to decrypt those secrets and expose it to the application. But ProdKit is basically new system, new infrastructure, so we have to go and look at a new tool. So we look at Google Secrets Manager. You know, it reduced the overhead of managing the secrets lifecycle and provided a safe means of loading secrets into workloads. So, with ProdKit there was the ProdSecretsCreate thing. You know, all this does is wrap the creation of a Google Secrets Manager secret. You provide a key value and some annotations and labels. But there was also infrastructure secrets created by those commands like ProdInit. So when you define a database, you know, your application needs to know what its database URL is, and this is how we load those things into the ProdKit applications. So what does this actually look like all put together? You have your secrets injection that occurs when the deployment is made when you push to like that GitHub CI piece. And so what all it does is it checks, hey, what Google Secrets Manager secrets are on my project. It looks and sees for like the labels and annotations we care about, like do we want to expose this to the application? And then when it creates the Cloud Run deploy request, it inserts these references to these GSM secrets. And then when Cloud Run actually deploys, it pulls those secrets so at no point during your CI are you decrypting and pushing secrets where they could potentially be exposed in logs or anywhere else. So you've gone and solved all these problems. How do you make sure these assumptions stay true? You know, we detect when safeguards are violated using a number of different things. Generally speaking, continuously scanning your cloud infrastructure helps. We use Security Command Center for this. You can define with Security Health Analytics these like custom modules that say, I want to know when a Cloud Run instance is, you know, setting this setting that we assume should never be set. And, you know, it'll notify us when that occurs using the asset inventory data, which is a tool that Google has that will pull all of the metadata for your applications and resources in Google. So you can scan them. Anyways, alternatively, if you don't have access to Security Command Center, you can just write effectively a GoCron job that's going to go and scan your infrastructure and check for settings you don't want to be set. Another thing, if you want to be more authoritative, we try to avoid using org policies because we don't like to block developers from doing things. We like to sort of provide safe guardrails is you can use organization policies. Organization policies allow you to just define conditions within your Google organization such as, like, what ingress settings we allow on Cloud Run instances. Like, what principles do we want to allow in IAM policies? You know, do we want to disallow anyone creating, like, a third party, like, adding... Let's add Archie to a ProdKit project. You know, we can block that. And alternatively, if you don't want to be authoritative, you just put it in dry run mode. So it'll tell you when your developers are bypassing these controls. So, last thing I want to talk about is runtime security monitoring. How do we monitor that containers are behaving properly? So, in GKE, this is easy. We used a tool called Falco. It uses eBPF to actually intercept syscalls and evaluate them against rules. Fine. But now we've shifted the shared responsibility model and Google manages all of the infrastructure. I have no means to get access to the node to inspect these sorts of things. So, what did we do? We asked Google to build Falco for Cloud Run. And it took about two years. So, we asked for it so you don't have to wait two years. So, Cloud Run Threat Detection is kind of managed runtime security monitoring for Cloud Run. It mostly focuses on high criticality detections like when there's malware in your instances. And it's very similar to their other product which is the Kubernetes Threat Detection. So, to give you an example of what this looks like, this is a detection for a reverse shell when it detects a stream redirect to a remote port, a remote IP. And this is, you know, all these detections are very important if you run very critical workloads in Cloud Run because it'll tell you when your workload has been compromised. So, in summary, we've gone and built all of these sort of end-to-end things as a comprehensive spread of security controls to help developers safely build containers but also gone all the way to runtime monitoring and making sure the containers are behaving there. This allows developers to ship quickly without needing to sweat security details. And if you want to talk about any more of these topics in depth because I just skimmed the surface, come talk to me after. And now on to Archie, a customer engineer we work closely with who will talk about observability in Cloud Run. Thank you. Quick show of hands with things security is easy. I think, Owen, you impressed everybody here. So, we have ProdKit set up and ready to deploy our applications. And thanks to security team and Owen, it's secure to deploy. Now, developers can deploy their applications, right? Well, we forgot, I think, something important. What if our application have an issue? How developers should aware that service is slow or provides a degraded experience for the end users? Or, once issue has been detected, how the developer is supposed to go and troubleshoot and find the root cause of the problem and fix it quickly? Is this an application issue or it's a cloud infrastructure issue? Any downtime or degradation of user experience across applications can cause significant losses for the business's revenue and reputation. And so, my favorite meme when I was working as a sysadmin and running VMs probably can, you know, explain it in more than a thousand words. But, with the differences in the world of serverless containers, you cannot SSH inside of the VM, you cannot see the logs, you cannot troubleshoot, and so it's a different paradigm that we need to get into. This has to be a better way to observe our system. So, that's where observability comes in play. Observability allows to understand the state of application by checking its external signals, things like metrics, logs, traces, and enabling developers to diagnose issues and ensure system reliability. Google Cloud provides Google Cloud observability tooling such as cloud logging, cloud tracing, cloud monitoring, and Google Cloud service for Prometheus. In addition to that, our goal with ProdKit team was to enable to have an ability for Cloud Run Services to send custom application metrics into Shopify's observed platform, which you can see here on the diagram. Shopify Observe is a centralized observability platform that has been built by Shopify. It's built on top of cloud native projects such as Grafana, Prometheus, Thanos, and it allows developers to go into one single place in the organization. They call it Observe Metric Explorer and find all the metrics across the company in one single pane of glass, rather than if the application is running on GKE or Cloud Run, and then they can create alerts and dashboards. So how are we going to achieve this with Cloud Run? Well, first of all, the good news, Cloud Run itself and its UI provides out of the box in context of the application your observability tools. So you don't have to jump to different tools to troubleshoot your Cloud Run. The first tab called Metrics. So in Metrics tab, developers can set up uptime checks and alerts, find key Cloud Run metrics and dashboards. In the Log tab, developers can find Cloud Run logs and application logs. And finally, service level objectives. SLO tab allows developers to become SREs and set the targets for uptime latency and target budgets. So now developers have access to Cloud Run specific metrics and the logs, but what about actual application metrics? We call them custom metrics sometimes. How we can expose applications from Cloud Run services. And in order to find the solution that we have to solve with the team, we have to understand how it works in Kubernetes and then kind of mimics this idea into the Cloud Run. And so the first step is that usually when you're doing this in Kubernetes, you have to instrument your application so it can produce your application metrics and emit them to the platforms. And so today, if you're using Kubernetes, you have two choices. You either use Prometheus format metrics instrumentation, which is like quite popular. It's been used for over 10 years. And then we have open telemetry instrumentation, which is relatively new, but super hot. And a lot of people are talking about it simply because you can instrument metrics, logs, and traces in one place and they can correlate with each other and it's so great for observability. And it's becoming a standard in industry right now. Once you instrument your application, you need to ship it to your collectors, we call it, or observability platforms. And to do that in Kubernetes, we use daemon sets pattern. And daemon set is basically running an agent of observability in your node, in Kubernetes nodes, and they are fetching the metrics from your application and sending to the destination. And this is like most common pattern in Kubernetes and everybody's using it, there's nothing new here. Another common approach is using sidecars. And so sidecars is basically, it involves deploying an additional container, which we call sidecar, alongside your primary application container in the same pod. And this sidecar extends the functionality of the main application without modifying its logic. And it's also widely used in Kubernetes for things like proxies, metrics, logs, and some others. So, so basically, we had to figure out which method is going to work best for us. And so because serverless platform doesn't have nodes, we had to go with the second option, is basically use sidecar pattern. And so we had to collaborate with our Cloud Run engineering team, and now Cloud Run supports sidecars. And once this was enabled for us, we were able to deploy Open Collector Sidecar from CNCF and being able to send metrics, logs, and traces information into Shopify Observe platform. If you don't have Observe from Shopify, you can always use Google Manage Service for Prometheus. We're providing two options to collect your application metrics with OpenTelemetry using Google built OpenTelemetry Collector. Or if you're using Prometheus instrumentation, you can use Manage Service for Prometheus Collector Sidecar that we also manage for you. However, we run into the challenge. So Cloud Run Jobs, which is like we talked about services, the Cloud Run Jobs, is a popular way to run light data processing at Shopify. It is used, for example, for the Shopify store management applications. And since Cloud Run Jobs didn't have sidecar support, we had to find a way how we can collect the application metrics for these production systems. And so our workaround was to use OpenTelemetry child process in that container, which is not recommended, way to run monitoring collection, and it worked for us as a stopgap, but we're not recommended to do this. The good news, maybe because of Google Next, Google engineering team released support for Cloud Run Sidecars for Cloud Run Jobs right now. So you can use your sidecars both in the Cloud Run Jobs and Cloud Run Services today. So now Shopify developers can deploy application with ProdKit, and it's fully automated in terms of setting up your observability and collections of the metrics. Okay, great. So now we're moving to the most interesting part of our presentation, using ProdKit in production to deploy real Shopify applications on Cloud Run. And so far, we've seen ProdKit has been used to deploy hundreds of different applications at Shopify. Most popular use cases are highly scalable API services, multi-tier web applications, internal services, and light data processing with Cloud Run Jobs. Recently, Cloud Run added support of GPUs, and so it's considered a candidate for running inferencing for Shopify applications. Another great use case we had is it was a use case where we're supposed to deploy an app across all regions of Google Cloud, and so the one option was to deploy GKE cluster in each region. What we did, we basically used Cloud Run to deploy this service automatically across all the regions without deploying any clusters. And with that, I would like to invite Daniel on stage. He is from Shopify customer behavior API team where he's going to share his journey how they migrated to Quadrant with ProdKit. Thank you. Thanks, Archie. Yeah, as Archie said, I'm going to talk about the story of my team and moving our application from where it was using ProdKit into Cloud Run. So I just want to set the stage of where our application was at before this migration. So initially, we were a Ruby on Rails application. It was very high throughput, handling two to four million requests per minute. And so being Rails, some of the things about that is, you know, it has a certain memory management model where it's garbage collected and single thread in the VM, which sometimes makes concurrency hard. And one of the things that we ran into with our application is it had memory issues. We discovered a bug in the gRPC gem where it essentially had unbounded memory growth. So after a deploy of our application, the containers would be killed by the process manager running and just constantly restarting. And so we were running on GKE and because of these restarts and the scale we were trying to run at, we were a bad neighbor in the cluster. We were causing instability for the applications and hurting their reliability and being able to do what they needed to do. To solve these problems, we came up with a plan. So first off, we migrated our application to Rust. This gave us better control over memory management. It allowed us to have really great concurrency primitives that Rust has. Secondly, we were going to use ProdKit to help us remove the need to manage any infrastructure on our own and help us move quicker. And because we were moving to ProdKit, we were going to move from GKE into Cloud Run to host our application. And this gave us the ability to have control over the sizing and the resource available for application. And we were no longer causing issues for other applications because they were no longer in the same cluster. My team is a team of application developers. And we focus hard on that. We want to focus on that. And they're not well versed in infrastructure and security like Nathan and Owen are. And so having ProdKit really helped us iterate quickly. It handled the networking issues, setting up the networking, setting up the load balancer, making sure it's secure, and handling all the complexity that's involved with that. And it allowed us to have our infrastructure provisioned and ready to go in days instead of weeks. Provisioning infrastructure can be very fiddly, getting it all correct and secure. And we didn't have to worry about that. We just ran those commands that Nathan showed and we had environments. We had a production environment and multiple staging environments to test our application. We did run into some growing pains that I want to touch on. First off, moving from Ruby to Rust, there's a very steep learning curve with Rust. Even though it has greater control over memory management, it requires sometimes a paradigm shift in how you think about memory within your application. And so it took us time to ramp up on that. Google APIs do not provide a native Rust SDK. And so we had to take the protobuf files for the services that we used, generate our own clients, which means we're missing a lot of the niceties that are built in. We had to build in our own retries and exponential backoff and handle all of those features that come out of the box with Google SDKs. Rails provides a framework called ActiveJob, which makes running jobs really simple and easy. That wasn't there anymore, so we had to build our own frameworks for handling jobs. Because of this, we ended up building a poll consumer that was watching PubSub for messages. Another growing pain that we ran into is initially we started with the VPC connector, which is an instance that runs in between your Cloud Learning containers and any services that you might talk to in your project, like Memory Store or Cloud SQL. And we moved to direct VPC egress, and I'll come back to that later to talk more about that. And finally, sometimes we needed more than what ProdKit provided. An example of that is we have two types of workloads in our system. One where their batch processes running in the background have longer request times and use up more resources. And then the second one is we want low latency, fast throughput, and we don't want those two workloads interfering with each other. And so we route the request to different Cloud Run services, and we did that via path-based routing, changing the routing map in the load balancer. While we did this, we were very careful not to overcome the security that had been put in place on ProdKit. For handling jobs, so there's two types of jobs that our system handled. One, things that are run on a schedule. And these work really well with Cloud Run jobs. So you've got a Cloud Scheduler set up a cron in there. It'll call the job, and it runs on its schedule, hourly, daily, whatever. And then the second approach, I mentioned there that we built the poll-based consumers. So you can see I put a little diagram up here. You have our application would receive an HTTP request into the Cloud Run service. Depending on what it was, we'd push a message into PubSub, say that we need to handle this maintenance task later. And then we had another Cloud Run service that would watch that PubSub queue, polling it, and then handle that request. We used, like I mentioned, we used the web service, listing on a port with no traffic. This is not really how Cloud Run services were meant to be used. They don't scale well this way. The scaling metrics for Cloud Run services, it looks at the number of concurrent HTTP requests you have and the CPU usage. Because we had no traffic, our containers never scaled. And so we had to come up with a way to handle this. We added alerts to watch the number of events waiting in the PubSub queue, and then we'd go in and manually scale and edit the revision, deploy a new revision that had more containers to handle the messages in the queue and bring them down. Some other options that we could have gone with here is PubSub has native push queues, so when an event comes into PubSub, it'll send an HP request to your Cloud Run service that you can handle it. There's also a new feature coming out where there's worker pools specifically for other messaging systems built in in Kafka, where can scale based on CPU usage only, and there's a presentation tomorrow, I've got it up there, called Autoscaling Kafka Consumers on Cloud Run. As I mentioned, we would go in and edit the revision and cause a full deploy of your Cloud Run service. Sometimes you don't want to be in the situation where you get a full redeploy. Cloud Run has features, so you can turn on manual scaling on your Cloud Run service, and this will not create a new revision, meaning you can just scale up and down without doing a full reset of your application, and this can be set via the YAML, in the console, via the gcloud CLI, or via the API. There's also another new feature that came out where you can set min instances directly at the service level, and those can all be set via the same option as well, and this does the same thing. It's just it's not at the revisions on your service directly. So getting back to the VPC connector versus direct VPC egress, so as you can see on the left there, this is how the networking looks. You've got your Cloud Run, connecting to the service connector, and then going to your Cloud SQL or your memory store, and the problem that we had there is it becomes a bottleneck, and it can have problems where it loses connections, and you just overload that one container, and your request latency spikes, and so we solved this by switching to direct VPC egress. The one con about that you can see on the bottom there on the right is that it will consume more IP addresses because it places your Cloud Run services directly in the same VPC as your internal Cloud SQL and memory store. So let's talk about VPCM. What is VPCM? This is the weekend following American Thanksgiving. It's Black Friday, Cyber Monday, and it is the biggest shopping event of the year and when Shopify sees the most traffic on its storefronts. So this here is a graph of the workload of the system. As you can see, of the month of November of last fall, last VPCM. So you can see we're about two to four million requests there on the left, slowly ramping up and spiking there on November 29th, Black Friday, at just over seven million requests per minute. Black Friday is the biggest shopping of the year. It's a day that we spend months preparing for beforehand. So how did my team prepare for that? So what we did is we took Apache Flink, which is a streaming solution. We're watching events that come off of the storefront and we took the input source and multiplied it five times and sent those through to our application. And what you see here is our staging environment overlaid on top of production. So it's basically the same graph with staging on top. And you can see there on November 11th, I threw 20 million requests per minute at our staging environment to make sure Cloud Run could scale and handle and we were confident in what we would see on Black Friday. And it scaled without issue and the team had confidence going into Black Friday knowing that our application could handle it. And over Black Friday, we had zero pages and it scaled flawlessly compared to the previous year where the system was on fire and we had multiple pages throughout that weekend. And so now I'd like to pass it over to Nathan just to wrap things up and give some final words. Thank you. Okay. All right. Okay, okay. Five minutes less. Let's see how fast we can do this. So final words. Let's start with some lessons learned. And I'm going to talk about even more lessons in a second. So I'm going to caveat this slide with the fact that Cloud Run has changed a lot since we built what we built. So like do your own research, figure it out. But this is what we ran into. We ran into just the generic fact that Cloud Run was not for everything, right? Stateless stuff. APIs, data processing. It was really awesome for. Background jobs did not fit well with Cloud Run's sort of HTTP everything model. Always on stuff. The scale to zero fact is a selling point for a reason. So if you leave it on all the time, it'll cost money. Highly stateful stuff. You know, great big user sessions didn't really play nicely with scaling to zero and some of Cloud Run's other features as well. Some other takeaways. I'm going to like rock through these. If you have more questions, please find me, you know. Balancing autonomy and debuggability was hard. Building ProdKit, some devs really took advantage of all the newfound autonomy. Some devs had issues kind of overcoming some of the like learning gaps, et cetera. Wrapping Terraform is tricky. It's pretty rigid. It doesn't like to be wrapped. It wants to own everything. Unobtrusive security is hard. As you've seen with Owen's section, there was a lot of work that went into making sure that the bad things were hard to do. Networking is hard. I don't have anything nuanced there. You all know networking is hard. Two people, okay, so with all this considered, I'd like to point out that two people built the majority of ProdKit. It was myself and a guy named Gabriel Paredes back at Shopify. Shout out to that guy, total 10x engineer superstar. So, you know, yeah. With all things considered, using Cloud Run, it is not that hard to build a framework like this. And I think you should, too. Thank you. And I'm going to briefly hand it back over to Archie to talk about some of the, you know, successes we had here. Thank you. So, today Shopify is running containers at scale using both GKE and Cloud Run. And thanks to the ProdKit initiative, Shopify developers are now able to reduce deployment of cloud application from weeks to hour. Applications deployed using ProdKit also following the best practices for security and observability is configured out of the box. Cloud Run scales Shopify applications to millions requests per second with ease and helps to run BFCM without many major issues and pager alerts. All that while removing the need to operate your clusters and troubleshooting your issues on the clusters. So, we want to thank Shopify team to collaborate with Google Cloud team and help to improve Cloud Run. And I hope all of you will be able to take advantage of all these new features that we were able to deliver together. And just to ramp up, I think, like, a lot of people are asking me, like, every day, which one I should use. So, I like to put this one. At Google, we provide several great ways to run your containers. And while GKE Autopilot, which is currently our default mode on GKE, can deploy applications of any complexity, both stateful and stateless, I find that the problem is, like, there's a steep learning of learning Kubernetes. There are some companies that are strong, like Shopify, they know Kubernetes very well, but there are some companies who are maybe just starting. And also, the part that you need to also manage GKE clusters, you need to still upgrade them from time to time. It's kind of pain for many organizations. So, Cloud Run can provide this great alternative to GKE to run serverless containers for you. And so, if you're starting your journey today with cloud applications, I would highly recommend start with the Cloud Run because it's serverless, it's easy to use and deploy, and toolkits like PropKit is going to help you to run this seamlessly and repetitively. However, if your organization is concerned of vendor lock-in because it's serverless thing, moving applications from GKE to, from Cloud Run to GKE is very easy because we use the same APIs and so, translating the deployment of Cloud Run to GKE is pretty straightforward. So, finally, if you'd like to learn more about Cloud Run progress, today we have a couple of sessions around enterprise security and also, if you'd like to learn more about scaling your Cloud Run applications for background jobs, out-of-scale Kafka consumers on Cloud Run could be a great session for you. And with that, thank you very much and I hope you're going to join us at the keynote, at the developer keynote today and have fun at Google Next. Thank you.