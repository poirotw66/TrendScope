 VIKAS KOLAVANIJANI Good afternoon, everybody. Welcome to Google's next generation network, which is AI-powered. My name is Vikas Kole. I lead the Google Global Networking and Infrastructure team. Joining on stage today would be Shubhas Rimondal, who is the tech lead for our wide area network, and also my esteemed colleague from the industry, Colleen Bannon, who is the CTO of British Telecom Business. It's a real privilege and a pleasure to really represent decades of work that various networking teams in Google have done, and I'm just representing this awesome work that has happened over all this time. Our company was founded more than 25 years back with a simple mission to organize the world's information and make it universally accessible and useful. And over the years, we have done that with our products. Today, we have 15 such products, each serving more than half a billion people and businesses around the world. And to build this product, we needed a network that ultimately powers how all of these people and businesses to experience them as they use products like Search, Gmail, Google Cloud, and all of the various products that you use every single day. This network has grown over time. Today, this network spans 2 plus million miles of late fiber. Just for reference, that is about 10 times the scale of the next largest cloud network that exists out there in the market. It includes 33 subsea cables that we have directly invested into, 202 network edges, 3,000-plus media city and locations, and of course, it connects the 42 Google Cloud regions with 124 zones around the world. We are also, and this is something that you may or may not know, we are also one of the most peered network in the world, substantially ahead of our peers in the hyperscale space. So what does all of this mean to you? What it really means for you is that if you connect to Google or Google Cloud, you get the most performance, the lowest latency, the most reliable network that you can expect. In this 25-plus years of building this network, we have encountered a set of pretty fundamental inflections. And as a result of those inflections, I think there were three distinct eras of networking that we have seen in Google. It all started with what I call the Internet era. This is when our singular focus was to deliver a homogeneous and predictable performance and latency across the world for anybody who was using products like Google Search, Gmail, photos, and other applications that you likely use every single day. In this era, we had to develop a set of innovations that were pretty groundbreaking in those times. Our internet-facing backbone, B2, the world's first SDN backbone, B4, the world's first SDN data center fabric, Jupiter, our bandwidth management system, bandwidth enforcer, and our SDN controller, Orion. Now, as time progressed with the advent of YouTube and other similar applications, came the streaming era. Streaming video became a very large part of the internet traffic. And by the way, that trend still continues. So to respond to that and to ensure that our users are getting very high-quality, low-jitter, low-latency video, we introduced another set of technologies. Those are the Google Global Cache, our SDN Edge called Espresso, the new transport protocols, things like Quick and TCP BBR, that ensured that the users can experience this video with great quality. The third large inflection was public cloud. Our cloud customers demanded a very high level of security, isolation of their data and their compute, as well as a market tendency. And to respond to those needs, we introduced another set of innovations. They were our new SDN Andromeda, the security protocol PSP, GRPC, as well as Swift for doing congestion control in this multi-tenant network. We are in the middle of the fourth large inflection in networking, and that is the AI era. You know, AI is a transformative technology. If you were in the keynote this morning, you have heard how people and companies around the world are using AI that are fundamentally transforming businesses and people around the world. At Google and Google Cloud, we have introduced the power of AI into every product that we have. And as a result, in the last 12 months itself, we have seen staggering increase in the compute and the applications that are used by these AI applications. Five times growth in training and inference compute capacity in the last 12 months. 8.5 times growth in AI developer APIs. And 15 times growth in Gen.AI agentic traffic that we support. Now, when you are serving some of the frontier models like Gemini, as well as supporting some of the very demanding fine-tuning and inference applications, it requires a lot of computing power, which is supported by TPUs and GPUs. But underlying this hyper-computer that we have built for AI applications is really the sheer scale of our network. And this AI era, just like the inflections that have come before this, they present a unique set of challenges that we must address, and the way that we are addressing them are with a completely new set of innovations that are built just for this AI era. So, you know, as I mentioned, we train these very large frontier models, like Gemini, in our infrastructure. When we train them, we not only train them using a large TPU cluster or a GPU cluster within a single data center, we train them across multiple data centers within a campus. We use multiple campuses across a metropolitan area. And in fact, we use multiple metropolitan areas to pull together a very large set of TPUs and GPUs that we then train these models on. Many of our AI unicorn customers are doing the same thing. They are not only using this multi-metro, multi-data center training infrastructure. Oftentimes, they are using their own data centers that they connect with hybrid connectivity to us. And in some cases, they are using GPU in other cloud. Now, in any of these applications, you know, there is a very distributed computing that is going on, and you must be thinking, well, distributed computing is not a new thing in enterprise applications. They have used it before. That is correct. But the way that the email applications use distributed computing is very different. When you have these very large frontier models that are doing billions of parameters that are getting updated together, they create this synchronous, spiky data flow that are very uncommon for a wide area network. That used to be very uncommon for a wide area network. You often saw them in data center fabric, but you never saw them in wide area networks. So what has happened really is that in this era, the wide area network is the new local area network, and frankly, the continent is your data center. And if you are sitting in there, and if you have architected one of these AI applications, you must have encountered the scenarios where the network gets in your way of designing the application that you really want to design, where you might be using inferencing on vertex AI, your data might be on a different data center, and because of the performance limitation of your network, you really can't build what you want to build. We see this with our internal applications as well. So it's a big challenge that we must address. Secondly, this AI foundational models, they run these intensive processes on these very expensive TPUs and GPUs. If you happen to have these processes interrupted because your network is not there, not only are you wasting cycles on these very expensive TPUs and GPUs, you are actually wasting the total amount of training time that you have allocated to this job, right? So it's very, very, very wasteful. So in the AI era, network just cannot disrupt applications that rely on the underlying infrastructure. Third, the AI model and the data that they're trained on, they must be protected in order to ensure integrity of these models. And as you all are probably aware, the whole regulatory landscape for AI is evolving, and increasingly there are more compliance requirements on sovereignty and data in transit. So that must be addressed with the underlying fabric that you build with. Last but not the least, our goal is to provide the world's best AI infrastructure in a very cost-effective way for all of you to innovate upon. And to get there, we cannot just do the traditional networking. We must introduce new ways of introducing efficiency and fault tolerance and operational excellence into this network to deliver all of this excellence. So when we take all of this together, what we realized is that we need to fundamentally re-architect our network. And we did exactly that. And it is done using four fundamental principles. Now, in this AI era, if you think about performance, security, and efficiency, you know, frankly, they're table stakes. Now, you know, our peer group in the industry are doing a lot of impressive work in improving those areas. But frankly, we have done a lot of work to get them right in the last three errors of networking. So in this era, we're looking at exponential scalability. Sorry, wrong button. There we go. Exponential scalability. The ability to handle massive amount of data, especially these very spiky elephant flows that I talked about when you're doing AI training application. Beyond nine, reliability. What do I mean by that? We have generally defined reliability as a percentage of uptime that you see every single year. Now, you know, and we call them 3949, 59, et cetera. The reality is that the average uptime really is not all that our customers want. They want extremely predictable and very fast mitigation time. They want predictable performance throughout their application running on this infrastructure. And they want very limited impact radius when outages happen. That's what we mean by going beyond nine. Intent-driven programmability. Our billions of users and customers, their unique requirements in terms of sovereignty, performance requirement, latency, so on and so forth. So we want to introduce a level of programmability in the network that goes to the finest grain of the performance parameter that the network supports. And we have done just that. Finally, the last decade has been full of buzzword around automation. Well, we have done a lot of work on automation over the last one decade. But we have been relentlessly focusing on building a network that is actually autonomous so that you don't have to worry about mitigations and restorations and also performance tuning. The network does it itself. So these are the four pillars that we're going to talk a little bit more about and explain how we're getting there. First, exponential scaling. Obviously, you want to scale your network with this data demand that you have. But what we realized is that it is not enough to scale the network in the traditional way where you introduce faster and bigger networking elements in the network. Instead, we took a page out of our data center fabric and we introduced what we call a multi-sharded network architecture. We have multiple shards. This could be 2, 4, 6, 8, 16, how many of our shards you need. When your demand goes up, especially imagine you have a region where you are now doing training of one of the foundation models. We can add new shards and therefore we not only are just scaling vertically by scaling within a shard, we're scaling horizontally. And in fact, that scaling is somewhat limitless and it's very, very elastic. That's what we really mean by when is the new LAN. And in reality, what has happened is that in the last five years, we have supported seven times increase. It's a staggering increase of bandwidth on this network by using this architecture. And this doesn't even account for this very spiky orders of magnitude increase in traffic that you see with this training flows, which is supported very gracefully because of this elasticity that we have in the scaling domain that we have. Second, if any of you here build, and I know we have some great networking binds in the audience as you build your network, you generally don't rely on a single provider to give you end-to-end network because you always worry about resiliency that you get. So you generally go to a multi-ISP, multi-carrier architecture, and it's actually complex. It's complex with failover, with load balancing, and so on and so forth. So wouldn't it be amazing if you could get that with the simplicity of cloud offering with single click? And you get that from Google and Google's partners that can build this infrastructure for you. That is exactly what we have done. So those multiple charts that I described, not only are they independent data plans, they're independent data control and management plans, and they have completely decoordinated operations. In other words, you're basically getting them exactly what you would get with multiple different ISPs. But you don't have to do anything in your applications to take advantage of them. All of these failovers happen magically between them. You might have seen a lot of the work that are happening, quite impressive work, in taking, you know, in many of our peer group and other hyperscalers, in taking data center fabric switches and using them in a wide area network. It's a good idea. We're doing it since 2012 in our B4 SDN, so we know it's a great idea. But we didn't stop there. In addition, we still do it. We actually use Jupyter fabric and the same components that goes into Jupyter fabric to build one of the shards that I've shown there. But we also introduced routers from four of the leading routing providers in the industry. And all of these routers, including our homegrown routers, they're 100% interoperable in data control and management plan. And they're controlled by the same SDN controller across the board, irrespective of whether they're commercial or homegrown, by using a set of open source technology that we have innovated over in the last decade. They're open config, GNMI, GNOI, GRIB, and P4. And that allows us to use all of these plans as completely interoperable but independent plans of network. Now, why did we go to this extent of doing this? Well, first, silicon diversity. In the current world, it is incredibly important to get supply chain continuity. And this network has three independent NPU families from three independent providers. But more importantly, software resiliency. Like any one of you who run a large network, our worst days are when a bug escapes testing and gets released through the production network and either brings the production network down or gets it pretty close to bringing the production network down. Well, in this network, that's generally going to be an annoying day but not a bad day. And the reason is the following. We would never release our software because it's a decorrelated operation. We never release our software in more than one shard. So if that has happened, the bug has gone into a shard, we'll simply take the shard down. And the traffic would magically fall back into the remaining shards. And it will happen in less than a second. And you wouldn't even know in your application level that something really major has happened. And while that is going on, we'll go and work on the shard by rolling back the software that we have released. And in order to make that possible, we have to introduce yet another innovation. We call it protective reroute. In protective reroute, we don't rely on routers to do the restoration of the traffic or the switching over of the traffic. It happens at the host. And all of that also has an introduction of regional isolation of the failure mode. So if you have a regional failure, you're only going to see a regional failure. You're not going to see any of the other regions being impacted. So all of this delivers you a level of technology that is just not possible with any of the other previous generation of technology. Finally, we have been investing into a highly programmable network for a long period of time. It is built on a stream controller with the APIs that I described. And it also has an universal network model that is built on a technology called multi-extraction layer topology or MALT. We took this programmability and took this to a very fine grain to implement things like I have a customer, which might be a British bank, because I have Colin who's going to join me. And let's say they rely on British telecom for their access to the enterprise network. And they have this need of this application. So they have branches in London and they have branches in North America. And they have this need for a very latency sensitive application. We can literally program that intent into a network that basically says that your flows are going to be limited to the two shortest latency paths between North America and Europe, even though we might have a dozen of them because of all the subsequent investment that we have done. You can do the same thing with sovereignty. You basically can say my data is never going to leave a sovereign part of the network and it's always going to be content within that sovereign area. So it gives you a level of capability that you just didn't have before. Last pillar, the one that I'm most excited about. How do we make AI better by using AI? We're doing that by using autonomous networking. Now, we have invested into automation in the network over the last decade plus. We started like everybody else with human driven. We've gone from that to workflow driven, to event driven, to machine driven. Machine driven is what we reached in 2017. That's when our network became fully intent driven. What that basically means is that at that point and beyond that point, any of the network changes are only done by changing the intent model and nothing happens. Nobody has access to any of the network elements. There is no changing config into network devices. It's only intent change. That's what I mean by machine driven. Since 2017, we have been relentlessly moving towards autonomy in the network and we have been heavily influenced by the work that Google DeepMind has been doing. One of those examples has been use of a deep neural network, it's called graph neural network, in predicting arrival time in Google Map that you all use every single day, I think. We found that there are lots of similarities between that and how we do traffic prediction in our network. So what we ended up doing is we used the same technology to build a digital twin of the network and we used the digital twin to predict performance, to do proactive restoration of the network, and do planning of the network where we get efficiency out of this model that are just not possible by traditional mechanism. So that was a lot. I've been talking for a while. You all must be thinking, great on slideware, show me the real thing and then I'll believe. So that's what I'm going to do. I'm going to hand this over to Shubhasree and Shubhasree is going to show us some of this technology right in action. Shubhasree, all yours. Thank you, Vikash. All right, everyone. It's late in the afternoon. I hope people are still awake. And if not, you're sipping your cup of coffee, but time to sit back, relax and enjoy some exciting demos. We are going to see two demos back to back. The first one is about reliability of our multi-shared network. Imagine you have a streaming video service running on our network. Let's say we are streaming today's event, GCP Next. And in the middle of that, we have a catastrophic global network failure. These are rare, but they're not unheard of. We have seen some in our network and we have read in news about other hyperscaler networks. We are going to see how this multi-shared network completely hides that outage from the user workload, which is the streaming video in this case. And how, with the help of protective readout, it protects it, protect the workload. And then, in the second demo, we are going to see autonomous network operations in action. We'll see how we can troubleshoot and root cause such an outage in order of minutes. We just get to trigger the mitigation and we get to go home on time. No late-night debugging. Sounds like fun, right? But before I get to the demo, I need to spend a few minutes explaining a key concept for this demo. It's the concept of protective readout. It's a fundamental shift in paradigm in how we think about network reliability. So, when there is a disruption in the network, can you imagine which entity gets to detect it first? It's the source. It's the host that's sending the packets. It gets to detect that the packets are actually not going through. It's not getting acknowledgement back. What if the host networking stack in the source could unilaterally fix the problem? It could just send it on a different path. And that's what happens in protective readout. The host networking stack just changes a few bits in the packet header, a few critical bits, that are used to compute a hash function that's used to pick one path among many viable paths through our network. As a result, in order of few packet RTT, packet round-trip time, which is of the order of a few milliseconds, we get to fix the problem. So, the workload won't even see an outage. And the shift can happen within the same network or to a different network if we have one. A salient point to note here, all this recovery happens in the order of few round-trip time. It's a function of round-trip time. And it's not a function of size of the network, how many nodes, how many links I have in the network. As Bikash just mentioned, our network continues to grow. But because of this technique, we should be able to recover the traffic still in fixed time, which is of the order of RTT. So, with that background, we are ready to jump into the demo. So, I'm going to show you side-by-side two scenarios. On the left, we have the classic network that looks like a single-shard network, the yellow shard. And on the right, we have the multi-shard network. We have four shards in this case. And we'll inject the same global failure in both cases. And we'll see how user experience differ in these two cases. Okay? Ready? Here you go. So, on the top, you can see the network diagram. There is exactly one streaming video, so one TCP flow. So, it's traversing on one path in the network. In the middle, we can see the streaming video. It's the next from last year. And at the bottom, you can see the traffic statistics. And you can see they're both identical. It's the same video. Now, in the middle of this, we are doing a software upgrade. We are upgrading the software in one of our key SDN controllers, which has an impact radius of one whole shard. What happens in this case? In production, we have a lot of protection, so that such a bad software would not go to the production. And a bad state would not transmit everywhere instantaneously. But we let it happen here. We remove all the protection. And we saw the bad software has taken over. On the left, the video is frozen. And packets are not going through, as you can see in the statistics. On the right, we didn't even see a glitch. As you can see, the traffic went to the other shard, the blue shard, because protective readout shifted it almost instantaneously. None of us noticed it. Did you? All right. Now, with that, we'll go to the next demo. We are going to see how autonomous network helps us. How do we troubleshoot this kind of an outage? It starts with an alert. The alert typically goes to our on-caller. We have a SRE. Some of you are probably familiar with NOC. Our on-caller friend gets a page which says something like, 100% traffic loss between A, X, and Y on shard 1. What do they do next in classic world? They would go sit in front of a computer and they would see through a whole bunch of data. They would read some playbooks. A lot would typically point to a playbook. And then the pointer would go to some set of dashboards. They would walk through those dashboards trying to find the needle in the haystack. Sometimes they were sufficient. Sometimes the playbook would point to other playbooks which would point to a lot more dashboards. And if the on-caller is not too expedient or if the problem is too complex, they pull in other experts. And then we all get in a war room and we all together analyze the situation and try to root cause it. It's a very, very stressful situation. If some of you have been in one or more of these, you would know. And to add to that, the car should ask for updates every 15 minutes. That's total distraction, right? Or even worse, you'd get in the war room and try suggesting mitigation ideas. Not a good idea. So after hours of debugging, we find a diagnosis. Someone figures out, oh, okay, there has been a software rollout going on. And it's correlated with the outage. Okay, we know what to do. We'll stop the rollout. We'll roll back the software. But by then, it's already hours. Sometimes even days if the issues are very complex. And then we'll see with the advent of autonomous operations, how things are getting better. We are in very early stage, but there is huge potential. We are starting to see something like this. Before our on-caller friend gets to their seat, gets in front of keyboard, our autonomous network operations bot, which is built on Gemini, have already started the troubleshooting exercise for us. It has access to all sorts of data. Real-time network telemetry, network topology intent. It has access to all the playbooks, all the past outages so that it can find some patterns, and network tools, of course. With all of that, it has already created a report with a hypothesis of the root cause. And it shows exactly the relevant data that there is a software rollout going on. So what does our on-caller friend need to do? They need to read the report. They need to ensure it makes sense. And if they are convinced that it's legitimate, they just need to trigger the mitigation that's suggested by the same report. And it's all done in order of minutes. So net-net, we see the troubleshooting experience going from order of hours or days to order of minutes, which is a win. We save a lot of engineering time, which can be used to build better AI infrastructure or deploy more of those. I'm sure you need that. You want that. And we still have human in the loop, which means it's still safe. We are not completely trusting the machines. Okay, so let me recap. We have gone through two demos. First one, we saw how our multi-shared network protects critical user workload in the face of major outages with the help of protective readout. And then we saw how autonomous network lets us troubleshoot such a massive outage in order of minutes. I hope you enjoyed the demo. With that, I'll turn the stage back to Vikash. Thank you. Thank you. Thank you very much, Subhasri. Honestly, I want nothing more than never have to go dull into a war room again. Let the agents take care of it. I'm very, very excited of where we're going with that. You know, really, all these innovations that we're doing, it's really for you all, our customers and partners. And just to talk about how one of our very key partners is leveraging some of these innovations that are brought in to build a combined offering that would delight our customers, it is my real pleasure to invite on stage, Colin Bannon. Thank you, Vikash. Hi, everybody. Still the clicker? Yes. Great. Colin, so welcome. Thank you. Welcome to Vegas. Welcome to Next. Many of our customers know British Telecom. But why don't we start with, why don't you familiarize our customers with your network and what we're doing together? Sure, absolutely. Well, BT, British Telecom, as you know, is one of the largest telecommunications companies around the world, serving primarily the Fortune 1000. But we're essentially serving many critical national infrastructures around the world with, whether it be 911 calls or air traffic controls or logistic systems or manufacturing companies or global banks. The hardest, toughest engineering, that's the sort of stuff that we love and we help delivering brilliant outcomes every day. And we've been doing that. In fact, we're the oldest telco in the world as well. But we're still trying to think like a startup. And you guys are inspiring us. So it's really exciting. And three things we're trying to do right now with this particular initiative, which is super exciting. One is to be the first and biggest and best managed service provider for Google Cloud One. So having that combination of being able to make it easier for a managed service offering. Two, to be, we're building like Fury right now, to be the biggest and best interconnection partner, both in your VPP program and also in the private peering as well with the infrastructure that we're building. Nestled, meshed. And there's a little bit more that we'll talk about that in a minute and some proof points. And three is, well, so what does a hyperscale telco network and a hyperscale cloud network look like together? What is the network as a service of the future? And what are the possibilities? And there's some really exciting things. You've built an abstracted path controller. We've done one as well, right? And, you know, if you think about telco networks before is they were really great at moving packets, but the core of our network, of telco networks, was a black box. And they were ignorant on all of those hops. You know, the path, the control plane, and the data plane were actually together. Yep. What we've done is, along the similar lines of what you've done, is abstracted the path controller and created an intent language that we can now start create and talk to each other and start to become transparent with each other and create incredible new opportunities of innovation together. So what you're really describing is that the interface is not just this blind data plane connectivity. We can exchange intelligence across with this API that we're doing. Yeah. And that ultimately benefits our common customers when we do this. 100%. So tell us more. Why would our customers care? Like, what do they get out of this thing? Obviously, very good technology. We're both technologists who are excited about it. Yeah. Why should our customers care? Well, we'll do four things. Maybe some out. So scale matters. You know, if I was listening to Ralph earlier, I think he's in the crowd there, a brilliant, knowledgeable person. He was talking about the subsea cable cuts on the west coast of Africa and the Red Sea. Scale matters in failure. You know, the bigger the mesh you have, you know, you talked about operational resilience. Scale super, super matters. So you've got your edge is about 202 pops or edge locations. What we're meshing and connecting into that is another 3,800 edge locations that filter and correlate and bring in. And where we're doing that, we're going to connect to 146 of those locations, which is more than anybody else in the world. So we are meshing together better than any other two networks in the world. And we're talking to each other at the intelligence layer, which opens up new opportunities. So scale matters because, of course, we're serving 198 countries. That allows us to do managed services, break fix, CPE, access networks in an incredible amount of scope. Your scale, our scale creates the biggest network in the world, which is really, really exciting. Speed is also super important for customers. Latency. Latency. By, you know, what we're creating here is that we can model across both of our networks to actually choose the fastest route. And that delivers great new outcomes for latency with all the new AI services, new really highly distributed workloads. You talked about the WAN is the new LAN. I absolutely believe that. You know, having fine-grained sort of east-west micro-segmentation control and latency is really important. Slow is the new down now. It's not just the hard down. It's the experiential side of things. And there is no plan B if the network is down. Most of the biggest enterprises in the world right now, if they don't have, I mean, MasterCard before, they used to have that little thing, you know, you put the piece of paper in and the card on it and you go, ch-ch-ch, around the world now, they don't have that. If it's down, you know, people are losing billions of pounds an hour without the network. The network is critical national infrastructure. And it's great that you're talking about this because, you know, to be fair, we've sat there with the hyperscalers for many years and said, don't worry about the network. And now you're saying the network actually matters, and I love that. And that's great that we've got this on there because it does matter. So latency is important and resilience, which I was talking about before. Being able to start to have new language, things like disjoint paths. So it's like the old Ghostbusters. You never cross the two lines at the same time. There's no node in that network, not just in the core with your shards, but in our core and in the access networks, where you have full diversity all the way through and also just can start doing the calculations of the lowest path, the lowest cost, but also start to deal with new things that we haven't thought about. Now that you have an intent language, you can start to bring intent into the core because we believe that, you know, before you had this sort of dumb core or dumb underlay, dumb pipe, and then SD-WAN that was trying to compensate for it. Now we're having the two talk to each other, and we think that you deliver better outcomes when you start to have intelligence. Imagine as if you had SD-WAN that controlled every hop, not just the first hop, which SD-WAN traditionally had before, where you were just flapping between two flavors of consumer-grade Internet. Now you have closed-loop automation and control around that, and this is where sovereignty comes in. Imagine where you want to start thinking about, in this geopolitical world, the Internet was great. It's a brilliant thing, but it's been there for a while. It was designed to actually survive a nuclear war, heaven forbid. But that organic rerouting is geographically ignorant. So when we were reinventing our platform called Global Fabric, which by the way is collapsing in not just a multi-service core but a multi-service edge, so we've abstracted the port and we have ELAN, ELINE, MPLS, Internet, IVPN services all in the same fabric. We don't really care what that services are. The multi-service edge, we can serve both private and public services. But having that, just think about the control and the language to say, well, actually, with data sovereignty, I don't want any of those packets to leave that country. That's hugely powerful. Or maybe there's a geopolitical issue where I don't want any of my packets to go in there, or maybe I have a class action suit or something like that. That fine-grained control, you can't solve that at the overlay level. You have to have a programmable underlay. And we've leaned in and ripped the band-aid off and redesigned the network. So if I hear you, you talked about all the four pillars that I talked about before. But I think from a customer's perspective, what you're describing is they're not buying a BT network and some cloud network services from Google anymore. What they're buying, they're going to you, and they're buying an end-to-end network fabric that uses your global fabric, our CloudWAN, uses the common intent language that you're describing, and can describe things like, I support, for example, in your case, a public sector entity. They have requirements for containing my data in transit within a geographical region. I don't have to worry when it leaves BT's network and goes to a cloud network or vice versa. I actually can have this end-to-end. That is, I think, amazingly powerful, and I think this is for the first time. It has never been possible before. Powerful, and an end-to-end SLA as well, which is awesome. That's right, yes. Wonderful. So I think moving forward, we talked about how the customers are going to appreciate this new offering that we're bringing to our joint customer base. What's next? Well, I just started sort of talking about this. So the build-out, let's just talk about the build-out. Scores on the doors. Up on the top right-hand side over here. So we're building like Fury right now. 146 locations to be the number one private peering partner. So that way, we're frictionless. So the ability to get onto Google and your networks, removing that. So private connectivity for those who want to have that. And also public, 104 locations as well, with immense build-out of edge capacity on that as well. If you think about our two networks overlaid with the deep access networks that we have and the fact that we're transparent, meshing things together with those two path controllers brings out incredible opportunities and incredible outcomes for our customers. And we're super excited about the future. It's amazing. I think we're just starting something that has such immense potential. Ultimately, it benefits our joint customer. And they get to have a network that is immensely programmable, highly resilient, exponentially scalable, and ready for the next generation of applications that they bring to the network. So thank you very much, Colleen, for all of your partnership and really appreciate where we're going in the future. Thank you so much. Appreciate it. Thanks for coming over. Thank you. Thank you. Thank you. Thank you. You know, as we have been talking about this, obviously you are thinking, how do I take advantage of all of this technology that Google is building and with partners like British Telecom that we want to bring to your customers? Well, we announced our CloudWAN both at the keynote today and then we have sessions tomorrow where we're going to go much deeper into what our CloudWAN is. So CloudWAN is really a fully managed Google-backed solution that unifies your private WAN, your software-defined WAN, and your secure edges required for your applications. So, you know, how do you use it? Like think of this as your enterprise network on a cloud wide-area network. It can connect your data centers, your campuses, your branches, your SaaS, your colos, your public clouds in a single unified WAN. Today, you know, earlier today, we announced a couple of key products in this portfolio. We announced Cross-Site Interconnect that allows you to use our global WAN for point-to-point connectivity with including extending that into partners like BT. 400-GIG interconnect, NCC Gateway, cloud-native SSC from multiple partners, Broadcom, Menlo, and PowerAlter Network, for example. We expanded our partnership with BT as well as Lumen and others with both becoming, they're both becoming verified peering partners and we're doing this deep integration of our network. So you can use this network as a single end-to-end resilient solution for all of you. And on top of that, we have a new billing option that is offering predictable monthly billing. So there is a deep dive session tomorrow and I encourage anyone who is interested and who want to take this for a spin, please attend and learn more about what you're doing there. So summarizing all of this, network is a key enabler of the AI era. It comes in the form of exponential scalability. We have scaled our network 7x or more by using a set of principles. You need beyond 9 reliability because you cannot afford downtime because of network. Neither do you want the network to be the blocker to your innovations in AI. We have increased our uptime by 93% by using this amazing technology called protective reroute. That is an order of magnitude improvement in overall uptime of the network. And finally, increased ROI and efficiency and performance by introducing AI and ML into network operations all the way from planning to mitigations to optimization. So it is a bold new era which is powered by AI and it is very real and you can experience it today as a Google Cloud customers and only in Google Cloud. No other cloud offers this. So please come and take it for a spin. Thank you very much. Really appreciate your time.