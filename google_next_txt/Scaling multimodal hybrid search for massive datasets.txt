 Hi, I'm Aron Lewis, Product Manager at Google Cloud, and today I'll be talking about scaling multimodal hybrid search for massive data sets. So let's talk about search. Why is search important? Search is fundamentally evolving. Large language models are now unlocking new ways for users to interact with technology, and users now expect AI-powered experiences that deeply understand their intent. As applications become increasingly multimodal, the need for fast, efficient data retrieval is critical. Vector search and embeddings are the foundational technologies enabling these intelligent experiences, allowing us to understand the meaning behind data, going far beyond simple keywords. Vertex-i vector search, which leverages years of cutting-edge research from Google, is built on Scan, the same approximate nearest neighbor search technology that powers massive scale Google services such as Google Search, Ads, Google Play, and YouTube. Our ongoing collaboration with the research team ensures we continue to deliver state-of-the-art performance and capabilities. Our generally available fully managed vector search is designed for any workload. It's consistently fast and scalable, finding nearest neighbors in milliseconds, even across billions of vectors. It's easy to use, fully managed with auto-scaling, and has native integrations with popular frameworks such as Llama Index and Langchain. Vertex-i vector search delivers high-quality results with high recall at scale. And it's incredibly cost-effective, up to four times more so than key competitors at scale. Essential features such as incremental indexing, filtering, result diversity, and hybrid search, combining vector and keyword techniques, are built in. And it's enterprise-ready with 99.9 SLA, VPCSC support, and more. Now let's talk about speed. These benchmarks show our P95 query latency across various standard datasets. Whether it's one million glove vectors or one billion big ANN vectors, we consistently achieve single-digit millisecond latency, while maintaining extremely high recall, often 96% or higher. Note that these numbers are measured at thousands of queries per second. For instance, on the big ANN 1 billion dataset, we achieve 99% recall with just 9.6 millisecond latency at almost 5,000 queries per second. This is performance at scale. Here's what's new with Vertex-i vector search. First, metadata storage. This is a big step forward. You'll be able to store arbitrary metadata, like descriptions, ratings, or other relevant fields, directly alongside vector embeddings within the index. This eliminates the need for a separate lookup step, or secondary database. Also coming soon is a direct BigQuery connector for Vertex-i vector search. This simplifies your data pipelines significantly. You can connect data directly from BigQuery for easier indexing, keep your index fresh with streaming updates, and get fast, scalable vector retrieval with minimal orchestration. We're also introducing storage-optimized vector search. This option is designed to be significantly more cost-effective, especially for deployments with very large datasets with lower traffic patterns. Let's talk about what one customer did with Vertex-i vector search. MercadoLibre, one of the largest e-commerce sites in the world, leverages Vertex-i vector search to serve their massive audience of tens of millions of users. They have been able to support their traffic and achieve fast response times at scale with Vertex-i vector search, while improving search quality significantly. We have built a demo that leverages the new agent developer kit and vector search to enable powerful agentic shopping experience. This was showcased at Cloud Next and will be published online soon. To get started, please follow the provided links for vector search and the agent developer kit. Thank you.