 . All right. Good afternoon and thank you for being here. Today, we'll explore unleashing the power of Apache Spark right here within BigQuery. My name's Kunal Vashi. I'm a senior outbound product manager here at Google Cloud. And I'll be joined today by Bhushan Mogul, senior product manager at Google Cloud, and James Callahan, head of core search and data engineering at Trivago. Let's take a quick look at our session today as we have an action-packed agenda. We'll start by talking about Apache Spark and the different ways you can run Spark at Google Cloud. We'll also take a look at some of the key feedback we've been collecting from our customers and see what they're really looking for. Next, we'll have a quick overview of the BigQuery Autonomous Data and AI platform and the ways in which it can be the ideal home for your Spark workloads. Then we'll jump right into Serverless Spark and we'll see a demo of Serverless Spark in action to show you how you too can unleash the power of Apache Spark right within BigQuery. After that, we'll hear from James to hear about how Trivago leverages SQL and Spark together on Google Cloud to bring you the best travel booking experience in the world. And finally, we'll go over the many new features and innovations we have in store for you. So, why Spark? Well, simply put, Apache Spark is one of the most popular large-scale data processing engines in the world. In fact, 80% of the Fortune 500 already uses it and it has over 2,000 contributors to its open source project. For many of our customers, it's often the go-to solution for a variety of use cases including big data processing, ETL, real-time analytics, data science, and machine learning, just to name a few. And here at Google Cloud, we believe in optionality. For maximum flexibility and control, you can DIY Spark on GKE. You configure your infrastructure environment and your services however you like, and you pay for whatever infrastructure you use without the headaches of actually having to manage that infrastructure. Alternatively, you can get a managed Spark experience with Dataproc. Here, you can still configure your clusters however you like, but you only pay for the time your clusters are active. You don't have to worry about managing Spark directly. Dataproc takes care of that for you. And lastly, we have Serverless Spark. With Serverless Spark, you simply write your Spark code and deploy, and Google Cloud takes care of the rest. No more cluster management. You get instant startup and dynamic auto-scaling all taken care of for you. And you only pay for what your jobs actually use. So building Spark applications, jobs, workflows, and pipelines has never been easier. Now, today, we really want to focus on Serverless Spark. This is what many of our customers prefer and want more of. We've seen this from some of our top customers, including Walmart, GMP, Wunderkin, SoundCloud, and Dun & Bradstreet, who've already leveraged this capability to power their businesses. In fact, we've seen that overall customer use of Serverless Spark has nearly doubled just in the last year. And that's at Google scale. And here's why. As we talk to our customers, it's becoming increasingly evident that it's all about delivering customer value. More than anything else, our customers care about delivering value to their customers. And it all centers around three main capabilities. Number one, speed. Customers want to deliver value quickly. They don't want to waste time grappling with low-value tasks like managing infrastructure or dealing with cluster management. They want to focus on building the best products for their customers. Number two, ease of use. Customers want optionality. Whether that means using Spark or SQL or both, or being able to access data in different formats or even in different locations. They want to be able to use the right tools for the right job. And not have to worry about being constrained or dealing with disparate tools that come with their own integration pain points just to get things to work. So in the end, they can easily build the best products, again, for their customers. And number three, confidence. Our customers want the confidence in their data platforms. They want to know that their data platforms work for them and not the other way around. And that's why, you know, capabilities like AI-powered assistance are so crucial. In the end, what our customers really want is an autonomous data platform so customers can focus on doing what they do best, delivering value to their customers. This brings us to BigQuery. BigQuery is Google Cloud's autonomous data and AI platform. It's built on the foundations of being unified, agentic, and flexible. Many themes you've probably already heard about this week. BigQuery already combines the best of data, analytics, and AI capabilities all in a single unified platform. So let's take a look to see how. BigQuery provides an AI-ready multimodal data foundation with unified governance and multi-engine support for SQL, Spark, and many other open source engines. With its multi-engine capabilities, its open and interoperable catalog and integrated UX that supports the needs of all data personas, BigQuery supports the full data to AI workflow. So what does this mean? It means no more multiple data copies, leading to lower costs. It means reduced complexity and increased interoperability. And ultimately, it means faster time to insight. So we thought to ourselves, how can we bring together all the benefits of BigQuery with the power of Apache Spark? And our answer? The ability to run serverless Spark directly within BigQuery, which is now available in preview. And here to tell us more about the benefits of serverless Spark in BigQuery, I'd like to introduce to you my good friend, Bhushan Mogul, Senior Product Manager at Google Cloud. Bhushan? Thanks, Funal. Hey, all. So I'm here to talk about what we've done over the past year in the area of Apache Spark and BigQuery together. If you all were here last year, you would have seen that we gave a sneak peek into some of the things that we were working on this area already. Well, we're proud to say that over the last year, we've had the privilege of working with some of our key customers to actually bring this thing to life now. So this is available in preview. And as you can see here, now if you open up your BigQuery Studio and you open a new notebook, you have the option of writing Spark code right in there. Again, no cluster management, no infrastructure management. This is serverless Spark like you're used to. One line of code, and you have a Spark session within less than a minute in most cases. It's open and flexible. You can bring your own Apache Spark code. No Google-specific things in there except that we've made it better, right? And then it's unified with BigQuery. So all the things that Funal was talking about in the area of security, governance, metadata management, all of that is unified under a single substrate so that you get that optionality that you are looking for, use the right tool for the right job. A little bit about how we do this, right? And it really boils down to data and metadata. As long as that substrate is uniform, universally accessible with a single API, you don't have to worry about it. You can get that optionality as well, right? So I wanted to focus on three things here. No matter if you're using SQL or you're using Spark, you go through a standard data access layer called the storage API, which gives you fine-grained access control. It gives you a row-level access control. It gives you consistent access management, policy management across the board. That gives you that uniform data access. One thing that we also like to indicate over here is going forward, all data access to BigQuery from Spark using the streaming read API that you use over here is included in the cost of Apache Spark, serverless Spark and Google Cloud. So no more extra line items in your bill as well. The second thing is in the area of unified metadata, and it's our universally accessible unified runtime meta store. What it contains is standard definitions of the location of your data, the schema, the formats. And this is consistent whether you're using BigQuery or you're using Spark. So you have some iceberg data located on GCS. You have a definition of that in the meta store. No matter if you access it from BigQuery or you're accessing it from Spark, you're using the same copy. No more data copies. No more data silos. And then the third piece of the puzzle is our unified governance solution, which houses all the business and technical metadata that your data stewards can use in order to get a data source. So discovery, that semantic layer, lineage, all that comes built in through that single layer across both the engines. And we've had some validation. Like I said, we've worked with several of our customers over the past year. Here is a quote from one of our customers, Trivago, who you will hear from today as well. The key here is customers don't care about product boundaries. They care about use cases. They want optionality. They want to use the right tool for the right job. And that's coming through all the way here. So enough talking. Let me show you how this all works. So this is a prerecorded demo. And I wanted you to imagine a situation where a company, an e-commerce company called The Look, has all their data, their product info, orders info, transactions, user data, all already housed within BigQuery. And now they've been using BigQuery for processing for the longest time. They've run into some use cases in which they'd rather use the value of and the innovation that's happening in Spark in the open source world. One example is, let's say they want to use open source libraries like scikit-learn, like XGBoost, that are open source compatible. Here I will show you how they can use all of these goodies, right, within BigQuery to predict whether a user is likely to purchase a product based on several features that we extract out of the data. So how this all starts is, in your BigQuery notebooks, you now get, when you open a notebook, a template to query using Spark. You can use that template to get some ready-to-use code. You can build on top of this code and iteratively develop your Apache Spark logic. For the purposes of this demo, I have created a notebook that we'll run through over here. The scenario is the one that I mentioned before. As you can see here, you only need a single line of code to get access to a serverless Spark session. That's this line of code right here. But for the purposes of this demo, since we're doing some ML work, let's imagine that you want to configure the Spark session. You have full control over that Spark session, so you can specify Spark properties if you'd like, and get access to that environment. Once the environment is ready, we also provide an on-demand monitoring solution for you, where it can show you all the jobs that have run on the situation, on that Spark environment, including the execution graphs for any SQL that you run. Now, I want you to also take a step back over here. If you were doing this in a classical Apache Spark way, you would have to specify or actually build out a persistent history cluster, maintain it on your own, and then ensure that it's up time. Ensure that it is up for the duration of your Spark jobs, right? None of that anymore. Always on monitoring solution for your Spark jobs. Let's keep going ahead. Once you have all of the monitoring done, the next step, you want to load data from BigQuery, right? This is where that uniform data access layer comes into the picture. You get access to optimized BigQuery and GCS connectors. Those connectors can be used to query that same read API that I was talking about earlier, and let's load that data that we were talking about. The next step in that journey is to explore the data. This is a typical model training, model development lifecycle, right? So we're doing some EDA over here. As you can see, there are some prompts over here. Those prompts are natural language prompts that are used to build PySpark code. So here is an example now of using some natural language intent to ask Gemini to generate some code for us. Taking a step back again, we're now announcing that this capability of PySpark code generation is now available in preview as well. So right in that BigQuery notebook, natural language intent, press enter, and then code should be generated for you. You can run it. It does not hallucinate. It has access to, it knows the schemas and the data sets that you have access to. So it doesn't hallucinate, right? It also can predict joint conditions. It can predict column types for projections. All of that sort of stuff. The next step is feature engineering. So we're typically, over here, we're taking some categorical features and extracting them in order to train our model. In this case, we're using things like location, gender, and other things. And then we perform our actual machine learning task. One other thing that I'd like you to take away from here is, at no point are you actually installing any packages. Serverless Spark now comes built in with standard Spark libraries for machine learning. So you don't have to spend cycles in doing this manually either. So typical steps here, train and test split, some scaling, followed by training using a logistic regression. And then you fit the pipeline, train the model, and then keep going. That's the trained model. Next step is evaluation. In this case, we are plotting an area under the curve. And then following that up with some visualization on how our model performs. So here is a precision recall curve. Most of this can also be done via PySpark code generation, which I spoke about earlier as well. We also have a confusion matrix over here, which will show that we predicted some excessive amount of false positives, in my opinion. So we may have to work on the training logic a little bit more. And then any additional plotting as well as to how our features were distributed and such. Now that's it as far as the development journey goes, right? But your journey doesn't stop there. If you just develop code, it's of no use until you actually deploy it and run it on a regular basis, right? If we don't stop over here, the next step in your journey is to collaborate with your teammates. And so you will see now that this also integrates with Git repositories, which you can easily configure in your, right inside the BigQuery Studio. You can see the Git repository there. It's connected to my GitHub account. And I can do things like code review, CI, CD, all of that, right inside the BigQuery Studio without ever leaving it. The other part of deploying is to be able to run this on a nightly basis. You have scheduling capabilities right in there. Just a few clicks, a schedule is set up. One other way of operationalizing this is also, usually notebooks don't exist in isolation. You split your work into several notebooks, plus some SQL queries, plus some data preparation things. And so all of that can be stitched together using what we call as BigQuery pipelines. As you can see over here, our notebook that we just coded up is the third step in this pipeline. You can easily build this and schedule this on an ongoing basis, right? So that's it as far as the demo went. And let me keep going. All right. I'd like to welcome James, who's been an awesome partner in this journey. And we'll hear from him on how Trivago's use these capabilities and how they will add value to their business. Thanks, James. James Callahan Okay. I got my wife to make this photo for me about two days after it was supposed to be submitted. If I had known that it was going to be so big on the screen, I would have tried a bit harder. I'm James Callahan. I work for Trivago. I've been with Trivago for about a year. For those of you who don't know who Trivago is, we're a hotel search site. So, you know, standard stuff. You search for London, search for the Coliseum. When you want to go, click search and you get your search details. I actually came from a Java engineering background. So I only sort of stepped into the data area about three months ago. And somebody said, do you want to give a talk about Google Cloud next? And I went, how hard can that be? And, yeah, I had to learn about Spark. What does Trivago do? So I said that we're a hotel search. And our mission is that when travelers are searching for a hotel, we want the obvious choice to be Trivago. We help travelers find the best place to stay, the best time to go, and the best deals to book. We're a meta search platform, which means that we don't actually hold any hotel inventory ourselves. We search other search sites. And if you get search results and you click on the view deal button, you'll get taken to the booking site, where you can actually make or not make the booking. Pretty much we do what you do when you go off and book a hotel. So you go and scan a whole load of different sites, and then you argue with your husband or wife about which hotel you're going to book. And then you go to the hotel site itself, and then there's some more arguments, and then finally she wins and you go there. And we do that, but we scan 400 different partners, and we process thousands of the price comparisons for your individual search. So we're constantly evolving our platform, and we run about 50 A-B tests at any one time. And you know how A-B tests work. You deploy things into production, and then you slowly iterate, slowly move your traffic from the old version to the new version, and see what happens. And so this is where the data starts coming in. So we process around 3 million searches a day. And we use that data to work out whether or not our changes have actually been useful at all. So we'll ask things like, do users like the feature? Which usually means they click on it or they look at it and they do something with it. Or maybe they hate the feature. We've actually implemented some changes that people have clicked on it and literally left the site. So that really sucked. And then most importantly is, does the feature help people make a booking decision? And that usually means they click on it, and then they're more likely to book at the other site. And sometimes, very disappointingly, we've got a great idea, we put it live, and nobody cares. Nobody looks at it. Nobody does anything. Along with the search data, yeah, we process 60 billion prices a day from 400 advertisers. About 2 terabytes of data a day. But there's also loads of other sources that we collect our data from. So we have to curate our content, for example. We're constantly getting updates to descriptions and images. And we're processing images through ML models to check for quality. We also gather a lot of reviews. And then we can use the review data from people who've actually stayed in those hotels to provide sentiment analysis. And we're actually now also providing AI highlights using the reviews that people have given to help people decide whether or not the hotel they're going to is what they're looking for. And we've got, yeah, about 100 data people at Trivargo. My job is basically to make sure that they've got the tools they need to answer the questions they get asked. And we'll talk about some of those questions in a minute. Let's talk about a use case. So this is just like basically a standard, really basic use case that we might want to pick up on. So we want to find hotels for our users where the prices drop by 20%. Because Trivargo is more than about comparing prices between different hotels and just giving you a sending or descending order. You know, it's like you might not want a budget hotel. You might want a spa hotel. So then if you just put the prices in order, you're not going to find what you're looking for. And finding customers' deals is why people will actually come back to Trivargo and use Trivargo. So for example, a room might normally be $200 a night when averaged over a 30-day period. But for whatever reason, next week it's $150. That's a good deal. You can book that and not tell husband or wife that you've saved $50 a night and then spend that money on a boys' or a girls' weekend. Although, I'm not sure how far $50 it gets you in Vegas. I was charged $47 for a sandwich the other day. So the challenges that we've got here are here in the scale. So it's two terabytes of new prices every day. Trends and exceptions for daily data for over 30 days. We just need to find those trends, but we need to extract the crap. There's loads of crap data that comes in that's just due to errors or anything. So we want to remove that data from our analysis. The solution to the problems I've mentioned? So the problems I've mentioned are easy, but they get hard when you do things at scale. And BigQuery enables our data scientists to work at the scale they need to. We use Colab notebooks so that, as Boushan was mentioning, we can flip between using SQL or Spark as they need to. So a lot of the time, SQL is enough. But sometimes then you realize that a programmatic solution is just better for what you're trying to do. And our Colab notebooks and just in BigQuery Studio, you can just mix those through. Boushan showed you that before. And what we've noticed is that people tend to try and use SQL more, especially in BigQuery, because it's just like super fast. But then when they wanted to move to Spark, then the Spark loads, they'd have to configure those Spark loads, they'd have to distribute those Spark loads, and they'd need to know about Spark to be able to use it. And now with this setup, they don't need to. And, yeah, we, so the engineers can, or scientists can actually spin up a runtime, a Spark runtime, that they can use themselves. And so, in the past, we actually maintained a Spark cluster ourselves on premise. And we had to worry about all the noisy neighbor problems and the configuring it and, like, all the problems that come with that. And it was sitting idle for 16 hours a day. So, much easier to use serverless. This is, let's talk about how our, that works. Let's talk about what our data scientists might do when they are trying to answer that 20% deal problem. So, do we have any deals that are actually 20% cheaper than their average? Like, if it's only 0.1% of the deals are actually 20% cheaper, what about 15%? What about 10%? Is 10% even a deal? Maybe not on a budget hotel that you'd use just on a work trip for one night, you might save six bucks. But if you're going on a spa retreat for a week, you know, with a bunch of you, that could be a good deal. And also, should we even be flagging deals to the users where you save, like, 20 bucks? Probably not. Probably not. So, BigQuery Studio gives us that platform that we need to rapidly explore that data. And, where is it? Yeah, and Colab gives us that ability to mix between SQL and Spark. I've mentioned that, mentioned that. Ah, we use, sometimes we just connect directly to the Vertex AR machine learning models, so that engineers can just leverage those straight out of it. And then, once a engineer or scientist has actually got to the point where they've got something that they actually think works, we can deploy that. And we actually deploy into Airflow sitting in Kubernetes. And then, we're going to do a lot of Kubernetes. But, that's like taking an idea for a feature from idea to production. But actually, this setup gives us a lot of other abilities. So, for the ad hoc analysis, which is something that I really like. So, people like me might ask the data scientists tricky questions like, if prices are cached for eight hours, by how much the prices differ between what we show the users and what they see when they hit the partner site? If the prices have changed between us caching them and the user actually going to the partner site, they'll see one price on Chivargo and another price on the partner site. And it's not okay for a user to see one price on our site and another price somewhere else. If it's only $1, they probably don't care very much. But if it's like $20 or $50, they may never come to Chivargo again. The other example I like to give is what is the volatility of prices as a function of time to travel in Berlin? So, I would expect like if I'm booking a hotel in Berlin in three months for the room not to change price very often. So, which means I can cash those prices for longer. I've got less load on our partners and customers. But if I'm booking a room for, you know, tonight, I would expect the prices to be changing quite rapidly and availabilities to be changing rapidly. So, I might want to know how often I need to go off and get new prices. In addition, we have, I might ask one of these questions, for example, the volatility question. And data scientists might just send me a graph or a chart straight away. And we might want to dig deeper into that. Or we might want to look at this data every day and see, you know, how things are changing on a daily basis. And they can just take the code that they've got and they can build dashboards with it or reports that go out on schedule and let people know what they need to know. Yeah, I've cherry-picked some of the quotes from data talents at Zhivago and honed in on these three. The first one really sums up what Bouchan and Kunal was saying. Our structured data is in BigQuery. We see SQL and Spark as two complementary ways of accessing and transforming it. I mean, this really, integration really does empower people to choose the right tool for the job. They use SQL for the basics like filtering, sorting, and joining. But in the same notebook, they can use Spark for when they really need a programmatic solution for a particular part of the analysis. And as a touch of my hat to AI, we've seen recently that people even not coming from a data background have started to build up their own notebooks using just natural language prompts to get them started. I did that recently. And I do not come from a data background. And I was able to get the data that I needed without having to pester other people and wait for them to get capacity to help me out. If I can do it, it's got to be easy. Yeah, so if I look at the two sides of our data sort of set up, we've got the data teams. They consist of the data scientists and the analysts that are doing all the cool stuff that I've been talking about. And now they don't need to think about how to bring in the dependencies or configure a Spark cluster. They can just focus on the business problem that they have been asked to solve. On the other side, we've got the platform team. The platform team consists of all the data engineers and the people that keep everything working. They used to be very busy in the past with keeping all the tools working in the data center. But they do much less of that now. A lot of that is a thing of the past. And we also know one of the great things is in terms of unified monitoring and logging. Before we started, before our GCP migration was complete, we had to look at error logs in two different places. We had to look in BigQuery and we'd have to check our Spark cluster. And checking the Spark cluster is a real pain. The engineers need to know quite a lot about Spark just before they can start looking at the errors that they were getting. We've actually built, finally on this slide, we've actually built tools so that we can show the engineer or the data scientists on a per query basis what their costs are. So we're building dashboards and we've built alerts so that if a data scientist or data analyst builds something that costs a few thousand, which I've seen just one run of a query cost a couple of thousand euros, a big red flag goes up and we can go and poke them and say, wait, what's that? Do you really need that? Do you really need that? Do you really need that? Do you really need that? Do you really need that? Do you need that? Do you need that? Yeah. And what comes next? So one of the great things about this integration is that our data scientists already had a lot of experience with Spark before we moved, before we were able to use this in BigQuery Studio. So they were able to leverage their existing skills to work with Spark. But as BigQuery adds further integrations, our data scientists can take advantage of the tools directly from the existing notebooks. When I talk to our data scientists, they say that they tell me that they want to use a lot more PyTorch and TensorFlow in the future. And we've got pre-configured environments to handle PyTorch in terms of flow. But until now, it's not been very easy to run these at scale. So we're super excited about running these loads in the same environment as the rest of the code is running. One thing that we really need to do is that we need to provide training and workshops for our data scientists. Because when I talk to a different group of people, some people know about all of these tools available to them and others don't. And so we need to help everybody to understand what tools they have, bring them up to speed. Okay, so I'll hand back over to Bhushan for a quick deep dive into the innovations we've built into Apache Spark. Thanks, James. That was fantastic. Yeah, always good to hear from customers, right? I can keep talking about Spark all day long, but you know, you wouldn't be here then. Cool. So we've spoken so far about how we've brought Apache Spark and BigQuery together. But I wanted to also stress that we are heavily invested at Google in actually making Spark better on its own as well. So let's take a look at some of the things that we've actually done specifically for Spark as well. One of our biggest complaints from our customers has been you do not have a first class development experience for Spark. I showed some of this during the demo, but now you have that notebook interface and we are continuing to innovate in this area. And this is not just, you know, here's how you can develop Spark code, but here's how you can develop Spark code. You can monitor it right in place. You can debug it in place. And we're continuing to innovate very, very heavily in this area. And again, not just open development, but also a few clicks, get integration, pipelines, schedules, all of that built right into this single pane of class, so to speak. The next thing, again, in that development journey, it's hard to get people up to speed with Spark, right? We have Gemini. Gemini is getting better day by day. Code generation is becoming unmatched in several areas, right? It already used to do things in Spark code generation pretty well. The problem was that it sort of hallucinated when it came to your data, right? So what we've done in this area is we've made Gemini context aware. So if you open up your BigQuery Studio and you start asking natural language questions, Gemini will actually know about your data, the data that you have access to, the data that you have access to via the BigQuery Metastore, via the Dataproc Metastore, and it will not hallucinate. The other thing that we've done in this area, I spoke a little bit about it, is it has the intelligence built in to also guess join conditions when joining two tables. It can guess column types for when you're generating projections code. And now this is available in preview. Shortly I'll share a link for how you can sign up as well. Next, James spoke a little bit about it, but you cannot get out of a session at Cloud Next 2025 without us talking about AI, right? So in Spark, now you do not ever have to, well, unless you bring your own packages, most of the popular open source packages that are compatible with Spark are built right into the pre-built Spark image. So no more installation. You have integrated MLOps via the vertex SDK all available as well. And on the area of packages, you don't have to worry about manually installing them in that development environment or distributing them on your Spark environment, right? But if you bring your own packages, we'll give you one function call, add your package in that function call, and automatically gets distributed. You don't have to worry about managing that Spark environment either. And then one thing that's coming soon, we're very close here, is GPU acceleration. We already have pre-installed NVIDIA libraries and drivers, but we're going to integrate all of this really soon. So stay tuned for an update on that front. One area that I really, really am proud of, of the team at Google, is the work that we've done on the performance side in Spark. It is all well, right, like innovating in AI and such, but if you cannot meet your SLAs, you can't make much progress. So I'm proud to say that on benchmark workloads, and we're comparing ourselves against Google, Spark on Google, one year ago, right? We are now 2.7 times faster than where we were a year ago. And we've even innovated in all layers of the Apache Spark stack. But I'd like to talk about three in particular. The first one, the first one being native query execution, which is our C++ based vectorized query execution, provides high performance, rips out the execution layer, and replaces it with our own C++ based execution layer. We've been working on this pretty aggressively with our customers. We've had a lot of validation in this area. We also ship a tool that you can run in your own environment, and it can tell you whether your workloads will benefit from something like this. So if you're interested in this, this is also generally available now. And then the other two areas are IO and query optimization. So I spoke a little bit about our BigQuery and GCS connectors. Lots and lots of new innovations in that area, help you with better IO, columnar shuffle, all of that good stuff there. And then the other area being query optimizations, where we've built innovations into several operators in Spark, made custom optimizer rules as well. Bloom filters, subquery fusion, advanced filtering, lots and lots of goodies here. Really, really proud of the team for the work that we've done in this area. All right. So, so far the journey's been great, right? This is all great. I can develop code, I can deploy it, but the real test, if you spoke, if you speak with any experienced Spark developer, they spend sleepless nights when things break, right? And that's when we want to also not leave your hands, right? We want to be there with you, help along. That's the crucial time, high pressure situations, SLA missing. I spoke a little bit about on-demand monitoring for Spark, and how you do not have to do any extra setup now for every single job. By default, you get a brand new, right there, Spark UI. If you know with Spark, you have several processes, executors, drivers, logs are scattered in like four different places per job. Metrics, same story. So now you get a central place where all of that information is aggregated and presented to you real-time as your job is running. This is available in GA, so you can try it out for any Spark job using serverless Spark today. Comprehensive, all kinds of logs in one central place, and metrics, and execution graphs, and DAGs, all the things that you need, typically for monitoring your Spark jobs. The next thing is how do we plug in more Gemini and make it sort of work for us, right? In this high pressure situation, your job's failed, or it's running low. You're running slow, rather. You're missing SLAs. Gemini Cloud Assist has been launched in preview. You're proudly one of the first partners with the Cloud Assist team in launching investigations for Apache Spark jobs that are fast or slow or failing, right? So if you can see in that GIF, you have an investigate button right next to every Spark job. Click it. Gemini has knowledge of typically what happens in Spark jobs. This knowledge has been built from support cases, internal playbooks that we have, internal bugs that we've seen, log aggregation, metrics, analysis. And all of that is used to surface actionable recommendations to you in a form of, like, top two or three things that you can try in order to fix your slow jobs. Again, available in private preview with that QR code. And look forward to feedback. We want to take this to general availability soon as well. Cool. So, we presented a sneak peek last year. We worked, like I said, with our customers to get to a point where now this is actually available in preview. That's the QR code. That's the QR code. This is specifically for Gemini and BQ, which gives you access to the PySpark code generation part. But it's no good to just be able to access, to generate code that cannot run, right? So it will give you access to that entire journey that we've seen today. So look forward to several sign-ups over here and working with you all as we build out the journey towards general availability. Here are some sessions that, that was our content for today in this area, but here are some sessions that you can also take a look at for all things Gemini, data analytics, Spark, BigQuery, all of that good stuff. Some of these may have happened, but you will share the recordings as well. And again, we, as always, welcome feedback as well. And we're ready, I think, for questions, if you have them. Oh. Looks like we're out of time, so we'll meet you in the back for questions. Thank you. Thank you.