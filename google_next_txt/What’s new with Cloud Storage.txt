 . Hello, everyone. Let's get started. Thank you. It's the last session of the day, so I know you must really love storage to be here. So it's great to have you all here. Thank you. Thank you. I'm super excited for the next 40, 45 minutes. We'll share all the new cool capabilities we've been working on in storage and cloud. Introduction. I am Samit Agarwal, VPN GM for storage in Google and Google Cloud. Thank you, Samit. Hi, everyone. I'm Asad Khan. I run the product management for the storage. Super excited to be here. We have so many exciting news and announcements to make. Actually, when we were putting these slides together, the struggle was what to include and what not to include. And me being a procrastinator, rather than focusing on the actual content, which I left it to Samit, I was surfing the web, trying to find the answer. How many are the topics we should cover before we will lose interest? So I found this interesting paper from George Miller, written in 1956, Harvard University, the magic number seven. And he talks about the human working memory and why seven is the magic number. So that's why we decided to go with seven topics that we will cover. And to make sure Mr. Miller continue to rest in peace, knowing that his thesis was correct, we will do a quiz at the end to make sure you grasp the seven things that we discussed today. So with that, storage has been front and center for every enterprise and every IT department. Sometimes it is very much in the face if you are building a very content centric application, like photo apps or YouTube and things like that. Many times it is in the background, powering every database in the world, ensuring that you can get to your data reliably. And one thing which Google was very lucky was that we had a head start. Even before cloud became a thing, we were working on Colossus, which powers all of the assets in Google, including Google.com, YouTube, Google Photos, anything you name it. And most recently, the Gemini as well. And some of the things that differentiated us has been the scale, the reliability, the durability, and the cost, especially the cost. Every meeting we have, they bring cost multiple times. So that has been a front and center for us. And you will be glad to know that today, Google Storage power some of the largest enterprise out there. And you can name any vertical, which, and you will find that they have Google Storage as the system, which ensures that their data is secure. And we have the enterprises, we have the startups, whether these are small startups, whether these are the unicorns, or these are the multi-building companies. So super exciting. But let's see where we are heading from here. So while all of you are continuing on this journey to the cloud and this transformation is happening, there was a phenomenon that happened in the last couple of years, which completely changed the landscape. And I'm not talking about Taylor Swift. We can talk about it, but that's a different matter. I'm talking about AI, of course. So AI has not only changed the way we consume things, the way applications are built, but actually changed how infrastructure is built, all the way from GPUs and TPUs to compute, to network, to storage. Especially for storage, what does AI want? More of everything. It wants more throughput, more IOPS, much lower latency. Everything has to happen parallel, reads, parallel reads, parallel writes. Storage has to be very closely co-located with compute. And of course, you still want all the regular things. You want your data to be safe, reliable, and consistent. So given this background and context, the journey to the cloud, the transformation that cloud is bringing, and AI, let's look at a few capabilities that we are bringing in. Let's start with AI and data lakes in particular. So the key thing for AI to work really well is speed and scale. Those are the two things that unlock the magic of AI. And we have been diligently working over the last year in continuing to increase our capabilities in high-performance storage. Both in cloud storage and in parallel file systems. And we have launched several new capabilities across both of these. We are not going to have a chance to go through all of them, but we'll pick a few highlights and dive deeper into them. So let's start off with rapid storage. If you caught the mention today in Thomas' keynote and you're wondering what is this rapid storage all about, let me give you a little bit of an insight. So at the simplest level, rapid storage is a cloud storage bucket. It's a zonal bucket, which has low latency, high throughput, and high QPS. So it's a really fast cloud storage system meant to accelerate your AI workloads. But saying rapid storage is faster than GCS is the same as saying Usain Bolt is as fast or faster than I am. Not saying a big deal. It's true, but it's not a big deal. He's not fast. He's much, much faster. Similarly, the stats on rapid storage are just insane. Less than a millisecond random read latency, no matter how small the IO is. So typically, small IOs is not a workload you want to run on object store. So you get block storage like latencies. This is 5x better than the closest competitor in any hyperscaler. You get insane amount of throughput. So you can get up to six terabytes per second of throughput and almost unlimited number of IOPS going against a single bucket. So what really powers rapid storage? So as Asad mentioned, underneath the covers, Google is built on a single distributed file system called Colossus, which powers all of Google. And in fact, most of our cloud storage products use Colossus underneath. This is the first time that we are bringing the power of Colossus directly in API form to our cloud customers. So you get to access Colossus in its raw form as a cloud customer. It's basically a bucket which is built off SSDs, placed as close to compute GPUs, TPUs, VMs as possible. So it's lowest latency. But it's deeply integrated in cloud storage. So most of the features of cloud storage that you're familiar with work seamlessly with this bucket. And we are storage people. We love graphs. So I put a couple of graphs just for the coolness factor. So these are read and write latencies for very small reads and writes. And you can see we can do reads and writes at half a millisecond reads and writes latencies. So truly exceptional. Switching gears. Rapid storage is definitely a very exciting announcement. But in the world of AI, the gold standard to do AI is still a parallel file system. And the gold standard for parallel file systems is Lustre. So you've probably all heard of Lustre. You're familiar with it. So as we started to think about how would we bring the capability of Lustre to the cloud, we think we thought about partners and the most obvious partner was DDN. For those of you who are not familiar with DDN, DDN has been company which has been working in HPC, AI, big data space for 20 years. They are the principal developer of Lustre, the open source software. So we are very excited to partner with DDN to bring this cloud service. To tell us more about this partnership and the managed service, I'm happy to bring James, SVP of DDN, to come and tell us more about this. Hello. Thanks a lot. Yeah, I'm James Coomer. I work for DDN. Who in the audience has heard of DDN? Let's have a look here. All the DDN people over here have heard of DDN. That's good. Well, I'm going to start off with a little bribe, by the way, before I start. If you find what I say interesting, we do have an event with Google and DDN in it. It's called Cloud and Cocktails. And there are some tickets over there with Xander. So with that, I've got maybe two minutes to explain not only what DDN does and why we're the best backers for this new cloud service, but also what managed Lustre is. Now, one more question for you to keep those arms moving. Who has heard of Lustre? I think that's about 10% of the audience, and most of them were from DDN. Okay. So Lustre is a scalable POSIX shared parallel file system. That means that you can mount it on one compute system, and we integrate with Google's Compute Engine, or GKE, or you can mount it on thousands. Right? So scalable in terms of number of clients. Scalable in terms of capacity, starting at just a few terabytes, and going initially to one petabyte, but rapidly expanding from there. Right now, we've proven performance up to a terabyte a second. So you can deploy a system very easily. It's a first party service. So you use your Google Cloud Console. You deploy a system really without any expertise, and the mount points appear on the clients, and you can get hundreds of gigabytes a second of throughput for reads and writes. Sub-millisecond latencies, millions of IOPS through a shared parallel file system. How many of this audience are running some kind of AI? One more set of hands, please. So this is a great solution for you guys, and it's really beautiful that we've got this new relationship together. So the Google Cloud Managed Luster Service allows you to, A, run extremely large-scale data analytics services. So running things like BigQuery or Spark, munging, cleaning, normalizing huge amounts of data. That's a great solution for Luster. We're going to minimize latencies, maximize throughput and IOPS. When you're training AI models, loading data, loading the models themselves into GPU memory, and then handling the checkpoints as those GPU counts get large, GPUs, TPUs, CPUs, and as the models themselves go from gigabytes to terabytes and beyond, and then the checkpoints go similarly up to terabytes, you can minimize the time spent on moving data and therefore maximize the productivity of your spend on the GPU infrastructure. So get more out in less time for less money using Luster. And then finally, when it comes to inference, typically whether we're talking about agentic AI or AI factories or running RAG inference models, we want to squeeze the latencies down. It's a latency challenge because each user experiences a pipeline. If we squeeze that pipeline, then they have a better experience and you get more value for your money. So we do that with Luster and you can scale and add concurrency. So you can have huge customer base running on this file system, experiencing very good latency. So that's my three minutes up. Thank you very much. So if you want to use this, you can go to your Google panel right now. You can spin up a system. If you want to talk to us, we'll be at this event later on. Otherwise, talk to your Google sales representative. Thanks very much. Thank you, James. Thank you, James. This next one is one of my favorite topics. You have heard a lot about how companies are coming and training their models or doing inferencing. But the question is also like what we, as the provider of the platform, using and how we are using AI to give a better service to you. So storage intelligence has that vision. And today it already, it's, I want to announce the GA of the capability, which is, we are bringing as part of the storage intelligence, but it has two parts to it. One, which I'm going to show you in a demo, what we have today. And then as part of the demo, we will also show you where we are heading with this product. So today we have customers who have billions of objects, petabyte and exabyte of data. And it eventually becomes a problem of how do I even manage all of that information? And how do I manage it from understanding of the data, from the cost perspective, from the security perspective? And it is both parts. How do I understand and how do I action it? So if I have an understanding of my data, now let's say I want to secure that data. I want to move that data. How do I do all those batch operations? And that is the goal that we want to go and deliver with storage intelligence. And let me show you a quick demo on that. So as I said, this is available today. So you can go to the Google console and you can look at the capabilities that it offers. More importantly, you can have a free trial. So all of you can go and try this product today. And once you go and enable the free trial, the one thing you will notice is that it will give you a full global view of all your enterprise data. This is not at the bucket level. This is not at a project level for your all enterprise data. You can look at how the data is distributed. You can look at all the metadata that you have applied. And you can also look at other capabilities that you want to surface in this dashboard. Now, the thing is that you can easily filter. For example, I want to go and see how much of my data is sitting in the US. But more importantly, think of it as like if I want to go and train my machine learning on the blueprints, I can go and click and see where is my data. And here I see that most of my data is in the US where my GPUs are. But some of this is outside. So now this is the insight I got. Not only that, you can go and with single click, you can do a bucket relocation. No code required. No app change required. We will ensure that we will keep the same name. But with that simple operation, you were able to move your data where your compute is. That is the power that this tool brings to you. Now, let me move forward where we want to go with it. In the previous one, you saw that I had my custom metadata. We want to change it into annotations that we can automatically generate and put it on top of your data. Now, imagine that you continue to add your data to the store. We will get the meaning out of those files with the right security settings. And with that one, you can do a lot more interesting stuff. And today we have the Gemini. Once we have the auto annotations, you can go to Gemini and ask very intelligent questions, which is that find all the factory images with safety hazard and ensure they are protected from accidental deletion. Now, if you think about it, this is a three-step process. I have to get the meaning out of your images that you uploaded. Filter it by the query that I put. Then I move it into a separate bucket. And then I put the right security settings on top of that bucket. Think of it. This is where we are going with the agentic part of the storage. And once you confirm that these are the right operations you want to go and perform, you click confirm, and then we will take care of all the three steps for you. This is the direction where we want to make sure the storage is more content aware and we can give you the meaning automatically. And we can do the operations which are multi-step. So, again, two things. One, the storage intelligence is GA today. You can go and try. And then the second part of the demo shows where we are heading with this product. Thank you, Asad. So, moving from the world of AI to analytics, the notion of data lakes is probably something you're all very familiar with. Almost everybody is now using some form of an open data lake with open table format. The architecture that has pretty much become an industry standard is where you store all your data in cloud storage in one of the many different open table formats, Delta Lake, Hodi, Iceberg. And then you can use your choice of compute engines, whether it's BigQuery or Spark or Trino, a variety of them. We have many successful customers running massive data lakes with this architecture. Snap is a social media platform you're all probably familiar with. They have hundreds of petabytes of data in cloud storage in open formats that they are running their data lake on. Uber, which was one of the premier companies running their data warehouse on Hadoop. They are doing a massive migration to Google Cloud using cloud storage as their data lake foundation. So, what capabilities are we adding for data lake customers? So, I want to talk about a new capability we talked about last, next. We just went GA called Anywhere Cache. So, let's take a look at Anywhere Cache. So, I'm old enough. You can see my gray hair. Who remembers the turbo button? Okay, there is a few people. So, turbo button was for the younger generation. We used to have these things called PCs. And the PCs had a button called a turbo button. You pressed it and it made things run faster. We didn't know why, but it made it faster. So, cloud storage now has a turbo button. You press it, it makes things faster. If you want to know why it makes it faster, I can give you more details. Otherwise, you just think of it as a button. So, basically, we would take the data that's stored in cloud storage, put it in an SSD-backed local cache. The cache would sit as close to the computer as possible. It's fully consistent. It's fully integrated into the APIs. It works with all the APIs that Google Cloud Storage has, so no changes to applications required. And you can, we have seen up to 70% reduction of latency, especially at tail. So, one of the things we see is at P99, P99.9, latencies are very slow. And that's where you see a big difference. And it's extremely efficient. You pay for exactly the bytes you store in the cache. And then the bytes are no longer cached. You don't pay for it. You don't pay for it. It lets you save a lot of cost on egress, especially when you are transiting data across regions. As I said, storage people love graphs. Here is another graph. When you look at the latency curves at the bottom, this is a latency of 2 megabyte reads, with and without cache. You notice, especially when you go towards the P90, P80, P99.9, latencies are very inconsistent with cloud storage. Anywhere cache makes the latencies much smoother. Now, when you think of a large query you are running on a data lake or a large pipeline, large jobs, these tail latencies can make your queries, pipelines run really slow. Which means more compute resources wasted, more frustrated data scientists and data engineers. And anywhere cache can let you make your performance much more predictable and stable. And if you are wondering, how would I know if anywhere cache is good for me? We have a recommendation engine built into the console. It tells you exactly if your data is cacheable or not. What is the right configuration to choose? How do you automatically scale the size of the cache? All of those. So you are making data driven decisions on whether the cache will be good for you or not. So rather than describe all of these, we will do a quick demo of what anywhere cache looks like. Yep. So as Sami said, this is the tower button, but let's give it a try. So what I have over here is I have a bucket on which I have enabled the anywhere cache. And next what I will do is that I will go and have the VM attached to that anywhere cache and the one on the standard store without any anywhere cache. And then we will do a side by side run. So for this one, what we will do is that I have a 10 gig of file. We will do a random seven megabyte read of this file. On the right side, we have the anywhere cache. On the left side, we have the standard one. And then we start the process of reading. As Sami said, the key thing to remember is anywhere cache is fast, simple and efficient. It is fast because it improves your latency and throughput. It is simple because you can just enable it whether you are in a single zone, dual region or multi region. If you are in a multi region, you can actually enable anywhere cache in multiple of the regions. You don't even have to pay. And that is where the efficient part comes. You only pay when the cache is actually being used. So on the right side, basically, if our claim is that you get a 40% improvement in the right times, and this one completed in 47 seconds, the left will continue to run and it should finish around 90 seconds. And that is the difference we are seeing. The other important thing is that we are already seeing that 74% of the customers who enabled it are getting their bills down on what they were spending on storage. This is not even including the money they save on the compute side because obviously the operations and the finish faster and they don't have to spend the same on the GPU. Just on the storage side, this is how much money they are seeing. So again, this is still continuing to go. Hopefully, any time this is finished. But that is the difference. Super easy button. You can go and enable and you will start to see the difference. So again, not only that, you can also go and look at the observability dashboard that we have made as part of the product. So you can actually see the difference. So we have the recommendation page, which will tell you when you should be using it. And then we have the observability that will give you full information when you get a cache hit and how many times. So with that one. Thanks, Asad. Always exciting storage demos where you do race on two things. So moving from AI and data lakes, while all of that is great, many of you are still in the journey of migration to the cloud. One of the things that is a very common pattern migration is you have a bunch of file data and you need to migrate file data. And we have a couple of solutions on how you can store file data in the cloud. The first one I want to talk about is Google Cloud NetApp volumes. Google Cloud and NetApp have been partners for many, many years. We have worked very, very closely together. Just last year, we launched Google Cloud NetApp volumes, which was a truly integrated first party service, which helped enterprises lift and shift their workloads very quickly to the cloud. Seamless migrations. Seamless migrations. It allowed you to optimize your cost. And you got the performance and security, which is the best of both worlds of Google Cloud and NetApp. This year, there are a couple of really cool, exciting new capabilities we are launching. The first is migration from on-prem to NetApp volumes in the cloud. So, anyone who administers NetApp is familiar with SnapMirror. And now SnapMirror works across on-prem and Google Cloud NetApp volumes. So, you want to migrate data from on-prem to cloud, you can just SnapMirror it over. No new tools, no third-party systems required. You can continue to use your NetApp volumes on-prem while the data is being migrated. So, it minimizes downtime. And we do a lot of work to make sure it's highly secure and very, very efficient. No conversation is complete without me adding AI into it. So, of course, once you move your data into Google Cloud, you have NetApp running in Google Cloud. You want to take advantage of all the services we have. So, we have a new connector which allows Vertex AI to directly work against NetApp. Rather than talk about it, we'll do a quick demo. Okay. So, again, as Samit said, that one of the reasons we are seeing a lot of customers choosing Google Cloud and NetApp is because it is not just about getting out of their data centers. It is about migrating, but also then modernizing and taking advantage of the capabilities which Vertex AI offers you. So, here we have a simple demo and it's a two-step thing. The first one is obviously the customer is using the SnapMirror to go and migrate. And that part is not shown as part of this demo. Once you have migrated, what I will go and do is that I will pick one of the volumes and then I will use it in my application. And that application I will fully build using the Vertex AI. So, this is the volume I'm going to go and use for my demo. From this one, the next step is I jump to the Vertex AI. If you scroll down, you will be able to see the new connector which is for the NetApp volumes. So, you go and pick this one. The next is you go and pick the location. And one thing you will notice is that this is unstructured and semi-structured data. But now, I can go and build embeddings on top of that which we will automatically generate and store it. The embeddings are all stored behind the scene. You don't have to go and worry about that. Next, I go and pick the type of the application I want to build. In this one, I'm building a chat application for my website. Once I do that, the data input is happening. This is the step where the embeddings are being generated and all the embeddings are stored in SpannerDB actually. Then, I pick the security settings. What is the authentication mechanism? I get the snippet of the code which I can put it as part of my web application. It is a simple application that we developed in India actually for the tiger population and how it is growing. And now, what you notice is that when I refreshed it, I have the chat experience on top of this. This shows that the import is complete which means the embeddings are fully now available to power my AI part of the chat experience. Here, I can ask simple questions. For example, the population of the tiger or anything more. Obviously, this is a very simple demo we are showing. But you can have a much more complicated interaction through the Gemini experience on the data which is sitting in your volume. So again, a very, very simple demo. But it shows as customers why you should migrate and why people are migrating and modernizing their applications. So the other way to store file data in cloud is Filestore. Filestore is our cloud native file system truly integrated into GKE, GCE, super simple to use, scales up and down. One of the consistent feedback we get about Filestore is while people love the Filestore service, they would like to customize and have more control over the level of performance and capacity they have. So we just launched the capability in Filestore where you can tailor Filestore to your desired level of performance and capacity. So as always, you can pick a capacity anywhere from 1 to 100 terabytes. Separately and independently, you can decide what level of performance you want. So you pick the level of IOPS you want all the way from 4,000 to 750,000. So you can go from an almost non-performance file system to a massive performance. And based on your capacity chosen and your performance chosen, your price differs. So you can optimize your TCO for a file system based on your individual needs. Not only that, once chosen, you can choose to scale up and down your capacity or your performance independently. So in case you decide you want more performance or more capacity, you can make that change. So this way you can truly optimize the cost of your file system for exactly your needs. So again, like so far what we have been talking about is a majority of the capabilities that we are bringing in the storage. This is a slightly different thing. This is how we are using the storage to give you the backup capabilities for your workload, which you might be running anywhere in the GCP. So what the promise that we are bringing and what is the challenge we often realize talking to the customer is they all are using either the primitive backups from the service or they are using some third party. The problem with that one is the lack of consistency and their ability to see a single view across all their workloads and ensure that are they properly backed up and are they secure. And that is the aim we have with the central backup service. So if you look on the top of this slide, you will see that the backup service gives you an integrated experience. What it means is that you have the same CLI, Terraform, the cloud API and the Google console. So one single console, one single set of APIs that you can work against. It gives you the central manageability and visibility. So no matter where you are running in the GCP, the intent is that the central backup service will give you that experience. And the last thing is that it gives you the resiliency and the security. And I will talk a little bit more about that in the next slide. And from the workload perspective, we already support the compute VMs, the VMware and the Cloud SQL, which is your own managed databases for both SQL and Oracle. And then we have a roadmap in terms of how we are expanding this backup capability to the other workloads that are coming. The other thing which I talk about, the security thing, is the ransomware is becoming a big thing. Where an external entity will come and either take over your data or delete your data. And what the backup wall provides you is defense against the ransomware. It ensures that even if you are the DBA, once you have set up the policy until when the data is backup and secured, nobody can go and touch that thing. So it gives you full immutability on the data which is in the backup wall. So the central management, the security and a single view is what we are seeing a lot of customers love and why they are using the central backup capability. On top of that, it is very important that you don't have to go to a third portal and try to figure out how to go and use it. As part of the product, we are ensuring that the experience is fully built in. So here is one example for the VM side where you can go and see that the data protection and the service is fully integrated as part of the portal experience. So again, if you have not given it a try, please go and look at it and you will be amazed at how we are thinking about the product end to end. So in closing, we talked about three things. First, we continue to innovate and push the boundaries of high performance storage, rapid store, anywhere cache, parallel file systems with managed luster. We want to make sure we are ready for the most challenging analytics and AI workloads that are out there. Secondly, we continue to take the AI capabilities and use that to build a smarter agentic storage, which can do work on your behalf. How do we make your life simpler? How do you make it simpler to manage storage and to get more value out of storage? And lastly, we want to make sure we are with you and helping you on your journey to modernize your infrastructure and migrate it to the cloud. Give you the right set of choices in terms of storage options and keep your data safe and protected. This was a world wheel tune of all the things we are doing in storage. We have deep dive sessions on all of these different topics. So if you want to learn more, please make sure you attend these sessions. They go into a lot more detail than I do. And your feedback is absolutely greatly appreciated. So thank you for attending. Thank you for attending.