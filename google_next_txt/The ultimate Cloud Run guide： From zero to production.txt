 Hello, everyone, and welcome to today's talk, the ultimate Cloud Run guide from zero to production. I'm Sarah Ford. I am a developer relations engineer on the Google Cloud Run team, and I'm joined by my teammate, Wietze. Yes, my name is Wietze Veenemann. I'm from the Netherlands, and I'm also part of the Cloud Run team here at Google. We put together this talk to introduce you to Cloud Run for those of you who are new to it. Cloud Run is Google Cloud's serverless engine. With Cloud Run, you can run any container on demand without any infrastructure management, no VMs or clusters to manage. In this talk, you will see an overview of Cloud Run, including the most common features to get your app up and running in production. To get started, Wietze will demonstrate an app he wrote and deployed to Cloud Run. It uses Gemini to generate short stories based on a selected emoji. This is secure. Yeah. This is not going to go how you think it's going to go. Like, you think, oh, he's showing a demo. No, you're going to see the demo. So go ahead, scan the QR code. You're going to try the demo for us. This is not a Rickroll, I promise. I see that one person there. Now we're going to scan this code. You see a whole list of emojis, and you can pick one, and that will be the protagonist of a very short fiction tale generated by Gemini, right? So maybe we can switch to the demo. Yeah, so, yeah, I see most of the phones have gone down. So, yeah, so let's see some emoji tales. Can we switch to the demo line? Thank you. So this is the website. For those of you who didn't open the app yet, let's here select an emoji, and I will be your hero, right? So let's try the juggler. Oh, this is waiting, the suspension. I'll go to the web console first, see what's happening there. So this is my service in Cloud Run, and I can see all the metrics, right? The request count, request latencies. Are the stories good? They're like, this is a corporate event. It's not supposed to be funny. Let's go back to the juggler, right? So Gemini is generating my story, right? So the juggler bowed. His silk costume shimmering under the lights. He dropped the pin. Wow. He caught it behind his back. Oh, that's nice. And the crowd roared. And the crowd roared. I'm not hearing them. Nice, I like this. This is going to be fun. A bit more participation. Yeah, that's good. Yeah, so this is the real app, right? We can see, like, in a few minutes, like, the metrics will show up, and we'll see the amazing request count here. And it's also fun if you go to the object, right? So I wonder what the story of the, I don't know, the tooth is. Oh, it's really struggling. Maybe I should have increased the quota. Yeah. Oh, there you go. Fluoride rinse left him stronger, ready for whatever came next. Okay, let's go back to the slides. Yeah. So if this talk ends up being very boring, at least you have something to look at and try. So I wrote my app in Python, right? I'm more of a Go developer myself originally, but Python is my destiny now. And my app runs in a container on Cloud Run, and it calls the Gemini API on Vertex AI to prompt for those stories, right? The prompt is literally write me a short fiction story, and it needs to end well for the protagonist. Try the poop emoji. It's fun. Okay. And as you can see, the stories are then streamed back to the browser using server sent events or SSE, right? So Cloud Run supports streaming responses. And this demo used a Cloud Run service. As a developer, you deploy a container image, and you get back an HTTPS endpoint. So let's go back to the demo for Vita to do a second demo. More demos. That's nice. Okay. So deploying a container image. So I go back to Cloud Run in the web console. I go to deploy container. That's right there. I'll choose a service. There are two resource types in Cloud Run services and jobs. Sarah's going to tell more about it later. So I'll choose service. And I will pick a container image that everyone knows. It's a web server, Nginx. And this is a container image in the Docker Hub, right? So it deploys any container image. Service name, Nginx, region, use center one. I'll say it's a public endpoint, right? So I will not enable IAM for authentication. There's two types of pricing models. We'll also dive into that later. Autoscaling configuration. And I can configure the port I need to send to, the Cloud Run needs to send the requests to. So for Nginx, it's going to be port 80. I'll click create. And now Cloud Run will pull my image in the background, import it into Cloud Run, and create the Cloud Run service for me. And it will start one container instance, ping the port, see if it's ready to receive requests, and then when it's ready, this will go green. I will show you a working Nginx. Unless I made an error, that would be very exciting. You know how it goes with live demos, right? I have a green checkbox. Let's see if it worked. Welcome to Nginx. Yeah, there you go. I mean, this is where I was impressed, right? This is what sold me on Cloud Run back in 2019, because it's so easy to deploy a container image and get it running. Like, it's as easy as Docker Run. Like, I used to, in a previous life... No, not a previous life. In my... Previous dev life. A previous developer life. This used to take a day, right? Configuring Nginx, auto-scaling, low balancer, HTTPS endpoint, and here, it was 30 seconds. I really liked that. So let's go back to the slides to add more depth. With Cloud Run, you have four deployment options. First, if you've already containerized your code, you can deploy your container image directly to Cloud Run. But if your code is in GitHub, you can set up a CICD and set up and deploy from there. The third way is, let's say you just have your source code, and maybe you don't have a Docker file. Cloud Run uses build packs to automatically build a container image from your source. And you'll learn more about build packs later. And the fourth type of deployment option is called a function. Cloud Functions is now Cloud Run Functions, and I will be demoing Cloud Run Functions tomorrow morning at 8.30 a.m. if you would like to learn more then. There are two types of resources in Cloud Run, services and jobs. A Cloud Run service follows purely request-driven scaling. Instances within the revision are scaled out behind a Google-managed low balancer. It provides out-of-the-box URLs and HTTPS. The built-in traffic splitting and gradual rollouts lets you reliably deploy new versions of your app without any disruptions or SLO breaches. A Cloud Run service has multiple trigger sources like HTTP2, GRPC, events, and WebSockets. A Cloud Run service provides multiple pricing options, such as pay-per-request or pay-per-instance-lifetime, which you'll hear about more later. The other type of resource type is a Cloud Run job. Cloud Run jobs are containers that run to completion. They are not triggered by HTTP requests. If you have a really large job, you can split it up and run it on multiple containers in parallel. Cloud Run jobs can be executed manually or automatically on a schedule, and you only pay while the job is being executed. Every time we add a region to Google Cloud, Cloud Run is available from the start. And with that, let's go back to the demo for another... That's me. So what am I going to do? Yes. I'm going to show how to change an app using the source deploying, right? So I have another service. It's really great. It's called Color. I will show you a page with a background and a text that says what the color of the background is. So let's open it up. It's a red background, and the text says this page has a blue background. So that's clearly not correct, right? We should fix this. You should. I'll do that right now. Okay, so this is a source-deployed service. So that means if I go to the source tab here, I can actually see my source code, like just how it was in App Engine in the before days, right? So it's a very simple service. It has an index.js file and a Docker file. So I'm able to edit this source and create a new deployment right here, but I will go to my local editor on my local host and make that change. So here, same Docker file. It's very simple. Just an index.js file. No dependencies at all. I can fix my bug. So my background color is going to be blue. I'll go to my console now. You know what you should also do? Thank you. You're welcome. I forgot to save the file. And that would have been very embarrassing, right? Because we would have been waiting on the deploy to succeed, and then the bug would still be there. Thank you, Sarah. Like, this is why we're a great team. Gcloud run deploy. No. Okay. Gcloud run deploy. Typing becomes so hard if everyone is watching you. Gcloud run deploy. There you go. It will ask me to confirm that I want to deploy this directory and to this service name. And while that's running, let's start. Yeah, while that's running, let's go back to the slides real quick. Let's break down what that gcloud run deploy command does for you. First, it uploads sources to a Google Cloud Storage bucket. Then it kicks off a build in Cloud Build. Now, note, if you don't have a Docker file, then BuildPacks is used to turn your source code into a container image for you. And BuildPacks is the open source version of the technology that powered App Engine since 2008. And again, you don't have to run these commands. This is done for you by the gcloud run deploy command. Once the image is built, it is pushed to Artifact Registry, which is Google Cloud's Container Registry service. Then Cloud Run creates a new revision, importing the full image. You'll learn more about revisions on the next slide. And lastly, 100% of the traffic is automatically migrated to the new revision as soon as that revision is healthy. What do we mean by a revision? A revision is an immutable or read-only copy of your container image and settings. Every time you make a change to the Cloud Run service, it creates a new revision. Cloud Run has support for configuring traffic migrations, allowing you to do rollbacks or other traffic migrations. And Cloud Run automatically pings the revision on the default port to verify that it is healthy or ready to go within a certain time frame. This is called a startup probe, and you can configure this startup probe as well. In the next demo, you'll see how you can use traffic migrations to configure a rollback. Let's switch to the demo. Yeah, let's do that. Okay, so my deploy was successful, right? So if I go back to my web console now, I should be able to see this new revision. Yes, one minute ago, and if I now open this link again, everything is correct. Good job. This is a really nice audience. Yes, they're super supportive. I like that. All right, so the page has a blue background. But let's say maybe I made a bug. I introduced a bug with this revision. I want to quickly roll back to the previous revision, right? So one thing I could do is I could go back to my source, make a change or do a git revert, and then trigger a new build. But, hey, I might introduce new errors that way, right? So a safer way is to do a rollback. So I'll go to my list of revisions in my service, click on the Manage Traffic button, and then select number 21, right? Number 22 is the most current, healthy revision. Number 21 was the previous one. I'll set traffic to 100%. 1,000, that's a bit too much. Let's not do that. And I'll click save. And Club Run will now roll back to the previous state. Oh, I heard someone. I heard it. I did it go. Back to the slides? Or? Let's go back to the slides. Git revert. Yeah, git reset dash dash hard. Yeah. All right. I heard one laugh. OK. So now let's move on about to adding more Google Cloud services to your application. The way you want to do this is by using the client libraries. It's not only convenient to use a client library, but you also do not have to manage API keys. You just initialize the client libraries, as shown here, and authentication is done for you using a strategy called application default credentials, or ADC. The Google client libraries automatically use ADC to find credentials based on the environment the application is currently running in. Then ADC uses credentials to authenticate to Google APIs. Let's say you have a Cloud Run service running in production. You set a service account as the identity of a Cloud Run service and grant the service account permissions to use Google Cloud APIs. The client libraries automatically use that Cloud Run service account for authentication. And here is how it would work on local hosts. First, you authenticate using gcloud auth login, and then you will run gcloud auth login application dash default, which stores your user credentials. The client libraries can now find the user credentials and use them to authenticate to Google Cloud APIs. Your take-home message is when you use Google Cloud's client libraries, authentication to Google APIs happens automatically for you. So what about services that are in your VPC? Perhaps you have a Cloud SQL instance that only has an internal IP address, or you have a memory store instance. You can use direct VPC egress for that. No need to create additional resources. Just check the checkbox to enable direct VPC egress, and you can connect to internal IP addresses. Direct VPC egress gives a Cloud Run instance its own internal IP address on the VPC. But it's for outbound connections or egress only. Inbound connections or ingress to a Cloud Run service always goes over the URL endpoint. Now, what if you have an app that has shared data and all instances need to be able to access it? You can use volume mounts to make a Google Cloud storage bucket or a file server appear as a local directory on every instance. Although client libraries are great for avoiding downloading some API keys, there are times when you do need API keys. Secret Manager is a secure and convenient storage system for API keys, passwords, certificates, and other sensitive data. It's really easy to use, which VPC will show you now. So let's switch to the demo. Well, easy to use. That's for them to judge, right? What? Easy to use. That's for them to judge, right? I'll demonstrate it, and you'll tell me if it's easy. I think it's easy. Fair enough, fair enough. Hey, but, Sarah, my Emoji Tales app doesn't even have a secret because I'm using the Google Cloud client libraries. So I'll just pretend that it does have a secret. So I'll go to my Emoji Tales app. I click Edit and deploy a new revision. And then scroll down to, where is it, Variables and Secrets. It's right there. And then this has an option to expose a secret as an environment variable. So there's three different ways how you can expose secrets on Cloud Run using Secret Manager. So this is the first option. You link the secret. You create a secret in Secret Manager. You link it to the Cloud Run service, and you expose it as an environment variable. That's option one. Option two is you mount the secret as a file. So in that way, you have a file that appears on your local file system, and it will always be the latest version of this secret. So you read from your application code. You would read the file, and you get the secret value. The third option is to ignore all of this together and use the client libraries for Secret Manager and get the secrets from there. So let's add one. Reference the secret. So I'll give it a name. So this is going to be the name of the environment variable. Let's call it database user. No, secrets. Secret. And then I will create a new secret. I will give it a name. Secret. That's a good name. My precious. Okay. My precious. If you have better suggestions, I'd like to hear them. That's okay. I'll create the secret now. Good. And I will always have... So secrets are versioned. I'll select it to always have the latest version. So whenever I deploy a new revision, because this is an environment variable, and you can only step them when you deploy the revision, it will always pull the latest version of the secret when you deploy a new revision. If you wanted to update continuously, use the volume mount. Now, I'm going to deploy this, and the deployment will fail. And why does it fail? Because the service identity, right? So there's a service account linked to this Club Run service, which is the service identity of the Club Run service. It doesn't have the permissions to access this secret, right? So that's what this message also says. The service account I'm using, I'll copy it right now because that's convenient for my next step, must be granted the secret manager accessor role. And then it will work, right? So let's go to secret manager and do just that. Did I copy it? Let's select it again. Secrets manager. By the way, if you're using the console a lot, there's this shortcut. You do slash, and then it will focus the search bar. It's super convenient. You can also type in the name of a Club Run service and go directly to... Service place? Yeah. I never tried that. Let's try that when I go back. Okay, so I'll select my secret, and I will click on permissions and then grant access to the service identity of my Club Run service and give it the role secret manager, secret accessor. I'll click save. Okay, so I'm going to use your trick now. We'll do slash and then emoji tails. It takes tails. Club Run service. Oh, that's convenient. Yeah. Thank you. Okay. You learn something new every day. Even on stage. We should do this more often. Yeah, we should. Okay, so edit and deploy new revision because I want to trigger a new deploy. I'll click deploy. And if I did everything right, it will now be successful. And my container or the process in my container will have access to an environment variable with the value of my secret, my precious. And viewer allows you to view the secret as it exists in the Cloud Console, but accessor allows you to actually see the contents of it. Yes. Yeah. In case I give it viewer and not accessor and you're wondering why you can't access it, that's why. It works. Yeah, it works. Yeah. Yeah, I think. Yeah. We've trained them well. This is nice. All right. So, yeah. Yeah, go back to the slides. Yeah, let's go back to the slides. Now, we're done building our app. Let's discuss what you need to know in production. For example, public domains, auto-scaling, and pricing. First is public domains. By default, you get two run.app domain names. The first one showed is a new deterministic URL, and the second one shown is the older URL that uses a project hash value. Then if you want to use features such as multi-regional load balancing, manage SSL certificates, or enable Cloud Armor, or set up a CDN. You'll want to create a global external application load balancer. And lastly, we have in public preview a custom domain mapping feature. Now, if you're serving production traffic, you might not want to send customers to the newest revision immediately. And that's where preview links comes in. So let's return to the demo for preview. Okay. Let's do that. So we're going to go back to my color app, right, because I have two versions of it running now. So there are... I go to the revisions tab. There are two ways to set the traffic, right? So either... And that's the mode I'm in now. I'm manually selecting which revision my production traffic goes to, right? So it's not going to the latest every time. It's now only going to this revision, the number 21. So that's the wrong version, right? The red one. Now, even if I deploy a new revision right now, the traffic routing will not change. It will still go to this 21 version. But... And that's... I like that for a production app, right? Because I want to maybe... I want to push a new revision with my latest changes, and I want to check if it works all right, before sending production traffic. So for that, I can create a revision tag. So let's do that. I'll click Add, and there's here an option for revision tags for the latest revision. So I'll add a tag right there. And what this will do, it will create a unique URL that will always point to the latest revision. There it is. Revision tag, always pointing to the latest. Even if I deploy a new revision, it will still point to the latest. And this is the blue version, right? So now I can preview my changes, and once I think they're right, I'll go to Manage Traffic. Like, I'm demoing this in the Web Console, but you can do this using Terraform and the CLI, whatever you prefer. But this just demos nicer. So I like my newest revision, so I'll manually migrate now to this latest version. So I'm 100% of traffic. I click Save. And now, also, my production traffic goes to the blue side. Well, that's preview links. Let's go back to the slides. They like that one. Thank you. All right, to recap what you just saw, there are two steps to using preview links. First, manually pin production traffic to a specific revision, and second, create a traffic tag pointing to the latest revision. Now, let's talk about autoscaling. Let's say the first request comes in. It could be the very first request ever or the first request after a long time. Cloud Run automatically spins up a container instance to respond to that incoming request. And as traffic comes and goes, Cloud Run will automatically adjust the number of instances needed. Cloud Run will automatically take down an instance when it has not received any traffic in a while, which is called scale to zero. Now, if you want to prevent scale to zero, you can use minimum instances to keep a given number of instances warm or ready to go. And while autoscaling is great, you may want to limit the number of instances that Cloud Run can autoscale to in order to limit costs. So far, we've only discussed having a public front-end website. But let's say you want to have a private back-end service that's not accessible from the public internet. You have two options. Option one is you require IAM authentication on the back-end service and then grant the Cloud Run invoker role for the public front-end service account to access the back-end service. The request from the front-end to the back-end adds an ID token for the back-end service to verify the IAM role. This is what the configuration of the back-end service looks like in the Cloud Console. You'll choose to use Cloud IAM to authenticate incoming requests and select do not allow unauthenticated requests. Or you can use option two. You can configure the public front-end to only send outbound or egress traffic through the VPC network. And the private back-end is configured to only allow internal traffic from the VPC. And here's the configuration for option two for the back-end service in the Cloud Console. With that, I'm going to turn it back over slides now to Vita. This is nice. Are you now going to do the demos? OK, so let's take a look at Cloud Run's pricing models. There are two modes, right? There's request-based pricing and instance-based pricing. So let's start with request-based pricing, which is the default if you deploy a new Cloud Run service. This is you pay for container instance time, right? So you allocate CPU and memory to a Cloud Run service. It starts containers. You pay for every container. That's how it usually works. However, on Cloud Run, there is no charge for idle instances, right? So if you have a container instance and it's not receiving any requests, that's what idle means on Cloud Run, you're not charged for it. So that's great. Very cost-efficient. What we do when an instance is not handling requests, we will slow down the CPU by a lot, right? So because otherwise we'll be giving out free CPU cycles. It would be stupid. So and also additionally, there is a fee per one million requests. What's interesting about this pricing model is that those minimum instances, right? So if you want to prevent a scale to zero, you configure minimum instances. So Cloud Run keeps that number of instances always running. If they are idle, not receiving requests, they are charged a lot less than the full rate. So for the exact details, refer to the pricing page. Okay. So there's another model, instance-based pricing. So then you also pay for container instance time, similar to request-based. However, now you also pay the full rate for idle instances. However, this price is less than when compared with request-based. There's no per-request fee, which is attractive if you have, like, a high-traffic, high-volume service. But also idle instances and minimum instances are charged at the full rate. And we guarantee that idle instances are shut down after a maximum of 15 minutes. Now, if you think, look at this explanation, and you think, well, that's pretty complicated, I agree. I agree. And we have this feature, cost recommendations. So because it's hard to figure out what cost model is going to be best for your service up front. So if you're using request-based pricing, we'll look at your usage, or Cloud Run looks at your usage, and it will recommend to enable instance-based pricing if it thinks that it's cheaper, and it will also make a prediction of your savings. Now, while we're getting to the end of this presentation, I want to call out that Cloud Run not only supports CPU, but also GPU-enabled services. So you can enable the GPU, and then you have an NVIDIA L4 with 24 gigabytes of VRAM for every instance. And it also scales to zero. So if you want to play around with running your own LLM inference server, this is really great to use. I want to call out that Sarah has created these three code labs that will help you put in practice what we've discussed in this session today. It's not the exact emoji tail, but it uses all of the components that Beats put together. It felt like less code is better when learning a new concept, so it's kind of like the minimum pieces to get you up and running. It's basically a chatbot UI that I've used instead of the exact emoji tails. Cool. They're really good. You should try them. And then finally, please complete, I know you've been getting a lot of these today, but please complete the survey because it will help us learn what to improve, and you can also tell us what you wish we would have covered. So with that, thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Bye-bye. Bye. Bye-bye. Bye-bye. Bye-bye. Bye-bye. Bye-bye. Bye-bye. Thank you.