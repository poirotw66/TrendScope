 Good afternoon everybody. All right, let's get started. How's everybody doing? I realize I'm sitting in between you and your favorite beverage. So, we will try and make this thing fun and entertaining. We are super excited and thrilled to be out here and talk about ways we can really improve the productivity for developers. Today we are going to be talking about leveraging AI agents to boost your code coverage with a specific focus on unit testing. So, really excited to have you on board. We have a team of folks today to entertain you folks and make your lives easier. I am Prithpal. I am a product manager within the Gemini Code Assist team at Google Cloud. And going to be joining me soon on the stage are Shreya and Pavel, also from Google Cloud. We all have been experiencing the developer life for a while. We've all been developers. So, we really understand the challenges and what it takes to get work done. Okay. Here's what we plan on covering today, focusing on your needs. Developer productivity boost. In this segment, we'll show you how we plan to use agents to automate the road tasks that slow you down, freeing you for the fun parts of what you like to do. Next, Pavel is going to come up and double-click a little bit into the agent architecture and unit testing. He'll walk us through and demystify how these agents work collectively, not just a set of isolated tools, but a highly coordinated multi-agent system that brings and harnesses the power of these tools and other agents to provide effective solutions in the area of unit testing. And then it'll also touch a little bit on some of the self-healing loops. Next, after that, we are really excited to walk you all through a beautiful demo on how we can use these agents to really help you improve your code coverage. We are going to have time towards the end for Q&A. So if you have any questions, please hold on to them. Towards the end, we'll have some live Q&A session. Okay. With that, just a quick show of hands. How many in the audience out here are developers? Right. Keep holding your hands up. If you have used any AI assistant tool, all right, keep holding your hands up. Look around. A lot of us out here are doing that today. You can put your hands down. Thank you. We do sincerely hope that we can turn each and every one of you into a Gemini code assist fan by the time you leave this session. Okay. Let's get right into it. Let's start with the core issue at hand. Developer productivity. We all want to be more productive, right? What's our mission? It's actually very simple. We want to cut down on the engineering toil and make you, the developer, more productive. We want to augment your software development lifecycle workflows with letting AI agents handling all of the tedious stuff so you can focus on the more creative and fun parts of development. This is a recent survey that happened. Stack Overflow does a developer survey. Let's take a quick look at some of these numbers. Developers already see benefits of using AI tools. 81% talk about increased productivity, while about 62% say that it helps them learn faster. And about a good 58% talk about greater efficiency by leveraging some of these AI tools. These are not just abstract benefits. These are tangible improvements to your daily grind. This is about making you, the developer, work better and smarter. Right now, most of us are using AI for coding, for searching for answers, and for debugging. These are great starting points, right? But let's take a look at the rest of the chart. Documentation, content generation, testing, project planning. These are all areas where AI adoption is low. And let's be honest, these are often the least enjoyable but the most time-consuming aspects of our job. Do you folks agree? Okay, thought so. All right, now check this out. Developers are most interested in AI for the exact same areas. Documentation, content generation, testing, planning. See the inverse relationship? We are using AI for the fun coding parts. But we want AI to take on the more tedious and the not-so-fun stuff parts of our job. That's exactly what we're trying to address here. We've seen three high-level industry trends. AI is increasingly being used across multiple enterprises to generate more and more code. The quality and stability of the code generated is declining because there aren't enough practices and reviews in place to be able to keep up. Software development lifecycle is an entire loop. It is not a single point in time. You can think of this as an assembly line where you have coding, testing, deployment, et cetera. A lot of folks are using AI today for the coding parts. Tackling and improving one part of the assembly line will create bottlenecks in some of the other areas. And lastly, maybe this resonates with everyone. Is anyone sure of hands out here if no one is being asked to do more or faster? Okay. Right answer. I think everyone has a lot of pressure. There's a constant need to be able to innovate faster, do more with less. Right? Sounds familiar? To align with some of these industry teams, there are three product teams that we have focused on. The first is, when we started, some of the folks who may have been here last year, the focus was on the overall product experience. Making sure we deliver a great interface in the IDE. The focus was on model and quality and performing some of the core tools that folks need while working inside the IDE. The focus then shifted very quickly to the enterprise entities. Enterprises and companies want to understand, well, how exactly is AI assistance helping me be more productive? Some metrics, tooling, became more and more front and center stage. And then lastly, the ability to be able to then focus on the entire software development lifecycle. What does that mean? Developers hang out on multiple ecosystems multiple developer tools. Using developer tools that we announced last month, late last year, we're able to now allow developers to be able to access a big part of the ecosystem right from within the IDE. This is also about bringing more automation and agents right into the IDE where you spend a lot of your time. Most AI tools start with coding. That's where the focus is, naturally, because that's kind of where developers were able to see the most benefits. But as I mentioned, SDLC or your software development lifecycle is a loop. It's not a point in time. Coding is one part of it, but it's the first step. Our approach recognizes the broader and the bigger picture. We are building agents for every stage of the software development lifecycle, starting with coding, migration, testing, security, monitoring, and deployment. And to manage this ecosystem of agents, we are introducing the Gemini Code Assist agent. Think of this as your central intelligent project manager that can coordinate all of these specialized agents through one unified interface so you don't have to. You interact with this one agent and it can delegate and orchestrate the problem that you throw at it across a bunch of different agents and bring all of that to bear. And to make it easy for you to interact with the agent, we are introducing the Gemini Code Assist Kanban board. This is one surface that you can use to submit a job to the agent, observe the status of all the jobs in flight, more importantly, be able to perform human-in-the-loop and interaction with the agent. As an example, when the agent presents you with a plan, you can help refine that plan. You can also use that modality to be able to provide any missing parameters or inputs. And then lastly, to review some of the artifacts that are generated by the agent, such as a code diff set. Okay. with that, I'm going to pass it over to Pavel, who's going to walk us through agent architecture and your testing. Pavel. Take it away. Thank you. Hello, everyone. My name is Pavel, and I'm a product manager working at Google on Gemini Code Assist. Okay. So how does our Code Assist agent actually work? Today, I will walk you through our agent architecture and show you our approach to unit test generation. But before that, let's start with noticing that the term agent has changed. Initially, when we talked about agents, we were often referring to much more simple concepts. Think about straightforward API calls, models for classification or feature extraction. Essentially, more like specialized tools performing simple tasks. But the landscape has changed significantly. The capabilities we now associate with agents are far more sophisticated. So, modern agents are designed to handle complexity. They can use various tools. They can decompose large problems into manageable steps. they can progressively refine their outputs and even incorporate self-reflection to improve their approach. So, we moved from simple function execution to complex multi-step problem solving. And our approach to agents has changed as well. So, we started with those simple API calls providing specific defined behaviors. behaviors. Then came the static agents, which were good for solving specific well-defined problems. And as the need arose to handle more complexity, the dynamic agents were developed. They offer far more flexibility. Here, you don't know all the steps, but it's still predictable, like in self-driving cars. But what about the tricky stuff? the highly complex, ambiguous problems, which are maybe new? That requires a more advanced strategy. And this is where the multi-agent approach comes in. So, let's see how it works in detail. When you face a difficult task, you need to establish a team. That's why we have specialized agent personas, and for a task like generating unit tests, you might have personas like software engineer, test engineer, maybe an architect, each bringing a specific skill and perspective. So, when a job is initiated, let's say, generate unit tests for a given piece of code, the relevant personas join the shared workspace, here visualize as a chat room, they all receive and understand the common goal. Then, the collaboration begins, they interact dynamically, they are in this chat room taking turns to contribute, reflect, feedback each other, and respond. And when the job is completed, they are ready to move to the next task. But what's special in this system besides simple turn taking? First, it's a constructive critique. So, the personas are designed to criticize the previous answers from their perspective. So, for example, the test engineer can criticize software engineer about some potential edge cases. Second, it's self-reflection. These aren't just tools executing steps. The personas evaluate the progress and the quality. And the last one, it's iteration. So, based on this constructive critique and self-reflection, the agent is going in phases, constantly thinking how to improve the next phase. And I want to focus on this iterative approach in aspect of unit test generation. Because we all know that writing good unit tests can be challenging. They need to be accurate, robust, we need to cover all the edge cases. And this is where our agent's self-healing loop provides a powerful advantage, automating much of this refinement work. So, before the loop even starts, the agent performs a build detection. So, it intelligently analyzes your project to understand its specific setup, the framework, build commands, setting the stage for you automatically so you don't have to do it yourself. And then the self-healing loop kicks in. It begins by generating tests tailored to your code and the specific goal. But crucially, it doesn't just generate them, it executes them. So, this provides the essential reality check. Do the tests run correctly? Do they pass? And in the next step, the raw results from execution, so all the passes, failures, errors, coverage gaps, are being analyzed. Here, the agent determines what happened and why. And here's the self-healing magic. So, that analysis immediately informs the next round of generation and the agent learns in every round how to improve the quality. So, this generate, execute, analyze cycle repeats till we get the desired quality. And with that, Briefpal, Shreya, would you mind showing us the demo? Thank you. All right. Super exciting stuff. So, who wants to see a demo? All right. Let's do it. We have awesome Shreya with us. Shreya, you want to introduce yourself? Everyone, I'm Shreya. I'm a software engineer on the Gemini Code Assist agents team. Awesome. So, here's what we have planned for the demo today. We are going to use an open source Spark repo. It's a Java-based web framework. And to make sure that we don't cheat, we've actually taken out all the unit tests from the repo out. Our goal is that we are going to run a code coverage report and show that the code coverage is pretty low, as you would expect. And then, we would want to show you two things. A, how you can interact with Gemini Code Assist. To do part of the puzzle, which is generating unit test. But then, we'll go a step further and show you how you can use the power of the code assist agent to help generate unit test for your entire folder or your entire repo. And then, we'll walk through what that does to your code coverage. And with that, if you're ready, we would like to switch over to your laptop. So, before I get into the demo, just by a show of hands, how many people here love writing unit tests? Writing unit tests is your favorite part of the entire SDLC life cycle. We had a few hands come down. I'd love to learn more. Personally, I find writing unit tests extremely tedious, which is why I'm so excited for the Gemini Code Assist agent. This agent doesn't just generate unit tests. It works on the entire SDLC life cycle, entire unit testing life cycle, end-to-end. So, like Paolo mentioned, in addition to generating the tests, the agent also compiles and runs the source code, runs the test, fixes errors, and iteratively works on the generated tests. So, for the demo, we'll be looking at the open source Spark repository. What I've done is I've cloned it to my desktop and opened it up in VS Code. So, this is an open source web framework for Java 8. Within this repository, we'll be looking at the route directory, which contains logic to map various incoming HTTP requests to different request handlers in the app. So, just opening up one of the files, you can see that the logic in these files is not extremely trivial. There are a lot of statements and branches that we'll need to generate unit tests for, in order to get sufficient code coverage. So, as Prithpal mentioned, we're starting off with 0% code coverage. So, behind the scenes, what I've done is I've run the Clover command to generate a test coverage report. Here, you can see the various classes that we're generating unit tests for. You can see the number of uncovered elements in the classes. Obviously, that will go down once we actually add tests. And then you can see the starting coverage, which is 0%. So, I'm going to go ahead and open up CodeAssist. And before I invoke the CodeAssist agent, I'm going to ask the chat to generate tests for the open file. So, we'll give it a few seconds. You can see that CodeAssist is already returning a response back. and since this is chat, you'll notice that in addition to the unit tests, we'll also see a helpful explanation of some of the tests that were generated and potentially also a list of dependencies that we'll need to install in order to run these tests. So, if I wanted to save these tests, there's a few different options I have. looking at the actual test file, looks like I can copy the code into my clipboard and then manually create a file. I can add the code into the existing file, or I can use a button to do both in one step. Back to you, Prithpa. Thank you, Shreya. So, let's do a quick recap of what we've done so far. You looked at a repo. Shreya walked us through the open source Spark repo, walked us through a code coverage report that was ran, and as we expected for it to have extremely low code coverage. You also saw Shreya walking us through how she was able to use Gemini Code Assist to be able to have it generate test cases for a specific file. So, think about how you as a developer can use AI assistance to work with it to start providing you more and more capabilities. But, I think it would take a good amount of time, something which developers don't want to do, when you want to have the tool generate unit tests for a set of folders or even a full repo. Right? So, let's see if you can leverage the Code Assist agent to augment your workflow by you delegating that task to the agent and see how that agent harnesses the power of other tools to generate unit tests for a larger set of files. Shreya. Sure. So, in order to invoke the Code Assist agent, I'm going to type at in the Gemini Code Assist extension. You'll notice that a new pop-up shows up. You can see the agents that we have along with the tools. You can think of tools as first-party and third-party ways of connecting to different APIs and then agents as ways of automating different parts of the SDLC lifecycle. For this demo, I'll be using the Code Assist agent, and I'm going to ask the agent to generate unit tests for the route directory that we were just looking at. Given the agentic flow, it can take several minutes or even hours to get a response back. So, for the rest of the demo, we're going to switch over to a recording so that we can speed things up. So, right where we left off, I'm going to enter my prompt. You'll notice that the new Kanban board UI pops up. I'm going to pause right there. So, what's happening behind the scenes is that our agent personas and our runtime environment are getting initialized. Once the task is ready to get started, you'll notice that the state will change on the card. You'll also notice that there's two other columns in this view. You can see the action needed column, which is where the job would move to if the agent personas needed any input from the human users. And then also the completed column, which is where the task will move once it's done running. Something I really want to highlight is this UI and this flow was made with long-running operations in mind. So, we have enough time to actually not just generate LLM-generated code, but also compile the source code, run the code, fix errors, and iteratively work on the files. So, we'll give it a few seconds to finish running, and then the task will move to the completed column. What I'm going to do now is click on view changes and open up the generated files. I'm just clicking through the files now, and I'm going to hit apply, and the files will save to my local desktop. I'm now going to open up the files. So, we'll just walk through the files. Notice we're not just generating one or two unit tests for each of the files, but we're generating enough tests so that we get sufficient code coverage on the source code. So, using the same tool I used earlier, Clover, I'm going to regenerate the test coverage report. The coverage is much higher now. We're almost at 80% test coverage per file, and the number of uncovered elements is also much lower now. Back to you, Prithpal. Thank you, Shreya. Could we switch back to the slide, please? Thank you. Thank you. Awesome. So, let's recap real quick. We focused on a few different things in the demo. Obviously, we sped up a lot of the pieces so we can fit the demo in the scope of this session. But we saw how we went from a very low code coverage report to using the unit testing capabilities of the code assist agent to generate test cases for an entire folder at a time, and then also improve your code coverage. So, overall, we looked at the benefits of AI tools. We saw the capabilities of a multi-agent platform and how you can use it to solve for a variety of different use cases, such as migrating your apps, generating apps from scratch. Today, we focused a little bit more in this session on the unit testing aspects of it. We also looked at how some of these capabilities really help you become more productive by the agent beginning to augment your stages of the software development lifecycle and helping you become more and more productive. Let's face it. Today, you saw the demo predominantly centered around the IDE surface. Let's face it. Many of us developers have a varied ecosystem of tools that we use for our everyday life. So, using collaboration and chat applications like Slack, Google Chat, using source control management systems such as GitHub, GitLab, and even project management systems such as Atlassian, Asana, and the likes. That's exactly where we're headed. We are really excited to push the boundaries of the Gemini Code Assist agent that you saw in the IDE today to bring a lot of these capabilities, exact same capabilities, into many of the developer tools that developers use today. So, keep your cameras ready. We're going to be making a few announcements, and there's some QR codes that you want to hang on to. Okay. We are super excited to announce the private preview for the Gemini Code Assist agents. Scan the QR code, take out a few pictures, walk through the steps. We will be opening up the private preview shortly to the folks who get accepted into the program. Okay. Announcement number two. We would love to hear from you folks and have you shape the future of the Gemini Code Assist agents product. Please follow the steps listed out here, and I'll do a special call out to Sarah. She's here in the audience. She'll be here with us later for the Q&A part. Please do not, you know, wait, stop by, by booth number 811 in the expo floor. I've heard there's some swag giveaway by the UX researchers. Okay. All right. How do you continue your learning journey from here on at Next? There are several sessions around Code Assist and agents, but these are two that I would like to call out. If you're able to see the developer keynote, it's happening tomorrow at 2.30 p.m. in the Ultra, McLobe Ultra Arena, where we will cover not only what you saw aspects of where the product is today, but where we're headed and some of the vision aspects of it. There's another session after that, the Spotlight session, where you may see the Code Assist agent in a broader set of use cases. So I just wanted to call these two things out. Again, your feedback is greatly appreciated. We would love for you to take a few minutes, use your next app, and then complete the session survey. What did we do good? What are things we can improve on? We would love to hear from you. And with that, we will end a little bit early and save time for Q&A. So with that, I would like to welcome back up Pavel and Shreya. Thank you. Thank you. I know they're going to be distributing microphones. They're going to be coming here soon. So if there are any questions, please raise your hand. I see one up front here. Okay. Sure. Hi. My question is around the code-based awareness feature. Can we get more details about how Code Assist indexes a particular code base? Is it abstract syntax trees or more advanced algorithms like that? And is that process generic across any repo of any size or any language, any framework, or how does that work? Okay. So I'll try and answer that. I think the question is more specifically around the Gemini Code Assist code customization feature. So code customization, we didn't see that today, but code customization is also in some instances known as RAG index for your code base, where we help the customers pointing us to a repo of their liking within the enterprise. It leverages and builds RAG indexes, getting a little bit more familiar with the code base. The way it works is, once you've used the code customization feature and we have enabled it for you, instead of you generally using Code Assist to answer questions, it then starts to match with, have I seen that question before? And if so, what is the best response that I can provide to the user? So it uses a bunch of different algorithms like similarity searching and other techniques to be able to make sure that whatever code is generated actually caters to a lot about the learned styles, behavior, and coding practices from within your organization. And maybe just to add to this, because you asked about the size, so at least for code customization and how many repos we can index, it's up to 20K repos, and the size here is, please try getting as much as you want. There's also a great session on Friday that will go into code customization, so I recommend checking that out. Yes, that's a great call-out. So yeah, please, please attend that session. All right, let's take one out there. Hello, hello. Yeah, my question circles around mainly hallucination. How do you feel, how do you determine the, I guess the competency slash confidence of the code that's being generated for the unit test? Yeah, I think that's a great question. So I think we've all come a long way. I mean, hallucinations have been a problem with LLMs, but there are a few things that we are doing to ensure that the code that's generated is of the right quality and it's not, you know, factually wrong, if you would. So I think I'll take the first cracker and then let kind of power talk a little bit about the self-healing loop. But essentially, what you saw was we have a multi-agent system in place and so we spawn all these different personas. The analogy is, think of that as like, you know, you have a chat room of sorts. The shared goal, the prompt that you submit and access to your code base, right? And so when the agents kick off, they take turns seeing the work of the previous agent persona and then they use a critiquing mechanism to see, well, did the code generator was factually correct? We don't stop there. We take it a step further and we run tests against that to see, well, are the unit tests passing, right? And so that's one of the few ways we ensure that we are not just sending results which come from the LLM right away. We have loops to improve the quality of response overall. Do you want to add something more to that, Pavel? Yeah, I think that's, first of all, it depends on the code because if you wrote bad code, then we're going to just generate tests to test this code, right? So there is this aspect and when it comes to testing this code and we have this loop going on, in every phase we're just trying to make sure if it's good enough and get to assert that the biggest possible level of confidence we can get. Awesome. Next question. Let's take one from the side here. So a lot of people love TDD and including, like, my manager, he loves it. Just thinking about this, it seems like it's backwards and sometimes they might generate, like, how you just said, a test that validates bad code. So is there any thoughts or ideas about how to maybe turn this backwards and make the test first? And to record that? Yeah, I mean, there are multiple use cases for unit tests and going with TDD is definitely one of them. So this is something which we definitely would like to explore. Here right now, for the moment, we were just focusing on starting from scratch in this demo session. So yeah, I think it's something worth it. I can talk a little bit towards, like, expanding on that a little bit. What you saw today was some of the early additions of the product. We focused on helping developers kind of, you know, offload and delegate the unit testing aspect of it, right? We are working on introducing support for integration tests, web tests. TDD comes up quite a bit. One of the things that we are planning on doing is there is some limited customization support that is going to be there in the agent as we go towards private preview. And some of those things are the ability to adhere to some best practices, coding standards, you know, unit testing aspects of it. As we see this thing go forward, our thought process is that we are able to leverage policy stores and repositories and use ways to be able to inject testing super early on in writing good code, right? So that's kind of where we're headed, taking some small steps, but, you know, that's where we are headed towards in the future. Okay. There's one here, and I think that gentleman had a hand up earlier out there. Okay. Sure, anyone. Hi. Thanks. I think one of the questions I have is when we talk about agents, I've noticed that a lot of the time there's the phrasing that these agents are going to be really helpful in taking away, you know, some of the rote, like, boring tasks. But the thing, I guess, I wonder is why would it just do the boring tasks? Like, why is it that when we talk about these agents, generally speaking, we talk about them as being, I guess, assistant tools rather than tools that can be, I guess, ultimately replacing? That's a great question. I think it's not that AI assistants and agents cannot help offloading some of the fun coding aspects as well, right? I think the idea is that just like you saw in the, some of the the data charts that we ran through with a Stack Overflow Survey, right? There's some aspects of the development lifecycle that we take to more naturally, such as, I want to help with coding because I enjoy that part, I want to problem solve, right? Agents are able to help with coding equally very well. What we have seen in our experience and talking to customers, that there are some tedious and less enjoyable aspects of our job. We would love for AI to take that on because ultimately at the end of the day, you as a developer are still on the hook to deliver that, right? Increasingly, as you see these agents become more mature and they're able to use techniques such as self-reflection, progressive refinement, iterative learning, they're going to get better at each and every task, right? Knowing your code base, becoming more familiar, leveraging these kind of tools that you allow it to access, so on and so forth. So the idea is that do they move from like simple tools to semi-autonomous to more autonomous agents? I think, yeah, it probably will get there at some point in time. But I think the most important thing is agents are under your control at the disposal of all of what you wanted to automate, right? Let me play this thing forward. One of the next things that I see happening is instead of you having to work with the agent more explicitly that you saw today, we said add code assist and go do this task for me. You want to make it more and more implicit, which means you may just enter your prompt in code assist as you do today. We would be able to detect that, oh, this is actually something which is not just generating a unit test for one file, but requires you generating unit tests for your entire code base, running them, making sure they're successful, and have a target code coverage score that is maybe pulled from a policy store or your best practices document, right? Well, that's kind of where it would head to and kind of become more and more autonomous. And so there are different layers, and I feel like as we kind of move along, as developers and users start to trust agents more and more with the right quality and aspects, I think what you're saying is probably something that we will start seeing in action on a much more frequent basis, where developers, I think would shift into other aspects of their job and let AI agents do a lot of the TDS, CI, CD, and the other aspects. And I think we're just starting out and beginning to see gravity towards some initial phases of the software development lifecycle, but there's, as I mentioned, SDLC is a loop. There are other friction points that we do want AI to take on. I have two more questions. Please. One from that. Yes. We have time. Sure. Hi, I'll be quick, I think. So I'm not a developer. I'm actually in software quality, but this topic is near and dear to my heart. So the software development lifecycle, as we all probably know, doesn't actually start with writing code. It starts with defining your business requirements, right? And then usually planning, and then we're jumping into writing code. So my question is, a few slides ago, you had the iterative approach of how the Code Assistant works, like it runs a code, it tests the code, it rewrites the code, and it does that until it passes, right? How do you know that you're not fundamentally changing the business requirement that code set out to provide? If you're changing source code through an agent, what if, like a simple example, I wrote a function to add two numbers, what if through that process now it's dividing or subtracting? Is there any kind of safeguards built into that? Or is that where we deploy the QA environment and then throw it to folks like me to test it? Well, we wouldn't want to do that to you generally, and we wouldn't want agents to do that to you, more importantly. So I think you bring up some very, very relevant and interesting points, right? So there's a starting point in the SDLC. We've taken coding as one of the steps in the software development lifecycle as an entry point, because that's where the gravity is, right? We're starting with that. You also saw, we had a session before this where we spoke about how we were able to leverage the power of tools to read a spec from a system such as Google Docs, or perhaps a user story in Atlassian, right? Jira. Those are starting points. What we're seeing is one part of the loop which is pulled at the right time by the user and or the agent on your behalf to be able to read the requirements and generate something. I think where we're headed to is more around self-healing but full-circle loops. What I mean by that is let's say the agent now is able to read an initial user story or a requirement written up in Google Docs. It may generate code. Today, there is an aspect of we running tests to make sure that the code, the unit tests pass. If not, we reinitiate that loop till we achieve a desired result. I can easily see us expanding a step beyond that to say, well, go back and loop back in with the requirement to make sure none of that has changed and or listen to real-time events that come in to be able to make sure this is self-sustaining and healing all the time. Right? So, with that, I know we're almost at time. I know there was one more question. We'll take that real quick but we're happy to step away after the session as well outside if there are more questions. Go ahead. So, this is more of a roadmap question. You've dealt with unit tests. Are you looking at any other type of tests? Because I'm a front-end developer so we do things like Cypress, Playwright, which is similar, written similar to unit tests. Yeah, I think the short answer is yes. I think we're starting here. Unit test is something that we are starting with. Yes, they tend to be more human-written. We're looking at integration tests, tests around, you know, you mentioned Playwright to be able to play certain things, web testing end-to-end. So, yes, these are all things which you're going to expand the capabilities. Today, we're starting with a set of few verified use cases. We definitely have plans to be able to expand this thing to a variety of different kinds of synthetic tests, human-generated integration, so on and so forth. Okay, well, we thank you for your time and we'll be outside if you have more questions. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.