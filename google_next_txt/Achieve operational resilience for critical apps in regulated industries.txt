 . Hello, welcome. Thank you all for joining us today. We're going to chat a little bit about some hard-won lessons learned with our partner friends here from CME Group. I'm Mary Koze. I am a product manager in cloud observability. Hi, I'm Preeti. Thank you, Mary, and thank you, Udaya, for accompanying me. I'm Preeti. I'm from CME Group, Chicago Merchandile Exchange. I manage the SRE team at CME Group. I've spent over a decade at CME. I've been working with Google Cloud and CME partnerships since the inception of Google Cloud in CME. The journey started with first architecting the solutions, designing them, and then I started building the SRE Foundation. And today we are going to share some insights about how we went through that journey and how other customers can follow that path. Super excited also to be joined by one of our amazing engineering managers here in Google Cloud. Thank you, Mary. Hi, everybody, and welcome to Cloud Next. My name is Udaya Pillala Murray, and I am a senior engineering manager in cloud observability. Mary and I work very closely together on a number of initiatives, and I also lead Cloud Assist work streams that build and improve upon the agentic frameworks as well. And with that, our agenda for today will be to cover the CME Group's journey to Google Cloud, the path to transition from an on-prem-based solution to a cloud-based solution, following which we're going to go into the principles for resiliency that will include a deep dive into observability, data resiliency, as well as change management within your organization. Finally, we want to wrap all of that up into critical key takeaways for your organization. And then we'll reserve some time for questions at the end. With that, Preeti, I'll hand that over to you. Sure. Thank you. So, firstly, let me introduce you to CME. So, hope this session you all can take away what CME is, how we moved on our journey and our partnership with Google Cloud, as well as thank you all firstly for joining here in the lovely city of Las Vegas. And let me start with what CME is. So CME is Chicago Monchon Tile Exchange. It is the world's largest derivative exchange where market participants come together to manage risks. There are various key products and services that we offer, the key ones being markets, clearing, and data. On the market side, we provide cutting-edge technologies in the space of low-latency trading platforms. For clearing, we act as a central counterparty for all the trades that happen at CME Group, being the buyer to every seller and seller to every buyer, thereby mitigating risk. And as you might realize, all these activities generate a lot of data. And for that, we have data platforms that we are building and building on as we initiated the Google partnership. We are building on more on those data platforms. So as you can see, we partnered with Google, and CME has always been innovative. So I'll start with the CME innovation story. So as part of CME innovation story, when did CME start? So CME was founded in 1898 as the first futures exchange based out of Chicago. We launched the first financial futures contract in 1972. We were the first exchange to go fully electronic with our trading platform in 1992. And we partnered with Google in 2021, being one of the first large exchanges and clearinghouse space to decide to be fully on Google Cloud. Now, as we say we migrate to Google Cloud, let's see what does it mean in terms of our regulators. So the partnership with Google Cloud is with the vision to leverage the advancement in technology that's happening and offer the best to our customers. As you can see on this slide, there are various regulators that regulate, us being a financial organization. And as we are modernizing and enhancing our technology stack, we need to ensure that we are compliant to the processes that have been set by the regulators, compliant towards ensuring that we can run our markets and clearing and data platforms in a resilient and robust manner and leverage all the engineering and observability practices that we have across Google. As I talk of observability, let's look at some numbers. So if you look at some numbers, these are numbers of CME Group for over a decade. As you can see, the volumes and numbers have been constantly increasing. And in this graph, you will see a couple of spikes. These spikes correspond to some significant activities that happen in market. It can correspond to, say, U.S. elections. It can correspond to market activity. It can correspond to market news, like events that happened this week. There was a spike in volume. So you see significant spike happens when there is volatility in market, when there is some significant news in market. And in order to support the volumes that we are seeing here, in order to support the applications, we need to ensure that we build scalable applications that are robust and resilient. And scalability and resilience requires observability. Requires observability without which we cannot operate at the scale that we need to. So bringing the topic of observability, I would like to hand over to Mary to brief a bit about observability and its practices at Google. Awesome. Thank you. Actually, go ahead. Mary, I was going to say, if I may. Please. One of the things that I learned out of this experience with the CME group in particular was the massive scale and the volumes of transactions that they handle. I'm going to bring back my old hat from my experience with trading organizations in the past. I was aware of all of the transaction requests per session per second. But when you compound that across multiple sessions, you generate a high amount of trading transactions. And you think about aggregating risk across all of that volume. That becomes a high, you know, scalability, reliability problem that we actually dug deeper into with observability tools. So, you know, from that angle, Pripy, I wanted to understand what were some of the guiding principles that you and your team actually adopted as you were starting your journey with observability? Thank you, Udaya. That's an excellent question. So, before we do a deep dive into the topic of observability, we'll talk about some guiding principles of resilience. Now, being a financial organization, we need to be observable. We need to be available. We need to follow principles of reliability, resilience, speed, security, performance, and many more. The list goes on and on. There are various architectural paradigms that we need to follow. We have been leveraging in our tech stack prior to Google migration. And that we built on as we embarked upon Google migration journey. Now, as you see on the slide, we have listed down a couple of these paradigms. We are not going to go over each one of them. We are going to go into a few items which can be the key. First one is observability. The second one would be availability. And reliability are the three principles that we are going to talk about today, which found the fundamentals of our platforms as we migrated. So, Mary, over to you for observability. All right. Thanks. And I have to say, one of my favorite parts of our partnership together has been getting to see how CME Group has a real appreciation for SLOs as part of your practice. So, we'll get into that in just a minute here. But I don't know. Anyone else in the audience fans of SLOs, using SLOs? All right. Cool. You guys are in the right place then. Awesome. So, why observability? I actually come from a mechanical engineering background. And observability is a key concept in systems design. You can only control a system that you can observe. So, what can we infer to drive internal systems based on the external outputs, right? And that's really the goal of observability and why I've been in the observability space for nine years now because it's so much fun. So, what we've seen is that elite performers are much more likely to incorporate observability into their overall system health really from the beginning, not as an afterthought. So, that's one of the things I've really enjoyed in getting to partner with the CME Group is sort of their approach to this. But let's level set before we dive into kind of some of the details here. We think of three building blocks for observability, right? So, our goal is that you can answer questions about the internal workings of your system. We're going to have three building blocks for that. Monitoring, logging, traces, metrics, logs, traces. Okay? And then these building blocks can be used to assemble all sorts of different tools to serve different personas. So, sec ops, developers, platform admins, SREs, business analysts, security analysts, all sorts of folks are going to benefit from what we can assemble out of these building blocks. In fact, as cloud observability on Google Cloud, we've got, you know, hundreds of thousands of folks who use, you know, our UIs in order to answer these types of questions every single month. All right. So, speaking of cloud observability and how we build solutions on top of these building blocks, in the past 12 months, we've had over 100 launches. So, since last, roughly since last Cloud Next, related to cloud observability. Don't worry. I know we're between you and lunch. So, we're not going to go over all 100 of those. But a lot of them were driven by customers like CME to serve specific use cases. So, you know, my ask to you all, the end here, if you have some feedback or requests, like, come talk to us. We'd love to connect. If I were going to call out maybe a specific theme around these different investments, I'd say it's really about bringing the logs, metrics, and traces together into a single pane of glass. All right. So, for example, here, we've upgraded the dashboarding experience to really make it much easier to create dashboards that span different types of observability. So, if anyone was users of the product way back when, back when it was called Stackdriver, we, you know, there were very different silos between your logs and your metrics and your traces. And what you'll see now is that they kind of all come together into a single unified experience. So, where do we begin? There are customizable, fully configurable dashboards that are available to you to bring various forms of telemetry data, like logs, metrics, and traces, into a single pane of glass. There are out-of-the-box dashboards available for your services where you can use to drive critical insights into your golden signals and slice and dice the infrastructure data for further troubleshooting issues within your environment. On top of that, you can actually iterate on these dashboards for your top-level metrics for your application data and start to monitor health of your services even further. For example, error reporting processes a lot of logs data and generates valuable error groups, which could be powerful starting points for your troubleshooting journey for products such as GKE, Compute Engine, Cloud SQL, and so many more. Actually, at this point, Preeti, I'm curious. As you started on your observability journey for CME Group, were there certain dashboards that you found that were particularly useful? That's a very good question, Udaya. So, OTEL, OpenTelemetry, has been our foundation building block on which we laid the foundations of observability in cloud. Now, it's important to note that CME did not do a big bank migration to cloud. We had a staged and a phased journey. Being in a phased journey, our observability also had to be a union of both what we had at on-prem and cloud. So, firstly, to answer your question of dashboards, the answer is absolutely yes. We leveraged everything that came out of the box, which included service dashboards, which included project dashboards, uptime check, health metrics, and many more, whatever capabilities the platform provided out of the box. In addition, we used Google Cloud APIs. We used G Cloud APIs to get more deeper insights from the platform, take those signals, merge with those signals that were coming from an on-prem applications, because we are in a hybrid state running in multiple areas right now, and bring it to a single unified dashboard and internal dashboards. We already had dashboards prior to migration. We ensured that we could plug in Google capabilities into that. So, we used all dashboards, any capability that we got, and the key is build a single pane of glass. Build a single pane of glass. As we migrate fully to cloud, we assume we'll have a single pane of clouds that is within Google. Right now, we are merging things together to get a single unified view that can help us observe and support this ecosystem better. So, I think that is a super powerful story, right? For us in cloud observability, we are always reviewing critical CUJs so that we can help organizations such as yourself and those in the audience become successful. Cloud Console platform provides in-context dashboards that give you access to a lot of observability data. And in this slide, what we're showing are predefined dashboards for GKE namespaces that utilize golden signals like CPU requests, memory utilization, container restarts, and many more. What's powerful about these dashboards is that you can overlay log-based events on top of them such that you can actually start to perform manual correlation between metrics and these critical events for further root cause remediation. In addition to that, you can also overlay events based off of incidents and alerts so that you can start to provide a richer context of a starting point from a troubleshooting standpoint on the resources that are experiencing issues in your environment. Actually, we just launched Cloud Hub and the Health and Troubleshooting Board that utilizes the similar concept of log-based events. And you can kick off AI-assisted application-driven investigations directly from that starting point. I think it's a good example that you brought of Cloud Hub. We are waiting to integrate with Cloud Hub. While we were waiting for Cloud Hub, what we also did was we took the metrics, alerts, incident, any events that we could get out of cloud monitoring platforms and integrate it, let that stream into BigQuery. From BigQuery, we had look at, we had look at, and look at where do we need to invest our energy in, what areas we need to harden, what areas we need to put our emphasis on in fixing our architecture or making it better. So definitely looking forward for Cloud Hub to integrate it soon. That's good. Awesome. Awesome. And that's actually a great example of, like, what motivates us, of pulling the entire Google Cloud ecosystem together to kind of help with that single pane of glass. So maybe another quick example of how we bring different signals together for the goal of a single pane of glass for observability. We've worked with a lot of the most common cloud tools, including GCE and GKE, for some common runbooks. So if you've ever, you know, kind of been like, oh, what's going on with GKE, for example, or GCE, MIG autoscaling failures, right? Like, this is actually something that we get a bunch of support cases about. We know exactly how to troubleshoot. So we've made these playbooks available in Cloud Observability so that hopefully it can be much, much easier for your team as they're building reliable applications here. All right. Another thing that is super common. Great. We have some stuff out of the box. But observability is really about those unknown unknowns, right? How do you troubleshoot what you didn't expect? There wasn't a playbook for it. And so users need to deep dive into our explorers. These are really, you know, my aspiration, like kind of what I tell our team is we want this to work for your team when they're woken up at 3 a.m., right? We want it to be super easy to find what you need and deep dive into your logs, metrics, or traces. And so we've maybe I'll do a short call out for the trace explorer, which has just gone through some massive upgrades. So it's pretty exciting. Check it out if you haven't already. Yeah. So, yes, leveraging and correlating the metrics, logs, traces helped us take our monitoring and our SRE practices from being reactive to proactive. We do not want to wait for an incident to happen. We want to ensure that we get alerts and signals so that we can avoid any kind of a disruption to our service. All right. So now that we've given an overview of all of the powerful visualization tools that you have in your observability toolkit, let's dig a little bit deeper into the lifecycle here for logs specifically. I want us to think about this in separate phases. There is collection, ingestion, routing, storage, and analytics. Preeti, you mentioned OpenTelemetry. That is a powerful tool, including any of your client libraries, to ingest and collect logs data by using the cloud logging API. Once you're there, a logs router is a powerful mechanism to filter, select, and route your logs data to the destinations as per your use cases. You can configure a logs syncs, that which is evaluated by a logs router, to take that input data and send it to or export it to a destination such as cloud logging, cloud storage, BigQuery, or PubSub as well. So what's powerful about this is when we provide you also with what is a log bucket, you can organize this information as per your organizational requirements and needs. And you can also add a configurable retention policy on top of these log buckets such that you can direct and dictate the retention period reads as per your team's requirements. I'll take an example. You have application debug logs. You may not really want to retain them for a long period of time. But I think, Preeti, you mentioned CME is a heavily regulated industry. So audit logs, for example, would require more or a maximum amount of retention time. And if I'm not mistaken, Mary, we have a maximum retention time for about 10 years, if I'm not saying. Yeah, right. So this is, you know, essentially something that I think we all want to think about when we're thinking about ingestion and routing and storing our information. Once you have all of these components in place, we index and store that data for powerful log analytic tools that we build on top of that data. Now, this is a lifecycle specifically for logs, but we are also looking at it for other telemetry types as well. And so think about your journey and do it in such a way that we can build a more compliant system as well on top of this entire infrastructure. And along with, as you talked about, the cloud observability lifecycle, thinking about my FinOps hat on, the data retention and how much log you are generating can also have an impact on your FinOps. So you have to be also thinking about how much log do you generate, what is the correct amount of log so that you can troubleshoot your system and also maintain it and also be compliant with other things that your organization might need. Right. Absolutely. So this is kind of like a quick overview of that like graphical version that you just had, Udaya. But like I want to maybe call out a couple of different decisions. The reason we wanted to go through this is to help you make the decisions you need when you're setting up your observability system within Google Cloud. Broadly, there are two different approaches you can use. You can aggregate your data at write and aggregate your data at read. Right. And so the thing that you want to think about ahead of time is how do you want to manage stuff across, you know, potentially hundreds of projects in GCP? Right. And so what we recommend is centralizing your logs and especially your your logs and traces because they tend to be the highest volume. And then something like metrics, you can aggregate it, read time as well. And so we've got sort of a couple of different tools in order to support this. So I already talked about them here. But sinks are going to help you do the right time aggregation. Buckets are going to be critical. You're going to have to think about that for controlling your region. So data residency, also resiliency. Right. Do you want your failures to be aligned with any, you know, if you're if you're running in region A, do you want your your failures to be correlated with that or do you want it to be uncorrelated with that region? You know, potential failure. Log views control access. So especially in a regulated industry in the financial space. Right. We see a lot of needs around subdividing who has access to exactly which logs. And then the last piece is scopes. And this applies, you know, kind of across. We're using logging as an example, but across the observability tools so that you can do read time aggregation. You can query logs, metrics, traces from different storage destinations. All right. So this hopefully gives us the vocab. To start talking about how do we want to structure our alignment and for observability platform. To recap, here are some of the best practices to create your single pane of glass for your observability journey. One, consider creating a clean project, a centralized project where you can aggregate all of your telemetry data, such as logs, metrics and traces. You have that. Now let's think about how to organize that data and tag it for access control using observability scopes so that you can configure your dashboards and your alerts and, you know, manage your team's observability needs. The next three points are really about what we've been talking about in terms of routing this information appropriately across the sinks and using your routers, and such that you can organize this data across various log buckets that are available for all the regions. And that could be for your business needs specifically. And in addition to all of that, please absolutely make sure to set up your org settings for the default log buckets that you create so that you can do it in a compliant fashion. Absolutely. Absolutely. In fact, when we were brainstorming about this talk, we put the slide in because you had mentioned, like, oh, we figured this all out, but it would have been nice if you had told us that first. So, sorry, we'll try and get better at that. But this is sort of our lessons learned from our collaboration here. All right. So, we talked a little bit about the thinking about data residency, CMEK. I wanted to talk a little bit about data resiliency. So, before talking about resiliency, I would talk about CME's journey into cloud. So, before our partnership with Google and before our partnership in 2021, we were primarily operating on on-prem ecosystem. On-prem meant our primary data center was in Chicago. Our secondary backup data center was in Secaucus. And we have multiple regional data centers in London, New York, and Tokyo. Now, we partnered in 2021, and our first initial duration went in building the foundations of the platform. As we were migrating our applications in parallel, we built our foundations of infrastructurist code. We built our CICD pipeline. We built our base open telemetry framework, what we would use so that applications can adopt them and build them. Along with that, now, the term CUJ, which I think Udyia brought up, is normally used more in SRE practices. But if you think about it, the CUJs are applicable right from how you think about designing and solutioning your applications. How you think about architecting them in cloud, how do you leverage the capabilities of cloud that comes in multi-regions and multi-zones, as Google provides, and be able to run your applications more efficiently. A few more things that the volume slide would have indicated is when you have volume trends where you can have spikes at any time, unpredictable times. You don't know when is the next spike going to be. You have to be able to deal with the volume on demand. Scale your servers on demand as needed and scale down when they are not needed. All these architectural paradigms we talked about and we touched upon very briefly as we started this talk, all these architectural paradigms were taken into consideration when we talked about how we'll build our applications. So I'll take on this slide, as you see here, risk monitoring as an example. So CME being a risk entity, risk monitoring is one of the key areas where we need to ensure that our users, both internal and external, for their risk monitoring purposes, should be able to see their real-time risk at any time. Our internal monitoring team, risk monitoring team, should have access to risk dashboards, access to real-time risk numbers. And for that, from Google perspective, we leveraged two regions. As you can see on this diagram, we are in Ohio and we are in Iowa. We are using two regions and we are using the capability of multiple zones. Multiple zones and multiple regions help us be resilient. If you can see, we use a subset for different applications. We use different subset of services, but we use a subset of over 60-plus services that are available in Google for us to use, and as we saw, one of the new ones that are coming up that we're looking forward to integrate. We use multiple services, and we rely on heavily scalable and highly available databases that replicate across the regions. So what did we accomplish with risk monitoring? Firstly, with our infrastructure as code, we did not have to wait for server procurement. So as compared to on-prem, if I had to deploy something new and something that required a lot of servers on my on-prem infrastructure, I would have to wait for the server procurement. I would have to go much ahead to my procurement team and say, I want to get X number of servers, get it installed in the data center, and use them. With Google Cloud, I don't have to wait for server procurement. With infrastructure as code, my developer can build the application, not just build, but scale the environment, can bring up any server at any time in any environment, and deploy the applications. We run our applications like risk monitoring applications hot, hot. We run them in a self-healing architecture. We run them in a load-balanced way. We leverage the power of both the regions. We leverage the power of highly available databases so that data is available in both regions and in all zones at any time. And if you see the paradigms that we talked about, in order to run and operate them as an SRE ecosystem and as developers as well, we need to ensure that we have sound and reliable observability. With key observability and as the terms that we use with a single pane of glass, we can support such an ecosystem and ensure that we are serving our customers with full efficiency. Can I interject here with a story? It actually really just came to me. Back when I was a C++ developer at SIG and I was developing the CME gateway, we rolled that all out in production. And, you know, as you're very well aware, we were all on on-prem solutions. And within about two days, we noticed that the gateway wasn't really processing the transactions and the requests. And it so ended up happening that it took us multiple days to realize that there may actually be a problem with the on-prem server in the data center. And then it took us, you know, about two hours of journey to the data center. And what ended up being the actual problem was a loose cable. And that took, I want to say, at least a week to troubleshoot. And we lost volume, right, on CME as a result. But that brings about the point that you're making, which is you're running HotHot. And you can run that confidently because you're on Google Cloud. And you don't have to worry about such situations, as I faced earlier. Really exciting to see this sort of architecture. Like, this is, you know, way back when I was in school, like, Ph.D. level systems design. So it's super cool to see. Thank you for sharing. Thank you. All right. So let's talk a little bit about how cloud observability, what, you know, little role we play in sort of the data resiliency design here, right? So I mentioned we need to think about, you know, kind of early on the data residency aspects. So kind of broke this down into, again, the pieces that Udaya laid out here, right? So on the generation side, we keep the â€“ this is, you know, kind of you're generating the data inside of CME group. And so that will kind of continue to just automatically absorb the data within the right cloud region. And when you route the data, even if you happen to be generating data, let's say receiving data from Europe and you wanted to keep that observability data in the U.S., for example, or in Europe or whatever, you've got full control over that. So logs will be processed in the region in which they're received, but then routed to whatever destination that your log syncs kind of are set up here. Again, using logs as an example, the other building blocks, metrics, traces are very similar, but logs tend to be sort of the most sensitive for a lot of our customers, right? The storage side, so they're going to be stored in the region that you choose here. That's maybe the biggest question that I get, especially from highly regulated customers like CME group, right? Where should we keep that data? Do we want to keep it? And many customers have to keep multiple copies even of the data. So a common strategy we'll hear is keep one data, one set of the logs in cloud logging, and maybe another set in cloud storage, so that if, you know, kind of the worst happens and an entire data center has an issue, you would have a backup in a completely different region, for example. And then we have other customers who are like, nope, we want to keep it all in that region. It has to stay in one region. So we hear kind of both directions of that. And then the last piece is sort of the processing. That will happen in the same region as the log buckets, and then we aggregate the results in the region in which the requests were received. So from a resiliency standpoint, maybe the one other thing I'll call out here, I talked a little bit about making multiple copies occasionally, but also another thing to keep in mind is that Google Cloud has, within a single region, multiple zones, and we will keep the logs data, the observability data in different zones that are separated. So kind of out of the box, you've got a lot of resiliency built into the system as well. All right. So let's talk about the change management, the human aspect of some of this journey. So as I said, migrating to Google, we took it as a very well-planned, phased approach. And in this phased journey, we took applications to cloud step by step. How did it help us? Our first range of applications that went us, we were able to get feedback from that applications. We were able to learn a couple of things that Mary just shared about, a couple of things that we learned during the journey, a couple of things we learned as best practices. It gave us time to learn those practices. It gave us time to automate those best practices and not just follow those best practices. So our phased journey was firstly the key. Along with, I would call out Google Collaboration. As Google team and my friends have been a constant partner with me in this journey. So Google Collaboration on best practices, Google Collaboration on how to do things, Google Collaboration on whenever we faced any problem, they were a phone call away and we could call and ask to say, how should we do this? That was the first point. The second was organization vision. Our organization vision, right from our management team to our development team to our business team to our customers, all were aligned on one vision, which helped us ensure that we have a smooth journey on this path. Infrastructure as code. Infrastructure in code and reusable frameworks helped us expedite this path. With infrastructure as code, we gave the power in the hands of developer, where they can spin up their own compute, where they can start their own services, where they can build and use any service that is available in Google and not ask for permission or not ask for resources. They can just spin it up, deploy it, and go ahead with an automated pipeline. So automation and infrastructure as code were the key to fast track the development. Along with this, we also regrouped ourselves or remodified ourselves to work in a pod model, as well as embarked on the journey for SRE. So aligning with the SRE culture helped ensure that we are able to support at scale the applications that are going on cloud, along with applications that are on-prem and support the hybrid environment in a seamless manner as a single pane of glass. Can you give me an example of the SRE culture at CME group? The culture at CME is very hands-on. So as SRE culture, we do ensure that we do not just work as SREs. We work as SREs in collaboration with our pods. So today our SREs are co-bingled with the development team. They sit within the pods. They are very hands-on. They help the teams not just in being reactive. They help us in being proactive to our problem statements. They are there with the teams right from the application when it's designed, when it's architected, to being able to deploy them in an automated manner and build any kind of an observability, stability, and scalability things that we need. Love it. Awesome. All right. And that brings us to some key takeaways to share with you all. So definitely we would say change is complex. What is the key? Defining the key principles, aligning users' expectations, aligning internal, external users' expectations, development teams' expectations, aligning your partner's expectations for collaboration is the key to navigating this complexity. Also, observability is definitely one of the key ingredients which can help migrations of this size be stable and be operatable without any pain. Totally biased, but I agree completely. All right. We'll do a final shout-out here for our friends in Google Cloud Consulting who have been a real partner in our journey, both for both of our teams, helping to make sure that we stay aligned. So if you're interested in incorporating them into your journey, definitely encourage you to go visit with them in the solution zone. All right. And I just want to say thank you to everyone for your time today. I know that you could be at lunch right now and really appreciate everybody's time. We would also really value your feedback on our session, on our products, whatever. So thank you so, so much. Thank you. Thank you, everybody. Thank you. Thank you, everybody. Thank you. Thank you. You're welcome. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.