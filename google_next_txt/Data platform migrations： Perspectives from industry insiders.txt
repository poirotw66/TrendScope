 Good morning, and welcome to day two of Cloud Next 2025. Most of you in this room are on a data platform migration journey. Some of you may be evaluating migration, actively migrating, or optimizing your data platform that's already in Google Cloud. With the help of my fellow speakers, I hope to share with you an overview of BigQuery migration services, some new things that are coming to BigQuery, especially around AI, some lessons learned in migrating, the business outcomes that came from them, and some tips and tricks that can help you have a great experience, a de-risked experience, and get the most out of your migration. So I'm joined today by my panel that will join us on stage shortly by Sam from QueryGraph, Doug from J.P. Hunt, Gaganpreet from CNA, and Raffle from DBS. And you're going to learn a lot from them and hopefully have the opportunity to ask some questions yourself. But first, let's take a look at what's stopping enterprises from achieving an AI-driven transformation. So I don't think it would be a surprise for everyone in this room that there's a lot of data, heterogeneous data, unstructured data, structured data. It is often siloed with multiple copies. And sometimes applications operate on different copies of the data. This has a lot of downsides. And often, a lack of integrated governance can cause some interesting effects. Next, data and analytics systems are often separate systems. Advanced analytics systems have been around for some time and are quite mature. But as AI platforms have become the norm, they have often been developed independently. And this leads to a complex, fragmented ecosystem where security, compliance, and orchestration, and governance can be different across these two systems. Let's go into a real-world example. Data that could live in your data and analytics system without any governance around data quality could impact your AI models. Now, scaling, this also leads to a couple of things around some issues with operating these systems independently. They're hard to scale separately. And they can lead to increased costs of running two systems instead of one integrated system. So how do we solve this? Well, Google Cloud's autonomous data and AI platform is BigQuery. At the foundation, we see that we can handle multimodal data, speech, audio, structured, unstructured information across any public cloud with integrated governance between both data and AI models, integrated AI to get you going with AI use cases on your data without any data movement. And you can bring to bear any analytical engine that you may require, whether it be open source like Spark or SQL through BigQuery or through Python with BigQuery data frames. We can do it all. And you can consume that information through Looker and AI agents. Now, we are the open data cloud. And we are the open data cloud. First and foremost, you know, we support all of the analytical engines and the Huda ecosystem that you may be familiar with, like Apache Spark, Apache Flink, Kafka, and Beam in open file formats like Iceberg, Hudi, and Delta. And second, we can kind of support you through a lot of resources. First off, there's incentives, which you'll hear about at the end of this conversation, to help you analyze and migrate your environment. Google Cloud Consulting Professional Services can help you plan and execute that migration. And lastly, deep migration experts, our black belts, can help you when issues arise. And there's more than just Google to help you on this journey. We have a very, very strong partner ecosystem that can help you deliver your migration from anywhere in the world. And this just builds on our commitment to being the most open cloud provider. Lastly, I want to leave you with some information about BigQuery migration services. As someone in professional services, we create accelerators and tools that have since made their way into our product in the form of BigQuery migration services. It makes it easy to migrate. We support assessments and migration activities from a multitude of systems, including popular data warehouses and data lakes, into BigQuery. Right now, we support over 15 sources of translation into BigQuery. We provide a programmatic API for our partners to access this service. And there are Gemini enhancements throughout and more to come. We've seen tremendous usage of this system in the last year, and we only anticipate more. After this brief introduction, I think it's time to welcome our panelists on the stage. So please join me in welcoming Sam, Doug, Roffel, and Godgain-Bree. The whistling. So for this next section of our talk, we're going to learn a little bit about our panelists, the firms, what kind of data platforms that they're working with, and here are some tips, tricks, and lessons learned. So with that, Doug, I want to pass it on to you. Sure. So I'm Doug Mettenberg, Vice President of Enterprise Analytics in our ENT organization at J.B. Hunt. I started the company in 1999 after graduating with my industrial engineering degree from Iowa State. I joined the engineering team at J.B. Hunt at that time. Our focus was on operational efficiencies, analytics, and just making the company more efficient. Some of the projects that I first worked on was our driver load assignment tool using CPLEX or mathematical models. Other items were like our routing tools where we would route our trucks and consolidate freight to save our company money. And then as time grew, the organizational grew. More data and analytics was added. So as today, I'm over our business intelligence team, our data architecture team, data engineering team, and our advisor team, which includes all the decision support tools using your common data science and operation techniques. J.B. Hunt itself has been around since the 1960s. It is currently around a $13 billion company. We are focused on the North American market where we move over-the-road containers using intermodal semi-trucks over the road, brokerage, and we even have a final mile division delivering to your home. We currently have over 30,000 employees, do close to 12 million moves a year with over 150,000 trailing assets. Okay. So why we started with BigQuery. Our main EDW up until this year was UDB or DB2 database that came from the mainframe. I'm dating it back to the 1990s. I probably don't have the official date on that, but that's where it was there when I came in 1999. And then, like I said, we started writing more optimization programs. We started building our analytical background in the early 2000s. And then around 2015, we started on our cloud journey. So at that point, we did some attempts at modernization using other cloud tools and, you know, SQL Server and so forth to upgrade EWs, which failed. Which even created more fragmentation in our system. And then in 2020, Google became our primary partner. So at that point, we started focusing on how do we become one centralized platform for our EDW. We chose BigQuery. So after some discussion contracts and funding, around three years ago, we got that all started. And then 18 months, we finished our BigQuery migration earlier this year. With that, we will now have one centralized platform. Retire the old platforms, which will give us, you know, more consistency. Help our customers, or my customer and turnable customers, be more efficient in the analytical world. But one of the things that we had to do before we even started that journey was perform a reduction on everything we had built over that 30, 40 years. Duplicate data sets. We had 12,000 reports that we got down to 3,000. I still think we can drive that down more. And I mentioned that you have to do this because as you go to cloud, it isn't just a server that gets slow. It is a lot of cost to move all that baggage over. And at the end of the day, we would get hoarders of data sometime. And really, only a few parts of our data is truly efficient and will help us get the ROI in the long term. In other areas, we were able to cut modeling time from days to hours using BigQuery ML and Vertex AI integrated with BigQuery. And then another main thing that I forgot to point out was most of our previous systems were nightly batched. So think about doing analytics when you were on a day-old data. Now, in BigQuery, everything is down to two hours, which is a great improvement over a nightly batch job. Well, thanks, Doug. I think you really hit the nail on the head when it comes to one of the top ten opportunities that companies have when migrating the cloud, which is to make sure that they have an inventory, that they understand what data and what pipelines that they need to move to the cloud, and only move what you need. I think next we're going to move to Sam. Thank you, Blake. And hello, everyone. My name is Sam Klakachopra, and I am SVP Product Enablement at CoreGraph. Now, CoreGraph is a data and technology company within WPP, and WPP is a global advertising agency group. So a lot of what we do at CoreGraph is about building products and solutions to help maximize value and unlock the value behind data and technology for our global clients and for our media agencies. We connect data to empower our marketers to build better, stronger media and marketing campaigns. So as we enhanced our product set, we knew that we needed to build a data platform which would scale across our global growth, it would provide robust data governance, and it would help unlock the value of our data for our clients, their data also. So we built Measure, which is a product that uses Google Cloud, BigQuery, and a few other partners and platforms to consolidate data and create a unified environment. This work and our BigQuery migration have improved cost efficiency. They've increased scalability and provided greater flexibility to all of our clients. And how did we do this? So BigQuery has helped us create a single source of truth for our critical marketing data. We've consolidated disparate platforms into a consolidated environment. Talking about some stats here, it's around 10,000 data sets, around 300 projects, 2,000 cloud run jobs, thousands of GitLab merge requests and Jira tickets, all across three environments. What this has done is it's maximized the value to our users and clients by bringing rapid insights. And to quantify that, compared to our legacy or grandfathered products, it's 25% faster. It's personalized their experiences and helped to create more innovative solutions by using the tool set that Google Cloud unlocks for us. BigQuery's serverless architecture and Google Cloud's enhanced security features mean that we can be very confident at handling our peak time loads. We can ensure that we are complying with all regulations. And we can make sure that we bring the data that our clients need to them when they need it and without any limitations. The key to our BigQuery success has been our strategic approach and a very phased implementation. It's not just about moving data. It's about transforming how we work. Our migration involved 50-plus markets across seven tenants and three environments. So we've placed a very high priority on things such as user enablement, governance, and feedback loops. Now, in the last couple of minutes, I must have said single source of truth, unified data environment many times. So everything that Blake said and everything that you've heard so far in Google Next talks about the models we build, the agents we train. If that solid data foundation doesn't exist, it's very difficult to make sure that you can be confident about the answers that your agents are providing. You can be confident that your queries are running at an optimal cost. We want to take that step where AI moves from being a graduate to a professor or it moves away from being an assistant to a colleague. And in order to do that, we need confidence in our data. We want to make sure that when we are opening our applications up to a human, an agent, other applications, APIs, we can be guaranteeing that they will get access to only the data that they are permissioned to do. So Choregraph, which powers the data and AI hub within WPP, we put our clients and agencies in the position where they can be confident and we can guarantee the accuracy, the timeliness, the scalability of the data. So I'm going to hand back to you, Blake. Oh, thank you, Sam. I think that's like some great information talking about handling so many markets and multi-tenant environments and having to introduce governance and data products, et cetera. That's a good story. And next, I think we're passing it to Rafa. Hey, thank you for having me. It's a pleasure to be here and share my thoughts about the cloud data migration. So my name is Rafa. I'm working at DPS. I'm leading the cloud for big data and AI analytics. Before we jump into the migration topic, let me quickly draw the picture of what data platform we have. So in DPS, we have in-house built data platform that we build around the open source tooling, around the Hadoop ecosystem. Most of our workloads are Spark-based workloads. That includes streaming jobs as well as regular batch jobs. We started this journey around 2017. And like it happens usually with the transformational projects, initially the adoption was more or less granular. But we invested a lot into education, into improving our foundational pieces. And at some point, it really paid back because we observed almost exponential growth in terms of the volume of the data, in terms of number of data sets. Currently, the platform is around six petabytes of data distributed across 40,000 data sets. So the platform is constantly growing. And because of that, we start facing some challenges. The first very main challenge is the scaling. We had the opportunity to either invest into our on-premise data center or move to the cloud. Because of the costs associated with building your on-prem data center, we decided to shift towards the hybrid design. And currently, we are operating one leg in on-premise data center and another leg in the cloud in GCP. So talking about the migrations, as I mentioned, for us, it's an ongoing journey. We are like half midway through. Recently, we achieved a major milestone. We moved another 20% of our platform to Google. And how we did that, we approached this as a so-called parallel run, meaning that we had our main infrastructure running in Singapore. And at the same time, we set up a brand new platform in Indonesia region. This use case was related to data regulatory requirements. We run these two platforms in parallel for some time. At the same time, we had automated reconciliation engine that was constantly checking if our data is consistent across these two regions. So we were able to leverage on the auto-scaling of the cloud and significantly optimize the costs. The huge benefit that we are observing right now is related to the integration with the entire ecosystem in GCP. So having the integration of BigQuery, for example, with generative AI tooling, with ML workflows, that's something that you can potentially recreate on-premise, but it's going to cost you quite a lot of time and effort. So why not just leverage existing tooling in the cloud? This achievement that I was talking about, this migration was pretty fast. We moved like 1.5 petabytes of data, around 12,000 data sets in less than four months. And the three key lessons learned from this is basically automate as much as you can, especially regarding reconciliation jobs, because for sure something will go wrong. There will be some discrepancies, so automate this process. The second thing is related. It might be a bit controversial. However, I believe that we shouldn't be afraid of modifying the architecture of our system during the migration, because when you're performing the migration, you're re-ingesting this data anyway. So we used this opportunity to fix our legacy hive schemas, and we migrated them fully to icebergs so we can leverage on the better partition handling, log compaction, and stuff like that. And the third, I believe the most important migration, yeah, it's a technical task, but it's all about the people. And it appears in two dimensions. The first dimension is related to the execution. So you need to bring the transparency to the table day one to get all the stakeholders engaged and everyone to be on the same page. That's the first dimension. The second dimension is kind of post-migration. We could enable cool technology for internal and external users, but if they are not using it properly, the benefit is not that significant. So there is this learning curve for business analysis that are using our platform to leverage on this integration with our lands and other tools. I hope you find these three key takeaways useful. I know that from stage it might sound like an obvious thing, but trust me, when you're executing such migration, these trivial things are becoming quite challenging. Over to you, Blake. Thank you. Agreed. Couldn't agree more. And I especially like your comment about changing your file format to Iceberg. I think that one thing that customers are interested to learn is that you can opportunistically optimize when migrating for little additional effort. So this is going to be in migrating Hive 2 to Hive 3 or Spark 2 to Spark 3, which can have tremendous cost savings and performance, or optimizing your file format. You're already going to be migrating these workloads over any way. You can take this opportunity to get more value. And that leads us to Gaganpreet. Thanks, Blake. So this is Gaganpreet. Good morning, everybody. My name is Gaganpreet Rondhava. I'm an AVP of Enterprise Architecture at CNA Insurance. So CNA is one of the largest property and casualty insurance companies in the United States. We have offices across the globe, Canada, and Europe. We have been in this business for 125 years, and insurance is a complex business. A lot of data, a lot of complexity. Over the... In 2024, we underwrote around $12 billion of cross-return premium. So, again, big company, very specialized skill sets, a lot of data. And as part of the CNA, my primary responsibility is in the data analytics space. I lead the data business intelligence, AIML architecture from a platform perspective. I also have global responsibility across all business domains for CNA Canada and Hardy. I also lead the innovation incubation lab at CNA, where I work with a lot of startups and insurtechs to bring in new solutions for the organization. We started our cloud journey, as I bet everybody else did. We looked at all the different cloud providers, all the major ones, and then we picked Google Cloud for a lot of different reasons. One of the main reasons, our on-prem environment, Oracle Exadata, was aging. We were not able to meet our SLAs, the data quality issues, and the things would take days and weeks to run. We wanted to transform the way we do business, right? And we, over the last nine years, actually we have doubled our net core income. As part of that journey, we wanted to pick an environment which is highly scalable, highly performant, we can optimize the cost, and make us future-ready, right? And I think we, by making us, by picking Google, BigQuery as that platform, if we look at all the, you're hearing all of these things in the AI space, generative AI things, that Google is making all these investments, I think we picked the right platform. Key learnings, and Rafa talked about it, right? So data migration is a journey. It's not once and done deal. So we have hundreds, hundred plus applications across our ecosystems. Actually, we have more than that. We moved the data from all of those applications into our cloud environment. And we will talk about it later, but there are different ways of doing things. You can do lift and shift. You can do lift and twist. We did decide to do modernization. And we'll get into detail as part of the Q&A. The key learnings from any data migration journeys is you need to make sure that your business is part of every step of the way. And you are locked in with your stakeholders. That's the only way to drive value. That's the only way to be successful. And that's the only way to leapfrog into next steps. That, I'll turn it back to Rafa. Thank you so much. I think that kind of concludes our introductions and kind of overviews. We're next going to move into some panel Q&A. We're going to give each panelist a question before taking questions from the audience. Apologies. So we'll start with you, Doug. Yes. You framed your move from a constrained on-premise infrastructure to cloud and its near limitless capacity as a positive adjustment. But, you know, that freedom requires discipline. So how did you approach educating end users on responsible usage as well as introducing the correct constraints into your platform and guardrails to prevent suboptimal use? Yeah. So one of my favorite features is the FinOps types of functionality you get within BigQuery. Because historically, with an on-prem server, yeah, you can add CPU. You have peak times during the year or during the day. People were always arguing about whose query was optimal and whose wasn't and who was still in the resources and who should get the resources, right? So, I mean, there's a lot of things you can do in the old days where you can try to move it around and all that. But at the end of the day, you really never knew how everything was being done, who was using the most, and the ROI on those processes. So when we went to BigQuery, we made sure that we set up project-level detail for, you know, not even just for business units, but for sub-business units and focus areas. And each of those project areas, we also set up quotas per day so that we can tell them that if this is what you get, you know, you've got to be able to do your job within this quota. We also told them to label the different processes within their project so that way we could say this analytical went towards this process and this one went towards this process, so for ROI analysis later. And then again, we didn't say that that was your end project, but we could say this is what you're spending. You know, you are the problem. You didn't think you were before, but we now know that you are, right? Or you aren't the problem and your value is great. So like when we get to budget conversations now, it's a lot easier than saying, here, you just get a blanket of IT expenses and we have no idea why, right? So with all that control, we can really be smart about what we're doing, you know, shut down jobs that aren't being performant, add jobs that are, and get everybody the resources they need and they don't interfere with the rest of the company. So it's the best thing ever, at least from a management perspective, is the FinOps functionality. No, that's a great insight. I think you said something important. It does take some upfront work to introduce that organization and that metadata to get the transparency and the observability into FinOps and then also using workload management within BigQuery, which can be quite fine-grained, you know, down to the application team. Yeah, that's good insight. Sam, you know, you commented this in your introduction. Pardon me. That migrations are more than just about tech. And you also talked about how you served a really heterogeneous, you know, customer base, multi-tenant, multi-market, multiple countries, global. So can you tell us a little bit about how you handled platform enablements, developer portal, training, and like what you learned along the way that you can share? Yeah, definitely. And Doug, our budget conversation is ever easy. It's always fun. Migrations are definitely more than just about tech. Our migration to BigQuery was successful because we embraced people and processes alongside of technology. We've placed a huge emphasis on user enablement, governance, and continuous feedback loops. So as an example, for user enablement and training, we wanted to ensure that our teams are equipped to use the new platform. We've implemented a comprehensive training program with the help of Google and our partners. We've included user training with eight distinct learning pathways. And so far this year, we've delivered four of those, Google Cloud Fundamentals, Transformation. We've built our own UI, so there needs to be some specific training around that. And Cloud Digital Leader. So this training is focused at all the personas who would interact with that platform. A myriad of documents have been created, self-start kits, deep dives, best practices. And again, we haven't reinvented the wheel where BigQuery best practices existed, where best practice existed for the partners that we're using. We've used partners and Google support to collate those. And for the cherry on top of that cake, we've trained an agent on all of these documents because it's always easy to point somebody to a document which could be 100 pages long, and they just want to ask a question. So we've got some very self-explanatory, the UI is intuitive to start with, the help, the annoying clippy that used to exist. This is the less annoying version of the clippy. I'm going to move on to governance. So clear governance is a very critical part of our project and the broader transformation and migration. We wanted to ensure order and consistency and a more successful onboarding. So from the beginning, governance features were included in the platform design and implementation. So as an example that the standard tenants which we spin up, they all have predefined structures for built-in governance features like discoverability and lineage, data quality, policy tags, observability, logging. Each tenant hooks into a platform governance framework and that creates a consistent governed platform for us. Strong communication. So I think when we were first setting the program of change and transformation for us, I must have spent time with pretty much everyone in the 50 plus markets. That's global. So across APAC, EMEA, and America. So you can imagine lots of sleepless nights which is why the jet lag is not affecting me as much today. We've got continuous feedback loops including very close collaboration with our partners, our teams, our vendors and Google PSO. We wanted to make sure that we are delivering the platform which will help the clients and agencies maximize media performance. Quickly collecting the feedback. No one wants to share feedback and that goes into a black hole and it's never addressed. So not only did we collect the feedback quickly, we were also very transparent in where in the life cycle that feedback now is. What we've implemented as a feature, the release processes, etc. Making the platform intuitive and easy to use obviously was the most important thing at the start. So prioritizing platform enablement, training and clear communication has helped us create a scalable, maintainable and a well governed platform that empowers our teams. Thanks, Sam. Yeah, I love that there's a recognition that data platforms are platforms too and that you need to manage them from a product perspective and really only build what you need and then iterate based on feedback from your end users. So that's really good. Next we're going to move to DBS and their focus on open source. So DBS made a significant enterprise architecture decision to standardize on open source Hadoop components. So how did that influence your public cloud evaluation process and ultimate choice of a cloud provider and what has your experience been since you've chosen Google Cloud? Okay, so as I mentioned during the introduction section we build our platform on top of open source and we to some degree we still need to keep lights on on our on-prem infrastructure and at the same time we would like to move the step forward and adopt the cloud at scale. Now what's important for us to have a compatibility between these environments so we can use the same tools, the same approaches on-premise and in the cloud and I truly believe that's also at the core of Google. All the technologies that you guys offer via GCP have equivalent technologies that we can adopt on-premises or we can extend Google Cloud to our on-premise environment to keep the compatibility between these environments. Just to give you an example, for example, data protection, format preserving protection in sorry, format preserving encryption in Google that is compatible with open source UBKey. So you can have the same security mechanism in the cloud as you have in your on-premise environment, which is great. It helps us operate our infrastructure, our environment efficiently. That's great. And you have your like-for-like as far as Hadoop, but you mentioned earlier in your talk that now that your data is in Google Cloud and you can rapidly use Vertex and the ML environment to do AI on top of all of that data. Yeah, that's also something we massively leverage right now. Awesome. And lastly, you mentioned your EDW was capacity-constrained and that your ML environment was just based on R and required a modern tool set. So what insights did you get when you moved to Google Cloud given how complex all of this insurance data was, how these legacy environments like mainframe and EDW? you and R, what did you learn in modernizing that into Google Cloud? And just to step back, customers typically can either lift and shift or lift and re-architect or optimize, which is the most common starting point in the cloud. But C&A chose to kind of leapfrog, go directly to lift and modernize, which is a bigger effort but can result in more value. So why did you choose that? What did you consider getting to that decision and how did it go for you? Thanks, Blake. It's great. Thanks for the question. Data migration is a complex journey. I think we all go through the same thought process. There may be a lot of people in this room who are thinking about moving. There may be some who just started the journey. There may be some who are in the middle of it. And there may be some who are thinking they're already done. Every business entity or an organization have a set of priorities and outcomes they want to get to. For some organizations, they want to get out of the data center business. For some organizations, they want to get to their insights quicker. For others, they want to transform the way they want to make that data available and get new insights out of it. For us, the number third was the most important. We were not in a rush to move our stuff to cloud because we were getting out of data center. we were not in a rush to get to the insights quicker because we wanted to give a whole new user experience. We do understand, and I think everybody in the crowd does, that it does take longer when you do that. As part of that journey, but it also gives you an opportunity where you can put a lot of bells and whistles, make the experience a lot better by bringing in data governance, data quality, so all of those historical issues that exist in your environment, you can try to work and tackle them as part of that journey. That journey will definitely take longer, but it will put you in a position where now you can use that data, not only your users, but with all the Gen AI advancements in the world that's going on, we can use Gen AI on that data because the data is organized, structures, we're following industry standard models, it's well cataloged. So that was our intent, and we are on that journey. That's a great insight. And I think this is a perfect time to see if there's any questions for our panelists from the audience. Thank you.