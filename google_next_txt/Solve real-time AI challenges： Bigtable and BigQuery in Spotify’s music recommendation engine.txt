 Hey, everybody. Welcome. Good morning. So hopefully you guys got to check out the keynote this morning and saw some cool stuff. There's a lot of AI in there, a lot of AI agents. This talk should hopefully be a nice follow-up to that because we're going to get into a little bit more of the nitty-gritty about how do you actually get some of this data to those AI agents and make this possible to have real-time AI. We're going to be talking specifically about BigTable and BigQuery and how those two products come together. We'll also be joined by Spotify, who's going to talk about essentially the life of a feature within the Spotify music recommendation engine and how they're leveraging BigTable and BigQuery to solve some of their real-time AI challenges. So I'm Chris Crosby, group product manager on our database team. You'll be hearing later from Sandeep, BigQuery product manager, who's going to talk about building offline features within BigQuery. And finally, we'll hear from Sunkit, a machine learning engineer at Spotify, who's going to, again, give that life of a feature request, of a feature in the machine learning platform. So in today's talk, I'm going to highlight just a couple of the few key elements in our Google's data cloud. This is our comprehensive analytics and database platform. Now, within this data cloud, you're going to have a variety of different database options. So regardless of what you're building, likely we have a database for you. In this talk, you're going to hear mostly about the non-relational side, specifically BigTable. That is Google's low-latency, fully managed key value database that works at an extremely high scale and is great for these globally scaling applications. I'm also going to be showing how BigTable combines with BigQuery, our analytics data platform, and we'll talk about some new capabilities about how they combine using things like continuous queries. Now, what's the benefit of bringing these two databases together? And it's really because it creates this real-time analytics database when you combine the two. So personally, I've been working in data analytics for almost 20 years now, and I can tell you over the last couple years with this race to AI, the conversations have really shifted into a lot more of how do I make data more real-time, how do I get stuff moving faster? So, like, a couple examples of what I'm talking about there. I'm working with a lot of machine learning engineers and data scientists, and their problems are how do I work together on all of these different features and make them more accessible to more people and then get them into user-facing applications faster and test those. I'm also working with a lot of folks in lines of business where they have these really nice looker dashboards that they look at on Friday, and they're asking questions like, hey, how come I can't have that great analysis in the CRM directly when the data comes in? So what we need is data that doesn't just inform but acts. We need to move beyond this passive data and into an era of more active intelligence. So a lot of customers, like Spotify, have already found this powerful synergy between BigQuery and Bigtable. If you scan that QR code on the upper right, there's a whole page that has a lot more details on the innovations we're making to bring these two databases together and why it makes sense and how other customers have done this. But just to give you a few highlights of customers that have already found this unification, for instance, we've had customers that are building fraud detection applications using this combination where they analyze patterns in BigQuery, but then they'll respond to live transactions in milliseconds as the data comes in in Bigtable. We also have customers that are building personalization engines that requires having multiple different types of storage engines. So data that needs to be frequently accessed will stay in Bigtable's roast storage where data questions that need to be asked, ad hoc on-demand analysis can still be asked, but of BigQuery's columnar engine. And then finally, we have a lot of telemetry customers, specifically this dashboard's about vehicle telemetry, where you have these operational metrics that can come in through something like Bigtable's counters, an aggregation framework for very high-throughput metrics that then you can analyze over time with a BigQuery external table. Now, to make it easier for developers to achieve these use cases and work across both Bigtable and BigQuery, we've added a Google SQL interface to Bigtable, which today we're going to announce is generally available. So this means that Bigtable now has the same dialect of SQL used by BigQuery, and this is really great if you are in BigQuery, but you need this really large-scale caching engine in front of it. So if you find yourself with requirements like single-digit millisecond responses or having a consistent and predictable response time, really high QPS, a flexible schema or global deployments, that's where Bigtable SQL can really come in and make it possible for anyone with SQL expertise to build these fully managed real-time application backends. The syntax here has also been extended for a bunch of new capabilities like map types that will let you to continue work with Bigtable's flexible schema, which is often a vital aspect of real-time applications, especially in ML. Now, extending that Bigtable SQL story, today we're also announcing continuous materialized views for Bigtable. If you're not familiar, a materialized view are database objects that store the results of a query as a physical table. That's going to allow for faster data retrieval and improved access. This is especially useful in situations like that fraud detection application we just mentioned, where you might have a very complex query, but you need to access it quickly, or you might want to improve the performance of a real-time tracking system that's generating metrics live, or maybe you just need to roll up time series data from a telemetry system. These are all great use cases for a continuous materialized view, and this is available in an open preview for you to go try today. Now, if you have worked with materialized views in the past and you've found some limitations, like stale data or maintenance complexity, Bigtable materialized views address a lot of that without impacting your user queries coming in. The views are fully managed, and they also work incrementally. So as data comes in, it will put that incrementally into the view. You don't have to wait on batch jobs. These views, unlike other NoSQL databases that also offer materialized views, these materialized views give you full SQL capabilities, so you can do aggregations and other SQL functions. So you can imagine you can have data coming in from all these different locations across the world, but it all consolidates into one view of that data. Even if you have updates or deletes, those will correctly propagate to the view. And this is all done directly within Bigtable. There is no ETLs or external processing tools you need. There's no upcharges or additional SKUs. It's all just part of Bigtable, and it's already hooked into the autoscaler. So with these new SQL capabilities and continuous materialized views, Bigtable can hopefully be a core part of your real-time applications. And we know that it's not just BigQuery that you might need for real-time applications, so that's why we're also extending the ecosystem to make it easier to ingest services, ingest from services like Kafka and Flink. And we're also enhancing our analytics access with Data Boost, Bigtable's offline access layer that can hook up with both BigQuery but also open-source frameworks like Apache Beam and Apache Spark. So with that introduction, I'm now going to hand off to Sunke to tell you about how Spotify approaches building and serving features using Bigtable and BigQuery. Thank you. Good morning, everyone. I'm Sunkeet, and I'm really excited to present about solving real-time AI challenges at Spotify. Before starting about Spotify, Spotify is an audio streaming app with over 650 million monthly active users present in over 180-plus markets. And in terms of catalog size, it's over 100 million tracks, podcasts, and audiobooks. So the scale is not just in terms of user numbers, but also in terms of the catalog size. We have done several studies at Spotify, and what we have found is that personalization is a key driver to retention. So the users keep coming back to Spotify for its unique personalization and recommendation system abilities. So how does this personalization actually happen, right? So if you're a Spotify user, you would have come across several surfaces similar to this, including AI DJ, home, video, and so on. And each of these surfaces are powered by dozens of machine learning models, which are powered through complex recommendation system algorithms. One unique challenge with Spotify's music recommendation system is that content replay is very high for music. So unlike the social media apps with short-form videos where, you know, people consume the video once and that's it, like when they come the next time, they want to see something different, in Spotify's use case, we have found is that the content replay is very high for music, right? So people do come back to their familiar favorites. At the same time, they do want to discover new tracks and items along the way. So we are fundamentally trading off familiarity with discovery. So behind these personalization algorithms are the complex recommender systems, as I mentioned, right? And how do they actually work, right? And if you're a Spotify user, you would have seen, like, if you start a track, you're going to see 15 or 20 tracks similar to that track coming up, right? So say you're listening to Bad Habits by Ed Sheeran, you're going to see 15 very similar tracks by similar artists right there. So the way that works is we take the track corpus, which is 100 million plus items, as I mentioned, and we go through two stages of candidate generation and ranking. Candidate generation is more focused on recall. So it's trying to, like, just filter down those millions of items to about 100 or 200 or about 1,000 or so items. Then the second stage is ranking, which is much more complex. It involves complex ML algorithms, and that's where all of the real-time AI challenges kind of come in. And today I'm going to focus primarily on the ranking use case, because that's where we see a lot of innovation happening around real-time and near real-time cases. And one thing which is unique and common to both of them is the user understanding part, right? And without user understanding, we really cannot make any personalization happen. And user understanding is a central tenet to all of the personalization that we do at Spotify. So today I'm going to focus a lot on the user understanding part and focus on the ranking use case to build up the intuition on some of the real-time AI challenges. So today we'll do something fun. We're going to actually build a new personalized playlist from scratch. Okay? We'll build a completely new playlist. We'll see different kinds of challenges and trade-offs, and we're going to understand them from five different levels in terms of complexity, going from batch inference to real-time inference. We're going to understand latency, freshness, and complexity trade-offs. We're going to take a couple of case studies at Spotify with production machine learning models. And finally, I'm going to end my talk with Goldilocks zone of recommender system models and where I see the future of recommender systems kind of going. And that's the bonus level six. So let's do it then. So let's make a new playlist, Top Genre Daily. Okay? And this playlist is basically going to be taking the top genre of every user and going to show about 20 or 25 tracks for that genre. Right? So if somebody loves rock, we want to show 20 rock songs for this user, personalized for that user, in that 20 tracks. And we've got a new logo here using Gemini. This logo is specifically Top Genre Daily. We want to build this new playlist. We want to make it on Spotify Home. And as I said, when you click on it, you see 20 tracks personalized to you for your genre. Okay? And we already know the top genre for every user. And since I'm not going to focus on the candidate generation part here, we assume that we have those thousand or so tracks already available to us for that top genre. And we want to build this playlist of about 20 or 25 tracks called Top Genre Daily, reverse sorted by the probability of stream. We want to maximize the chance of Spotify users engaging with this playlist. And two requirements that we have are personalized and fresh. Okay? So if personalized was not a requirement, what we could just do, a naive thing that we could do, is take those thousand tracks and just return the top 20 most popular songs. Right? So all users whose rock is a top genre are going to get exactly the same playlist. And that's something we want to avoid. We want every user to get a unique playlist that is personalized for them. Right? And the second requirement is that it should be fresh. And by fresh, that means that throughout the day, as you change your habit from morning to afternoon to evening, this playlist should update. So just an example for that, like you may start your day with soft rock, which may change into hard rock or metal in the afternoon. So this playlist should be updated alongside your habits throughout the day. So it's personalized across users. And it's fresh throughout the day for you. So let's peel the layers and see how we can build this playlist. First is level one, batch inference. Okay? This is something very simple. We really don't have any model yet. So what we are going to do is take the features such as light songs, country, top genre, popular songs, and so on, and we load them into an offline feature store, which is BigQuery. BigQuery, and the way to do that is we first load them in a data lake, which is Google Cloud Storage in our case. Then we do data flow jobs using Shio, which is a Scala binding for Apache Beam. We run those jobs and we load them into BigQuery. BigQuery at this stage is very nice because what it does is it loads the features but also allows the analytics to kick in and you can have data scientists to analyze your data while you are building the model, right? And in this talk, I'm not going to focus too much on the modeling front, but you can assume that we have a batch inference and model training coming after BigQuery and this is a very simple and easy to maintain pipeline, right? We take user understanding, we build a simple pipeline and load all of the features and we do the predictions and all of these predictions are happening once a day for all of Spotify users. So one key thing here is, one big con is that it is stale, so we only update it once a day and we need to run inference on all users, right? Because this is happening in batch, we don't really know who is going to click on top journal daily tomorrow, right? So tonight, when you're processing the data, you need to actually process it for 650 million users and have everything being ready in a database, right? So can we actually do something better? This is where the online feature store comes in, okay? We keep the bottom part the same as level one, which is a BigQuery, which is the offline feature store. We add an online feature store, which is Bigtable, okay? And what Bigtable allows us to do is to move away from the inference of like all of the users. We move them to on-demand inference where only the users who kind of click on top journal daily, they get inferred and you build the playlist for them live on the request path. And we do that by processing all of the features once a day, as I mentioned, and we load them to Bigtable and it's ready to go. So when the service does a real-time inference, it kind of fetches the features from Bigtable in about 10 to 50 millisecond latency plus minus does a real-time inference. Okay, so now we have solved the problem of on-demand inference. We have added another problem here, right? Which is we need to now sync the offline and online feature store. Also, features are still stale. So like level one, we are still doing all of our feature processing once a day. So your liked songs are still updated only once a day, right? So you're not really being fresh. That's where we bring in the real-time features. Okay? And the real-time features, I think, which really work well for Spotify use cases is time of the day and device. And as I said, the soft rock and the hard rock example, in the morning, you're listening to soft rock, in the afternoon, you're listening to hard rock. That kind of intuition and analysis kind of happens through the features like time of the day. And to do that, we take the service and we add time of the day and device, which fetches from the client, and we kind of do the inference. Okay? And the main challenge with this is that we need to recreate these real-time features offline, right? Because all of our models are still being processed offline, so we need to do a complex point-in-time join with all of the rest of the batch features like light songs, country, top genre. We need to join those rows with time of the day and device. And this is where a new problem comes up for us, which is train serve data skew. And I'll give you a real-life example of what I've seen in real life. Our service, the time of the day was in UTC, and time of the day in the batch pipelines was in local time zone, right? This is a very simple example. The format is the same. This is very easy to miss when you're building the models. And this local time zone and UTC time zone basically changes the inference from morning to evening for users, and it really messes up the model, right? So that's the train serve data skew, which is a massive problem, and you do want to avoid it. So to avoid it, we have a, we can use logs, okay? And the way to do this is we take out BigQuery completely from this, okay? We make a canonical feature store which is Bigtable. None of the features get loaded to BigQuery. You just load them, everything, to Bigtable, and you do, when you're doing the real-time inference on the service, you actually log all the features and collect them in Google Cloud Storage that you can then offload to BigQuery if you need to. So what this does is it completely removes the train serve data skew altogether. You're not really syncing, if you see the previous diagram, you're syncing this BigQuery Bigtable, right? And now, you remove the sync part, you just have Bigtable, and what it does very brilliantly is it completely removes this separate path between model training and inference and kind of unifies everything in a single feature store, okay? One con it actually ends up adding, though, is a log and wait is needed for new features. So if you have a new idea, your data engineer is like, okay, I have a new feature now, and so you would have to wait seven days or 14 days to build up all of the data to start training the model, okay? And this is the inference-first approach instead of the training-first approach. And you need to run the fake inferences till the ML model is ready. And I want to give a real-life example of this and move away from music streaming for a second. And imagine you are an architect, okay? And you love making blueprints for houses, and you do them offline in a laptop, you make a beautiful blueprint, and you then send them to a construction crew, right? And their job is to take that blueprint and they're supposed to build houses from it. So what's going to happen inevitably is your blueprints might expect certain materials to be there, but when the construction crew kind of goes and starts building these houses, your materials might be a little bit off, right? You might miss a certain doorknob or something like that. So you're going to get a, this is what I mean by train-serves-queue. Your blueprints and your real life is slightly different. So what this level four actually ends up doing is it takes out this problem, and what you do in the architecture example is you go to different zip codes, you start collecting all of these materials first, okay? So you go to a particular zip code, you see what are the doorknobs available, what's the different kind of lumber, what are the different things available, you log all of them, and then you come back to your office and you start actually making blueprints from the logs that you found in that zip code, okay? So that completely removes the skew, right? Now you know exactly the materials that are available in that zip code and you build the blueprints from that, okay? And this is one of the key takeaways of the talk is that you might need to remove this syncing step and use logs instead of doing this syncing, right? And in the architecture example, you completely remove this extra step of translating the blueprint to a particular house and similarly over here, you kind of do that by removing BigQuery and you kind of do everything by Bigtable. So this is the level five, okay? So level four does quite a lot for us, it already adds batch features and real-time features. Over here now, the question is, if in, say, someday your friend introduces you to GrungeRock, okay, this is a completely new sub-genre in Rock, how will our playlist update to it, okay? So now we want to add artists heard in last 30 minutes. So we want to, like, really, really become personalized throughout the day and change the playlist as the artist kind of, as your activity changes throughout the day. And this is an example of near real-time feature where a slight delay of a few seconds or minutes is okay, but it's still, you do want to maintain latency low. This is also sometimes called as streaming feature, right? Streaming or near real-time is kind of synonymous. I prefer to call it near real-time because it makes it very clear what we mean by this, okay? And most of our architecture at this stage is very much similar to level four, which was this. Now we are just adding a pub sub-component, okay? And that pub sub-component is going to basically take all of your activity and it adds them to a queue. We have a separate data flow job that uploads to Bigtable. and Bigtable has basically different column families, right? So at this stage, you can imagine that the column families representing batch features are getting updated once a day and the column families which are representing the near real-time features are updating throughout the day. So a very active Spotify user is going to get their column families updated several times throughout the day by the data flow job at the top. So we have added freshness and responsiveness. The main thing that we have, the con that we have added is complexity. Right? So it is definitely more complex. We have pub sub-component, we have a data lake, we have different data flow jobs right into the same Bigtable. But what we get from it with all this added complexity is that now all of the two requirements that we wanted with Top Journal Daily are met. Right? So we wanted to make it personalized and we wanted to make it fresh. Now this architecture allows us to update throughout the day it's personalized and we are able to update to user activity throughout the day. So the natural question to ask is which level is the best? Right? And I think you can guess my answer. It actually depends on the product requirements which kind of determine the features. So some features such as language are perfectly okay in batch. You don't really need near real-time updates throughout the day. People's languages are not going to change in like minute latency or whatever, right? Like you can just update it like once a day. But the features such as artists in last 30 minutes they need the real-time or the near real-time component. There is no other alternative. Right? There is another set of features which are in the gray area which is kind of the last five artists heard. Okay? And this is a kind of feature that can be implemented in any of the five levels. So you could implement it in batch but what will happen is when the batch pipeline kicks off it takes the last five artists. So it's going to reflect some version of the user which is behind. Okay? It's going to be say midnight, UTC so the last five artists you heard at that point. The level three which is the real-time features is the freshest but you're doing complex SQL queries on the fly. But the level five with near real-time features is the best option as it balances freshness, complexity and latency. There are further trade-offs in near real-time features with materialized views as Chris was saying it offers lower latency than calculating everything on the fly. We also have two options around how to actually process this data. Okay? One is that every time a user listens to a new song we re-trigger this pipeline and we update the last five artists that's more expensive but more fresh. We could also re-trigger 10 minutes after a stream. So every time you hear a song we wait for 10 minutes and then we re-trigger this pipeline and that is more efficient because we're pooling in the artists that you're hearing in 10 minute blocks. Right? But as you can imagine it's slightly unfresh. We are like behind by about 10 minutes. So there is always some level of trade-off that we are making. I think I hope I'm making this clear with some of these examples and I think what we have kind of with all these five levels we have been looking at like different aspects of this triangle. Okay? There are three main characteristics of recommender systems that are desirable. Low latency, low complexity and high freshness. When we started with level one batch features we were basically just doing low complexity. Low latency was not there and high freshness was not there. Then we added an online feature store with Bigtable that added the low latency component with low complexity. We were still missing the freshness because we were updating everything once a day. Then we moved to real time. We added a lot of freshness but when we do the historic features in real time we kind of start losing on latency. That's where the near real time and the streaming features kind of come in which is a level five which is kind of combining low latency with high freshness but we give up on complexity. So we need to put in a little bit more effort in kind of hooking everything up. So now we are going to look at the case studies at Spotify with real life examples. First is artist preference model. Okay. And this is a mega hit mix and you can see for me it's showing Ed Sheeran at the top even though Ed Sheeran is not in my three or four top songs and the way it's doing it is it's going to take my historic listening and it's showing Ed Sheeran is most likely to make me click this playlist and stream more from it. It uses a level two approach which is basically the batch pipelines which update once a day. All of these features are uploaded to Bigtable and the online inference is run fetching features from Bigtable. So whenever I click on mega hit mix it's actually doing this computation live and it's showing it's pulling from this model and does a real time inference and shows up Ed Sheeran over here. And there are many many models similar to this. We have models like very similar to this building different other things across different surfaces and cost and complexity add up very quickly if every model separately calculates similar features and uploads to Bigtable. Right? So if every model at Spotify starts to process country, listening, demographics, other features again and again and they all start writing to Bigtable you can imagine the complexity really, really growing and really making a problem. So the question is can we do something better? This is where user representation embeddings kind of come in. So instead of building all of these features separately we build a single representation model which is the representation learning based approaches and which takes the users past listening their batch and near real time features and builds a foundational embedding which is around 128 dimensions and that 128 dimensional foundational user embedding can then be used by all of these different surfaces without really kind of doing all of the streaming and real time complexity they just call this embedding which has all of this information within it so it creates a base understanding of users that can be reused by many models and it combines the batch and near real time features so this is more of a level 5 approach we do use a lot of logs for this so whenever the features are coming in we log everything and then we train the model and the updates are being made throughout the day as users engage with Spotify it also helps level 5 also helps with the cold start problem because whenever a user kind of a new user comes in we really don't know anything about them and then we basically start adapting to new users interests as they kind of start using the platform we start to see what artists you're liking what artists you're not liking and kind of adapting to that it also follows the transfer learning approach which is basically keeping this embedding stored in a shared big table cluster that we can scale up and down via auto scaling and the downstream models add specific features as necessary and this allows us to really scale personalization and really distribute personalization across multiple models at Spotify finally I do want to take a couple of minutes to talk about this Goldilocks zone of recommender system models and this is a really important slide because it kind of summarizes and condenses everything that we have talked about today and as you can see we have on the y-axis is how much data freshness do you want okay so some products may be okay having data freshness at one to two day level but some other models may need more freshness on the x-axis is how long can you wait okay and as you can see one thing to really pay attention to here is even if you're able to fetch features at very low latency your freshness may be very bad okay so for example we keep all of these batch features in big table we fetch them they may be representing a version of a user which is one or two days behind okay so you it may be just using your follows and likes from one or two days behind it's not really up to date right so to really be fresh we need to be in one of the three top boxes with either real time or near real time entrance which is basically either using like real time or near real time features in big table or using pub sub and perhaps for some model an RT inference is actually better so in today's talk I focused primarily on real time inference so the level four and level five are all using real time features but my point here is that maybe there is a better in the future there may be better that we use near real time inference and there is one reason for that as we saw in the keynote LLM based agents will play a big role in various systems in the future including recommender systems and one thing with agents is that the latency is really low it takes maybe one or two minutes for the agent to come back with the results so we want to move off the request path so I think what is going to happen is we may need to be in the three boxes on the right which are two minutes to a few hours waiting basically right because you can't really process agent tick responses in about 50 milliseconds so the intersection of the freshness and how long can you wait ends up being in the middle which is the near real time features so you are going to move off a lot of this logic of the request path and move it into near real time so as you can imagine say you are at spotify you say I'm at the beach the agents are going to work in the background and they're going to update spotify behind the scenes so a lot of the complexity is going to happen behind the scenes in near real time we still would need some real time components to fetch that but it's going to be a hybrid approach which combines the real time inference with near real time features so this is it from my side I'm going to pass it off to Sandeep thank you Sankit that was super insightful now in the next few slides what I'm going to do is I'm going to pick up a pattern that Sankit mentioned here I think it's level two and describe how to do that in Python using BigQuery and Bigtable why Python well Python is probably the most popular language when it comes to data science and data analytics but here's an interesting thing about Python and Python data science as long as you're working in a small data set size and your compute requirements are small everything works really really smoothly but as soon as you relax one of those two dimensions maybe you bring in more data to train your model or maybe you need to train on a lot of features at that point we see that most of the data practitioners switch from a single notebook experience and they adopt one of the distributing computing frameworks now these frameworks bring in their own challenges first of all you have to manage some new infrastructures so there are high costs in setting up and managing this infrastructure second you now need to retool and rewrite the program that was working in a single notebook you have to learn new libraries and you have to squeeze out the performance from these infrastructures and these frameworks that are new to you so you have to learn all those tunables but if you really step back what's really happening here is you're spending more time on the infrastructure details and less time on the business logic which is what you want to do and this is the problem we wanted to solve when we launched a new Python open source library called BigQuery data frames or big frames to be short this was launched a year back at the it enables Python data science on a large scale data so how does it work what big frames really is is a transpiler so it gives you as a user pandas and scikit like APIs but behind the scene it is transparently converting these into SQL BigQuery SQL BigQuery machine learning SQL we are building the SQL tree on the client side and lazily pushing it down to BigQuery for an optimal execution so now what's happening here is you as a user are getting the APIs that you are used to but then the BigQuery distributed computing power is made available to you in the same notebook what's even better is data never leaves BigQuery so a data frame in our abstraction is a temporary or a permanent table on BigQuery so now you have access to hundreds of terabytes of data in the same notebook where you are processing tens of gigabytes of data so the real story here is you can go from gigabytes of data to terabytes of data with a simple switch of a library and you can also bring in your custom functions now we have a session dedicated to this tomorrow another breakout session but let me connect this to what we were discussing here I'm going to pick a pattern where you can table in near real time how do we do that in Python when it comes to feature preparation we really think of it in two steps you typically begin with a lot of raw data and you want to clean it up you want to prepare it so that in the next step you can featurize the data so what we recommend you starting is with starting with our bigframes.pandas library or package you can import that package and start loading your data into BigQuery data frames data frame this is where you can now apply typical pandas like operation aggregating joining splitting your data frames getting your data ready for the next step now that your data is ready you can move towards transforming and featurizing your data this is where you can import our other module which is bigframes.ml which emulates scikit APIs and behind the scene we are utilizing BigQuery's machine learning for a lot of this feature feature feature transforms in fact we have some of the featureizations readily available with us like one hot encoding or min-max scaling but you can bring in your own transforms as well you can create a column transformer you can create scikit pipelines you can train models you can evaluate models you can also export models so at the end BigQuery table now you can transport them to online store like Bigtable but before I do that let me also give you a quick preview of something exciting coming in BigQuery we not only want you to get features out of your structured data but we also want to make sure you get those insights from your unstructured data and getting insights from unstructured data has always been challenging and BigQuery is going AI query engine this is where we bring a lot of the LLM techniques and technologies within the core BigQuery planner and query execution so that you as a user who are writing SQL for your structured data can also get semantic insights from your unstructured data with the equal ease AI query engine will be coming out in experimental mode very soon you need to use that QR code at the top to get in but within Python we already have a version available of these AI operators for you to get a sense of what this means and you can use that to also get features from your unstructured data so now your data and features are ready from your structured data as well as unstructured data this is the time you want to push it to Big table and this is where we recommend so this BigQuery table that you just prepared you can load it into a streaming data frame and simply call an API to Big Table and it will start streaming your features as soon as they get ready into Big Table behind the scene we are utilizing BigQuery continuous queries now G8 feature to achieve this but then that's an implementation detail for you right you as a Python user you are focusing on the APIs that you are used to while we do all the heavy lifting if that sounds interesting to you Chris has a demo for you Chris over to you all right so we've been talking a little bit about you know music recommendations but we've also thrown a lot of new features at you and so what I'm going to do quickly is just run through a very short demo that is going to set up this entire architecture okay so what we're going to start with is that batch inference the AI operators that we just talked about in big frames so we go ahead and set this up and we are in a big query studio notebook and the first thing I'm going to do here is I don't have access to Spotify's catalog but that's fine we can bring in some open music data sets that's publicly available and this will load into a big frame and if I take a look at what's in here you can see it's just some track information some artist information I have lyrics and some information about the songs which I'll use to essentially get a start on some of these playlists that I'd like to go and generate and so what we're doing here is we're going to use that AI operator and I'm going to take a look I'm going to use a semantic filter and I'm going to look for lyrics to discuss love that are also songs that we know are danceable so combining the structured and unstructured information and I came up with a nice little playlist of things like Timberlake for dance music the next operator I'm going to use is this semantic map where I'm going to look for female pop stars my data set has no information about gender or who's a pop star but the LLM has that knowledge so we can use that and directly apply it to the data and say here's some female pop stars and here's their playlist and we'll see if that works we'll find Grace Jones in our playlist now this is my favorite operator right now it is the semantic join so what I'm going to do here is I'm going to actually take a bunch of demographic information that I have specifically in this scenario I have people that have just moved somewhere and I'm going to look say okay where did this person move to and can I actually join that on lyric data where I'm matching up by the state here and so if I do a quick look at what be a great playlist for somebody right the next playlist we're going to create I have a whole Beatles catalog which is all different types of music you know the Beatles were pretty prolific here but I'm going to use semantic clustering to break all of the Beatles catalog into three distinct playlists and so I'm going to go ahead and do that with this clustering capability and then I want to know what's up with what you think this playlist is about and in this example it found that this playlist is primarily about relationships and heartbreak and so we can save that as a potential playlist great so I have a bunch of offline features that I've built how do I synchronize those up to big table how do I turn these and promote these into key value pairs the first query that you're looking at here is consolidating all of big frame all this is a little config I can add additional pandas capabilities within this and it's going to synchronize everything up with big table I don't have to keep this notebook running this job is going to continually run if you look under the hood of what actually happened here is generating this continually running job that's exporting data any changes into that table are going to be reflected up in big table so we can go ahead and take a quick look at big table and again we're going to run SQL against this the same SQL dialect that I'm using within big query and I can take a look and make sure this playlist came up and they certainly did and again this is a continually running job against that table so even if I came back to big query and I inserted another playlist from another process or anything outside this is just a quick insert statement directly from SQL but in a couple seconds that result that that playlist should also come back and show up in big table which I will hopefully be able to pull the key of that playlist and there it is so that was synchronized up and so you don't have to worry about managing ETL jobs now we're going to add some of those real time information that we had talked about that Spotify uses it's really important and so to generate these and to fake these I'm essentially going to use a Kafka job that's going to start sending in Kafka into big table via a connector I'm using managed service for Apache Spark to do this and I'm going to be using a Kafka connect cluster the big table sync is not quite ready but it will be there soon as you can see in the UI and so that will be just a native connector that you can start to use to move your Kafka topics into big table directly so I can see this data coming in and just do some quick counts on it so there's about 180 so far 180,000 so far now I have and so what I might want to do is take data per minute and I'm going to look at how many likes are occurring per minute and that way I can react if how our recommendations are doing make sure we're good there and you can see this query does chug along and it takes a bit to come back but what I would love to do is for each minute to have this stuff save that off and now I can just run SQL directly against that view what's happening here is it's going to initially do a back load of information everything in that table that's happened it's going to synchronize that to the materialized view I'll have a copy of the data that I can now query against and this is currently still back loading but what I do is just like I would any other SQL statement I'm going to query against that view that is being incrementally updated and you can see now I have much snappier results than you saw before where it was taking some time to crunch that and these are live numbers still coming in so what that would let me do is now I can create these real-time analytics dashboards where I can come in and run queries against this and this is sped up it's in minutes but I sped it up per seconds and so every minute we're at 180,000 likes are coming in so we know things are going well but then the first time that we have a change in that okay all of a sudden we had this big drop off what happened people aren't liking our recommendations as much and now we know we can go back into BigQuery and build some new playlists so that's about a 40 minute demo that I crammed into six minutes so if you want to learn more two other sessions you should go attend tomorrow there is another big frame session if you're interested in that and there's another big table session that's going to get a deeper dive into these materialized views and other aspects of the SQL do to to to you