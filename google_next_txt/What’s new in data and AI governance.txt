 Again, welcome. Thanks a lot for joining us at Cloud Next. We have an amazing topic today. We want to talk about what is new in governance, in data and AI, what's happening, what are the innovations that we are doing. And I'll be joined by Vinay from Levi's and Arvind from Verizon, and they're going to share their stories around governance. So I'll just quickly go through what we are thinking at Google and Google Cloud from a governance standpoint. One of the biggest things that is happening right now, and we have seen this in last year, is AI is truly transforming how we do things. It's not just transforming industries, but it's transforming lives and the world, the way we look at it today. And what happens is that with this, we know that every customer is embarking on AI projects. And we also know that this is a stat from Gartner, which actually says that 60% of the AI projects are bound to fail. Okay? Because I think, you know, it's not easy. When you do AI, when you do Gen AI, when you build agents and all that stuff, you know, it's still very nascent. And one of the reasons why these projects are bound to fail is because governance is really, really, really important and pivotal to make this project successful. And governance is scary. Why is it scary? You know, governance was always scary. Governance is not a new word. It has been there for a while in data management. Since the time immemorial, when data management has been around, governance has always been there. Right? But AI is making it scary. Why? Because now you can actually interact with this complex data management system through natural language. You can ask questions. Now, anybody can ask questions. Anybody can speak the language of data management systems and they can ask questions. And when you ask questions, you have to be extremely careful because you don't know what answers are going to be given by the system to the people. Are these right people? Do they have right privileges? They have right authorization. Now, governance is always hard, right? It's not going to be easy. First is silos. Data comes in a lot of variety, shapes. You know, we know we talked about the three Vs long time ago, 10 years ago. It's always in silos. When you go to enterprises, you know, data is distributed all across the enterprise. It's in different data management systems, different data warehouses, lake houses, et cetera. The second thing is the change that is happening is massive and it's rapid. What we used to do one year ago is completely changed. What we spoke about at Google Cloud Next last year is completely changed. Now, we are talking about agents. We are talking about, you know, agents making decisions around, based on data, right? And then the culture. Nobody speaks the same language when we talk about governance because governance is not just a product problem. We always talk about it. It's a product, people, and a process problem. So when you don't have a great culture of data within the enterprise, right, then it becomes extremely hard to actually govern your data and AI assets very well in an organization. And why do you need governance? Right, I'll talk about all the innovations that we are doing and we have follow-up sessions on it. Why do you need governance? If you can't understand your data, if you can't trust your data, and if you can't discover your data, then there is no point you can, like, you know, even if you are actually adopting to all these Gen.AI technologies, building agents, and all that stuff, it doesn't matter in an enterprise because data is the key to make these decisions that will affect your top line and bottom line as an enterprise, right? And one of the things that governance does is if you do have governance baked in, then you can simplify your data management first, right? It becomes very easy to take these massive silos of data and manage them at scale. You will be able to get to insights faster because right now, if you look at the data lifecycle, you start ingesting the data, and from there onwards, you have the, you know, prep and the curating the data and then going forward and then trying to, like, you know, model the data in such a way and then using your ML models and everything to get insights. But that journey typically takes about three to eight months in an enterprise. That actually drastically gets reduced. Then you actually are complying to the changing regulations that are happening. So if you look at, you know, GDPR and the new EU AI Act or anything, the compliance is going to be critical because the, and how do you actually comply to all these new regulations that are going to be evolving? So if you have governance foundation laid properly, then you don't have to worry about compliance. And finally, if you really want to embark on AI and ML projects, you have to have a strong governance foundation. So what are we doing? Why do we think it's important at Google, right? The first and foremost thing is governance is pervasive and universal. It has to be universal. It has to be pervasive. You shouldn't be thinking about it as an afterthought. You should be, it should be seamlessly integrated with your data and AI platform systems, right? The second thing is it has to be intelligent. All the manual way of doing things and extracting metadata out of the systems and then trying to make sense out of the metadata, trying to explain the data, trying to secure the data, trying to share the data at scale. All this journey has to be automated. You cannot do the same manual things that you used to do 10 years ago, right? And finally, you want to have an architecture which is completely open, which means that any kind of formats that you have seen, the whole evolution of iceberg formats and everything and the runtime catalogs, the catalog has to be in the front and center of your data strategy and it has to be open. It has to support interoperability so that you are not actually logged into a system. So we have actually taken all of these things and we have laid the foundation at Google from a governance standpoint. So what are we doing? First thing, there is more detailed presentations that you'll see and you'll learn more. But the first thing is we are bringing in governance into your BigQuery platform, right? So Dataplex is the governance foundation for GCP and then what we have taken is we have actually all the governance capabilities in BigQuery is powered by Dataplex and we are brought into the BigQuery platform as BigQuery Universal Catalog. And what is Universal Catalog? Universal Catalog is basically your Dataplex catalog plus your runtime metadata. And all of that stuff now comes into BigQuery and what does it do? When we talk about catalogs, catalogs again is the nerve center and it is actually the heart of your data platform. And there are three types of metadata. What we have been able to do is when you looked at Dataplex, Dataplex had the informational physical metadata in the system. Your table names, your column names, your profiling information, so on and so forth. Then the second thing is in order, nobody understands the physical metadata so we brought in the business metadata as part of the platform. So now you can actually take the physical metadata and look at the data and we can augment the metadata and then we can provide business semantics on top of it. That is baked into Universal Catalog. And finally, we have brought in the runtime metadata like your iceberg formats, everything, is actually part of this Universal Catalog. So this Universal Catalog is powering all your governance journeys on data analytics and BigQuery. and BigQuery. So I want to, I'm very excited, I don't know if you have been to the keynote today, you know, but one of the key things that is happening in the data and AI platform is that there is, you need to have this, what we have built is a central nervous system for BigQuery. It's called the knowledge engine. What knowledge engine does is we can automatically harvest all the metadata from all these systems within GCP, right? And once you have the metadata, we actually use knowledge engine to augment the metadata so we can enrich the metadata, we can explain the metadata. Not only that, we have gone above and beyond. Obviously, based on your profiling scans, we can tell you all the relationships. This is all relationship discovery has always, always been there. But the most important thing that we can do is we can actually look at your data and data sets and then we can derive insights in natural language. So, let's say, let's take an example of, you know, if you had like, you know, 50 tables in a data set and if you had to write a SQL query, it takes a lot of time to write a SQL query. Instead of writing a SQL query, what we have done with the knowledge engine is that we can power data insights and it will give you a natural language question. It will tell you, you know, you can ask questions like, what are the top 10 customers in your business? Prior to this, if you had to do it, you always had to face the cold start problem because you don't know where to start. So, what knowledge engine is doing is it is actually bridging the gap between your physical metadata, the business metadata and accelerating your insights. And one of the key things that we are working on right now is, and you will see more, you'll learn more about it on Friday is, you know, we are launching semantic search, which is natural language-based search, which is powered by knowledge engine. We are actually going to augment all the physical metadata through knowledge engine. We can actually provide insights through knowledge engine. We can actually give you quality recommendations and tell you exactly if your data is of good quality or not through knowledge engine. So, this is a huge leap in the way you have been doing governance because, again, I want to say, you cannot go back and do the manual ways of governing your data anymore. You have to automate in this new era of AI and what we have done is we are providing you all the platform capabilities integrated with Gemini so that you don't have to do anything and then it will actually start explaining the data better within the enterprise. So, with that, I am going to, I know my clock is not working, so I want to make sure that Vinay has time. So, you will see that we have an experience that is coming in BigQuery so that you don't have to leave BigQuery and go anywhere and then you can actually do governance within BigQuery. And, you know, in order to learn more, you know, there is a session on Friday and then we'll have a lot more details. With that, let me hand over to Vinay who is going to talk about their journey. Thanks, Jai. Hi, everybody. My name is Vinay Narayana. I work at Levi's Strasen Company. I run their data and AI platform engineering. Today, I'm here to talk about the Levi's data platform journey. Let's dig in. Most of you probably know Levi's as a brand, as a company. What you might not know is that Levi's is a 172-year-old company and it's the inventor of blue jeans that many of us are wearing today, including myself. For the most part of Levi's history, Levi's has been a men's denim brand and they have been sold predominantly in a U.S. wholesale. Over the past decade, we have been very intentional in transforming this company. One of the key priorities for our CEO, Michel Gass, is to make this company DTC first while also ensuring we maintain wholesale as a critical part of our business. As a result, Levi's has been going through lots of transformations and one of the key things is for us to become a data-driven company. We aspire to make our, or empower our business analysts to have access to real-time data that is also accurate. Today's session is primarily going to focus on how we went on this journey of making Levi's data rich and insights poor to truly being insights rich. Before we dig in to where we are with the current-day platform, I want to quickly walk you through our legacy analytics ecosystem. As you can see, on the left side, we have the various data sources. We are a big SAP shop, so SAP becomes a big data source for our data platform. We were moving data from the various data sources in numerous ways, and that is represented by so many arrows that you see. We started down on our Google journey back in 2020, but then we lacked the overall analytics strategy within the company. As a result, there were a few business units that created their own data platforms within a SQL server, a data warehouse, rather, in a SQL server. They bought tools like Alteryx and Dataiku. They moved data from point A to point B in any way they can just so they can meet their business demands. As you can see, our visualization ecosystem also was pretty scattered with too many tools, and this is a classic example of a company without any analytics strategy. Before we embarked on our journey to build a new platform, we did a discovery of the various user challenges our customers or our stakeholders had. One of the primary things was our business users didn't know where the data lived, and if they knew where the data lived or the data source, they didn't really know who was the owner of that system. And if they knew the owner and they somehow got access to the data source, there was really no documentation for the data, so they wouldn't know what those columns or tables really meant. And they used that to create dashboards and insights, and you can all imagine how big of, I mean, how the quality of that dashboard could really be. That led to, obviously, trust issues in those data sets. Our users also used Excel spreadsheets to run really, really complex formula, really, really, really complex map to come up with insights. And some of these spreadsheets had numerous worksheets with very complex formula in it. Again, we can all imagine how bad it is for the analyst who actually maintains that spreadsheet month over month and year over year. So with these user challenges in our mind, we wanted to come up with a vision for the company so we can all rally behind it. We want to enable our analysts to have access to or drive accelerated insights by giving them access to data that is easily discoverable, easily understandable, and also it's seamlessly integrated with the rest of the analytics stack. This is very important because most of our analysts, as you can imagine, are non-technical folks. You want to package your entire data platform solution in one way so it's all easy for them to access the data and the various components of a data platform. As you can imagine, a data platform could have multiple components like a data lake or a data warehouse or data governance layer, et cetera. You want to package all of these things in one way so it's easy for your analysts to be able to use the data and also work with it. And this led us to building this system design for our organization. You still have all the data sources on the left. We ingest the data in one way. There are no so many lines like you saw in the legacy ecosystem. We ingest all the data into Google Cloud. We use Google Cloud Storage as our data lake. All our unstructured and semi-structured data lands there. Then we use technologies like dbt to process the data and we create a data warehouse layer in BigQuery. Mostly it's a star schema. And there are a few other abstractions we build that I'm going to talk about on the next slide. We use Dataplex for our data governance layer. Every data product, by the way, we build data as a product. Every data product comes with a data lineage, a data profile, some data quality checks, and all of that. We also leverage Dataplex for user and security management. And all these data products that we build, we publish them into Analytics Hub, which is our data marketplace. We use Analytics Hub a bit more creatively by publishing our data products to our internal users within Analytics Hub. We have some use cases to expose our data to external customers, too, but that's still a work in progress. We also leverage Gemini. We have several plans around how we want to leverage Gemini for our data platform. I'll talk about that in a few minutes. We use Data Fusion for, again, a drag-and-drop interface for creating data pipelines or simple aggregations for our less tech-savvy analysts. And that's our entire data platform. We also have consolidated all our visualization tooling. We are looking to consolidate all our visualizations in Looker, Looker Studio, and Looker Core. We are also leveraging the WordX AI platform in Google Cloud as our ML platform. So all our data science models are built on WordX AI. Since we use WordX AI, we also use WordX Feature Store. And if you realize this, WordX Feature Store is built on BigQuery. So all the features that are in the Feature Store, we expose those also as a data product for any of the analysts or other engineering teams if they want to leverage those features that we built it for us model. And also data science models obviously leverage the same data platform. This data platform is for both analytics for ML as well as any other enterprise apps that want to leverage our data products. Excel is still on that list just because Excel, I believe, is going to be there for the foreseeable future. Our analysts love using Excel, so we're going to continue to have that within our portfolio of reporting and analytics tooling. And lastly, observability is across the stack from the source to everything in Google Cloud. Now, this slide, I'm going to talk a little bit about how we leveraged Dataplex to implement a data mesh. And this is our interpretation, Levi's interpretation of a data mesh. Firstly, I want to talk about Levi's business. We have a pretty big supply chain operations organization, and then we have our e-commerce and consumer. Our supply chain, as you can imagine, we have thousands of stores across the globe. We divided our supply chain operations into three data domains, sales, distribution, and material management, retail, supply chain and planning, and lastly, e-commerce and consumer. Each of these data domains has a leader within my organization, and many of them are here today. And each of these data domain leaders are experts in these individual data domains. They know the data really well. They also know their stakeholders really well. They know what the stakeholders' use cases are for the data, what insights their stakeholders are looking to get out of the data. This is very critical for us because this was lacking in the past. With everybody doing everything, this became a super big hit, if you imagine, or if you will, within how we implemented the data mesh and my organization itself. The business stakeholders are very happy that they know who to contact for any of the data products that we build. They have an escalation point, and so on and so forth. And digging into a little bit more deeper into the middle part of the slide, we use a star schema like I alluded to earlier in BigQuery. That is where we have all our facts and dimension tables. On top of that, we came up with an abstraction called a data mart. A data mart is just a thing that Levi's does. For example, orders. A data mart is a table or a group of tables that has all the orders information. All the sales data will be in a data mart called sales. Any web analytics data is in a data mart called web analytics, and so on and so forth. Each of these data marts are created within their own data domains. Then we built yet another abstraction layer called a data product. A data product provides a business value to the company. It could be something like global point of sale. It could be the performance of all the stores across the globe. Several other data product examples that you can imagine. Most of our data products have an enterprise dashboard associated with them. Because these data products provide value to the company, there's also an enterprise dashboard that provides insights based on that data product. And all of this is self-service, and I'll show you when I show you a quick demo about how this self-service data product layer works. Then I will spend a couple of minutes on the bottom part of the slide, which is our self-serve data infrastructure. This is a work in progress. We have many users that want access to this data platform. This data platform has been around for about seven months, and we are onboarding users pretty rapidly at Levi's. And every day, there's some user out there, some part of the world that needs access. So we want to be able to give them a way to get access with a single click. We want to be able to give them an ability to create buckets or get access to buckets with data if need be. All of that through a centralized portal. Then if you look at the top, this is the governance team. So we have a central team that is focused on creating the data model. This team is focused on how the architecture should be defined. Let's say if there is any streaming use case or if there is any other new use case that we are onboarding into the company, for example, metadata management. This is something new that we are working on. How we do source code management, the CICD, how we do the code reviews. Everything is managed centrally through this organization. Governance, infrastructure modernization, we have quite a few components within our legacy infrastructure where we use GKE or VMs quite a bit still, for example, with Airflow. We want to migrate all of that to serverless capabilities. And I'll talk about that a little bit in a few minutes. And lastly, security. So this is the centrally, or this is the central organization that kind of does all the most, well, things that are critical for all of these data domains. And we push these best practices and architectures, et cetera, from this central organization. And lastly, we have an incident management team to coordinate all the incidents within the company. This is where we are with the data platform. Firstly, we are 50x faster than our legacy data platform. And this is on the low end, by the way. A significant number of visualizations are 100x faster compared to the old data warehouse that we have or old visualizations that we built. We have over 700 users already using the platform on a daily basis. We have 45 new data products or 45 data products overall. We are adding about seven data products every month. We make a release of the data platform on a monthly cadence. And each release goes out with about seven data products along with the business impact they give the company. We also have a KPI where we say every visualization that we build has to download in seven seconds or less. Hardly 5%, I would say, hit that seven second or less mark. Well, seven second mark. 95%, if not higher, download within a second or two thanks to the BI engine, which works miracles for us with getting our dashboards to download very, very fast irrespective of where you are located in the world. Every data product that we release has three data quality SLOs. One, data freshness, data availability, and data quality or data accuracy, sorry. And we make this all visible to our users and I'll show you how we make it transparent for our users to get information about the SLOs. One of the critical things after we built the data platform was educating our global team and most of them were new to the cloud. My team, along with Google, we created eight hours worth of training material, training videos and exercises for our users to take and we identified a few trainers who would go train their individual teams. So we train about 100 of those people across the company and that has led for us to adopt the platform much faster. 100% of our data products come with a data dictionary, data lineage, and data accuracy checks. Lastly, we have an operational data mesh. The one that you saw is all real. I want to quickly walk you through a demo so you can see everything comes together, how everything comes together. So this is our landing page. with all the data products in Analytics Hub. There are many of them, so a typical analyst would come down here, look for the, just browse for the data products on their own if that's what they want to do. The other option for them is obviously search. For example, this analyst wants to know what kind of shipment-related data products exist. They go and look for shipments and we show three data products for those. next, they want to look at the point of sale and they're very interested in the global point of sale. So they open up that data product and you can see there is a subscribe button. So they go here, they hit the subscribe button and when they do that, this data product is available to them as a link data set. It's not a copy, it's a link. So we are cost efficient by doing that. If they do want to mess around with the data product, they can always make a copy and do whatever they need to do with it. But they won't touch the production copy of the data. We provide pretty detailed documentation about the data product. This is where we provide all the tables, the BigQuery tables that make up the data product by each region. Since it's a global POS data product, we have three regions, US, Europe, and Asia. And we also have a global data set. we talk about, or we at least demonstrate or show all the sources of the data from which this data product was built. Oops. Okay. Sorry about that. And this is the SLOs. Like I said, we make our SLOs transparent for every data product. In this case, we show a data freshness of every 24 hours. Now, this is really going to be history very soon, because we are doing a live integration between our data source as well as BigQuery, and that will enable us to basically make this near real time. We should demonstrate here that the data freshness for this, in this case, is about 24 hours. Data availability per region is also shown, and also we'll talk about the number of data quality checks that were run for this data product. We actually run several data checks, data quality checks, in addition to this, but these are the ones that we expose to our users, which are more focused on the table itself, the data product table. And there's a bunch of other information about the various data marts that we use that power this data product, some key columns the users might be interested in, and lastly, we also have this area where we talk about where we have the links for the various templates, as well as some dashboards that were built on top of this data product. Global POS happens to be a very popular data product at Levi's, we created this template called Retail Analytics in Lucas Studio, which any of our analysts can just leverage, do copy-paste, make, you know, change around things on the UI very easily, and they can create their own ad hoc use cases or insights. There is also a support area we had to grade out just because of privacy reasons, but what is grade out is basically the escalation point for this data product. So we have an owner identified, we have an escalation point identified. We also provide a link to a support forum where people can go ask any questions they have. Since we built the data platform as a product, we have to offer brilliant support, world class support, which we do. And it's chat-based. Now what we will do is we'll go back to the top, we'll click on one of the BigQuery tables, in this case the European dataset. this is the Dataplex interface, and you can see some of the details about this BigQuery table, and this is where we have the data dictionary. Now imagine an analyst landing on the analytics hub page, now they have, they were able to easily access the data. We make all the data products accessible to our users, but we protect the data at the source in BigQuery. So nobody can complain they don't have access to data, they do. Based on their role, we anonymize data, or we don't give them access, for example, to PII data. So they know how to search, how to discover data, how to access the data. Now it's a question of understanding the data. If you go back to my vision, we want to make the data easily understandable. This is how they understand the data. We have very detailed descriptions for each and every column in the table. We provide business terms for each of them. Sometimes that's easier for the analyst to make sense of what that column really is. Now this is how they make sense of the data. Now the next challenge is the trust. We provide very detailed lineage information. Many of our users said they really want to know where the source comes from, and this is the place where they go to get the source information. We also have a data profile which gives some statistics of the data itself without opening up the table in BigQuery. They can go look at the data quality and we provide a bunch of checks. We have observability or monitoring and alerting on all our data sets. Before the SLO for data availability becomes a reality, if there's any issue, we go resolve it. We run all kinds of checks, reconciliation checks, row checks, row condition checks, null checks, et cetera. And if something is failed here, the user can make a decision if they really want to use this data set or not or if they want to go to the support forum to ask any questions. Now the users have understanding as well as trust in the data. Now they are ready to gather insights and all they have to do is click on explore with Lucas Studio. We open up this data product in context for the user and all the columns within the table show up on the right. It's now just a matter of dragging and dropping to gather insights. Okay. If I can make this work. Okay. There we go. For next steps, we would like to make our data platform 100% serverless. Like I said, we have some airflow and some legacy infrastructure. We want to make all of that serverless because it's not just cheaper for us, it's also extremely efficient for us to run everything in a serverless manner. We want to be able to consolidate all our data pipeline configurations and metadata in our Dataplex catalog. We want to be able to explore and enable conversational analytics within Looker. We also want to explore and test Notebook LM, which has already been giving us quite a bit of good results. Lastly, we want to expose Data Canvas to our analysts. And we are constantly analyzing and exploring these new tools to ensure our data platform remains a modern data platform. We are also actively working on integrating both SAP and non-SAP data sources into Google Cloud in real time. That's also going to be a huge upgrade for us as we continue to work on this platform this year. With that, I'll hand it over to my colleague, Arvind. Arvind. Thank you, Vinay. Nice job, Vinay. I think Vinay took a couple of minutes from my segment, so he's going to give me a free pair of jeans. So I think that works out a pretty good deal for me, actually. So good afternoon, everybody. I know I'm in the middle between you and happy hour, so hang tight for 10 more minutes, and we're going to get through this very quickly. I'm sure everyone knows Verizon, so I'm not going to go through these. Many of you may be customers. You don't need to raise hands. I don't need to check how many of you dozed off or not, so we'll kind of leave it easy there. But we're a global provider, operate across 150 countries, a lot of fiber coverage, number eight on the best workplaces to work for, and what this really shows and what you wanted to take away from this whole thing is that we are massive in terms of scale, right? So that's the key from this particular slide here. I'm going to go here, and what are we talking about from Verizon's vision of AI? Verizon believes in humanized AI. You can read what it says on the left, but humanized AI is really about enabling all of our frontline agents, our employees, as well as our customers. That's really what it is about. It's not about displacing. It's about helping them. It's about enabling them. We have four major lines of businesses and stakeholders. These are the key stakeholder groups, as well as the three of them are business units. I'm going to refer to them on the next slide, so you will actually get the feel for it. Chai talked about the roadmap in terms of how the data silos have kind of gone away. So I'm going to talk a little bit about that. Our journey of transformation at Verizon for data, we call it as One Verizon Data. We started that journey in 2018, 2019 time frame, and this is what we were looking like at that time. So we had all the disjointed data sets that you see here are really referring to the data silos that we had. These are primarily different business lines or business units. Everyone had their own little mini warehouse and island of data. When I say mini, they're fairly large, but they had their own little islands, if you will. And our first phase was our big data migration to Google Cloud. And that's how fast it happened. It was the fastest Hadoop to GCP migration given the scale and complexity. It was a great partnership. It took a lot of work to get there, but that's something that we got done around 2023 time frame. So here is kind of where we are at. We have partially linked data sets. We are working our way into getting better at it. We are moving away from our traditional legacy data warehouse into BigQuery. As we work through that, that's our phase two from an EDW modernization perspective is what we call it. This is going to be the largest telco data warehouse in North America running on BigQuery. BigQuery. So that's something that we are underway. We are in the middle of the phase two. We should be done by 2026 on this. And then we'll jump into future phases where we get to unified data sets. We have a couple of other smaller islands, if you will, in our business that we are working on bringing it together. And the net result is going to be a future state data lake house architecture where all of this data is unified. It's easier to query. It's easier to understand. And can do a lot of things that Vinay talked about, a lot of things that I'm sure all of you are interested in. How do you get there? That's really been our journey. So I'm going to spend a little bit of time in terms of our data estate. What does this look like? So this is primarily from users who are running queries directly on the warehouse standpoint. If you just think about it from that perspective, you know, we have about 3,500 users. We run about 50 million queries. This includes all the dashboards and everything else that queries our data sets, 35K pipelines. We built a lot of frameworks to go with the Google suite of products and services. We have 40 plus petabytes of data that's on Google Cloud, and we are growing. Right? Tons of different workflows, data assets, and quality rules. This is an area where we are still maturing. As you can see, you know, we have about 5,000 or so quality rules right now, but this needs to be much larger. So that's something that we are constantly focusing on and working on. What has been our journey, as I talked about one Verizon data, as our journey on transformation, from a governance lens, what does that look like? So this is primarily how it has been. You know, we've been working on this journey for several years now, maturing the governance across data and AI lifecycle. As we think through this, you know, as you see the data and analytics taxonomy, it's really about defining the ontologies and the standards across the enterprise. This has been super critical. And we are leveraging a lot of the services and capabilities that you saw and heard about, and you're going to hear more about in the various Google sessions around governance. Self-service analytics, you know, we are a big Looker, Looker core. We just finished our migration to Looker core. We were leveraging Looker on-prem for a while now, and we are getting into Studio and Pro and a bunch of other things, expanding the capabilities as we grow our capability on business semantic layer. So this helps answer questions very similar to the examples that Vinay was talking about. We have several data products in the mix. We are building more. This is something that's exciting for us as we have gone through this journey. A couple other things, you know, we are leveraging in terms of capabilities. We have data sharing. You know, I'm not going to talk about the culture aspect of it. That's something that is ingrained into our organization as a whole. We are a data-driven enterprise, and we made a lot of efforts around that. And we leverage capabilities around quality, lineage, privacy control, and responsible AI. So, you know, you see a lot of these in here. How does this come to life? Right? I probably need two hours to go through that, and I know we don't have that time here today. So I'm going to spend, like, two areas I'm going to double click on. So I'm just going to keep it very simple. Modular design for enterprise data. How are we managing it today? This is a real experience of how our users go through in the enterprise. How do they use the data? How do they use it on a day-to-day basis? What you see here on the top left-hand side is our production trusted area. Right? We have two different spaces there. We call it the PR, the producer space. This is where all of the data from the golden sources gets ingested. You know, think of it as an ELT landing ground. Right? This is where all of that comes through. And then we have another space called DO, data owner space. So these are both the production areas. This is a trusted zone, if you will. So how does the user do, you know, how does the user today? So, you know, beyond querying, if they want to make some changes, if they want to add more attributes, they want to add some features, et cetera, they take that data into the exploration space, either through references, like the linked example that Vinay was giving, or you could also take a copy of it, put it in your exploration space, play around with it. This is your little mini sandbox. It's your safe space. You can do a lot of things. You can share data with other users to collaborate, et cetera. So that is the kind of work that happens in exploration space. Once you have all that analysis done, you know exactly what you're looking for. You have the insights that you are looking to derive and hypothesize. Once you have all those details, you go into the development environment, and it's an automated process, and then we promote code back into production using CICD. So this is a simple example of how we are looking at it and enabling users to leverage data across the enterprise. The next area that I'm going to go into is super critical for us, driving seamless user experience. What do we mean by seamless user experience? Seamless user experience for us is somebody who can come into a marketplace, search for a data set or a particular attribute. They find it, they can gain access to it with a proper business use, and they can go, get access to it, leverage it for their analysis, and perform the duties that they need to, whatever their job may be. That is really what seamless user experience is. And we are leveraging a lot of the capabilities from governance perspective, and we have augmented them with some of the frameworks that we have built ourselves at Verizon as well. So you see the ones that have the V logo in there, those are frameworks that Verizon has built on top of capabilities that Google Cloud is providing. So this is a simple use case of how data comes through. An onboarding request comes in. You know, those assets are deployed in BigQuery. We leverage DLP to scan. We get the info types identified, and we also augment it with custom info types that we have. We pull all that information into SDP. We do the policy tagging and metadata enrichment there. I think Chai was referring to some of that as well. And then we load all of that information into a Verizon data marketplace. This is, you know, an area which is available. And the primary reason why we have our own little marketplace is because we're still heterogeneous, you know, in terms of our data transformation journey that you saw. Not all of our data is in, you know, BigQuery yet. So we run that process. So once everything is in there, we can leverage more capabilities like Analytics Hub and other things. And then we have custom view creation. This is, you know, primarily enabling global access of data because we are a regulated business. We have data that relates to government bodies, et cetera, which cannot be viewed outside. So we have some filtration that happens there. And then data lands into these governed assets and project spaces, which enables seamless access. So that's really how we go about doing this at scale day in and day out. So that is my presentation. Thank you for being patient with me. I'm going to hand it over to Chai. Thank you. Thank you so much. Thank you. Thank you. There is a lot of announcements. Tomorrow the blog is going to come out. You will see all the new things. I think it's always amazing as a product manager to see when customers are ahead in terms of innovation. When I talked about data products, we are going to be launching data products here. You'll have a first class construct in BigQuery, in Analytics Hub called Data Products. Arvind was talking about all the metadata tagging and everything. All that stuff is getting automated using Gen AI. So I think really proud to have some amazing customers who are innovating and actually forcing us to catch up. Hopefully, there are a lot of other things that you are going to be excited about. Go to the Friday session, which Lou is going to be presenting. There is a data fabric session, which also talks about governance with Virgin Media. And I would say don't miss this session. This one is the redefining data and AI governance session. It's the most important session. It has all the new launch announcements that we are going to be doing. Lou is going to be leading it. So we are really excited. Thanks a lot. Again, we'll be here. I'll be outside for at least 30 minutes. Ask me any questions. I'll share our experience and hopefully Vinay and Arvind also can share their experience. Thanks. Thank you again.