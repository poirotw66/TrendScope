 Hello and welcome everyone. Good afternoon. This is day two. And today, this particular session, we're going to talk about what's new in cloud networking, especially around the new cross-cloud innovations that we're bringing. My name is Manandar Singh Sambi. I'm part of the cloud networking team here at Google Cloud, and I'm joined with Anna Berenberg, who is the engineering fellow at Google Cloud. She's been with the company for almost 19-plus years and has brought a tremendous amount of innovations on networking. We are super excited to be presenting all the innovations today. We do have a packed agenda, so I'm going to try my level best to cover as much as possible for Q&A. You guys can find us. If you have any specific questions after the session, the team will be around here. I will be here, and I will be here as well. Now, cross-cloud network. This is something that we introduced back in August 2023. We've seen a phenomenal adoption of cross-cloud network within our customer base, with 50% of the Fortune 500 adopting at least one use case, whether it be about multi-cloud networking with distributed applications, using our global front-end to protect and secure and deliver Internet-facing applications, and we recently launched CloudWAN. Roughly about 23 exabytes of data per month is what we are seeing that is actually being used and being sent by our customers. So I'm going to give a big round of applause for our customers who have believed in this journey and also have consumed many of our products to make cross-cloud networking much simpler and easier and secure. Thank you. Now, we are at a very interesting inflection point. Google Cloud has been known for innovations. 25 years of networking innovations that have shaped the networking industry. It started off with the Internet era. It came to the streaming. It went to cloud. But now we are at a very important inflection point with AI. And let's talk about why that inflection point is important and from a networking, what do we all need to think about as we think about networking for AI ML workloads? The AI era, obviously, is going to be a multi-cloud, multi-model. About 93% of the Fortune 500 customers are going to be using more than two or three providers and more than two or three models. About 70% are actually going to be investing heavily in AI-inferencing workloads. And let's not forget security. 90% of prompt leaking attacks have actually resulted in data loss. And as we all know, in enterprise, data is key. Data is the IP. It's very important to protect and have a security stack for all your ML needs. Now, in the AI era, we also upgraded and brought in our next generation Google Cloud Network that actually builds based upon the table stakes of high performance, reliability, and scale. We had to evolve our network to provide exponential scalability beyond nine reliability. We've been used to three nines, four nines, five nines. The team's done a tremendous job on this multi-shard architecture using protective reroute to provide the best reliability that you can experience in a cloud environment. It's fully programmable, intent-driven, and very importantly, we are going from automation to autonomous behavior. Automation, zero-touch provisioning, these have been the buzzwords for the last decade. And now we will have AI agents that will actually run, configure, monitor, and debug our backbone network. So a big round of applause to our GGN team that's actually innovated. Now let's not forget the SDN layer. Since 2012, we've been investing in our SDN layer, which is called Andromeda. With Andromeda 2.0, it was all about scaling our global VPC infrastructure, encryption everywhere, and improving reliability. But with Andromeda 3, it's about multiple VPCs being able to communicate with each other. Being able to build for large-scale AI training and influencing workloads. Increasing our connections per second 12 times better connections per second. And lastly, we all know how service stitching in the cloud in an SDN world is extremely hard. And the team's done a fantastic job delivering cloud stitching with Andromeda 3.0, where not only can you integrate our cloud NGFW, or any third-party, best-of-breed security appliance. So a big round of applause to the Andromeda team. Now, while those are fundamental, foundational layers of having the best network and one of the most innovative, SDN stacks, let's talk about how we can secure and optimize AI ML infrastructure. It starts off with data ingestion, where we have cloud interconnect, we have cross-cloud interconnect that you connect to any other cloud. You can move petabytes or zettabytes of data back and forth to use our GPU infrastructure. For training, you'll need the best network, which is the Google backbone. And I'll talk about other innovations that can help you provide scale from a GPU-TPU cluster, as well as optimizing GPU utilization for inferencing. For ingestion, we are very pleased to announce 400 gig on our cross-cloud interconnect and cloud interconnect. High-bandwidth, fully managed, SLA-backed. On top of it, we also provide the flexibility for you as a customer to configure application-aware policies. You can decide at the time of day which applications are more important that you want to be able to protect as you egress out of the cloud. Extremely important, extremely needed, especially as you move bulks and bulks of data from an AI perspective. We've been partnering with Lumen, one of the leaders of offering 400 gig to almost any place in the United States. This will help our customers reduce time to value as they adopt AI ML. From a scale throughput, we are now introducing GKE scale where we can go up to 65,000 nodes on GPU and TPU, enabling trillion-parameter AI models. This improves performance, traffic isolation, dual homing, and network segmentation that customers need in a GKE cluster. We're also introducing a 3.2 terabits per second non-blocking bandwidth with the RDMA VPC to have GPU to GPU communication. One of the largest companies that we all know about, Enthropic, in AI, is actually using our GKE scale infrastructure to do exactly that. As we move to AI inferencing, it brings a whole different amount of challenge. As we all know, AI workloads are not exactly the same as web workloads. So traditional load-balancing algorithms result in inefficient use of GPU and TPU. the network bottlenecks will lead to subpar performance for your AI, ML workloads, especially response times. The growth in number of LLMs is going to be exponential. There's going to be more and more LLMs, there's going to be more agents, more LoRa adapters. And as customers, you will need to quickly adapt. In fact, I was actually at a panel in the morning where one of the CIOs of a telecom company said, the LLM model that I used last year, I already have to replace it within six months. And that is because they want to be able to use the best-in-breed models to bring more value to their infrastructure and to their customers. Security, top of mind. I'm sure you've all had read the news, of the 100% attack success that happened with one of the open-source LLMs that was introduced. They could break into with 100% of harm bench test case that was done on that open-source LLM model and they were able to infiltrate data, do prompt injection to make the LLM model hallucinate. You cannot expect that to have your enterprise applications ever get into that kind of a mode because not only will it break your brand, your applications will not perform and your users will not have the best experience. To solve these challenges, we are extremely happy and excited about GKE Inference Gateway that will allow you to do 60% lower latency, 40% higher throughput, reduce costs by 30%, and integrate and provide you consistent security and compliance. And I'm going to talk more details on GKE Inference Gateway. But first, let's look at traditional load balancing algorithms. What you see on the screen here is that traditional load balancing algorithms are built for web applications where round-robin or CPU-based algorithms will actually shift traffic to a container that might actually be connected to a GPU cluster or GPU node which is running high both from a pending request queue as well as the KV cache. With GKE Inference Gateway, we will now be able to optimize based on parameters that you offer. so almost like smart traffic routing that will look at the pending request queue, look at the KV cache, and identify which GPU is best optimized for your model. There will be models which will be chatbots that require less processing time. There will be reasoning models that require higher processing time. Having that intelligence, having that smartness, to be able to leverage that GPU infrastructure to provide the best inferencing for the right application type is going to be critical and very, very important. We believe with this architecture, you should be able to load and have better throughput, 40% and 60% lower latency. Some customers can also now consolidate the infrastructure that they need from a GPU-TQ perspective, thereby saving costs. We can also allow you as a customer to now have a better efficiency use of your GPU infrastructure that you're getting in the cloud. So if you're actually using dynamic, LoRa, fine-tuned models, we can actually now point it towards a GPU or LLM that is loaded on the same GPU infrastructure and provide you the best inferencing latency. So imagine you have a LLM model for German and one for Spanish. You can now load these two models on the same GPU versus having separate GPU infrastructure for every model. This was not possible before and is not possible with traditional load balancing. We believe with this architecture, with GKE Inferencing Gateway, you will be able to save up to 30% costs. Now, GPU capacity is quite constrained, so how do you leverage the multi-region architectures that you might have? You might get capacity in region A, you might have separate capacity in region B, and as a customer, a burst of traffic comes in, today's traditional load balancers cannot move the traffic pattern and load balance between regions, especially from an AI ML workload perspective. GKE Inference Gateway helps move and create a pool of capacity across multiple Google Cloud regions. It will automatically reroute the request to the right region, optimized based on utilization of GPU in that infrastructure. We are also working with hybrid NEG support to be able to not only extend this within Google Cloud regions, but eventually to other clouds for DR or backup perspective. And finally, let's talk about security. It's extremely important as a customer to make sure that we can provide you the tools to be able to protect your AI ML infrastructure, providing you consistent protection, making sure that your protection against every LLM with the OWAS top 10 is compliant, protect your data, data, and ensure that no data expletration happens. We also want to be able to provide you centralized visibility and control and extend it to a third party. So you have model armor that integrates with GKE inference gateway, but we also give you the choice with NVIDIA guardrails or even with Palo Alto networks from an AI firewall perspective. Now let's see how GKE inference gateway actually performs. Sorry. Can we play the video, please? Oops. Oops. Here we have a couple of clusters running. This is the one that we are actively using. It has got a few nodes that are up and running. They are using this H100 GPUs. And on there, we are using this VLM Lama 2.7b deployment. In order to see GKE inference gateway in action, we have run a load test using a publicly available shared GPP dataset. First, against a traditional GKE gateway with traditional round robin load balancing. And then, against GKE inference gateway with the inference optimized load balancing. First, let's look at the KV cache utilization. When using traditional round robin load balancing, we see that there is a lot of variance across the model servers in the KV cache utilization. And in some of the cases, it also saturates some of the modern servers. In contrast, when you look at inference optimized load balancing, that is routing based on the KV cache utilization, it results in a much more even distribution of load across all of the different modern servers. Here, you see that there were certain requests that are getting queued up when using traditional load balancing. However, when using inference optimized load balancing, there was actually no queuing of requests at all. Which means, overall, you've got a better serving performance, and your users have a better experience when using GKE inference gateway for their modern server. We have roughly about, this particular product is in preview, we have roughly about 10 to 15 code development customers. If you are interested to help partner with us, do reach out to us, we can absolutely extend the preview to all of you. can you go to the slides back again? Can you go to the slides back again? Sathish, can you come and help here? I know where the mouse is. thank you, Sathish. Sathish, the speaker notes. So, we have customers like Snap, who are actually using our AI infrastructure to deliver high performance and security for all their AI experiences. Thank you. With that, I will pass it to Anna to come and talk about the future of what more can we expect on AI ML infrastructure. Thank you, Anna. Thank you, Muninder. There is a debate going on right now. Is a prompt a programming language or not? Please raise your hand who thinks that prompt is a programming language. Not working. Tough crowd, right? Okay. Well, I believe that prompt is a programming language. And let me walk you through today's presentation, and I hope you agree with me. And this is why we actually, because of prompt, a natural language, that's why we actually have to rethink what networking is in AI era. Here is similar to how code is being translated from a programming language down to the byte code. Here is the, from natural language, create API definitions, and from API definitions, create a code, right? Sounds like a compiler, right? Okay. And now let's look at application design center we just launched. So we call it ADC. Similar to how compiled parts were assembled together into one binary, here is Gemini design agent in ADC puts together multiple deployment modules into one application. Sounds like a linker, right? Okay. And here's Gemini Cloud Assist, the troubleshoot running application based on the configuration, signals, and domain knowledge. And it optionally hands off the investigation to the human customer support. This really sounds like a debugger, right? Okay. Everything I showed you actually built as a mesh or orchestration of agents. And where does it leave networking in this agent-based stack? Well, let's recap. Last year, we were in the same room here. We delivered on many last year predictions. Muninder just announced. There are big pipes. There are our service extensions, power, AI safety, with the model armor and the other providers to follow. we were talking about AI routers. So, Muninder just introduced you to GKE Inference Gateway, which is what actually happened. With inference optimized load balancing, intelligent routing, and AI safety. Now, but AI technology moves so fast. And last year, we were talking about request-response communication. But this year, we need agentic networking. And this is an example of prompt-driven travel booking experience, what it looks under the food. We start from the top, from the surface. The prompt comes via natural language interface. Then there is a whole mesh of agents powered by LLM. and each agent operates on the data and tools. And they orchestrate it via planning, execution, and monitoring. And there is always a policy because actually we build for enterprises. And there is never enough policies for enterprises. Now, that brings three challenges for a genetic network. One, how you uniformly access models, data, and tools, and other agents. Two, how to route to agents instead of routing, with a prompt instead of routing the HTTP header. And three, how do you have uniform policies when you have mix of RPCs and events because proactive agents introduce events all the time. So, let's consider each of the challenges in detail. For agent to model, we need standard access to models and to functions. And look at this stack, how the calls are made. We need to be able to interpret Gemini APIs or open APIs to provide security and privacy via AI safety guardrails. We need to be able to extract prompt for the routing. And based on the configuration, we need to be able to decide whether to call this function either locally within an agent or remotely. Because guess what? LLM can generate a function and have it run, for example, on Cloud Run. Two, we need to standardize access to data and the tools. And you probably, many of you heard about model context protocol, which is called MCP, and it rapidly gets adoption recently. But what happens is that every API provider produces its own MCP server. So instead, we are saying we are going to shift it down to the gateway and have agent gateway to support MCP server to have uniform access where you can provide support the same policy no matter which tools or which data you are trying to access. And C, we need agent-to-agent communication. So what happens before yesterday, there was no agent-to-agent protocol. A new agent-to-agent protocol was announced just yesterday, and we call it A to A. And before that, agents need to know each other APIs. And so the adoption of intercommunication is limited because it's a mesh where everybody needs to know about everybody's API. With agent-to-agent, you create standardization where not only the application developer doesn't need to know about other agents' API, but actually LLM itself can generate an agent because APIs are known, and so they can talk to each other. And with that, there is this agent gateway that probably is going to come out, which represents a single server, a single this A to A and MCP server for multiple agents coming in, and it's going to front, again, multiple APIs, and also it's going to federate to another MCP server. In this way, this gateway is going to have both MCP server fronting, as well as MCP client sending it out. And with that, what would you gain by that? So you'll have, again, universal access control, you have telemetry, analytics, security, model armor, and the other policies. Now, the second challenge is how to route to agents. In the normal, non-prompt world, you'll have a URL being called from the code, and then the regular HTTP routing takes you to the services and microservices. What actually happens with the prompt? Well, the prompt comes, and it's not the only URL you can do is send it to somebody up front. What happens today is everybody who builds up gen AI application have to do routing. And so instead, we propose to push down the routing down into the gateways and have whatever every application developer does today, they look up into RUG. Now you have gateway that can look up into RUG and find appropriate service based on the lookups and send via agent route, and then agent route can redirect it down to HTTP route. Now the third one is interagent communication, specifically how to mix RPCs and events. In this example, a travel vacation planner, this example is travel vacation planner where AI agents decide to book your vacation based on your calendar, how busy you are, based on your preferences, based on travel advisory that is happening, it's tracking the prices, it's tracking the flight schedules and weather. And with that you can see the communication that happens that both proactive agents that are running on their own and they're event driven, as well as regular agents who are going to be talking through RPCs. So this mix of RPCs and events creates situation that they both need to have common policies. And while the RPC communication always have developed proper policy enforcement and the configuration, the events typically don't. And so that has to change and we just released event arc advanced into public preview, which allows you to set up policies on events. Now, we need obviously to orchestrate it. Now, let's look into a single agent communication. Over last year, this inference evolved into becomes heavily asynchronous. And we got to insert, before we got to insert inference gateway only in request response path. and now you have this 10 hops for example here. Where do you insert gateway? So, again, look, this can reside in local VPC, in another VPC, on internet, or it could be even served as a SAS. So, we need to be able to understand all these topologies and be able to control all of them. So, to orchestrate all of them, we're introducing ambient agent network, and we have a solution, we call it one network, and one network is an architecture that allows you control and manage every load service mesh egress proxy in our service network. Not a product, just an architecture. And here's your travel planner under the hood with one network. That is built on the principles of one proxy and one control plane. And this architecture powers all cloud networking service products. So, in short, the orchestration on one network consists of four points. One, it's a governance. We define hub application perimeter around LLM, agent, tools, and functions. We will provide the ability to govern all the path and understand all effective policies. For the policies themselves, we will orchestrate ingress and egress security. We need agent and inference gateways everywhere to assure AI safety. On every network path, always on and bin. End-to-end observability, they are betting on open telemetry and its semantic AI agent and AI agent framework schema to provide metrics, logging, and tracing across all different ownership of the functions, LLMs, the data, and agents. And traffic management. Did you see those loops? And basically dynamic nature of communication between agents create an opportunity to deliver a seamless self-healing network by orchestrating retries, dampening, temporarily throttling, and fast auto-scaling. 75 years ago, Grace Hopper had an ambitious idea. Create entire programming language based on the language that would use English phrases. But she was told immediately that it cannot be done because computers didn't speak English and didn't understand English. And we came a full circle because today computers don't understand English in any other language. Grace Hopper would be proud. Let's complete her mission. Thank you. Thank you, Anna. At least the way I understood, as all of you probably realized, you've got to have the best backbone to be able to power your workloads, your cloud infrastructure, the SDN layer followed with it, being able to service both from our data ingestion training and inferencing with GK Inference Gateway. But the future that Anna just pointed out is all about agentic networking. And one of the reasons Anna chose to come and talk about agentic networking on how networking is going to evolve is because we got to deliver on the promises and the products that she talked about. Last year, Anna talked about Inference Gateway, or she talked about a concept of what inferencing needs to be and how web workloads are different. And I think the team's done a fantastic job this year by getting GKE Inference Gateway into preview. So thank you, Anna, and a big round of applause for the engineering team that continues to deliver on the Inference Gateway side. Now, as I switch gears, I'm going to go back to service networking. Now, last year when we talked about service networking, it was all about providing a consistent, secure experience for you to be able to connect your workloads from any cloud to any service that Google offers. It could be a Google native service, Cloud SQL, BigQuery, and the likes. It could be a third-party service, a MongoDB database, RICs, and the likes of them. It could be your own managed SaaS service. We use technologies like Private Service Connect, PSE, that provides seamless, secure connectivity and consistent. We are one of the few cloud providers that actually provides security as well as a level of simplicity and consistency to connect to any service that is there. We also provide consistent security policies that for all services that can be accessed through a service VPC. But we didn't want to stop there. The next phase that enterprises are asking us is to build a services-based architecture with DevOps team in mind so that they can build one service that can be applicable and be reused by multiple applications. And DevOps people, when you talk to them, I'm sure, they tell you networking is hard and complex. and one of the reasons why they come to us and tell us is infrastructure provisioning and ticketing systems are stopping us are huge friction points. So the team took that on to themselves and we're actually extending service centric networking for developers. Now service owners, as you can see in the slide, can now use isolated GKE clusters to build services. We can either use existing IP address or new IP address and they all can be private IP addresses without actually having to expose public IP address thereby accelerating the application rollout. The dependency on IP addressing was so big for the developers but this unique flexible capability that we're providing can actually enhance it. We're also introducing Cloud App Hub which completely dismantles this complexity of producer consumer divide by fully automating all the discovery and cataloging of services that developers can now publish and connect to. We're also introducing service health that will actually provide better reliability for global services. Now in this architecture developers can also simplify operations. They can use our multi-cluster PSE services that enable services to connect to multiple services in a GKE cluster with a single PSE connectivity. security. And security like I said top of mind they can now introduce and integrate with Cloud NGFW to protect any service that has been published into the service EPC as well. So for folks who are actually looking and working with DevOps do talk to them about how we have simplified service networking and made it easier for DevOps to actually consume and publish services that can be leveraged by other parts of the business in a much more agile faster time to market. Oh sorry. We've been working with Goldman Sachs as you can see the quote here that our collaboration with Google has enabled us to streamline service discovery and help to empower our developers to iterate faster and more efficiently. Now Anna talked about interagent communication and I'm sure the first time it was proposed to me I was like what does this mean from networking? But I've come to the realization of how important this is especially in the AI world. Today the pub sub mechanism that you see with the publisher and the subscribers they're using a message bus that has no policy control. when someone says no policy control I'm sure as networking and security advisors you're looking back and saying how is that possible? How am I going to protect it? How am I going to make sure my IEM policies are applied? Can I do call outs to it? And that was scary and that got my attention and I'm like as a networking leader and as a person who's done networking for a while I think this message bus should evolve and Anna's team have done exactly that. With event arc advanced we are now using the same construct of the proxy as well as our service extension to give you the controls of integrating model armor that is to protect your models give you IEM policies of which publishers and subscribers can talk to along the same time giving you the flexibility with service extension call outs to write your code and control and define policies that are needed. Custom policies could be custom routing and any other infrastructure requirement that you need. Very very very powerful. I would definitely encourage all of you as network cloud networking leads who are working with developers who are looking at interagent communication to have this level of discussion on how important it is to have the message bus provide security and networking controls. I'll move to network security. Now network security has been top of mind for many of us. I mean we want to be able to protect you from the initial compromise. We want to make sure there's less lateral movement when it happens. No data exfiltration is the north star and when it happens there should be no data loss. To do that we're introducing new capabilities in our network security portfolio. For securing the workload I'm going to talk about how we have made cloud native firewall policies with zero trust network access. We are introducing layer 7 domain SNI filtering. For some people you can think of it as web URL filtering. We're also introducing DNS armor. For securing the data you will now have a very simple way for integrating both data at rest either with cloud DLP or with your preferred choice of DLP for data in transit. And obviously we have to do it with an open ecosystem. While we bring the best of breed we want to provide the flexibility to you as a customer to be able to consume and bring your own security appliance and integrate it into your workloads. From a zero trust perspective we did talk about how you can define policies between workloads and destinations could be a managed SaaS service by Google, your own or a third party. What we have done now is actually introduce firewall tags with hierarchical policies. This allows you to now define policies that you see in that table where you can actually define constructs and have policies which can be ACLs or even firewall insertion in between that communication of workload to workload. Even when the workload moves the policy will automatically get associated. you don't have to redefine your security architecture as your workload expands, moves, or even downsizes. We're also introducing the concept of network types where you can now use VPC construct and internet construct to define a firewall or an ACL policy. As we all know, networking and network security works on IP addressing, but in the cloud it cannot be automatically integrated. These constructs are really important. VPC constructs, internet constructs, the firewall tag constructs. That's how developers develop workloads and applications. Adapting network security policies to provide something that is cloud native has been our number one focus. We're also introducing layer 7 SNI filtering, which is AKA web filtering. We're really extremely proud of DNS armor. Now, as all of us know, SolarStorm, Log4j, and I can give a big long list of all the previous attacks that you've seen, were started with DNS. DNS protocol was used to do data exfiltration. It made it the news. A lot of enterprises got impacted. DNS is the number one protocol that gets ignored, and that's what adversaries actually use to infiltrate your infrastructure and do data exfiltration on it. We are now partnering with Infoblox, which is the leader in DNS security, to offer you the controls that you need, where you can protect DNS traffic. DNS armor is now going to be in preview. For folks who would like to use it, please reach out to your account teams. We're happy to extend it to you. Moving to global front-end, a global front-end that we had with Global Load Balancer, our Cloud Armor portfolio, which is DDoS, and high performance caching with Cloud CDN, was able to now service all back-ends, whether it's on Google Cloud, on-prem, or any other cloud, without having to expose your public IP address. that has a huge PCO saving, because you do not have to apply NAT gateways, firewalls, because you are not exposing your back-end IP addressing. We know that the applications are going to migrate from what you have today to Gen AI, so we extended service extension call-outs to our model armor, where you can now protect your Gen AI applications. It also works with third-party. It could be your own firewall or your own best-of-breed ecosystem provider that you would like to bring. In addition to that, we're also introducing new edge programmability capabilities with service extension plugins, what we also call as WASM. We're extending our WASM plugins for load balancing, where you can actually run WebAssembly code directly in your data path, on your edge, using Rust, C++, and Go. Extremely powerful, giving you that customization, where you can customize routing, traffic management, and even event-adapt content on the fly. We're also doing pre-built libraries of about 60-plus WASM examples. So you can take one of those examples, customize it to your needs, for your enterprise, and use them. We're also extending what we had as service extension plugins for pre-cached objects in Cloud CDN. And obviously, that is going to be something that's going to come later in this calendar year. We talked about CloudVan. You heard Thomas on the talk about CloudVan. CloudVan. This is one network for all your enterprise needs, built for the AI era, a fully managed, SLA-backed, reliable backbone for any-to-any connectivity that offers up to 40% improvement in application performance, integrates the best-of-breed security and other branch services, and also offering you up to 40% TCO savings. Now, thank you very much for today. I hope this session was interesting. More innovations to come from Cloud Networking. Feel free to please submit your feedback. And we really greatly appreciate the feedback. Thank you very much. Thank you. Thank you. Thank you.