 . Please welcome Datadog senior SRE and DevOps advocate, Ajuna Kirruzzi. Hello, everyone. Thank you so much for joining us today. My name is Ajuna. I'm here to talk to you about out-of-the-box thinking, automated observability with Datadog and Project 44. I just introduced myself, so I'll jump to my next slide. So if you all bear with me, I'm going to start with a joke. You're going to have to laugh. It's okay. But what do developers and LEDs have in common? You can shout it out if you want. If you thought bright, you were partially right but mostly wrong. I'm talking about energy efficiency. And I don't mean it in the way that I called myself energy efficient as a team, though my parents probably would have preferred lazy. I mean it in the sense that developers have a lot that they need to do. The world is growing and changing and we're just getting all of these tools and technologies that we have to balance. There has been an explosion of complexity, right? We're on so many more computing units, meeting with so many other different types of people, so many organizations, and releasing multiple times an hour. But when you lean into tech innovation, this is what happens. That energy efficiency starts to matter a little bit more because you need to start automating things. In my opinion, automation is a core part of being a developer and wanting to make things simpler for yourself. Maybe it might take a lot of time to do that, but that goes with it. The first time I learned how to SSH was because I didn't want to walk to my computer science lab in the winter. So just using that to make yourself automate and innovate really matters. I mentioned I work at Datadog. If you haven't heard of us, we are an observability and security platform. It gives organizations insights into how their infrastructure and applications are running and ways to improve the security of their systems. The reason why this matters when it comes to automation is because Datadog was built for developers by developers. So a lot of the tools that we built for ourselves eventually became products that we've released out. When we first started in 2010, we were much more of a monitoring solution, mostly helping you gain insights into your systems and optimize them from there. But since then, we've grown. We realized that that data was helpful to multiple types of people, so we added security, same data, just providing it at a different context for folks who also need to know how your systems are running. And over time, that continued to grow as well, shifted left to gain more insight into the processes before you go into production, right? How do you make your systems better while you're building? How do you make your build systems better? And even expanded into understanding our users and supporting them. And of course, our users mostly being developers means that dogfooding at Datadog is one of the things that we're doing. So, fooding at Datadog is one of my favorite places I've done it, pun intended. We also have a lot of intersectional technology that overlaps all of these products, including a service catalog, giving you insights into how your services connect, but also an instant management tool, and recently, on-call. All in all, the focus on the developer experience, automating it, making their lives easier, is a core part of what it's like working at Datadog and hopefully using Datadog. The way that we're able to gather all of the data that we get is through our over 850 integrations. We have a few very cool Google Cloud ones, but all of them automatically feed data into Datadog when you connect them, making it easy for you to get information out of the box. We use that information over a variety of different ways, obviously, to make our products better, to learn how people are using them, but also because we're getting trillions of logs and metrics, and even as an organization generating them too, we want to use that information. We release different reports every year. The State of DevSecOps report came out almost exactly a year ago that we used to look into how different organizations are using their tooling, specifically how it's making people's lives better in this intersection of dev ops and security. One of the things that we looked at was automation specifically as infrastructure as code and looking at how on different cloud logs what infrastructure as code tooling people were using on different clouds with Datadog. And we actually found out that within Google Cloud, 45% of the folks that were sending logs through Google Cloud to Datadog weren't using any infrastructure as code tool at all. I point this out because infrastructure as code can be really helpful. If you haven't been using infrastructure as code, you probably use it in a slightly different format. It's existed for a long time with tools like Puppet and Chef from the 90s, but it's also grown and changed even though those companies still are doing very cool things. Terraform is one of the main ways that people are using infrastructure as code across different clouds. The benefit of infrastructure as code is that it lives alongside your code. So you're able to use it to allow your teams to define, manage, and provision their infrastructure using code as opposed to clicking through different consoles or using a CLI command tool. The adoption of infrastructure as code is widespread, especially in helping your team secure their cloud infrastructure. But there are still ways to improve on that. There are very cool things that infrastructure as code allows you to do. Mostly that when you use it, all of your changes in launching your services are peer reviewed living alongside your code, making it easier to roll them back when something goes wrong or migrate and use that cloud flexibility to their highest extent. Developers really get an opportunity to be more technical. There are a lot of technical and cultural benefits to developers that infrastructure as code provides because it makes it easy to spin up new instances, replace parts of your services, and more. I have my friend coming on to come chat with you a little bit about how Project 44 is using infrastructure as code in Datadog to automate their observability. That's a huge topic. I'm going to go ahead and start my time. Thank you. Thank you. Thank you for the opportunity to start sharing my insights and share my insights. Thank you. Thank you. Thank you. Hey. My name is Manan Katari. I'm a staff SRE at Project 44 and I work on our observability platform as well as kind of our entire cloud infrastructure. So what does Project 44 do? Our vision is to make supply chains work, right? We are looking to build the connective tissue across your entire supply chain. your entire supply chain and to improve our customers' experience through that supply chain. It's a very complex ecosystem. There's hundreds of thousands of carriers all over the globe. And the way we've built our product is we track shipments across all modes and geos. That's ocean, air, rail, truckload, across the world. We've built products around inventory visibility so that you can track your products across various SKUs. We've built some transfer point efficiency products. And then on top of all of this, we've built some AI-powered supply chain intelligence which allows enhanced ETAs for tracking your shipments, exception management, and collaboration, and more. And all of this is pretty complex, right? Building all of this, we're integrating with hundreds of thousands of carriers. We have tons of different API connections all over the place. And so we kind of wanted to simplify our lives for our developers, right? So, you know, observability on its own can be pretty overwhelming, right? You have your application or service or whatever instrumented, and that's great. But, you know, we want to reduce that overhead for our developers because there's a ton of data out there, you know, just by saying, oh, yeah, we have this set up with monitoring or whatever, right? So what we wanted to do is, right, we've built a platform for our developers that uses their lives and, you know, it makes it easier to focus on, hey, like, this is what actually matters when you're, you know, when an issue pops up. So kind of our transformation over the years, right? Before, you know, we were very reactive. We were focused a lot on, like, log-based monitoring. Everything was kind of reactive to, hey, this happened, and then, okay, now we have to, like, look at our logs, whatever, and investigate what's going on, right? And then even as we've implemented, you know, tracing tools like Datadog or whether you're using open telemetry or whatever else, right, it's a ton of data. There's, you know, hundreds of millions of data points a day coming in through, you know, all your various monitoring tools. And so what we wanted to do is we wanted to build a platform that makes it easier for our devs to get the right targeted visibility without having to, like, say, hey, I need to look at, you know, so much data, and how do I get there, right? And kind of all of this leads up to, like, hey, the end goal is, right, one, to reduce the number of, you know, issues that our customers catch before us, and also, as a developer, to reduce the times, you know, you get interrupted, whether that's during the business day or, you know, at night, because no one likes being woken up that late, right? And kind of the last part of this is, you know, not every engineer, developer is an expert on observability, right? So we wanted to make it easy for our developers to say, like, hey, this is what actually matters, right? And this is what you should alert and monitor on, and kind of go from there, right? But before we kind of go there, talk about observability, we kind of want to talk about a little bit about how we've built out a self-service platform for our developers, right? So this, you know, is about, you know, reducing the bottlenecks between, you know, the traditional operations, infrastructure teams, and our developers, right? So it allows our engineers to take a little bit more ownership of the infrastructure that they need and they want to own, right? And the other part of this, right, is great, that's all great, but, again, not all engineers are experts on infrastructure, right, around security, networking, et cetera. So we've built some guardrails in place, right, so that, hey, as a developer, I say, hey, I need a database, and it's pretty simple, right? They all have to do is provide, you know, a name for that database, the size, whatever, and, you know, everything gets spun up, right? With that, we've also kind of built some stuff around, hey, how do we spin up, you know, a new service quickly, right? So, you know, before this, we, you know, it would take about a week, maybe two weeks to get a fully, you know, a new service up and running and deployed out into our Kubernetes clusters, right? What we've done to make that faster is we've built some skeleton repos, right? So we, at Project 44, most of our applications are written in Java, but we have some Python and Node.js as well, and we've built out some skeleton repos that allow our developers to kind of easily spin up a new microservice and includes all the batteries, right? CI, CD, observability, everything, and it's deployed, you know, within minutes. This also includes, you know, optional basic building blocks, like, you know, we, if you need Postgres or you're using Kafka or whatever other technologies that you need, we have all of those kind of built in optionally into these skeletons, right? So as a developer, you know, this is kind of what the workflow looks like through GitHub Actions. You know, you select the repository type, so we have, you know, templates for, again, you know, like a Cloud Run function or, you know, a Java microservice skeleton or, you know, Java Postgres skeleton. You give it, you know, some other information about who owns it, you know, a few other things, and then you run that workflow and your repo will get created, right? So this is kind of what the repo looks like, you know, it creates the code owners, the name, and then if you look at, like, the files, you know, it'll include all this stuff, right? So this, for example, is a Java service, and, you know, it has all those build systems with, we use Gradle, so, you know, it has all of that kind of set up for you. So that all you have to do is, you know, add your actual feature logic and application code, you know, in your next PR, but the service is already ready. So next, I want to talk about kind of how we've automated Terraform, right? So, you know, we talk a lot about automation, but great, you know, Terraform, you can run that locally, but that doesn't scale well when you're working across, teams, you know, globally, you know, and we want to just kind of go through how we've set that up, right? So we use a tool called Atlantis. It's an open source tool for Terraform automation, and the way we've run this is we chose this because, one, it's fairly easy to deploy, right? It has its own Helm chart, and we're able to deploy it using, we use Argo CD, and so we deploy it fairly easily, right? We include that Helm chart, we deploy this out, and it's very similar to kind of how your Terraform workflow runs locally, but, you know, it's just all run through GitHub, right? So the workflow kind of looks like, hey, as a developer, I create my PR, right, with the changes I need, whether that's I'm creating a new piece of infrastructure, a new monitor in Datadog, whatever that is, right? It will, there's an Atlantis app that runs on GitHub, and then there's the deployment that runs on our Kubernetes cluster, and the remote state is stored in GCS, right, in a storage bucket on Google Cloud. So, you know, the developer pushes their PR up, they get the Atlantis plan, which is the same thing as the Terraform plan, right? It'll show that output. You get it approved, you apply it, merges, and it's kind of the same workflow that you would see locally. So with that, right, we use that for, you know, all of our infrastructure, whether it's across Google Cloud, AWS, MongoDB, Elastic Cloud, but we really want to focus kind of on how we've automated observability, right, because ultimately that's, the infrastructure is great, but, like, when you're, as a developer, what I really care about is, hey, when something goes wrong, can I, like, figure it out quickly, right, and reduce the number of, you know, bugs, issues, incidents, and try to get to that, you know, reduce the MDTR overall, right? So the way we've done this is we've tried to build kind of as close to a zero-touch setup for this as possible, right? So from kind of the templates that we have, right, those microservice templates, we also have a kind of centralized Helm chart repo that we use. These are full of various templates for our Java services, Python, et cetera, and they include kind of all the different, you know, things that you might need for service running in Kubernetes, right, your service object, your ingress, secrets, whatever else, right? And so as a developer, they don't need to know kind of the internals of, hey, you know, get super in-depth with Kubernetes objects, you know, the various resources. All they have to do is say, hey, this is my service. If I need to, like, you know, extend it, we've added some support for that, you know, with some environment variables. But then also kind of on the observability side, right, we've set up defaults for injecting the Datadog tracing library via the admission controller, right? So all you have to do is nothing. It's all included, right? It's all set up for you, right? And the reason, right, you know, four or five years ago, before a lot of this was kind of built up, is, you know, you would have to sit there, you know, and instrument each and every one of your services, right? You would have to add the tracing library. You would have to potentially instrument, you know, your code. And that can take, you know, a lot of time, right? At Project 44, we have, you know, hundreds of microservices, and you're doing that for each repo, just getting that tracing library. Then you have to worry about, you know, the build process. You've got to deploy it out. That takes minutes, right, per service, and then you have to wait for it to get released. And that can take, you know, days, weeks, whatever, right? So by implementing the submission controller for that, we've basically sped up the process to no time at all, right? Additionally, kind of with this, right, we've enabled a lot of the environment variables. Specifically with Datadog, right, they support a lot of integrations around, you know, MongoDB, Postgres, et cetera, to, like, track those metrics, you know, at the infrastructure level. And so we've set up, you know, those environment variables by default. You know, we include things like, hey, do you want profiling? Do you want, you know, header extraction, injection? All of that, right? And so all of this is set up by default, right? And so kind of the way that looks is, you know, this is in our Helm chart, our default values file, is, you know, we have all these variables set up. But, you know, you can see, like, hey, you know, if you're sending some custom metrics, you can provide that list. You know, if you want profiling on, you can turn that on. But specifically, you know, you can see where we have the library version and the library injection, and that's kind of where the admission controller happens, right? And that happens at runtime, right, so that when your application gets deployed, before your application actually starts up, the admission controller will inject that library, and the tracing will be set up. So, great. You have all that set up now, and you have all this data coming in, right, to whatever APM tooling you're using, but now what, right? That's great. But, again, as I mentioned, there's, you know, hundreds of millions of points coming in for us, you know, per day, and it can be pretty overwhelming, right? So, what we've done is we've built out some kind of modules to manage those alerts and kind of focus our developers on, hey, these are the things that you should focus on, and this is where, you know, this is how you should kind of build, this is what you need so that you're covered, right? So, before we kind of get there, there's also kind of a lot of different ways to set up your repo, right? And so, what we've done is, one, we've built a lot of alerting modules kind of that live in our, the same repo that we have all of the actual Terraform code in, right? So, there's various ways to do this organizationally, but that's kind of how we've set it up. And so, what we've done is we've built some, you know, standardized modules on some golden signal metrics, as well as kind of some other business-specific monitors, you know, for various things, and that allows our developers to just provide a minimal amount of detail to get kind of the right monitor set up, right? This also allows us to kind of make some global changes, right? So, if we want to update that module and say, like, hey, you know, everyone should be, like, doing this X, Y, or Z thing, right, it's very easy to just make that change once, and it gets applied everywhere, right? It also allows our engineers to easily make, you know, to spin up new monitors for a new service or an existing service fairly quickly, right? Instead of having to go through a UI and touching, you know, potentially, hey, I have a service that has, you know, 30 or 40 public API endpoints, and I have to, like, set up monitors for each of those, you know, latency monitor or whatever. That can take time, right? This speeds that up fairly quickly, right? So, kind of just touching a little bit about the golden signals, right? These are kind of the main modules that we want our developers to kind of pay attention to from an alerting perspective, right? So, we talk a lot about kind of the latency, right, which is, you know, time to require to fulfill a request, right? The traffic, which is, you know, how many hits the rate of requests coming into your service, you know, saturation, which is the percent of resources consumed or your capacity kind of remaining on your service, and then also error rate, right? So, kind of how many errors are happening over time. And so, the way we've kind of built this out, as I mentioned, is, you know, we have a repo, and this is kind of how it looks, right? We have, you know, all of our teams set up, and each team gets their own folder. We have kind of our module directory in the same repo, and, you know, there's a ton of different ways to organize Terraform repos. This is specifically what we do for Datadog, for our, you know, Google Cloud infrastructure. That's more segregated by project, and then, you know, by service. But this is kind of what we found works best for monitoring, specifically. And so, you know, this is kind of an example of what our error rate module looks like, and so, you know, as a developer, all they have to do is specify, you know, if they want just the defaults, we've set up a bunch of defaults, you know, saying, like, hey, for example, you know, I want 99.95% success rate, or whatever, right? That's our default threshold, and so, as a developer, all they have to do is say, hey, here's my service name, and great, you get that. But potentially, you know, we've also added a lot of customization, like, hey, you want to change, you know, the specific endpoints that you're monitoring, or the, you know, the operations, whether it's, you know, an HTTP request, or a Kafka production, or whatever that is, right? And also the ability to update those thresholds, right? And so, you know, a developer can use this module, they'll spin it up, and then, you know, this is that monitor, that same monitor, right, in Datadog. So this is an example of a service called OnRamp Conduction Manager, and all they've done is they've specified the service name, and this one, you know, it's tracking all of their endpoints for all of that, right? And so all they had to do was specify that name of that service, and boom, right? They get this. Next, kind of showing what this looks like, right, in code. So this one is a specific example of our latency module, but, you know, using Terraform, you can see, like, hey, this is a service, and they want to monitor three different API endpoints, right? And each of these have a different threshold, because, you know, not each API endpoint needs to respond at the same time, and, you know, what the developer cares about or what the customer experiences is different based on those, right? So all they have to do is, right, you know, here's just kind of a simple, they're calling the module, they're providing some different API endpoints, and this makes it really easy, right, to extend, right? So if they have, you know, they want to add, like, 18 more endpoints or whatever, all they have to do is add those, right? And they don't need to copy that module over and over again, right, because Terraform, even though it's not as great as writing, you know, regular code, you can, you know, it has a lot of the same conditionals, loops, et cetera, even though it's not as fun to write, right? And so, you know, this is an example of a latency module, and then kind of as we've matured as an engineering org, right, we've moved kind of towards how we think about our monitoring, right? So the golden signals monitoring, all of that is great, but ultimately, right, like, those kind of spikes can happen, you know, at any time, but what really matters is kind of what we have set our agreements with with our customers, right? So we have internal SLOs across our org, you know, internally saying, like, hey, this API endpoint needs to be available for, you know, 99.95% of the time, or the latency needs to be under, you know, one second for, you know, for 99.5% of the time in this time period, right? And so whatever that is, is, what we've done is we've kind of built out burn rate modules, right? So this is using SLOs, and you're tracking, hey, as you get closer to breaching that SLO, right, so you know, if that threshold was 99.9% or 99.5% or whatever, as you get closer to kind of hitting that threshold, the burn rate saying, hey, you're using up what is termed your error budget, right? And so that's really where, like, hey, we need to focus on preventing that, like, preventing us from using up that error budget, and let's focus on getting that fix in place. Because ultimately, right, like, one latency spike at, you know, 3 p.m. or 2 a.m. or whatever might not actually matter for a customer, right? What matters is, hey, over time, how are we performing, right? And this allows us to kind of reduce our noise further for our developers, right? So that they're actually focused on, like, hey, what does the actual experience look like for all of our customers? And so, you know, it allows us to balance kind of that reliability along with our user experience. And so, you know, this is an example of kind of how that looks, right, is you set up that, you know, you call that module, you give it the service name, you can set up those thresholds, and you get that SLO created in Datadog, right, with that burn rate alert created with you, right? And so it gives you kind of an overtime view of how you're performing. In this case, this is a, you know, an error rate SLO saying, like, hey, how many errors do I have in a, you know, seven-day period or 30-day period, and how many can I have before I, you know, run out of time? And so, you know, that's great. So this is kind of what the experience looks like for a developer, right? Again, going back to kind of how Atlantis works, you know, they'll create that PR, and it'll show you, hey, this is what that query will look like in Datadog, right? This is what the monitor will look like, thresholds. This is the same thing you would see kind of locally if you were running, you know, Terraform plan locally. So whether that's creating a new monitor or you're changing something, this is what it looks like, right? And then, you know, you'll run that apply, and, you know, it'll get applied. You can see the output of that apply, you can see the resources that are created. And again, all of this is stored, you know, for Terraform in our remote state in GCS. So kind of with that, right, just kind of concluding all of this, right, it's great, like, we've built all this tooling, but how do we actually measure the success of this, right? And so we want to make sure that our engineers, one, are adopting this stuff, right? They're actually using these alerting modules, but also we want to make sure, it's also that can attract, hey, does it reduce our, you know, MTTR during an incident? Are we actually, like, improving the time it takes us to identify an issue and all that, right? So the way we've done that is we've built out, using, you know, Datadog scorecards features, we track, you know, various different custom rules that we've built out, saying, like, hey, are our developers, you know, using these modules? So it doesn't show it here, but in that it's sort of the best practices, you know, it'll show, like, hey, do they have a burn rate monitor? Do they have SLO set up? Are they using kind of our advanced deploys here where they're using, you know, Argo CD with, you know, rollouts or canaries or blue-green deployments or other strategies, right? And so we want to track that and be able to say, like, hey, is this actually leading to measurable improvements in, one, for our engineers, right, is it reducing the overhead for them? And then, two, is it actually, like, improving our customers' experience with our products, right, by reducing the number of incidents and that kind of thing, right? So with all that, right, kind of all the stuff that we've built has really, it's all about kind of the time savings, right? We've reduced the time it takes to spin up a new microservice by almost a week, right, including the kind of associated infrastructure that you might need with that, right? So whether it's a memory store instance, Cloud SQL or whatever, you know, it takes maybe less than an hour now. You know, it's very close to, you know, our engineers, we want them to have the right monitors and observability built out with that service as well, right? And this is, you know, fairly close to zero-touch setup, right? All you have to do is potentially, you know, adjust some thresholds. And, you know, as kind of we've moved along and, you know, with all the new, you know, AI tools and things like that, we've been exploring, like, how do we even automate this even further, right? Can we build out these monitors automatically and adjust those thresholds automatically? We haven't kind of gotten there yet, but we're kind of exploring that so that, hey, as an engineer, you don't even have to do anything, right? It's kind of all there. I do have some kind of, let's say, thoughts about that because I don't think, like, hey, I think some of that, like, it should be very deliberate, right? Like, you don't want to have something automated, like setting up an alert that you don't, like, it just gets created automatically, and then you're like, great, like, was that threshold even right? Like, I got woken up? Like, I think there is a value to being a little bit more deliberate about, hey, setting up these monitors, right? So there is some value in, like, having a little bit of manual work there because that also allows the engineers to kind of validate, like, this is what we actually care about and what we need, right? And again, we've reduced that kind of overhead for our engineers with all of this, right? By building those modules, they don't have to, like, worry about, hey, oh, I care about, like, the error rate or this. They have a module. They don't have to, like, one, you know, we have 15, 20 different engineering teams. Not everyone needs to build the same thing over and over again, right? We've kind of centralized this. We have a module that everyone can use, and our developers have really kind of run with this. They've built out modules that are like, oh, we think this is useful for other teams, and so they'll add that module in and they'll make an announcement and they'll be like, hey, if anyone is, like, interested in monitoring this kind of thing, you know, whether it's some custom metric or whatever else, you know, our engineers kind of run with this as well, and it's become kind of a shared ownership between our platform team and our engineering teams. And kind of lastly, by building all this, right, the ultimate goal is, again, as I said, like improving that customer experience, and so we've reduced kind of the number of incidents caught by our customers first, right? Four or five years ago, almost everything, all of the issues that we had were caught by our customers first. By building all this out, we've kind of, we've gotten to the point where we're still not perfect, right? Our customers still catch stuff, but we've drastically reduced that down by about 70%, right? Most of the stuff we're catching before, and that's all thanks to kind of what we've built out here. And so with that, I'm going to bring back Ajuna up on stage to talk a little bit more about how Datadog has kind of helped, can help you build all of this stuff without having to do it all on your own. Thank you. So to help close out, would love to leave you all with a challenge to go look at ways to automate your observability. If you are already using observability tool like Datadog, there are ways to track areas that are potential for improvement, like the scorecard feature that Manan talked about. But generally having a centralized place, whatever you are using, for all of that information to know how developers can work with each other within your team and improve on that and make it easier in an instant to find each other is something that definitely matters. So since I talked about automation, I have to plug our workflow automation tool because I think that it can make it easier for all of you to automate some of your observability. A lot of the things that came from workflow automation were from internal hackathons that we had. One is if you do want to automatically make monitors with Terraform and Datadog, we recently launched a workflow to make that easier for you to do. You don't have to build out what Project 44 does. It doesn't obviously do all of the cool things that theirs does yet, but right now you can make it do what you're looking for because it is a template, so you can edit it, change it, and it'll make GitHub pull requests for you to launch monitors in Terraform. Other ways that I think you could improve on your observability is if you're automatically making your monitors, you should automatically be able to turn them off. So consider a way to mute and unmute your monitors, especially during an incident or planned downtime, as well as scaling your... In workflow automation, you can also do things like scale your GKE tool, your GKE node pool, so you don't have to just do things related to observability. There are lots of ways that workflow automation can help. So the purposes of this challenge is I would really love to see this number get much higher in our next report, and I really hope you can think about ways to make your developers' lives a little bit better and automate your lives a bit. A little bit more energy efficient. The Airing