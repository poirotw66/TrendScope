 . Love the energy. Okay, so this is enterprise-grade security and scale for serverless workloads with Cloud Run. My name is Xia Wen. I'm a product manager on a Cloud Run team focused on serving our enterprise customers. And with me is my very talented co-worker, Thomas, and our amazing customer, Tejas from ANZ Bank. Thank you. So we have a lot of great content and a lot of great updates for everyone here today. But we'll start with the basics. What is Cloud Run? So Cloud Run is Google's fully managed serverless container platform to run your code functions or containers on Google's infrastructure. So you give us your container or some code that can be turned into a container and we'll run it for you. Cloud Run can scale up quickly to meet your traffic needs from zero to thousands of instances in just a few seconds. And we automatically scale down to zero during off hours. Cloud Run is focused on doing one thing well, which is to run your container. And for the rest of it, we leverage the best of Google Cloud. So Cloud Run is integrated with Google Cloud services like Secret Manager, Cloud Monitoring, Cloud Storage, and others. And your Cloud Run workloads participate in your advanced network and security architecture alongside your VM and Kubernetes workloads. And throughout this entire journey, our priority is simplicity and velocity for everyone. It's easy to use. There are no VMs or clusters to manage. And we're proud that Cloud Run consistently gets top satisfaction and usability scores from our customers. So is Cloud Run ready for the enterprise? Is Cloud Run ready for my enterprise? The answer is absolutely yes. For many reasons. First, security and compliance. Cloud Run was created with security built in. There's always two layers of sandboxing for all customer workloads. All your data is encrypted and there's access control built in for all traffic to our apps. Cloud Run is integrated with Google's best-in-class security products and compliant with major certifications like FedRAMP, HIPAA, and PCI. Cloud Run was built for the diverse workloads you find within a typical enterprise. AI inferencing, web serving, API backends, data processing. And whether you have a public-facing app or a private-facing internal app. Whether you have a cloud-native app or an app written 10 years ago and recently migrated to the cloud. Whether you have a lighter-weight app or a heavier-weight Java Spring boot app that takes 45 seconds to start up. Cloud Run is built for all of these different kinds of apps. And third, Cloud Run is cost-effective. Because you don't have to provision, pre-provision, for your peak traffic. With blazing-fast auto-scaling and automatic scale down to zero, you can use and pay for only as much resources as you need. And with zonal redundancy out of the box, you don't have to over-provision in multiple zones in a region to enable zonal failover. And beyond that, there are many more ways to save with multiple billing modes and committed use discounts. With all these benefits, many, many customers, enterprise customers, are choosing Cloud Run. We have customers in the retail space, in financial services, and many other industries. And in fact, within Google, Cloud Run is now the preferred platform for Google's internal apps, which are apps written by Google for Google. So now that we've covered the basics, let's review what's new for enterprise apps this year. We'll start with AI. If you're new, try using Cloud Run together with Vertex AI. You can host your AI model in Vertex AI and put your front end in Cloud Run. Many customers are already using this design pattern in production. If you want more flexibility, you can host your own model directly in Cloud Run with GPUs on demand. And this is now GA. It takes just five seconds to start up a new Cloud Run instance with an NVIDIA L4 GPU and pre-installed drivers. So your AI inferencing app can start up quickly, start up and scale up quickly in Cloud Run. And like other Cloud Run workloads, your AI app can also automatically scale down to zero, and you pay by the second and only when in use. Now, speaking of AI apps, if you have your AI model in Cloud Storage, then I have a tip for you. Consider using a feature called Cloud Storage Volume Mounts, which just reached GA the second half of last year. With just a few clicks in the Cloud Run UI, you can mount a Cloud Storage bucket in your Cloud Run container, and the contents of the bucket appear as part of your file system. So your app can now easily access the AI model in Cloud Storage. And of course, beyond AI apps, you can use this feature for many other use cases, too. If you have large media files, or you have config files, or other static assets in Cloud Storage, you can now bring them easily into your container. Now, AI models are very big. Many, many gigabytes in size. If it's not built directly into your container image, then you need a fast network connection. So your AI, so your app can download your AI model quickly and start up quickly. And for that, we have DirectVPC Egress, which is a great feature that's gotten even better this year. DirectVPC Egress is Cloud Run's built-in VPC networking solution, where you get IP addresses, where your Cloud Run instances get IP addresses directly on the VPC. It's faster, more scalable, more reliable, and lower cost than the previous solution. And this year, DirectVPC Egress now has better support for IP-constrained environments. Your Cloud Run services now use half as many IP addresses as before. And we support smaller subnets. And support for hybrid and private NAT is coming soon. Beyond that, DirectVPC Egress support for Cloud Run jobs is now GA. You get the same performance, scalability, reliability, and cost benefits as services. And we have also reduced our IP address requirements for Cloud Run jobs. So now that you have your AI app running Cloud Run with a fast network connection, you're ready to welcome your first user. But how do you authenticate them? The solution is to use Identity Aware Proxy, or IAP. And that is now built into Cloud Run. IAP is Google's fully managed service for end-user access control. With just a few clicks in the Cloud Run UI, you can now enable and configure IAP for your Cloud Run service. And when your end-users come to your web app, they will now be presented with a login screen. And they can log in with their Google identity or an external identity. And under the hood, because this is a direct integration, your IAP setup is now dramatically simplified. You no longer need to manually create a separate Google Cloud load balancer. And because it's a direct integration, all ingress paths are protected, including the built-in run.app URL. All right. After you welcome a few users, you might start thinking about creating a highly available architecture. Now, Cloud Run is a regional service. So you get zonal replication and zonal failover out of the box. And that is good enough for a lot of apps. But some customers want more. So if you want a multi-region deployment, you can now use a single GCloud command to deploy your Cloud Run service to multiple regions at the same time. If you put your multi-region Cloud Run service behind a single Google Cloud load balancer, your end-user traffic will be automatically sent to the closest region. And coming soon, you'll soon be able to create or configure cross-regional failover. So you can create a readiness probe. And if that readiness probe fails due to a problem in your app or an upstream dependency, then your end-user traffic will be sent to the closest healthy region. Let's now talk about your data processing needs. So, for example, if you're a bank, maybe you want to do a portfolio risk analysis once a day. For that use case, you can use Cloud Run jobs, which is great for running scheduled batch jobs. And if you want to process events that are pushed from a queue like PubSub, you can use Cloud Run services. Now, what if you want to continuously pull the data, for example, from Kafka? For that use case, we'll be launching soon a new resource called Cloud Run Worker Pools. Worker Pools are designed for non-request-based workloads, like Kafka consumers. And unlike Cloud Run services, Worker Pools do not have HTTP ingress. No built-in run.app URL. No HTTP ingress means they do not scale based on HTTP traffic. Instead, Worker Pools can automatically scale based on CPU utilization, or you can manually scale it based on a custom signal like Kafka queue depth. All right. So now that we've architected a great app with Cloud Run, I'll pass it over to my talented coworker, Thomas, to talk about managing and securing all your apps at scale. Thank you. Thank you, Xiaowen. Hi, my name is Thomas, and I'm here to talk about how Cloud Run is also built for admins and operators. Since the beginning of Cloud Run, we've been building tools and features to help make enterprise things easy, such as security, compliance, and reliability, so you can focus on building. Let's start with authorization. Cloud Run heavily utilizes IAM for both control plane and data plane access. Cloud Run provides fine-grained permissioning, as well as convenient roles for common patterns, such as a Cloud Run developer or Cloud Run admin. All permissions are now deniable. This allows administrators to prevent specific permissions from being used within roles that are granted to a principal. So a common use case for this would be preventing deletion of backups or services. IAM deny can ensure that service accounts that are used to create and update your service aren't used to delete the service, which would unintentionally cause an outage. You might have a separate process for how to turn down a workload. Disable IAM will previously, in order to create a publicly accessible Cloud Run service, the all users principle must be granted access to invoke the Cloud Run service. But by disabling the IAM invoker, this allows public ingress even if your organization makes use of the domain-restricted sharing organization policy. And finally, principal access boundaries ensures that users and service accounts cannot be granted access outside of your organization. By ensuring that IAM access can't be granted on a service account or user outside of the organization, that can help prevent phishing attacks or data exfiltration. Additionally, access boundaries can be placed on a project level. So you might enforce that a service account that is granted access in a production project is never granted access in a development project as well. Again, preventing possible data exfiltration. Cloud Run threat detection is now built directly into the security command center, enabling always-on and intelligent security. This is a one-click integration that improves your security and operational efficiency. Cloud Run threat detection continuously monitors your Cloud Run workloads for remote access attempts and most common runtime attacks like a malicious URL observed, reverse shell, unexpected child shell. This monitors kernel analysis as well as network logs and container vulnerabilities. The security command center powered by unparalleled threat intelligence of Mandiant and Gemini AI is a single view for monitoring and investigating threats. If Cloud Run threat detection detects an attack, it generates in near real-time a finding in the security command center and alerts you. This dramatically reduces your exposure and saves you costly downtime due to incidents. The security command center matches artifact analysis findings to running Cloud Run services, allowing you to pinpoint the base artifact that introduced a vulnerability. Ultimately, this integration empowers you to deploy Cloud Run with unparalleled competence. It's about simplifying security, strengthening your defenses, and ultimately accelerating your innovation by removing security as a bottleneck. Cloud Run now supports custom organization policies, which is in general availability. Previously, Cloud Run provided a handful of common organization policies around controlling ingress, egress, and binary authorization. With custom organization policies, your organization can provide enforcement on any Cloud Run resource specification, tailoring the needs directly to your organization. Additionally, custom organization policies supports Dry Run, which can provide a report of all the resources that would be in violation of a potential new policy. I want to show a few examples of the power of custom org policies. Org policies are defined using the common expression language, or CEL, which enables unique and powerful constraints. Okay, bear with me. I'm about to show you a wall of text, but I think this will help show the power and flexibility of custom org policies. Here we go. The first is you can now create conditional org policies. So, for example, you can write a check that if your service disables IAM, that we talked about in the previous slide, then that service must use internal ingress, right here. But if the service is not disabling IAM, then public access protected via IAM is still allowed. This second one demonstrates that you can tailor your organization policy to specific supported fields that your organization has defined, such as enforcing an individual network, in this case the production network that is defined by your organization. How's that for a wall of text? All right. Cloud Run has always provided a convenient and predict default URL. Now it has a predictable URL of service plus project plus region plus run.app. But now cloud run allows a simple way to disable this URL with the aptly name dash dash no dash default URL in the gcloud argument, also in the API. Disabling the default run.app URL of a cloud run service only allows traffic from the service's other ingress paths, being cloud load balancing and any configured domain mapping you have. When you disable the run.app URL and enforce that all traffic is coming through the ILB, now you can rely on any ILB specific settings, such as custom auth Z. Additionally, you can enforce traffic to only come from the same VPC. So if there are other VPCs, those services won't be able to access this cloud run service. Cloud run now supports automatic base image updates, enabling automatic security patches. So how do you get this? Well, if you build your container using the cloud run source deploy workflow, such as cloud run functions and gcloud run deploy dash dash source, that will trigger a build using Google Cloud's build pack base images. The other way is to use a multi-stage Docker file, and then you copy the built components into a scratch image. When you have enabled this, this allows for Google to automatically patch the base image, providing important security fixes to your running applications. Additionally, and importantly, this all happens behind the scenes, with no need to redeploy on your side. Cloud run will provide a system-generated log, allowing your team to track any updates that happen to your service. And finally, in 2024 and 2025, Cloud run has onboarded to many new assured workloads, including FedRAMP High, ITAR, sovereign controls. But this was all enabled through Cloud run investing in tooling that benefits every workload, such as enforcing data residency, ensuring that your data is stored and stays within specific geographic boundaries. Access transparency and access approval. This enables your organization to know what data is accessed when you file a support request, including the option to approve or deny that data access. Customer managed encryption keys. Using keys. Those allow for your organization to maintain control over the keys used to encrypt data at rest. And usage restrictions. Allowing your team to control which Cloud run services can be used within their Google Cloud resource hierarchy. And with that, my time is up. I want to thank all of the Googlers hard at work to make these things possible. And importantly, thank you, audience, for your interest, and those of you who use these features and provide feedback. And now I am honored to hand over the microphone to Tejas from ANZ. Thank you, Thomas. I give you my word that you're not far off from your drinks, right? You'll be, soon you'll be karaokeing bright side. Absolutely. So my name is Tejas. I'm a principal platform engineer at ANZ Plus. And today, I'm here to talk to you about Cloud run at ANZ Plus. And why it's a runtime you can bank on. ANZ Plus is a cloud native digital bank first established in 2022 and has recently grown to over a million customers and $20 billion in funds under management. It's not just an app. We're reinventing banking products, our internal processes, and the customer experience. It's a proposition that our customers love. With a 4.8 rating and high NPS scores, we manage to maintain that as we have scaled. At ANZ Plus, we're keen on financial well-being of the customers. Designed to help customers manage their money better with features such as spending categorization, savings goals, and financial coaching. More than just transactions and savings, though, our home lending proposition was released about a year ago. Built ground up for using cloud native technologies, we use Golang, GRPC, containerized applications, running on Cloud Run and Kubernetes, and other Google products. Today, I'm here to take you on a journey that ANZ Plus went with Cloud Run. Kicking things off, we'll talk about where we were prior to Cloud Run, where we are today, and how we got there, and where we're going. Like many organizations, we started our cloud native journey using Kubernetes. It gave us a lot of control over our infrastructure, and the large open source community was a big plus. However, we quickly encountered the Kubernetes complexity trap, with a steep learning curve, having to manage and upgrade pet-like clusters, optimizing for resources, and navigating network and compliance complexities of a highly regulated financial environment. Deemed a significant amount of our team's time and expertise. As the saying goes, with great configurability comes even greater compliance. We needed a solution that would allow our developers to focus on building great features, not managing infrastructure. Then came Cloud Run. In its early days, it offered great promise of serverless simplicity, streamlined developer experience, where developers could just focus on building, deploying, and scaling their applications. Resilience, with built-in scaling across all zones inside of a region, and handling any traffic spikes, making sure our applications are available. Cloud Run's pay-per-use model was also incredibly attractive. As you can imagine, as many organizations, we tend to over-provision everything. But the fact that we only pay for what Cloud Run used meant that we eliminated any idle costs. On the flip side, though, in the early days, the security posture, the private network connectivity, and use cases were limited. Now, in 2019 was when Cloud Run initially released, and we were happy to see that it supported GRPC because we had a lot of GRPC applications, but it wasn't quite enterprise-ready. 2020 to 2021 was a big year for networking and security. The addition of the Australia region, VPC access connector, VPC service controls for Cloud Run, and C-MEC were all instrumental in ensuring that our data exfiltration, data residency, and data encryption needs were met. 2022 to 2023 was really a year for expanding the use cases for Cloud Run, with features such as always-on CPU to run those background processes, sidecar to run those extra processes inside of your service, and Cloud Run jobs. These were the years where Cloud Run really met a vast majority of our compatibility requirements for our workloads. And finally, 2024 to 2025, I personally think it's a big year for performance and ease of use, with features such as direct VPC egress, which allows for greater throughput and reduces the prerequisite step for getting your Cloud Run up and running in an enterprise environment. Having deterministic URLs and native multi-region deployments in Cloud Run really meant that Cloud Run was really that much more easier to consume as an enterprise. Now, once Cloud Run has met all of our requirements, we made it available for our developers, and we saw a natural and organic adoption of Cloud Run. Developers had chosen Cloud Run for new projects and even started migrating some of their existing projects by themselves. Then we did a study internally between Kubernetes and Cloud Run, and these were the results. First, infrastructure costs. Cloud Run does have a higher cost per compute initially, but as a banking organization, we, for redundancy purposes, we had over-provisioned our Kubernetes clusters to make sure that it was available in all zones across all regions. And the platform tooling required in order to run the Kubernetes clusters also took up a lot of extra costs. So, when we did the study internally, we actually noticed that Cloud Run had a lower overall platform infra cost. Cloud Run significantly simplifies deployment and scaling of your applications. It reduces the cognitive load required for your developers, allowing them to focus on performance and scaling rather than label selectors and pod disruption budgets. Kubernetes supports a wide range of applications, including stateful sets and complex deployments. But internally, a vast majority of our applications were stateless as we used Spanner as our data source. So, Cloud Run was an ideal runtime for us. From an operator's perspective, Kubernetes requires significant expertise in infrastructure management, security, and networking. Because of the increased configuration, it had a massive attack surface, which requires careful configuration and management to ensure security and compliance. Cloud Run, on the other hand, simplifies all of this by providing the essential knobs required to the developers and the operators. So, after all this analysis, the developer feedback, the performance benchmarks, and the cost comparisons, we arrived at a clear conclusion. We made the strategic decision to formally establish Cloud Run as the default runtime for a vast majority of applications in ANZ+. Where we are today? For about a year now, Cloud Run has been the go-to runtime of choice. And the numbers speak for themselves. With over 1,600 Cloud Run services deployed and 300 plus in production, we run some of our most critical functions in Cloud Run, including customer entitlements, transaction history, managing cards and settings, and sending push notifications. It's truly become an integral compute runtime and an engine for innovation and velocity at ANZ+. Let's take a closer look at our push notification service. It serves about 100 million requests to our customers every single day. These notifications include transactions, security alerts, and other major updates. Cloud Run's automatic scaling is absolutely essential here. It ensures that we can handle massive traffic spikes, especially during those shopping events and those promotional campaigns, without any performance degradation. High availability and regional failover are absolutely essential for a financial institution. Our customers need to be able to access their accounts and make payments even when a region goes down. With Kubernetes, achieving multi-region is a major undertaking. It requires setting up multiple clusters in multiple regions and multiple zones, configuring complex networking and failover mechanisms. Cloud Run completely transforms this. It makes multi-region dramatically simpler. There's no clusters or nodes to provision. Deploying a service, there's native commands such as G Cloud to deploy the service and manage it. But what I personally like about GCP is the ecosystem of tooling that it has built around Cloud Run as well. And one of them is the cross-region load balancer, which is able to serve traffic across through to both the regions and even achieve failover from one region to another. In addition to that, the Spanner dual region is another great feature that we use in the Australia, Sydney, and Melbourne region in order to have a truly HA service. Now, we didn't just flip a switch and then migrate everybody to Cloud Run. We made some strategic investments in order to ensure that there was a smooth migration for our applications. One of them is having a load balancer between our services. This meant that Service A could now migrate their applications from Kubernetes to Cloud Run without having to change their DNS or having to affect their clients and dependents. Second, we invested in an internal developer platform called the X Framework. I'll talk more about this in the coming slides. And third, we invested in a transformer service, which allowed us to manage all of the platform configuration that's present in our customer's repositories at scale. Now, prior to having an internal developer platform, in order to get an application up and running, our developers were exposed to a dozen different tooling, in each of which had anywhere between 50 to 500 configurations that you could have in any of them. This involved a lot of scripting and deep understanding of the underlying infrastructure. It was a time-consuming and error-prone process, slowing down development and increasing risk of misconfiguration. Then comes X Framework. It's an API-driven platform. So, developers define their applications as CRDs, and we built golden plots. One for deploying your project, one for getting your repository up and running, and deploying each type of your application. Then, we made sure that it was declarative and intent-driven. So, the developers give us the intent, and we configure the tooling with the best practices of compliance and security as needed in order to get their outcome. Both, we made sure that it was self-serviced. We really wanted to create that one-stop shop for all of our developers' needs. Fifth, and most importantly, we wanted to ensure that there's consistent user experience, regardless of what runtime you're deploying into or what task you're trying to do. Now, X Framework has been in use by more than 600 applications, and just last year alone, it's done more than 250,000 deployments, which sums up to around 700 deployments a day. Now, our developers have gone from needing to know dozens of different tools to just defining their intent in X, and X takes care of configuring all the underlying tooling for them. What this also meant was we could swap the underlying tooling around without affecting the customers themselves or affecting the user space or the interface. Even with an internal developer platform, getting your application up and running is easy, but troubleshooting your application really requires you to know what are the inside CRDs that are being generated by these patterns. Let's now take a look at the Kubernetes pattern as an example. It creates a deployment, a service, a service account, Istio virtual service, Flagler canary, HPA, PDB, config maps, et cetera. Now, as a developer, in order to troubleshoot this mess, it's still very complex and draining. That's where Cloud Run comes in to tame the chaos. The equivalent slide on Cloud Run is just creating a Knative service, creating a GCP service account, and a GSM secret for your application to consume. Now, this becomes much easier for the developers to understand and troubleshoot, which makes their life much easier. The Transformer service. So, one of the key principles of the internal developer platform was to host the config right next to the code. But this meant, as a platform engineer, that I had to now make changes to thousands of repositories if I wanted to change anything inside of their config. So, we built the Transformer service. It handles the lifecycle of platform configuration that's present in our customer's repository. It does so in even a gradual fashion. So, you could do it service by service, repo by repo, or team by team. And it has ability to roll back configuration in case something does go wrong. Now, while we made it easy to migrate to Cloud Run, we needed to ensure that we had enough controls in place to ensure that we didn't go back to where we were before. So, we made use of these features, such as committed use discounts. And I would particularly recommend looking at the flexi cards, which allows you to get those discounts, regardless of whether your application runs in Cloud Run or Kubernetes or migrating between the two as well. And we've also enforced non-production to scale to zero, except for those event-driven applications, which we enforced to scale to one instead. Then, we had billing mode optimization. So, we provide recommendations to our customers on whether it's cheaper for them to have instance-based billing or request-based billing, and they can swap between the two. Then, we took up the direct VPC egress, which eliminated the cost of having a serverless VPC access connector proxy running, and it increased the throughput of our Cloud Run applications as well. Then, we do regular resource optimizations with our applications team to ensure that they have the right CPU, memory, and concurrency settings set for their application. Lastly, our FinOps team have implemented monitoring and alerts on any unexpected cost spikes or anomalies. This allows us to quickly identify any issues that could lead to unnecessary expenditure. Where we are going. Now, we're incredibly optimistic on the future of Cloud Run. We are keen to try out the Cloud Run worker pool and eagerly waiting for GPUs to come to Australia so that we can start running those inferences in Australia. Next, we're also very much looking forward to strengthening our security posture using Cloud Run with features such as the authorization policies on ILBs and service-to-service firewall with tags. We're really looking for that additional defense in depth over what IAM provides in order to provide service-to-service network segmentation and least privilege. And finally, we're very much looking forward to getting our hands on the service-to-service server-to-service API so that we can do more sophisticated and automated multi-region failover strategies. Now, none of this would have been possible without the close collaboration that we've had with the Google Cloud engineering teams. This collaboration allows us to get access to early features, try them out in the real world, give our feedback, influence the roadmap, and most importantly, to join problem-solving with the team in order to innovate, push the boundaries of serverless computing, and meet the unique needs of a digital bank operating at scale. On that note, Sharon. Thank you. Thanks, Tejas. I'm so happy to hear that ANZ Plus has chosen Cloud Run for the vast majority of their applications, over 1,600 workloads, all running with enterprise-grade security, higher developer productivity, and lower infrastructure costs. We'll be right back.