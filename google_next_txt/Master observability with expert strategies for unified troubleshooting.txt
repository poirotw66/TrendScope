 How's everyone enjoying Cloud Next? Yeah? Good? Excellent. See, some people are still sleeping. I know it's the session after lunch, so I'll do my best to be entertaining. So, quick show of hands. How many people in this room have had trouble dealing with handoffs during a production incident? You know? So, it sounds like some people, you know, the teams maybe don't work so well. There's a blame game going on. How about costs? Everyone feel great about spending lots of money on observability? Yeah? Your businesses love that? How about Gen AI? Why? And observing Gen AI workloads. Does everyone feel pretty good about observing those? You're totally confident? No non-determinism can hurt your stack? Not seeing so many hands. So, good news. You're in the right session. Today we're going to learn about some expert strategies for solving those problems. My name is Punya Biswal. I'm joined today by my colleague, Samit Tuladhar, from Google Pittsburgh, and Alex Van Boxel, who's a principal architect at Colibra, joining us from all the way over in Belgium. And we're going to be talking about a few things. So, we think of observability problems as data problems. And, you know, the journey that we've been on with these, I think, is best experienced through the eyes of our customers. Alex is going to talk to us about how, over the past five years, Colibra has been on a journey with open telemetry, building up their data foundation for observability. We will take some of the principles that they've learned and apply that to a case study, a problem that's on many people's minds, which is applying distributed tracing to a Gen AI agent observability problem. And, finally, Summit will bring us back and kind of put this in a larger picture, give you a framework for applying some of these ideas to your workloads, you know, realizing some of these techniques on the systems you care about. So, let's get started. Let's talk about complexity, right? We're observability practitioners here, and it often feels like the world's just always throwing new stuff at us, right? We're already dealing with distributed systems, microservices running wild on different cloud infrastructures, some running on-premises. Another is Gen AI in the mix, adding lots more non-determinism, a lot more cost sensitivity, a lot more rapid iteration. And all these different components of our systems are generating just massive volumes of telemetry data, right? When you have massive volumes of telemetry, you're talking about building scalable storage systems, keeping query times fast and interactive so you can debug during an emergency, and in the end, supporting the mission, right? We can get lost in designing these observability systems, but at the end of the day, we need to go from something is wrong to understanding why to fixing it fast, right? It doesn't matter if we build really amazing systems if they can't do that. So, how do we do that? Like I said, we think of observability problems as data problems, and data generally looks like, well, you generate the data, and then you consume the data, right? How do you generate the data? How do you generate the data? How do you generate the data? How do you generate the data? The data generation story starts with things like OTEL auto instrumentation and EBPF, giving you a strong foundation. Semantic conventions and structured wide events help you to make your data be interpretable and reusable in a variety of contexts. At the other side, you have different consumption experiences for different personas, right? So, you have your interactive exploration for looking through all your data and looking for a needle in a haystack. You have powerful query languages for your expert users who know exactly what they're looking for. And then you have dashboards, alerts, natural language, right? All these tools that help different kinds of people engage with data in different ways. Now, what we've learned as we've scaled up these systems, as we've brought them to enterprise settings, is that, you know, generation and consumption aren't the only parts at play. I didn't just make a slide with a huge amount of empty space in the middle. We found it really helpful to have collection, routing, and storage. So, what's the role of collection, routing, and storage? It's really helpful to have something in your stack that can quickly offload telemetry from the systems generating it. Enrich it, filter, sample, right? Make all these small adjustments to data that make it possible for you to correlate them in the end. We've also found it helpful to have a routing layer so you can pick and choose the destinations for each of your data. You can send your data to Google if you want or to non-Google destinations if that best serves your needs. And finally, when we're thinking about storage, again, we've talked about scale, and that's definitely a big aspect of what's going on, scale and performance. But compliance and governance have proven to be really important in our storage systems. There's a through line here, which is that these systems are never done, right? Your first iteration on generate, collect, route, store, consume will not be perfect. So, you also have to embrace the idea that you're going to constantly be improving these systems. You're going to be taking lessons learned from one production incident or one analytic scenario, and then taking that all the way back to the generation part. And, again, what we found is the open source community, the open telemetry community in particular, has been a huge tailwind, especially on the left side of the stack. So, I've said a lot here about open telemetry and, you know, seeing what our customers are doing. And honestly, we have been on this journey together, right? We've worked, we've watched customers and we've watched the community embrace these ideas and really build some amazing, amazing systems. So, who better to talk you through that than Alex? I'd love to welcome you to talk more about it. Thank you, Poonja. So, at Calibra, working on our Calibra platform, a product for delivering you unified governance for your data and AI, we've been working with open telemetry for a while. Our journey started five years ago. We were really struggling with open telemetry. We were hooked into a vendor-specific observability system and costs were rising out of control. This was the right time where you actually, we could see open telemetry coming up really as a well-defined specification and instrumentation in SDK. It really has the promises that you can start owning your data instead of like a vendor login into a vendor silo where all your data lives. Open telemetry gives you a way to own your data. Owning your data is very important because observability data is very important because your incidents, it impacts your business, right? So, business data and observability data, they're very close together. Also, we have a future plan to actually deliver some of our observability to our customers. So, open telemetry has a way to transport those data throughout our system, collect them, and then route it to whatever you want. If you really have a way to transport those data, that makes it way more hard. So, open telemetry starts at the collection point, right? So, with the open telemetry collector throughout your fleet, you can start collecting your data of vendor agnostic. At that time, you can then route it to different destinations. So, those destinations or observability backends can then deliver the right observability job for yourself, for your observability stack. So, on the observability side, part of our data, or most of our data actually, goes to observability clouds. So, in a very cost-efficient way, it delivers like nice correlation between all the data points. But we're also exporting things to data, to Elastic, where some of our analysts can dig deep into hot and high cardinality raw data. They really like that. We also export to Datadog, because our developers really like it. On the business side, we're routing our data to BigQuery, where we store our data, and with a higher attention. So, we can do analysis on that. And we have data flows doing kind of aggregations on a daily basis or streaming way. It enables different personas, right? So, you have like the performance analysts diving into Elastic, data in Elastic, that dives deep, what are the performance problems in our hot data? And the business analysts going to BigQuery to see how our data evolves over time and do deep analysis and follow the customers. And we have support engineers that really like to go deep and use Cloud Trace to see where the incidents are happening. So, you see here are a few of our dashboards. So, those personas actually have their preferred tools. We provide for more observability use cases, Grafana, that hooks into our different observability backends. And for our business use cases, we use Tableau for more deeper analysis on the long term. So, let's dive into a few of our examples that we really use business data. One of our first ones that we did was our API deprecation strategy. This is one of the first ones five years ago because Traces were only available. So, we use Traces as our data source. We extract the data, aggregate it and put it in BigQuery. And then see who is using what. The funny thing is that we, Colibra, were the biggest offender. And we were the one that was using our obsolete APIs the most. So, at least we have an actionable item and actually decrease our own usage. As soon as OpenTelemetry logs started to be available, we really start pushing developers to go to structured data or structured events. For them, they're really comfortable with that because it's actually just a step above like just the standard log line. You make it way structured. But to make it really useful, you have to start doing or actually add more good semantics on top of that. And then we're coming to the real hero of OpenTelemetry and the semantic conventions. Semantic conventions enables any tool actually to discover what is what. So, with a description of what is the cloud you're running on? Are you running on Kubernetes? Are you running on a VM? Are you running on Cloud Run? Or what describes an HTTP call? That is all in semantic conventions. It's easy to start with. It's all built in. But I really encourage you to really go a step further. I guess infrastructure or routing, that's not the only thing. You want to have business value. So, you start building your own conventions. For example, we have a notion of tenancy. We have tenant group, tenant environment. So, we started defining our own semantic conventions. For example, colibra.tenant.environmentid. That's how we describe a tenant environment ID. We use that throughout our system and it enables you to connect, us to connect everything together, no matter what system is producing those metrics. So, later we'll see how you can start doing that without the instrumentation or manual instrumentation. But without proper infrastructure, this doesn't work, right? So, you have here an architecture diagram. Don't dive too deep. But, like, it is way more simple than we started on five years ago. We had done those vendor-specific agents and collectors on top of that, open telemetric collectors, and all those things kind of grouped together. But now, after five years, we only have open telemetry from beginning to end. So, let's simplify this, right? So, with the building blocks completely to your left, yes, you have the collection, the collection really close to your application. So, it collects application data, infrastructure data, system data, and even more to the left, we even have, like, collection of open telemetry signals at our customers. Whereas, Colibra platform, we have an edge system where we actually have capabilities to extract metadata. Those systems need to be monitored as well. They send open telemetry data to our backends as well. Even in a browser, we're starting to collect open telemetry data. A bit unusual, but very useful. We start to dump everything as soon as possible in PubSub. Google PubSub is a system that has our buffery layer, but it enables, like, a lot of use cases. One of them is data processing. So, we have two kinds of data processing. The batch workloads that is enabled by the PubSub backup system, and when we have just daily jobs that read from the cloud storage backup and to do a reporting. That API deprecation strategy is one of those ones. And an exciting one is streaming processing, where we actually start to look at envoy logs and see what calls that they're making. And a URL, we're mapping it back to our open API specs. So, we're an API-first company, so all our systems have their own open API specs. That data flow just gets all the open API specs and do reverse mapping. And we'll create metrics out of that with a way lower cardinality, and it seems to be a very useful metric. Those signals, those calculated metrics go back to PubSub. And along with the original raw data, it is ready to be exported to one of the backends. So, we have several parallel pipelines at the right that actually takes the data, subscribes to PubSub, and then dumps the data to the right observability backend. So, the real workhorse here is OpenTelemetric Collector. There are two roles in our architecture where we use a collector. One at a collection point. So, we have them running our Kubernetes nodes. Each node has, like, a collector. Also on our VMs, we also have, like, a central collection point for some miscellaneous data collection, like our CICD pipelines. Then, as soon as they run PubSub, we have other subscribers. We have our export pipelines that subscribe to the PubSub and then start doing transformations, enriching, doing kind of governance, filtering, filtering, and so on in those pipelines and then export the data. We are very proud that we actually contribute back into the collector our importer and exporter of PubSub. So, it is available to everybody for use. If you have, like, more classical or on-premise solutions, there are also things like Kafka Collector where you can actually hook into that as well. So, for five years into our journey, we're very happy that we started and we were an early adapter. It was a bumpy road. Not everything is perfect, but, like, the stability of the protocol really helped us to be very vendor-agnostic for collecting data and that really makes choosing any backend really possible. It keeps everything in control, so we are now free to do and reach owner data to whatever we want. So, for us, we also were very happy that we started building on top of Google on PubSub, using PubSub, GKE, BigQuery. It just scaled with us without a problem on our use cases and our customers. So, I'm going to hand it back over to Punja so he can actually talk and hopefully incorporate some of our learnings into the product and see that we can even simplify our product even more. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. So, I think everyone can agree that was a fascinating deep dive. Five years compressed into ten minutes is not easy, but, you know, it's clear that Colibra has really figured out how to solve some hard problems by taking open source and Google technologies and other vendors of their choice and pulling it together into something cohesive that works for them, right? And there was a lot going on there, right? Big architecture diagram, lots of components. You're here with me. It's afternoon after lunch. What's in it for you? Well, so I'm going to bring it back and talk about a problem that's maybe hot, right? That's maybe on everyone's mind. How can we apply some of the same ideas when we're trying to observe an AI agent, right? As I said, we're observability practitioners. You know, we squint at everything and we see distributed systems. And I'm going to do the same thing to AI agents. It's probably the same slide I had last year. Just an AI agent in there. Kidding. An AI agent for an observability practitioner is a distributed system. It's an orchestration program that talks to a language model. The language model could be local, could be an API like Gemini, and it talks to a set of tools, right? Each tool is backed by APIs. Now, if you're a distributed tracing person like me looking at this, you're thinking, wait a minute, how do I tie together all the things that happen when backing a single conversation? And the answer is context propagation, right? So, it's really important that when you put together this AI agent with its orchestration and whatnot, that you ensure that trace context gets propagated between all the components. Once you have that, you're able to generate correlated telemetry, which means metrics, log events, trace spans, and send them all into a single cohesive data store. In our case, we're going to use cloud observability, and that powers a variety of different experiences based off of the same cohesive data. Now, before I dive into the demo part of this, I want to talk quickly about that first part, you know, if you're not familiar with it, context propagation, traces, this all sounds complicated. Sounds like a lot of work to set up, and really, it's not, right? Open telemetry makes this problem really, really easy. So, for a Python program, which is what my example is, there's a single command, open telemetry-instrument. If you wrap that around your program and you give it some configuration, it'll set up context propagation, it will instrument a variety of popular libraries, give you spans, metrics, logs for all of them, and just give you a great starting point, right? But, and this is the great thing about being an open standard, it's also very easy to extend. Alex talked about building your own semantic conventions, augmenting your telemetry with business data, and this is a great example of doing that. As you can see, one line gets you an extra span. With a couple of extra lines, you can add, you can augment that span with attributes relevant to your business. So, this is where you could add in a tenant ID if you wanted. This is where you could add in a customer segment if you wanted. So, with that out of the way, I'm going to dive into the demo part and, you know, could we switch to the demo, please? Thank you. So, for people who aren't familiar, this is Cloud Trace. It's a part of the Cloud Observability product. Specifically, we're looking at the Trace Explorer. For this demo, I've set it up to look at all spans coming out of our Gen AI agent. It's called the Next25 Telebot, and I'm looking at a particular API call, get slash ask hybrid, which represents top-level conversations. So, there are spans for every individual thing that happens within the conversation flow, and we'll look at that a little bit later in the demo, but this is a good starting point for us because we can start to spot some patterns in our data. Looking at the data, we can see, are people able to see from the back of the room that our shortest conversation interactions take about two seconds, and our longest ones take about 16 seconds. So, that's coming across from the UI. We can also look at the colors. So, darker colors mean more spans in that latency segment. So, you can already see that somewhere between three and ten seconds is where most of our conversations take, but the outliers can take as much as 25 seconds, right? So, you need to talk to your UX researchers and talk to your users to see, is this an acceptable latency, or do you need to improve something, right? The numbers don't mean anything in of themselves. You need to tie them to your business context. But sometimes, you know, what you're interested in isn't just the average or the median latency, right, or the prevailing error rate. You want to go deeper. You want to know something like, what's happening in this specific conversation? Well, tracing gives you a way to look at what's happening really, really, in really great detail, right? So, this Gantt chart gives you a timeline of everything that happens during one conversational interaction. Even for an on-caller who is brand new to this system, they can look at this picture and quickly get a grasp of what's going on. What are the parts interacting to make it happen? We start out by choosing a strategy we spend most of our time answering a question according to the strategy that was chosen. And then finally, we spend a little bit of extra time grading the interaction, meaning figuring out, you know, do we think this was good for the customer? Do we think the answer was relevant? Things like that. Each of those steps in the interaction is powered by a call to Gemini in this demo, right? And one of the coolest things you can do with Trace, I love this, is you can actually inspect what happens in each of those model calls in great detail. So, I clicked on this Gen AI chip and we can see here we sent the model a question, determined whether this question is about open telemetry in general or about ongoing development and discussions in the OTEL community. And the model got back to us with an answer which is it's a general question, right? So, that's telling us something about how the strategy step works. There's other cool stuff here from the, again, all drawn from the OTEL semantic conventions for Gen AI. We can see that this step used 43 input tokens and one output token. So, counting tokens is really important if you're trying to manage costs for a Gen AI system. And, in fact, the UI also tells us how many Gen AI tokens total were used by this conversation. So, 650 input tokens, 445 output tokens. Again, you can see here a nice balance between totally generic functionality, being able to dive into any hierarchical structured interaction of APIs, and then some specifics helping you with Gen AI. Well, this is great if you're trying to figure out what's going on in one conversation, but sometimes, you know, maybe your SREs and data analysts need to work together because they're trying to understand why did customer satisfaction drop last week, right? They're not just trying to answer a simple question about latency or error rates. they need to dive deeper into the data. They might need to write some complex SQL queries like Alex talked about. How do we do that? Well, in Alex's case, they had to route the data to BigQuery and then they had to, you know, set up custom retention and then have a totally different environment where people could manipulate the data. Luckily, in Cloud Observability, we have Observability Analytics. So, quick show of hands, how many people have used log analytics before? Say a few hands. Awesome. Hope you guys love it. I love it. Observability Analytics is our upcoming expansion of log analytics to cover all forms of observability data. So, logs, traces, metrics, costs, all these things are going to get pulled together. And this is a great place to start doing whatever kind of complex analysis you want on your data. So, I'll start by showing you the raw data. The raw data in our case is spans, right? And you can see here the span schema is really, really detailed. So, for folks who are familiar with the open telemetry schema, the span schema contains every field in the OTEL schema. All the attributes, the resource attributes, the scope attributes. You can start to see here some of the commonly used attributes in our data set, right? This is great. It's comprehensive. It's also overwhelming, right? If you're an on-caller and you see this, you probably run screaming because you don't know what to do with it. That's okay. What we recommend to all of our customers is to create your own semantic layer, right? To create curated views on top of this data that correspond to different known use cases. And these are not materialized views. These are just schema on read views. Summit will talk more about how that gets done. But just to give you a taste of what that looks like, we have a GenEI spans view. And in that view, you don't have to pull out input tokens and output tokens by digging deep into the attributes. Everything is properly typed. Everything is available in the schema itself. And you can even do things like get all of the prompts and responses concatenated together so you can review them. Nice part about this is that you can also take any of these queries, you can chart them, you can save those charts to a dashboard, and you can pull that dashboard up from your playbook anytime you have a relevant incident. So it's really a reusable asset. Again, there's a lot going on here, so to dive deeper, give you a framework for thinking about these and about adopting these. I'd love to welcome Summit. Thanks. Thank you, Pune. Great demo, by the way. So I want to take you behind the scenes and kind of talk about what powers cloud observability, what are the different components of this architecture? Alex talked about a lot of the data pipelines and data lakes. We are also building similar pipelines to handle observability data. So let's start with the ingestion side of things. So in cloud observability, we get data from your applications running in GCP as well as any other cloud providers if you are multi-cloud, as well as on-prem servers. All of this gets into our APIs. So for this, we embrace open telemetry. We have actually built a Google-owned open telemetry collector that gets you more secure version of open telemetry that is more scalable to run in production. We also support a managed Prometheus API as well as other traditional logs, metrics, and trace APIs. All of this comes into this router layer. This is where it's very flexible and powerful on pick and choose your data and send it to any other destination you want. So you could export it to a BigQuery dataset that you own or a GCS bucket or a Cloud Pub sub-topic. This is how we integrate with our third-party partners, let's say Splunk or Elastic and others. You can consume this data from elsewhere and the tools that you choose to. Some folks love Grafana for open source solution. That's also great. But you can also bring this data into the observability data lake that we are building on top of BigQuery. So on our own storage stack, we have our time series database, Google internal, very highly scalable, that gets like quadrillions of points per day, which is Monarch. And for the event data, we use BigQuery to create this observability data lake. So on top, we have metrics, logs, traces, metadata about different resources. But we are more thinking about how do we model this data as less events, events with a schema. So if you can send data as events or time series, this is a great place to store it, very highly scalable, exabyte scale performance storage layer. So our recommendation is to centralize all telemetry data that are related in the same place. And then on top of that, you can create different views of the data for different slices of the data. You can have access control on top of who gets to access what data. That's also quite important. And you can group different views as scopes. Scopes are the things that are kind of, for a particular application, you can say, application scope that says, here's the related metrics, here's the related logs and traces. You can group them together. That's what's powering our query APIs and observability experience that you all love and use every day. This includes things like cloud logging, cloud trace, cloud monitoring, alerting, as well as the log analytics, which is now, we're calling it observability analytics. So not only on the observability experience, but you can use the same data in BigQuery. This is powered by something called the link dataset that gets you access to the data without doing any copies or ETL pipelines. You can directly access it from either BigQuery, Looker, you might have a data flow pipeline, or you can use a vertex AI model, build something on top of the data. So this is all possible because we have one scalable single-story solution for telemetry. So how do we think about this data? What is the right way to think about logs, metrics, and traces? A good way to frame this is this metrics about how well you can understand something versus how aware that something is happening. Starting with the known knowns, things you can prepare for. If you know ahead of time, then you can create a log-based metric, for example. You can set up an alert. You can create log alerts. And for this, you can design playbooks on how to handle such incidents, things that are known knowns. But when you come into a dashboard, a monitoring dashboard, you might see some charts and some lines that are, some anomalies happening, something that looks weird. It's easy to find those data. However, it's really hard to understand why it's happening. To get to the root cause, you generally need to go look at the logs and traces. Once you find it, once you spot the problem, it's very easy to understand. But finding that one single log line is the challenging part. That's where it's the unknown known stuff. And then the hardest part is the unknown unknowns. This is where observability comes in. This is where you have to explore, ask questions that you had not anticipated on. This is where you need analytics to understand and find the trends that are hidden in the data. Now, to get to this model, first of all, you have to have the right data model. And the good way to model this is using wide events. You collect all of the attributes, all of the different dimensions that may be useful, store them as event data, wide event data. And some of them could be high cardinality, for example, customer IDs or request URLs. Some of them could be low cardinality, like your HTTP status codes or method names. But the high cardinality is where you get a lot of the value because a lot of the times you can spot what is the experience for a single customer or a cohort of customers. You can figure out some patterns over that high cardinality data. And this is where BigQuery is really good at looking for patterns over vast amounts of data. So where do we store all of this? This is what we are creating in Observability Data Lake. Some of the key properties is real-time, scalable ingestion layer. You can ingest from ingest to queries in just a few seconds or some second layer of latency. But the nice thing is it's a columnar database. That means your white events are a perfect fit for a columnar store. And the thing about telemetry data is it's ever-evolving. Your status code may be a string today. It could be an integer tomorrow. So it's able to handle that because it can use a native JSON type that allows evolving the schema over time. That means very flexible, ingest in schema where you can then enforce schema at the query time. It also offers a full-text index. If you're looking for that needle in a haystack, you can look for those tokens and run queries very fast. So once you have this data as unstructured or semi-structured data in this data lake, it's very important to also have a semantic layer on top. So Alex and Punya talked about semantic conventions. Semantic conventions are how you model the data at the instrumentation side, but there's always going to be some data that you might have to fix. Data is messy. You have to do some transformation or some calculations at query time. And you can do this with what we call analytic views, which is now available in Log Analytics as preview. And you can model in whatever makes the best sense for you to make it easy for queries. Once you have that, you can ask arbitrary questions. SQL is really good for asking such questions. And then you can visualize the data, create charts. You can also create SQL-based alerts on this data. However, most people struggle writing SQL queries. I myself, especially during an outage, it's very hard to come up with an initial query. And we are now launching a UI-based SQL query builder, which is coming soon, which allows you to select the data source, write your filter predicates, write your aggregations without writing an actual SQL query, but using just the UI elements. Another extension we are adding to the language is pipe syntax that you probably noticed in this demo. This is basically flipping the order of the SQL query. It's an extension of the language where you can, instead of using the convoluted original syntax, you can now say which data source, how to filter it, how to aggregate it, and what kind of data you want to project out. In that sequence, that's the way you would think about creating a data pipeline. What's the source? What kind of filtering and transformation you want to do? And how do you want to extend the project it out? So I'll show you an example of this. This is the same example that Punya had showed where you are joining data, two different data sources, logs and traces, joining by the trace ID and the span ID, and you're also doing some extraction of the different fields. For example, in here, you're extracting the payloads as well as the trace ID, and you are doing some calculations on input and output tokens and latency as well. So you can set this up as one time as your semantic layer, and all of your analytical queries will run on top of this. All right, so what have we learned today? What are some of the key strategies for mastering observability? It all starts with data, right? At the very bottom, instrumenting with generic event data. Using high-dimensional, capture as much data as you can, and use semantic conventions on top of that. Converse all of it in a data lake, and then add a semantic layer on top. This allows you to do a lot of the data exploration, ad hoc exploration, as well as analytical queries to get the real value that's hidden in this data. With that, I would like to maybe hand it off to the questions and answers, and invite Punya and Alex back to the stage. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. So any questions about observability, open telemetry, or anything that you have seen today, happy to answer. There's a mic. Rule five? Yeah. I think there's on the back of it. Yes. Hey, I have a question about handling the sensitive data in terms of ingesting any PI data, how we handle besides DLP. is there any best practice or are you introducing considering the GNI integration? Are you introducing any new mechanism for anonymizing the data? I could not pretty hear it well, but I think you were talking about ingesting APIs. So today, we have logs, metrics, trace ingesting APIs, and we are kind of converting it into also supporting open telemetry protocol. If that's not the right question, please find us after the session and we can talk more. Okay. Yeah. I was referring the confidential data handling. We'll connect later. Yes, let's take it offline after the session. One more question from here. Yes. Will there be a way to sync GMP metrics into the observability data lake in the future? Metrics, yes. We are getting metrics data out of our time series database and storing into observability data lake. This is currently available as private preview and we are making it more scalable and which allows you to select which metrics you want to take into the data lake. So this is also coming later this year. Hello. Yeah. Here I'm Mahesh. So it's a lot of stuff exciting like logs, traces, metrics and everything. Technically speaking, all this storing the data into the global cloud it's a cost. So how do we balance the cost and the observability? Do you have any solutions around that? Definitely the cost is a top concern. There's various aspects to it. So you can do fine-grained cost analysis. We're actually adding more granular cost data as part of FinOps. But you can also look at where the cost is coming from. Whether it's logs, metrics, and we actually have a dashboard that tells you the uses patterns of which kind of metrics you're actually querying and what makes more sense. So there's a lot of optimization that you can do. We're actually building a lot of tooling to help you do that optimization. And to add to what Samit said, we talked about a collection and routing layer. So we provide facilities for you to filter out data early in the lifecycle so you don't incur a cost for it. Hi. Building on the last question, I was concerned about the trace for the Jane AI, seeing that you're adding that context on the answers and the questions and the answers, right? I'd imagine that will increase the cost of ingesting these trays a lot if you're doing it for every question to your LLMs, right? So what's the plan there so cost doesn't skyrocket for traces? Again, I'm not sure what's going on with the audio, but I think you asked a question about the cost, managing the cost of trace. So I think for us, trace and logging are fairly similar in terms of cost. So if you're looking to, so we talked about some ways to reduce cost for logging, like you can decide to stop emitting certain kinds of logs, you can decide to filter out certain kinds of logs, and you're going to have access to that full suite of functionality for traces. But in addition, trace sampling is a common way that people manage costs. So you can decide that you only want to trace, say, 0.01% of all requests, or you could decide to only sample the slowest requests or the requests that generate errors. And OpenTelemetry has very flexible sampling strategies that you can use to fine-tune that.