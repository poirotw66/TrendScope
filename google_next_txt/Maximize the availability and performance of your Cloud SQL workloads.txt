 Good morning. Welcome to the last day of Next. Hopefully you folks enjoyed the party yesterday night. My name is Gopal Ashok. I am a group product manager in Cloud SQL. I'm joined by Subra Chandramali, who is also another product manager in Cloud SQL. And I'm super excited to have Govindraj Palniswamy. He's a principal architect at Global Payments. And Govindraj has been a close partner for us. And he's going to walk you through some of the things that they've been doing to basically get the best practices around architecting for availability and performance. So we have a lot to cover. So let me quickly go through what we are going to cover and just set an expectation in terms of what you can take out of the session. We'll start off with an overview of Cloud SQL and Cloud SQL Enterprise Plus Edition. We've done a lot of work around performance and scale, and we have some really exciting announcements and also demos. Hopefully that will be super beneficial for you. And we'll talk through some of the best practices for designing for maximum availability. And then Govindraj will come on stage and talk through their journey in terms of using Cloud SQL and how they architected Cloud SQL to achieve the business continuity needs of global payments. Okay. How many of you use Cloud SQL here today? That's pretty good. How many of you use Cloud SQL Enterprise Plus? Okay. Quite a few. So over the last two years, one of the things that we've done in Cloud SQL is that, you know, the product has been in the market for a long time. And, you know, we took a very hard look in terms of understanding, hey, what do we need to do to basically enable customers to run the most critical apps on Cloud SQL? And when you look at the, you know, pillars for running critical applications, there are primarily, you know, three main pillars. One is, obviously, you want to have the best performance and best price performance. The second is, obviously, high availability and reliability. And the third is data protection. So when we launched Cloud SQL Enterprise Plus Edition, one of the goals that we set is, hey, how can we actually push the boundaries in terms of performance, availability, and data protection? So what you get with Enterprise Plus is that you'll get approximately, about 3x performance compared to the Enterprise Edition. And we have done a lot of, and the way we get that is by basically investing in the multiple layers of the stack, including the hardware, software, and software optimizations. And then we also do some optimizations at the storage level. The other part is availability and reliability. So with Enterprise Plus, we basically provide pronouns of high availability. And one of the key things that we have done is we have basically looked at all the different downtimes and ask the question, hey, how do we basically make it very close to zero in terms of downtime when it comes to failure modes? And lastly, data protection. So if you have needs for higher retention of logs, et cetera, Enterprise Plus will give you up to 35 days of logs. So what we want to do now is we want to basically walk you through what's new with performance. And for that, I invite Subra on to stage. Thanks, Bilal. All right. I'm here to talk about how do you maximize performance for Cloud SQL workloads. When you really think about performance for Cloud SQL, it's very useful to think about it in three buckets, right? One, you want to choose the right addition that's optimal for you running your Cloud SQL database workload. As Gopal alluded, about a year and a half ago, we launched a new addition, or we launched additions for Cloud SQL. Currently, we have two additions, Enterprise Edition and Enterprise Plus Edition. The second thing, that you want to also look at the right compute and storage offerings within Enterprise Plus that best meets your performance requirements for your workload, right? And lastly, you want to think about the features that is available in Enterprise Plus. For instance, do you want to enable read replicas or enable data cache to maximize performance both at the single instance level and to scale out performance in the case of read replicas? Let's take a quick look at what Cloud SQL Enterprise Plus offers today. With Enterprise Plus, as Gopal alluded, we can achieve up to 3x higher performance compared to Enterprise Edition. And we do this with three different things that we offer underneath. We offer the latest and greatest hardware, optimized hardware, and we also include engine optimizations, and we also enable the new feature called data cache. Data cache essentially just extends your buffer pool into a local SSD which is on the host that enables caching of pages on a much faster storage which is on the host, right? And this innovation helps us deliver up to 3x higher read performance compared to Enterprise Edition. Currently, Enterprise Plus Edition also offers two different machine CDs. We offer N2, or what we call as N2 machine CDs with data cache, where we offer one vCPU to 8 gigabytes of memory. And we also have memory-optimized N2, which also offers data cache, where we offer one vCPU to 30 gigabytes of memory. Memory-optimized N2 is currently only available for SQL Server. Okay. So that's what we have currently, but today I'm really excited to announce the preview availability of Google Axion machines for Cloud SQL, right? With Google Axion, we are able to deliver much higher price performance compared to our existing offering. Currently, we offer, I mean, with Google Axion C4A-based machine CDs, we can offer up to 48% better price performance than our existing N2-based machine CDs. To put this in perspective, we also, this C4A machine series also enables a much higher performance compared to Amazon's RDS running on Graviton 4. We are super excited about the homegrown C4A platform. We have also enabled some of our customers early access to this platform, and based on our engagements with some of our customers, they are really excited about what C4A delivers, and they can't wait to get their hands on these machines. Let's take a deeper look at what Axion machines provide. One of the fundamental key characteristics of Axion is that Axion, the processor, is built with single-threaded hardware cores, which essentially enables a service like Cloud SQL to deliver consistent high performance for every software thread, right? We also offer up to 72 vCPUs with Axion C4A-based machine CDs, and currently we support MySQL and Postgres, right? But we've also made other innovations with C4A, not just at the processor level. We're also enabling HyperDisbalanced with our C4A machine CDs. HyperDisbalanced is a new storage architecture, and that essentially delivers much higher performance, much higher I.O. performance for your I.O.-intensive workloads. With HyperDisbalanced, we are able to deliver 2x higher throughput, so we are offering about 2,400 megabytes per second throughput with HyperDisbalanced, compared to our previous generation, where we were only offering about 1,200 megabytes per second, so we've doubled our throughput offering with HyperDisbalanced, right? With HyperDisbalanced, we're also delivering 160,000 IOPs. To compare this to our previous generation, we were offering 100,000 IOPs, so that's a 1.6x improvement in IOPs performance, right? But HyperDisbalanced also delivers other key value, the ability to disaggregate disk performance from capacity. This is primarily to... that will help you optimize your TCO. Along with that, HyperDisbalance also lets you choose your IOPs and throughput every four hours, so you can actually increase your disk performance during your peak hours of usage and reduce it after your peak hours, thereby also reducing your... optimizing your TCO. As you can see, we've been really hard at work with respect to enabling the latest and greatest hardware to improve performance for Cloud SQL workloads. But we also continue to optimize software to increase performance, right? So based on our conversations with customers, one of the most common challenge MySQL users face is how to tune their MySQL workload to achieve optimal performance, right? To address this challenge, we've introduced OptimizeWrite, a suite of features designed to monitor and adjust MySQL's configuration dynamically based on real-time workload demands and underlying infrastructure to optimize write performance and reduce latency, right? Really, three categories that I'd like to highlight here today. Adaptive Purge, that provides foreground operations such as... that prioritizes foreground operations such as new queries over database bookkeeping operations. We also introduce adaptive I.O. limits that intelligently tunes database I.O. limits to match the underlying infrastructure. And we also introduce I.O. mutex optimizations that reduce contention at the storage layer. A combination of all these three enable us to deliver up to 3X higher write throughput, while at the same time also delivering a whopping 98% lower latency. All right. So, I hope that all of you are able to take advantage of these new innovations that we are bringing to market very shortly. Now, to talk about scaling, I'm going to hand this back to Gopal. So, how many of you are really interested in ARM? I'm curious if folks are looking to adopt ARM. That's very exciting, and I think one of the things that customers have been very excited about is not performance, obviously, but also the sustainability aspect of ARM. So, I'm glad to see there's a lot of interest in that. So, we talked a lot about performance. There are really, you know, great things that we are doing, and hopefully, you know, the things that we are doing on the performance side will really benefit you and you can take advantage of it. But we are also looking at some... We have been doing some work on... In terms of, hey, how do we address some of the common customer challenges that we've been having? So, one of the things that we constantly hear is that if you're using Postgres and MySQL, hey, Cloud SQL, can you help me manage connections, right? I think there's such a common problem in the Postgres MySQL world. I'm sure some of you are already deploying things like PG Balancer, et cetera, to manage the connections. And the problem statement here is that if you have an application with a lot of short-lived connections and opens up a lot of connections against a database, that really causes performance issues. So, that's been a main ask. And the second thing that, you know, everybody asks is, hey, you guys have read replicas. Hey, why can't you just make it easy for us to scale reads using read replicas? And can you just, hey, just do it with a single click, right? So, these are kind of the couple of things that has been constantly coming to us over the last year or so. So, I'm curious. Who has this problem statement in the room? A few folks. Okay. So, I'm very excited to announce managed connection polling for MySQL and PostgreSQL. So, what we are essentially doing here is that in Enterprise Edition, you can basically enable managed connection polling, which fundamentally improves your performance by up to 5x, and you can also see reduction in latency by up to 85%. Right? So, I don't think, if you're familiar with connection polling, I don't have to explain this a lot, but fundamentally, what we have done behind the scenes is that we have built a scalable connection polling layer. So, what that means is that we basically scale the connection poolers based on the size of your machine. So, we understand how much load is coming into the system. We basically, you know, scale the connection poolers based on the configuration of the VM that you're using. And if you're scaling your VMs, we'll automatically scale the poolers along with it so that you get the benefit of, you know, the higher number of cores that you're running. So, this is something that I'm very excited about. I'll talk more about this in the demo. I think the demo is actually going to be the fun part. The second thing that I'm very excited to announce is read pool. So, manage read pool fundamentally is that it gives you the ability to create read pools in Cloud SQL. So, Cloud SQL today supports read replicas. You can create a number of read replicas, and you can have your application connect to the read replicas. The challenge there is that you basically have to manage the connections yourself. You basically have to understand which node you want to connect to, manage the connections, and scale the reads that way. We're making it super simple. So, with read pool, what you get is you basically can have a pool of read replicas up to 20 nodes. We provide a read endpoint, so that means you connect to the read endpoint. We fan the read queries across all the nodes that is in the read pool, and you can basically scale the read pool up and down. So, if you think about it, if you have a read scale, you know, kind of a scenario, and you're already using read replicas, this is actually super beneficial, and we are, you know, in the future, we are looking at, hey, how can we make it more of an audit scaling functionality, but we are super excited about that. Both of these are in public preview right now, and you should be able to take advantage of it today. So, let me just quickly walk you through a demo here and see the benefits that you can see from these capabilities. So, what you're seeing here is that I've pre-created a C4A instance, and I've also created, pre-created a read pool for that particular instance. So, now let's take a look at how do you create a read pool, right? So, to create a read pool, it's very similar to creating a replica if you're using Cloud SQL. So, if you click on create replica, and you scroll down, we have basically enabled a new option called read pool. So, if you click on read pool, it gives you the ability to set the note count, and you can set the note count up to 20, right? So, and that's it. That's essentially how you basically go and create a read replica, or read pool, rather. So, one of the things that I want to really quickly point out is that this particular read pool that I created, I've intentionally created only with one note, and there's a reason for that. So, you can see, we list the read pools in the read pool details, but the other key thing that I want to point out is that the internal IP address that you see here is actually the read endpoint. So, if you connect to that IP, the 10.192.10, you're essentially connecting to the read endpoint, and we will fan the queries across however number of nodes that you have. The other key thing is that I've already enabled managed connection pool in this particular read pool. So, as you can see, you have the default port, but we now have a new port which is 6432 because I've enabled managed connection pool. So, now let's take a look at how you actually enable the managed connection pool. It's actually very simple. If you have an existing instance, you can go to edit, and in the edit page, if you go down to connections, that's where you configure all your connections, you'll see this new option, enable managed connection pool. It's already checked because I've already pre-created it, but we also give you advanced connection pooling options. Some of you must be familiar with it. We give you the option to configure things like maximum client connections, what is a connection mode, et cetera, right? So, we made it actually very, very simple for you to configure it. One thing I will note here is that when you enable this on an existing instance, we have to restart the instance. We are working on making it zero downtime, but for now, keep that in mind. So, now what I'm going to do is now I have two ports. I have the default port and I have the managed connection pool. What I'm going to do is I'm going to run a baseline workload. So, there's a sysbench workload that's a read-only workload but using short-lived transactions. So, we basically reconnect in this benchmark every 10 transactions. So, I'm going to kick that off. So, now let's go and take a look at how the read pool is actually working. And remember, I'm connecting to the read pool in this particular case with one node. So, as you can see, the CPU utilization has gone up, right, to 50%. That means things are kicking in. It's about 47% right now. In terms of the transactions per second, it's about 35,000 transactions per second. The key thing to note here in this graph is that you can see that the managed connection pool is not enabled right now. We are not using the managed connection pool. And the other key thing, interesting metric that you want to look at is the new connections that are coming into the read pool. So, you can see that there are about 3,000 connections right now. Just keep that in mind. But you'll see that later how this is going to fundamentally change with connection pooling. And then we open up a number of backends to basically manage that particular workload. So, now what I'm doing here is I'm going to basically stop the workload and run the workload against port 6432, which is a managed connection pooling workload. And let's see how the instance performs. So, I've kicked off the workload. It's the exact same workload. It's just connecting to the managed connection pool node. Wow, look at that, right? So, you can immediately see, obviously, there's a warm-up that happened here. The CPU actually went down from 50% to about 22%, but look at the throughput. The throughput essentially went up by 4x, from 35,000 to about 120,000. So, you're getting immediate scaling of your application by just enabling managed connection pooling. And the point about managed connection pooling with Cloud SQL is that we have just made it very simple and we do all the scaling behind the scenes. And as you can see, we are utilizing the managed connection pool. We can see from the metric that the managed connection pool is being used. And look at this, right? We are essentially, with connection pooling enabled, the number of connections literally goes down to zero. That's more of a scaling thing in the monitoring, but it's a few connections compared to thousands of connections. And that's the reason why you're seeing more throughput and less CPU utilization. And we will basically ramp up the backend accordingly. So, that's, you immediately got about a 3x, 4x benefit from enabling connection pooling. So, now, what if you want to scale your applications even further? Let's say you have this sales event that's happening. So, what I'm doing here is I'm changing the node count in the replica pool to 10. So, I'm going from one node to 10 nodes within the same replica pool. So, we hear, you can see that, you know, there are 10 nodes that are getting created. It takes about, you know, if you're creating about 10 nodes, it takes about, you know, 10, 15 minutes. So, I've done, fast forwarded this a little bit. So, the instance gets updated. And once the instance is updated, what we're going to do is we're going to run the same workload against a managed connection pool port. And let's see what happens. So, over here, you can see that a lot of these nodes are getting spun up, right? That's why you see in the CPU graph a number of different lines coming up. But look at the throughput. Fundamentally, we have essentially increased the throughput by 10x, right? By basically scaling the nodes. And the thing is, we did not change the port. We did not change anything on the application. You simply got that benefit. And over here, we then correspondingly opens up, you know, how many of the pools that we need to do based on the number of nodes that you have. And as you can see, the number of new connections remains steady, right? And we ramp up the number of backends that is needed to basically do the connections, the connection management, right? So, what we essentially demonstrated here is that starting from the beginning, there was no workload. We basically ran a baseline workload against a default port with about 35k. Then we ran that same workload against the managed connection pool port. Then we got a 3x improvement in performance. And then we scaled the number of nodes from 1 to 10 and we got a 10x increase in performance. So, that's a linear scaling that you can get with very, very minimal change to your application. So, that's essentially what, you know, a combination of managed connection pool and managed connection pooling and read pool can get you. So, hopefully, you know, this is going to be beneficial for you. Hopefully, we made it very simple for you to leverage this. And, yeah, so, we are super excited about it. Hopefully, you can leverage it, too. That's essentially all the things that we, you know, announcements that we had around performance and scaling and we are very, very excited about that. And I'm really hoping that, you know, you guys can take advantage of this. Okay. So, now, let's move on to availability. So, when it comes to availability, I think pretty much there is a standard framework, you know, in the database world or in general, I would say, that people think about availability, right? All of us have been talking about planned and unplanned downtime for decades now when it comes to databases. And fundamentally, you know, that's how we think about, you know, we think about availability and downtime. And one of the things that we've been doing, like I said earlier, is with Enterprise Plus and all the work that we've been doing, our goal with Cloud SQL is to basically provide the best service in terms of availability in the market. So, one of the things that we have done with Cloud SQL Enterprise Plus is for planned downtime, for planned maintenance, rather, the downtime is less than one second. And so, the way we achieve that is we do a lot of orchestration behind the scenes. And the scenarios where, when I say planned downtime, what I mean is we do a lot of, we do maintenance, quarterly maintenance. I'm sure you, you know, you do operations like scaling up your nodes, scaling down your nodes, et cetera. So, all these planned operations, you basically can do with less than one second of downtime. Just think about that, right? I think it fundamentally makes a difference because if you're doing scaling operations and if your downtime is only like less than one second, that fundamentally changes how you think about, you know, how you want to optimize your instances and the fleet. So, that's one of the key benefits, you know, when it comes to Enterprise Plus. And we have a roadmap to basically bring near-zero downtime to pretty much all of the operations. So, for example, we just recently launched a minor version upgrade as a near-zero downtime operation. we're going to basically enable near-zero downtime operation for flags, major version upgrades, et cetera. So, that's kind of the path that, you know, we are down and we have come a long way in terms of all the different plan maintenance that we can actually do with near-zero downtime. So, even the other thing that, just from a best practice perspective, it's always good to understand, you know, your own downtime considerations and what Cloud SQL provides is a lot of controls in terms of when you can actually, when you want that planned maintenance that we do happen. So, this is an area that I think it's good, if you're not doing it today, I think you should think about doing it, which is you can basically control when the maintenance happens, right? You can, once you set the maintenance, you have the ability to defer the maintenance up to 42 days. You also have the ability to deny maintenance, for example, if you have like a large, a critical event in your business where you do not want anything to touch your systems, you basically can put a deny maintenance window for about 90 days, and even if we do roll out any maintenance, we will not touch your instance. So, even though the planned maintenance with Enterprise Plus is only sub-second, even if you don't want to take that, you have absolute control in terms of how you want to manage it. If you want full control, we also offer the ability to do self-service maintenance. So, when we publish a maintenance patch, you can basically say, hey, you know what, I don't want Google to do this for me, I'm going to do it myself based on a time that works for me. So, we have the ability for you to basically do self-service maintenance at the window of your choice. So, the bottom line here is that we have given you enough controls and flexibility so that you can manage when that plant maintenance downtime happens. So, now let's look into unplanned failures. So, I just want to walk you through how you want to think about architecting for unplanned failures. So, in a cloud world, I think all of you are familiar, right, there are regions, right? So, when you think about unplanned failures, you want to start with the region and ask the question, hey, how do I protect my instance from all the different failures that can happen within a region? The number one thing that you can do to protect your instance is to enable high availability for Cloud SQL. It's fundamentally a click of a button. What we essentially do behind the scenes is we use storage level replication with synchronous replication across two different zones. So, that means even if a zone goes down, RPO is equal to zero, and we will automatically fail over the instance over to the new zone, and you are always writing to a single endpoint, and we will automatically redirect the connection to the new zone. So, that's something that if your application has high availability requirements, that is something that we highly recommend that you do. If you want added protection, if you want protection from dual zone failures, dual zone failures are extremely rare, even if it happens, it's kind of like a regional outage, but if you want to protect, again, from two zone outages, you can also create a read replica to the third zone, and in case two zones go down, you can promote that read replica to become the primary. So, that's kind of the fundamental set of things that you should be doing to protect against regional failures, but what about, sorry, from zonal failures, but what about regional failures? So, I'm not sure how many of you have DR set up today in your environment, across region, a few. The reason, you know, we don't see a lot of customers doing that, everybody wants it, but the thing is, it is very complex, right? It is, obviously, there's a cost aspect to it, but there's a whole lot of complexity, because it's not just the database that you need to worry about, you need to worry about the entire stack. So, quickly, I just want to touch upon a key feature that we delivered in Enterprise Plus, which is advanced DR. Fundamentally, it takes away the complexity of doing DR across regions, and I'll quickly touch upon that in the next slide. And lastly, backups. I think it is critical that you configure your backups. One of the couple of things that we announced today, and a lot of folks have been asking for this, is today the backups in Cloud SQL are tied to the instance. We have essentially decoupled that. So, now what can happen is that if an instance gets deleted, you can actually retain your backups, both your automated backups and on-demand backups, even after the instance has been deleted, and you can retain it for the retention period that you had set on the instance. And you can also take a final backup at the time of instance deletion. So, we have made a lot of progress on the backup front, so I highly encourage you to take a look at that, and if you haven't configured it just from a data protection perspective, I think it is super important that you configure that property in the instance even for your existing instances. And quickly, I just want to touch upon what I just mentioned earlier, which is the advanced DR. So, the reason DR is complex is because you have to do a lot of orchestration. You have to write a lot of scripts. You have to coordinate a lot of things. And one of the main things that problems with enterprise edition was that, in general, is that when you fail over, the entire topology breaks apart. So, what we have done with advanced DR is that even if a region goes down, you can fail over, but we will maintain the topology. So, the moment the region comes back up, we will just rewire everything so that you don't have to do anything to basically bring your topology back to the right state. The second thing is switch over. So, DR testing, you want to be able to do it with zero data loss. So, with switch over, we ensure that you can easily fail over back and forth, and we ensure that we only fail over when all the data is replicated, and we make sure that RPO is equal to zero. And more importantly, we provide a right endpoint. endpoint. So, that means when you're right to the endpoint, even if a region failover happens, the right endpoint will redirect it across to the other region so that you don't have to orchestrate all the different things that you have to do to make that pretty seamless. So, this is something that a lot of customers have been taking advantage of because we made it very, very simple to do disaster recovery. So, it's great for me to talk about all these things, but I really want, you know, Guvind Raj to come on stage and talk about exactly how they are using all these things. Hopefully, you know, this has been, all the announcements that we've been made has been pretty useful. Guvind Raj, go ahead. Hi, everyone. Hope you're enjoying the conference. Since it's the last day, everything is going well. So, our company, like Global Payments, it's a worldwide commerce provider where we focus on different industries. Payments is our speciality, and we also, like, work on the industry software segments, where, like, schools, hospitals, gyms, those kind of areas, as well as we do business-to-business payments, and we also, like, provide payroll processing services for employers as well. Our industry specialization, like, our, like, retail, restaurant, stadium venue, like, if you're wanted to, like, see, like, a football game or other games, we provide that fan experience, too. And if you're actually, like, purchasing something in a convenience store or filling a gas, we are there everywhere. Like, we may be, like, touching your life in some areas as well. And we are actually, like, spread across, like, 100 countries and operate truly global by the name. wanted to actually, like, touch on the use cases that what we have. As the company is global, right, we provide, like, payment services. Our customers, like, expect the services to be always be available and there are certain, like, business requirements that drive the availability because we don't want any downtime from a system standpoint. it has to be always available. One of the areas that what we will be double-clicking on is where somebody wanted to make payments for their invoices, what they receive from their hospitals or, like, from the local counties or wanted to make some recurring payments for utilities, et cetera, right, for which, like, we classify that system as a tier one because it has to be always available to make payments 24 bar 7, right? And the aspect we look at is it has to be, like, 99.99 availability is critical for that application. And then the downtime has to be actually, like, less when it comes to either planned maintenance or unplanned maintenance. We cannot actually, like, take much of a downtime and our RPO requirements is actually, like, the data has to be there in the other region less than a minute, so that way we don't actually, like, lose any transactions as well. And our other requirements are around, it has to be multi-zoned, like, considering, like, zonal failures, regional failures. That's why cross-region read replicas and advanced DR are important features for us. And then as our industry, like, we have to, like, meet a lot of, like, regulatory compliance requirements, such as PCI DSS, GDPR, CCPA, and also, like, the NIST security CSF framework, et cetera. So that's one of the use case that driving some of the business requirement made us to select, like, Cloud SQL. And let's check on the other area, where we have an application which is not that critical, like, the payment application, but it does actually, like, provide support to certain payment services, what we call it as a value-added services, which has got, like, brief planned outage acceptable, like, during some maintenance windows, as well as their recovery should be within, like, 15 minutes, but RPO is always, like, we actually, like, try to keep it one minute or less because we don't want any data loss, for which, like, again, similar, like, areas, but, like, from a capacity perspective, it may be a little bit, like, lower capacity requirements, and still, like, the same compliance mandates applies over there as well. And let's actually, like, double-click on the architecture, what we use from a Google standpoint. Say, for example, these are, like, clients interfacing with us through APIs or UIs, which actually hits our global load balancing services and a presentation layer, and the presentation layer reaches out to application layer, application talks to the database instances, and where we have actually, like, three-region topology over here, it's very important to have the three-region topology to make sure you're actually, like, always have two-region available to provide your availability so that way you can actually, like, perform things in a much better fashion, and in case if there are issues that you see in other region, and if you want to bring your application in the tertiary region, it's very easy to recover it and have the application, like, also be available in the third region, but we wanted our data tier to have, like, at least, like, the three-region art coverage be available as well, and let's look at, like, some of the key benefits why we actually, like, selected Cloud SQL for our, like, use cases. One is, you can have, like, multiple read replicas per region. At the same time, you can actually, like, distribute the load for improved availability across, like, zones as well, right? And when we are actually, like, using the cascading, like, replica architecture between, like, multi-region where there are certain traffic we could actually, like, route them for read purposes through other region to keep the lightweight operations on the right nodes. So that way, your transaction, you're not going to, like, miss getting committed in and getting replicated, and if you wanted to access them for any get operations, you could actually, like, do using the read replicas. That's the application logic, like, our app developers has actually, like, put in the code to make sure they actually, like, utilize those capabilities to have much better performance and throughput from an application standpoint. And other thing is, like, recovery automation. So, there are, before this cross-region, like, switchover, like, we used to, like, orchestrate the failover strips with application reconnecting from one region to the other region to make sure we failover things properly across, like, regions when there is a need. And then, point in time, recovery, like, where, like, we can actually, like, restore it quickly within a matter of, like, minutes to have that, like, data retained as well. And then, some of the business benefits that we have achieved is zero downtime during maintenance, and we also, like, do comprehensive, like, DR testing as we have to meet certain regulatory requirements, and that helps us to prove those DR testing capabilities using a pre-production environment, which mimics similar to the prod environments. And then, we do actually also do, like, quarterly, like, failover testing to make sure our application performs very well, as well as we have, when we need to do some preventive maintenance on the app stack and other areas as well. And it helped us to reduce the, like, the management overhead in terms of, like, maintenance standpoint, operations standpoint, monitoring standpoint, et cetera. And it, what it actually, like, provided us is, because it's the topology is a multi-region, multi-zonal, it provides the best resiliency for us to have the services always available to meet the SLAs that what we commit to our clients. And then, other thing we have to look at is, it actually, like, reduced drastically our administrative overheads, which given some peace of mind for our SRE teams and the database teams as well. And, as everyone knows that, like, we have, as part of implementation, you would have, like, lessons learned. There are certain, like, key lessons that we learned, is, untested, like, failover processes often fail when needed. and one of the practices that we actually, like, try to follow is the chaos engineering where you have to actually, like, look at fault injections and inject, like, failures as part of, like, your testing and release management process. And we do actually, like, those tests in our non-prod to till, like, our pre-prod and pre-prod configurations are mimic similar to, like, what's the production one. So, that way, you can uncover some of the issues up front before you actually, like, roll any potential changes to your production environment. And also, like, we have implemented, like, quarterly automated, like, failover tests for application connectivity validation to make sure app performs well and it's able to do its operation for both read and write operations. And also, like, as needed, like, we actually also do the load testing to make sure it provides the right performance that what we needed. And when it comes to the resource optimization, often, if you look at the production instance, there will be a drift between production and pre-production. So, what we actually made sure is we right-size those Cloud SQL instances and make sure the pre-prod environment, it's, we execute, like, if we need to, like, do a load test for an upcoming event of one of our clients or something, we actually, like, do those testing and make sure our systems are up and available for meeting the customer demands. And then, we extensively use, like, Terraform, no click ops, not much of our click ops happens because it actually, like, impacts the state of the systems. systems. So, we actually, like, try to do everything through the Terraform as a code with our pipelines executing things from a system standpoint or even from a schema releases standpoint, we use, like, certain tools, like, available within the Google Cloud using Liquibase and other things to update schema, et cetera, as well. And then, we are looking forward, really, like, the upcoming features, like, what Subra and Gopal mentioned about the connection pooling, et cetera, to improve the performances as well. And other thing, we are also using the Cloud SQL Insights to provide automated alerting capabilities and look at the key performance indicators. And thanks again for having me here. Thank you very much. Thank you for being here. Thanks again.