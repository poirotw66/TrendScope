 . Good morning, everyone. All right. We're on our third day for next. And we're the cloud heroes. Everyone that made it here today, 8.30 in the morning, the third day deserves like a big applause. Thank you, everyone, for showing up for the best show. This is the best show in town. Okay. We're going to talk about today how we can help you all to manage your applications in cloud. We've been working hard with the team all the year to make sure we remove toil, we take the complexity, and you are all, you know, doing what matters for your business instead of dealing with what we can help with. So let's get started. My name is Ines Envid. I'm a product director at Google Cloud. I'm going to be joined by Ram, who's in Swap, and Nandan, who's another product in this area. So, all right. Managing your applications in cloud. What's the deal? Like, you know, everyone has to do this. Why is this a topic that is worth looking into, investing into, and making it simpler? Okay. So, your application deployments, all of yours, I'm pretty sure they generate a sea of resources. It can be overwhelming, and probably not saying any, you know, groundbreaking fact here. You all deal with this all the time. Let's have a little bit of a game. I'm going to show you a picture, and you tell me what you see here. Okay. What do you see? Pixels? Blue, yellow, you know, some of them like lighter blue, some of them darker blue. Like, what's the meaning of this? Like, any idea? Hard to say. Hard to say, because it's a lot of information, little context. Sometimes you need a little bit of context to figure what it means. Okay. Now, you start seeing some trees on the side. You start seeing some little houses, village. Might not be the best high-fidelity picture, but this you can make. It is a starry night from back up. So, what's the deal? Context matters. It's important that we are able to decorate the data that we get, so we understand how it comes together. What is the meaning in the context of other information that we're getting? And then we can actually get a picture of what it means. Now, game's over. What do you all need to accomplish in cloud? Well, I'm pretty sure a flavor of this might not be exactly this, but I'm sure all these have like a flavor of I need to make sure my applications meet some KPIs. I need to make sure my applications can grow with the business. I have more demand. I can grow. I'm spending a dollar. I'm making the most of it. I'm making sure I'm spending efficiently. So, it's very common sense goals that at the top level, they make sense for all of us. When it comes to realizing those in cloud, sometimes it's hard, and we'll go through what we can do about it. But all you need really is, in a nutshell, is you need to get the critical insights. It's like how far you are from those goals and what you need to achieve those goals. So, if you actually get the information about what are the issues that might be on the way of achieving these goals, and you have some help on what you need to get done to achieve these goals, then you're much better off on your way of doing what it matters. How can we do that? We'll talk about that in the talk. But let's take a look of why, in reality, this is not as simple as it seemed in the previous slide. It can be tedious. Let's take the first one of the first topics we had. Let's assume, you know, my client authentication, like from some of our customers in this case, swap, is not meeting a service-level KPI. Okay. I'll have to go and probably take a look into a bunch of metrics. So, I'll go try to figure out, you know, what the metrics are showing, what these KPIs are looking like. And I'll do that for a couple of, you know, maybe the load balancers, maybe I'll have like a front-end. Deployment that is GKE. Go take a look at the metrics. See something wrong. I'm probably gonna go and take a look at the logs. Are there any errors that are associated? I might take a look at the same resources, but maybe I see, okay, I've actually a GKE node associated. It has a VM that it's running into. I'll see whether there's errors that might actually be pointing into some direction that makes sense, or how they work together. I'm gonna take a look at configs. I might go to asset inventory. Has anyone deployed anything that has changed in the last, you know, hour or so? And by the way, is there possibly a Google incident that might be impacting what I'm doing? I'll go take a look in service health, see whether there's any Google incidents that might be impacted. And another thing, maybe somebody's scheduling a maintenance window. Possibly. Do I have an upgrade event coming up? So, a lot of information, and it matters in the context. It matters in the context of what are the things that compose my application. It matters in the context of how they happen and how they relate to each other. That is the crux of the issue, and you probably need to jump to different places to actually take a look at that. So, don't take it from me. Take it from Ram, who does this every day for real. And let's hear from him what's the challenges that they're facing in their deployments. Ram, welcome. Ram. Thank you. Thank you, Ines. Good morning, everyone. Happy Friday. So, I'm sure you're all excited to go back to home. All right. I hope you're having some good learning sessions in the past two days. I'm sure you will pick a thing or two of this as well. So, thank you for being here. My name is Ram Guganini. I'm a director of technology, SRE director Charles Schwab. I'm responsible for managing several critical applications across the Schwab, and also involved in building the scalable observability solutions with AI-powered reliability. If you guys haven't attended yesterday, we have a good session on AI observability. Please try to watch it. So, let's talk about who is Schwab, right? Charles Schwab is a leading financial institution in the U.S., which manages about 10.28 trillion assets with 37 million brokerage accounts. When I say that, yes, it is 10.2 trillion assets, when you put that into perspective of a country's GDP, Schwab stands third in the ranking. And we are managing assets even bigger than Germany, Japan, and India. So, the reason why I'm giving that context is it tells the scale we are operating and the complexities of the applications we are managing. And the time we spend to issues, analyzing deployments and everything, it is very critical for us. On a daily basis, we process millions of logins and trades. At any given point of time, if there is an issue, that means you are impacting the customers, logging to their website, checking their balances, making their trades, doing the move money and everything. So, there is a real impact. And that's the reason, and we want to be really looking at close on what the team does, how the teams are operating, what the tools they are able to do is really matter for us. So, let's look into one of the applications. I know we have several applications. Schwab runs their data, security, and several critical applications in the cloud. Today, I'm a little bit more focused on one application. We call it as a client authentication in cloud. This is the digital gateway for Schwab entry. That means if you are logging into either a Schwab.com website or a Schwab mobile, or even Thinkorswim, this is where it gets authenticated and probably able to log into the websites and take the rest of the stuff. So, what is the criticality of this application? In case, if any application fails, right, users are unable to log in, they won't be able to do any of their balances, not trades, everything, right? That is really critical. I may be okay if I didn't get my Facebook picture for a minute or my Twitter feed, but if I'm unable to make a trade, I'm losing my money, we are impacting the customer. So, that is the criticality we are talking about here. Okay. Now, let's talk about some of the challenges. I know Ines just touched the scratch, but we want to hear from us, like real customer, what is really happening in a day-to-day, and what else Google can help us to take this from to the next stage, right? So, it's not about one application. If you look into the enterprise, there are several applications, maybe hundreds of magnitudes. All applications get deployed. The key is how well we are able to manage the applications. You know, multi-team complexity, right? Usually, for any enterprises, even for small companies, there will be a team that is developing the cloud modules, and developers who are the ones that are deploying the deployments. So, oftentimes, you lose the context of, and whenever there is a new deployments that are deployed, or when there is a cloud team builds a new modules, we don't know whether they are really available or not. And the next is cloud providers unpredictability. I'm sure we all move from on-prem to cloud. When you are at on-prem, you have a real control on what you are trying to do, especially, I know when I am deploying my apps, even I know when I am scheduling my infrastructure maintenance. Mostly, we do over the weekend so that you don't have any impact or anything. But once you move to the cloud, you lose that context, right? You know, all you have is, you know, when you are deploying your applications, but under the hood, it remains a black box for us. All the cloud providers, they change the things continuously. All the time, changes are happening, and there is no way to know exactly what are all the real changes that are happening, how they could potentially impact, right? So, that is the real issue. There is a fragmented management. If you look at today, sorry, I'm not, you know, a little bit of teasing Nandan or Ines, but say, like, I am from an SRE team. We manage SRE teams. Several SREs are there. Today, if I have to troubleshoot an incident, I got an alert. I need to go to three or four different places to start investigating my thing. I will start with Logs Explorer to go look into what else is happening. Oh, no, no, I see the LSA is fine. Okay, let me look into my SLOs. Then I need to switch to monitoring dashboard, which has metrics and SLOs. Oh, I see the SLOs, they went down. Now, what is the impact? How many participants are unable to log in? From Logs Explorer, I won't be able to do an analytical query. Then I need to go to log analytics, write a query to see how many people got impacted. Or I need to look into if somebody is using BigQuery or local, right? I mean, there is a lot of context which is happening here. And, you know, in the incident management, every second matters. Whenever there is an incident, SRE teams are really strangled together trying to find information as quick as they can. But when the context switch is happening, it is really harder to find it. And, you know, I think it depends on, right? Each time when our application is running, it has its own parameters means, you know, sometimes there is a predictable load patterns. If you look at our use case, when the stock market opens, we see a spike of the load, and the evening unit comes down, right? But during the market volatility, I'm sure we are all observing from the past two weeks what's happening. You never know what kind of a load you would expect. So, I'm sure, you know, we try to do as much as we can, the pre-scaled capacity or, you know, planned capacity. However, it would be really good to have what is my projected capacity. Today, I'm running at, you know, a certain number of logins per minute is using GKE, this many number of nodes, your CPU utilization is at this, and memory utilization this. But I wish there is another way to project, okay, if you need to move your load from 100 logins per minute to 200 logins per minute, what is the real capacity needed? If there is a better way to forecast the capacity needs, that would help us to really understand, okay, are we ready for this one? Or sometimes, you know, you may not have enough capacity, or you need to request the pre-capacity as well. And another one is the FinOps complexity. It depends from small organizations, even for the bigger one as well. I need to really understanding how my costs are occurring. If we are using the BigQuery, and I have real experience, you know, we, some of our teammates, they rate a query, which is really doing the entire table scan, right? You know, if you don't have, you know, right parameters, it does the entire table scan. And you don't know how much costs are occurring, right? And that's what we are talking about, one application or one load. How about if I am, I have to deal with like tens of projects, multiples of projects. There isn't a one better way to bring all the cost level occurring at a folder level, or even at the org level, right? But you need to go to the today's cost explorer to spend a lot of time in understanding, using the SKUs and all that stuff. But I wish there is a better way to do the cost management as well. And another thing is, when SRE teams are really trying to understand their challenges and problems, most of the time, it's like siloed, right? You are looking at application level. So, sometimes, you know, an SME knows about one application, another SME doesn't know about other application. When you try to bring the overall impact, so it is very hard to get a high level broader picture of your enterprise, like, you know, how things are working, then, you know, fragmented way to get into each nitty-gritty details. For example, today, if you are running an application, it's maybe like the web-based application, I'm talking about the login and all. So, it starts from the cloud armor, and then go to load balancers, and then ingress gateways, GKEs, all the way, right? It will go down to the database and all that stuff. For us to look, understanding what is really happening, each product will have at least, you know, 10 to 15 metrics. What are the real metrics we need to look into it? Some say, right, at least we can start looking into the golden signals. Not necessarily all the time golden signals will bring the details. So, as you add more products, more services, trying to consume to use, your complexity of observability also increasing. And I wish there would be a unified way to bring the observability together. Also, and I just touched it, right, for any SRE to go back and look into their stuff, it is taking a lot of time that is impacting your MTTD and MTTR. So, probably a better way to troubleshoot, better way to bring all the tools together. So, that would really help and empower our users to support these critical applications at any given point of time. So, I would like to pause here. I'm sure you didn't get bored with my problems. Now, I am excited to call Ines and see what they got for us to help us to move away from these problems. And I'm back. Yay. Okay. This time with solutions, no more problems. I promise you. Okay. Enough of this. Like, we understand the challenges here. Okay. So, we're introducing CloudHub. How are we looking at tackling all the issues that RAM was articulating to us? Well, this is what you need to really understand and really deal with for your deployments. I'm sure you will have a flavor of this. You deploy your applications, and then when they get deployed, they get all converted into a set of resources that are running. They have a set of relationships. And we don't have a good representation of your application that is really carrying on to the runtime and then the deployment. So, that is at the cross of what we need to solve first. We need to model and organize this information before we actually show anything around that. So, this is the application-centric platform that we have announced, and that is going to help us to really understand the relationship of the resources, model them, and associate to the specific applications that matter to you to achieve a business function. We do this at runtime. We do that preserving the context. What is the environment that the application is running? What is the criticality? What is the owner? And we are constantly understanding the changes that happen in the deployments and modeling it as a group with all the real-time state that these resources have in the context of the application. So, once we do this, first we model these applications that we can understand the relationship and the semantics. The relationship of how these resources connect with each other, talk with each other, relate to each other to form this application. What are the semantics? Is this a production application? Is this a test environment? Is it a critical application? Then we can actually set perimeters around that, have the security posture and policies associated to that. We can start troubleshooting them. And we can understand the data and correlate the data associated to the application because we understand the relationships. And then we can as well understand things like, you know, cost attribution because we understand the composition and then we can start optimizing it. So, very important concept that we're building on top, modeling the application at the platform runtime. Then next, we need to provide the insight. Remember at the beginning we said, well, if we just understand the application, we understand the insights of what's getting on the way, we can start providing action. So, then the second thing is Cloud Hub. Cloud Hub is the central hub for the contextualized actionable insights. What it means is that we are unifying the data sources across multiple products that belong to that application and across multiple domains. It could be monitoring, it could be config changes, it could be maintenance events. We are providing contextualized because we are understanding the relationship of the composition for that application. And it's also goal and task oriented because we are using this as an entry point from those insights to trigger Gemini Cloud Assist and our AI assistance workloads to help you troubleshoot, optimize the cost for your applications, etc. So, what it looks like is you actually have a set of dashboards that are all brought together in a central place. Right now, this set of dashboards are coming off the shelf and we will work on providing more customization for you to create your own dashboards for the information that matters for your own insights, for your own insights, for your own goals. And very important, we talk about this, how does the application relates to that. When we are consolidating the data in these dashboards, we are doing for the group of services, workloads that become part of that application. And we keep that state and that composition, that group, that relationship always true to the state of the deployment so that you can continuously see the data in context. And with this, like, I'm sure you all want to see it in action and this is the most fun part of the presentation. So, I welcome Nandan, who's going to walk us through the real way this all works. Welcome, Nandan. Thank you so much, Ines. And thank you, Ram, for walking us through some of the challenges SRE teams face with it. So, what we are going to do now is we'll take a closer look at CloudHub. But as we do so, we'll run it through some of the scenarios Ram explained to us. So, we'll do it by picking an example application. So, here I have a Gen.AI application that many of you have or are considering to build. In this case, we decided to have some fun and we are building a movie recommendation chatbot. Okay? We'll take plot lines from you, directors, all the other things that you may want to watch, look up a database and provide suggestions for movies. Now, I built this application on GKE. I've got microservices for the inference, for my front end, the web server. And to do this, I needed different node pools. For my GPUs, I needed a node pool. For my CPUs, another. And then for the vector embeddings, I used Cloud SQL to cache some of the chat responses. I had Cloud Memory Store, static images. I had GCS. That's a lot of components and moving parts for a single application. And in some cases, those components may even be hosted on different projects. So, how are we going to help the life of an SRE with an application like this? We're going to first start by having app developers provide context by logically grouping those resources that you saw. The GKE deployments, the storage, all of them into an application. App Hub defines an application as containing services or workloads. So, in this example, my Gemma inference is going to be a workload. And then storage bucket, Cloud SQL, other such services are also registered into the application. So, you've defined your application for your chatbot. But now, so what? Right? Now that we have defined what an application is, we can start to derive insights and context into what your application does. I can try to find out if there are errors. I can try to find out how much it costs. I can define capacity, quotas for that application and start to derive insights into it. Let's take a closer look at each of those aspects. One of the common scenarios that we see is production deployments that configure application. Right? And they're notorious for having causing errors because you misconfigure your application. So, let's hear from Ram. What are some of the common situations he runs into in a production configuration situation? Ram. Ram, you still want me to talk about problems? People may not like further. Please. Just kidding. Yeah. So, yeah, it seems exciting, Ram. So, especially when you talk about the deployments, right? As I said, in enterprise you may have not just one app or two apps, multiple of apps, right? As I said, it's deploying using your scripts and all. What is the best way to know, okay, whether my app deployments are working? I know for a particular app, you may have, write the deployment logs in the cloud logging and all. I can look into it. But how about when it's spanning through across the projects? Right? I have the projects and folders depending on the LOBs and all that stuff. How do you get, you know, a combined view of, you know, how is all my deployments working at any given point of time? That is one of the challenge. Other one, you know, we deploy the apps and also there is some other teams, you know, who manages the entire Terraform level of, right? New modules and whatever is needed, right? Whenever there is a new module is available, there isn't a better way to notify, okay, you are using this module today. There is another update is available. Do you want to consume that, right? Where that is all through traditional sending an email to the people. We know how good we are reading thousands of emails every day. So, my clear problem is how to get better at the deployments or how to understand what is really happening in your deployment landscape. That is one of my top challenges. Awesome. Thank you, Ram. So, let's take a look at how we can address that through Cloud Hub. To the demo, please. All right. So, what you are looking at is the overview page for Cloud Hub. You can access Cloud Hub right here on your Cloud Console or you can also search for Cloud Hub on the navigation search bar there. On the home page, and I'll take a minute on the home page before we get into the scenario that Ram explained to us, you get to see any Google incidents that may be affecting your applications as well as some of the emerging issues that you may be interested in as an SRE. Thankfully, there are no Google incidents at the moment, but we also provide a way for you to find out if there were Google incidents, which of your applications may be impacted as a result. But back to topic. Let's take a look at the scenario that Ram told where in production your deployments may be misconfigured. You want to know more about what happened, when did it happen, and how you can address them. So, the first board we have introduced in Cloud Hub is the deployments board. The deployments board shows you all the production configuration that have gone wrong, why they have gone wrong, and how you can possibly address them. This is powered through App Design Center, and App Design Center is where I created my application, my chatbot, and I deployed this on my project using App Design Center. App Design Center can also manage your Terrapom state and allow you to maintain revisions of it. So, if you had a pipeline that deployed your application, and those applications had a failure, I, as an SRE, can find out why that failure happened. I can also have a call to action to address that failure in App Design Center. The other thing available here in the dashboard are to do with what platform engineering teams often do. They maintain Terraform modules for critical infrastructure, whether it's GKE clusters, Cloud SQL instances. They maintain these modules for use by development teams when they build their applications. Oftentimes, these modules need new revisions because there are new features available or vulnerabilities that have been patched, and they need to make them available to teams that are consuming these. So, when new modules are published, they're available here as action items for developer teams to consume those new revisions of those templates so that you can upgrade your application. You also have guidance to allow those updates to first happen in dev, test, and promote those changes eventually to production. And that covers the deployment board that we have in Cloud Hub. In the London, it seems exciting, man. Sorry, I need to put you on the spot. We didn't practice this, though. Okay. So, I just realized, you know, we need to use this App Design Center, right? How about if someone is existing apps which are not part of the app design, how that's going to work? Yeah, that's something that we are going to work on on our roadmap to try and integrate those additional scenarios. Awesome. Looking for that. Back to the slides, please. Cool. Now that we have looked at deployment, another critical aspect in the day of the life of an SRE are errors affecting applications. Whether there are latency issues, problems with people logging in that Ram talked about. Let's take a closer look at what we can do to help improve errors with applications. Ram. Thank you, Randan. Yeah. I mean, today, right, as, you know, kind of touched me a little bit back, there are multiple ways errors can occur, right? It could be anywhere from our infrastructure errors or application errors, database and everything. Today, everything is funneled into the logs explorer, right? All, everything goes into the log. Then we need to write a definite queries to understand, okay, whether this is a part of an app error or a database error or, you know, anything related to, like, networking or something, right? And, you know, I think, you know, it would be better to do some kind of, you know, labeling or something automatically, right? Okay, this error is coming from a particular source versus, you know, your infrastructure versus errors and all. So, it's very hard for us, right, to comb through all the errors. And, again, I don't want to go to the same problems. Hard to write the queries as well. Yeah. Thank you, Ram. All right. Let's take a closer look at how we can address some of the issues that Ram just had. So, to the demo, please. So, Ram talked about, you know, needing to perhaps label things. Well, that is now unnecessary because you've already told GCP what your application is composed of. You've provided us the context that your application includes a GKE deployment, a bucket, a SQL instance, and so on. So, we know about your application. That means that we can now provide insights into the health about your application. Now, this is the health and troubleshooting board in CloudHub. I see all the applications that I currently have. I also know what environment and region they're running in. Let's look at the production app for this chatbot. I have eight alerts that have gone off. These are alerts that I configured. I will also see if there are any Google Cloud incidents that affect this application. And in the details, I get to see all the components that make up my application. My application here, as discussed previously, has GKE deployments, SQL instances, and so on. Those show up here. And I also get to see important metrics about the service at hand. So, whether it is Cloud SQL, whether it's GKE, I've got them all in one place. And if I want to know more details about what may be happening here, you'll notice here that there are a few container restarts. And we'll come back to this towards the end where we'll investigate why my containers have been restarting. But if I wanted to know more about what's happening with my application, I can have that deep dive in the app-centric monitoring page. This aggregates all the metrics and logs for my application in a single dashboard. This GKE deployment has these metrics available. And if I scroll further down, I also get to see all the logs that my application has been producing. This reduces the time that an SRE needs to spend trying to understand different sources of errors for your application. This is great, Nandan. This is what we are looking for. It's more of like a unified monitoring, right? Exactly. Bringing one common view across the... A unified view for your app. So this is going to help the SREs to, you know, no need to do more context switching, right? Focus on a main page, right? That way you can traverse to the rest. Exactly. All right. Back to the slides, please. Okay. Another important aspect is to have your applications run efficiently. It's not just enough that they run without errors, without problems, but you also want to run them as efficiently as possible. And oftentimes a burden on utilization and efficiencies falls on the shoulders of SRE teams. Let's hear from Ram. What are some of the challenges that he sees in looking at the efficiency of applications? Ram. Sure. Thank you. Yeah. Again, you know, an application is made up of several products, right? You're starting from your load balancers all the way to the caches, databases, and everything. But when you go to the production first, to be sacred, we want to be cautious, make sure, you know, we don't know how to even plan the capacity planning or something. You go and start with, you know, a certain capacity. But over a period of time, you know, we need to look into see how our resources utilization is. Sometimes you could be utilizing very underutilized, sometimes even overutilized. But I wish there is a better way to, okay, on a regular basis, something is analyzing for us to tell. Okay. You provisioned a GKE cluster, but you are utilizing only 5% of it. I know today if you go to some of these VMs page or anything, Google displays, okay, you can reduce, save this by doing this one. But again, I would expect more context, right? Okay. Even if you reduce this, you still continue to function. You are able to maintain your SLOs and all. That level of details we need so that that way it is easy for SREs. Yes, I know this is my peak capacity. This is my lower capacity. These are my planned one here versus, you know, what I need to do. Great point, Ram. Let's take a look at how we can address some of these things. To the demo, please. All right. So, Ram wanted to know how an application is utilizing resources to it. A GKE cluster is not your application. Your application uses some of that cluster's resources. It also uses databases, storage, and other items. Let's take a look at how CloudHub can help understand those costs. So, here we have a breakdown of cost and utilization by the application that you have designed. Your application contains, as we've seen before, multiple products. It's not just GKE. And so, you can get to see here the cost of your GKE deployments combined with other services like Cloud Memory Store, Cloud SQL, what the cost changes are. And on the right-hand side to that, you also get to see the utilization that those GKE deployments have as opposed to what the GKE deployments have requested as CPU usage. Very often, developers request a bunch of memory and CPU for their deployments, but you're not entirely sure if they're actually utilizing them. What you get to see here on a timeframe between 1 and 30 days is the utilization that your GKE, Cloud SQL, and other services are doing all with the lens of your application. So, for the first time, you'll be able to tell how your dev app is consuming resources or utilizing resources as opposed to staging, production, and so on, giving you all the opportunity to build your apps more efficiently. Back to the slides, please. Cool. So, we've looked at some of the challenges and questions that SRI teams have about an application. But it's never about one application. You don't have an SRI team for one app, right? SRI teams are managing many apps, and even if you took the chat bot, the chat bot is in dev, but it's in prod, it's in staging, just the chat bot has many instances of it. And it's never just one application. We have the client authentication application, a trading application, many, many applications in large enterprises. So, how do we make sense of all of these applications coming together? So, that's my question, Nandan. I know you're talking about one application, movie guru, right? I mean, in real world, it's not about one application, right? If you take about a small ARGA enterprise, you're talking about hundreds of applications span across projects, folders, maybe ARGA, right? That's how we need to, yeah. Thank you. Thanks. Great point, Ram. So, with this release of CloudHub, we are introducing the ability to enable application management at a folder level. Your resources are already in projects. Those projects presumably are in folders, and those folders may be in other folders and eventually in an org. You can take any of your existing folders, which very commonly enterprises create folders by environment, by business units, by teams. You can take any of your existing folders and enable app management on it. And when you do so, we go and discover all the resources in the projects, subfolders, everything, and bring it to a single place. By doing so, now you can create your applications at that folder level, and you get all the benefits we talked about in the previous slides. So, if you had a production folder and you enable app management on it, all your production applications will be available at that folder level, giving you insights into the cost, the errors, the deployments, all of these aspects of managing your applications at the folder level. We'll take a quick look at how easy it is to enable app management at a folder. Thank you. All right. So, to do this, you need to be a folder admin. You head to the IAM manage resources page and select any of the folders that you currently have access to. You go to settings, and in settings, you have the ability to enable the folder for application management. In this case, I've already done it. We'll pick one where I have not, and hopefully, I can show you how that's done. Oh, I've already done it on a bunch of these, but this is the place where you would be enabling your folder for application management. Back to the slides, please. Cool. But it's not just about cost, errors, and deployments. The life of an SRE is also planning about the future. It's capacity planning, quotas, maintenance, and a bunch of other issues. Let's hear from Ram on some of the scenarios that he faces on issues like this. Yeah, I would like to quickly, you know, summarize it. I'm talking more problems here. But one thing about it, it's more about like you versus me, right? You know, whenever there is an issue, obviously, we start thinking, you know, maybe some deployment changes issues or something occurred. Because we didn't know what exactly happening, there is no known way to know, hey, here is your scheduled maintenance. These are the changes coming from Google on this infrastructure level or any cloud provider. Or maybe something else is coming, right? So there is one of the thing versus kind of like a black box and, you know, trying to spend a lot of time in figuring out, oh, something happened, right? We're already seeing some, you know, Google issues going on in central network or something, right? Yeah, we need to know exactly when things could be planned to change. Perfect. Thank you, Ram. Back to the demo, please. We'll do a quick walkthrough now of the three cases that Ram talked about. Cloud Hub provides you with the ability to look at all the quotas that are currently being affected. So here I have in the danger of running out of quota for two of these APIs that I'm using. We also provide the ability to forecast usage for GPUs, for the CPUs that you're using in various regions. And finally, also provide the ability to tell you how you're doing against the capacity reservations that you're making. In addition to this, Cloud Hub also provides a window into all the maintenance that Google does on various products. GKE has a maintenance schedule. Cloud SQL has a maintenance schedule. These maintenance schedules are published here. You get to see things that are planned, things that have already happened, if they happened successfully. And you can use that to correlate if any of these maintenance cycles impacted your applications. And lastly, Cloud Hub also provides a quick overview into support cases that need your attention and that require responses from you so that support teams can help. Back to the slides, please. What we'd like to do, finally, is to investigate a problem in Cloud Hub and see how quickly SRE teams can troubleshoot issues with Cloud Hub. Okay. Back to the demo one last time. All right. So an SRE team has been paged because some of the chat messages that have been going back and forth, they're seeing latencies or errors with it. That's all the information the SRE team has. They got paged because of it. And so the SRE team jumps on the call and they land on the Cloud Hub page. The first thing they'll notice is that the nine alerts that have gone off on my Kubernetes service, which is the web server. All right. That's interesting. But the Kubernetes service itself isn't going to tell me much. So I'm going to look at the deployment that backs the Kubernetes service, which I know exists in here. All right. When I look at the Kubernetes deployment, and I'm going to include pod crashes here, you'll notice that in the last hour or so there have been 22 pod crashes that have happened. Well, that's a bit interesting. I'm going to take a look at why that may have happened. When I hover over the annotation here, I notice that my alert has obviously gone off. But I also see here that the GKE pod has indeed crashed. And right here, I can use Cloud Assist investigation to kick off an investigation into why this happened. So I'm going to hit investigate here. That's going to take me to an investigation page. And in the interest of time, I'm going to switch to an investigation that I already ran so you can see some of the insights I got. So here, I get to see some important hypotheses. There's an out-of-memory error that affected my pod. And we are running out of time. So I'm going to quickly wrap up. And the out-of-memory error was related to a deployment change that I made about an hour ago where I upgraded from version 3.4 to 3.5. This was correlated because Gemini Cloud Assist investigation was able to make sense of your application, kept track of the changes, correlated the incidents from the logs and metrics, and could trace it back to this problem. All right. With that, I thank you so much for giving us your time. Thank you.