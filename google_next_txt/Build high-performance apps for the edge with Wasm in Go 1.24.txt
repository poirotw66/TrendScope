 Thanks for making it out. I hope you've had a great Google Cloud Next so far. It's always great to present on the very last day because you get all the most dedicated people here. So, you know, thank you for making it out. I'm Cameron, and I'm the product lead for the Go programming language at Google. I'm here with my colleague Armand and Justin. I'm Armand's a product manager for Google Cloud Service Extensions, and Justin is a principal engineer at Shopify. I'll let them introduce themselves properly in a bit. We're here to show you why and how you can use Go to build high-performance web assembly applications for the edge, including on Google Cloud. So before we get started, a quick second about me. As I said, I'm the PM for Go. I've been on the Go team for almost five years. Prior to Go, I led product for the Google Cloud SDU, including language and framework integrations. And before Google, I was a software developer at a high-frequency trading firm where I worked on low-latency trading systems for U.S. equities and all of that in C++ and a little bit of Java. And now I'm a Go convert, you know, obviously. The stuff we'll discuss today is just a small part of why I like Go a whole lot more. And as I mentioned, I'll let my colleague Armand and Justin introduce themselves properly when they come up. So real quick, how many of you use Go today? You know, either yourself? All right, amazing. Almost all of you. So I've done a lot of talks on Go, but today's talk is just a little bit extra special because I have a good chunk of the Go team here in the audience. We actually even have Robert Grisimer. Robert's the co-creator of Go, along with Ken Thompson and Rob Pike. He may get a little uncomfortable when I put too much attention on him, but he will be at the community hub down at our booth if you want to get autographs later. So all of you obviously know what Go is, but I think it's still helpful to tell you what we, the Go team, think Go is and, you know, how we look forward from there. And the first and probably most important thing I want to say about that is that Go is not just a programming language. It is an end-to-end, complete platform for building production systems. You know, this has been our vision from the very start. It's to facilitate real software engineering at scale. So this is the sort of software that's written in teams where members come and go, where there are deadlines, SLAs, all that stuff. And internally, we reduce that vision to the mission statement that I've put in the title of this slide. Go provides the most productive platform for building production systems. It's productive because it's easy to learn and maintain and scale across teams. It's a platform because, as I said, it's more than a language. It's an end-to-end developer experience, and this includes IDE integrations, building deployment tools, monitoring tools, vulnerability management, runtime tools, all of it, you know, out of the box. And it's production-ready because it's reliable, efficient, stable, secure, which is why you'll see it all over the enterprise, especially in critical business systems and infrastructure. And, in fact, it's why the modern cloud itself was built on Go. And by that, I don't just mean Google Cloud, but I mean AWS, Azure, and all the other major players in cloud tools and technology. So, you know, thinking about that some more, you know, how many of you are familiar with the DevOps infinity loop? Most of you, it seems. So, for those of you who aren't, this is just one way to visualize kind of the broader software development lifecycle. You know, you can think of the loop on the left as the inner dev loop where you're writing code, you're iterating quickly, looking for quick feedback, you know, productivity. And you can think of the loop on the right as the outer loop, the one where you deploy your code into production, and then you monitor and operate it. So, when we think about Go as a platform, we think about solving end-to-end across, you know, both of these loops. So, I just want to give you two quick examples. Let's take developer velocity. You know, Go has features, tools, and libraries that take everything into account to maximize your team's developer velocity from when they're writing code to when they push it into production and then operate it reliably thereafter. We provide IDE integrations, including our own plugin for VS Code to make it easy, you know, to leverage the features of the rest of the tool chain. We provide a powerful concurrency model through Go routines. We have built-in formatting, a built-in test framework, a built-in debugger. And all of these things create an end-to-end solution for gaining and maintaining developer velocity for your teams. The second example I have is security. So, Go is really well known in security, which we also address end-to-end. We take this really seriously, and it's because we've seen what can happen in other language ecosystems when a popular dependency is compromised. And because Go is used in all this critical infrastructure in the cloud, we sort of recognize that security is among the very most important things that we need to provide and take seriously. So, this starts with our dependency management system, where the Go module mirror, checksum database, and package website at pkg.go.dev warn you if a library that you depend on is tampered with or suffers from some known vulnerability. You know, how this works is actually extremely interesting, but it's beyond the scope of this talk. But there's a lot of materials online if you're interested, and I encourage you to check that out. Goes also the only major language with fuzz testing built into and integrated with its tool chain. Fuzzing is an automated type of testing that, you know, intelligently manipulates inputs to your program to sort of flesh out bugs and vulnerabilities. And there's a lot of other stuff on the slide, but in the interest of time, I'll just say we have an end-to-end approach to security, and, you know, we really take that seriously. And there's a ton more, you know, stuff focused on performance, on compatibility, ecosystem health, you know, all this stuff. And it all fits into this framework of providing end-to-end solutions out of the box. And that's what makes Go the most productive platform for building production systems. And then we do all of this with really extraordinary feedback from our users. You know, for the most part, Go users really love Go. It makes my job really great. We consistently see really high customer satisfaction in our surveys, levels that are, you know, practically unheard of in the industry. So I'll close this section with just a quick overview of how we position these features as value to you. You know, in these four rows, you can think of this top row as stuff related to productivity. So it's quick to get started, quick to iterate, quick to build real, scalable production applications. And this translates into faster time to value for you. The second row is about reliability. You know, and this includes security and compatibility and all the stuff that reduces your long-term maintenance and operation burden. You know, and so the less burden you've got, the lower your total cost of ownership, and the more time and resources you can use to build the new stuff that grows your business. The third row is about the cloud. So Go is purpose-built for the cloud. You know, the libraries and integrations and architectures that Go enables, they were meant for the cloud. You know, they weren't retrofitted for it later. And this means that you can realize the cloud's benefits faster and easier than you can in other languages. Finally, Go users are happy. You know, they're happy everywhere, and they're really happy on Google Cloud as well. And I think everybody likes happy developers and operators. So that was a lot about Go, but I promised you a little bit about WebAssembly. So let me get into that a bit. In Go 124, which we released this past February, we introduced some really important new improvements to Go's support for WebAssembly that unlock a lot of new opportunities for Go-based WASM applications. So all of that stuff I just presented on Go, that productivity, the platform, the production readiness, now you can bring that to your WASM applications. So before I get into it, how many of you have experience with WebAssembly today? All right, great. So you're all familiar. For those of you who only kind of know what WASM is, I'll just want to have a couple of slides on what the benefits are. So first of all, WASM is a binary instruction format that runs in a sandbox runtime environment. This means that any host that understands the WASM instruction format and can provide that sandbox environment can run your code. So in other words, it's portable. And usually, when you have some sort of bytecode interpretation, you pay for it in performance, but not so much here. You know, WASM is incredibly performant, and WASM applications are capable of achieving near-native performance on their hosts, which makes them particularly useful for the sort of low-latency, high-performance use cases you often see them used for, like edge computing, which my co-presenters will go over with you soon. They kind of resemble containers, you know, but they're much lighter. No operating system, no extraneous binaries, all that stuff that sort of adds a layer between the host and the application that impacts performance and potentially security. And then there's great, like, cold start performance as well. And finally, WASM is more secure. So the runtime environment is in a WASM host is a sandbox with structured flows and verification, and all your system calls go through the WASM host. And what you may have inferred from all of this is that it doesn't matter what language you write your WASM code in. You know, so as long as you can compile WASM bytecode, you can run that code on a WASM host, and you can mix and match with other languages that are capable of compiling WASM as well. So this is all really cool, and I'm really excited that Go124 supports it all so well. And next, I'd like to sort of show you what it looks like, and I'd like to start by walking you through how support for WASM kind of got to go over the years. So, you know, WASM itself was originally designed for web browsers. You know, people wrote games and other performance-demanding applications that ran in the browser using WASM. So in 2018, with Go111, Go introduced the JS WASM port for compiling WASM for the browser. You know, so to use it, you just import syscall slash JS, and then you can add Go code as elements of the DOM or manipulate the DOM some other way. People made some really interesting stuff with this capability, and if you check this engine, eBit engine, you'll find that someone even created a 2D game engine for Go that many others have used to build Tetris and Flappy Bird-style games and a lot of other cool stuff. But anyway, it's easy and performant. But WASM kept evolving, you know, and developers started to use it beyond the browser, which brought about WASM, the WebAssembly system interface. So WASM provides an ABI and an API for WASM programs to interact with system resources like the file system, the, you know, system clock, random data utilities, stuff like that. It's kind of similar to a POSIX sort of layer. So this unlocked a lot of new opportunities. So now WASM could be used for portable, more secure server-side applications or CLIs. And that's when Go introduced support for WASM preview 1 in Go 121. So in the example on this slide, you know, we have this regular Hello World program. And next, we compile it with this Go OS WASM P1 and Go ARC WASM build flag. And then we use this off-the-shelf WASM runtime. In this case, WASM 0 to run the program. There are many free open source WASM runtimes and libraries. WASM 0 happens to be a pure Go one. And I'll show you how we're going to build our own host with it on the next slide. But there are many others, you know, just for the record. There's WASM time, WASM edge, WASM ur. And the point is that you can see that by supporting WASI, the WebAssembly system interface, Go 121 enabled you to build WASM modules as an executable where you started in the WASM runtime, run the main function, and done. And that brings us to Go 124. So, Go 124 expands Go's WASM capabilities in two big ways. First, Go 124 enables you to export Go functions to the WASM host with the Go WASM export compiler directive or pragma on line four of my example. So, now, once we compile this to a WASM binary, the WASM hosts will be able to directly call my add function. And remember, we can do this with a lot of different languages all running in the same WASM host, so maybe you'll have a subtract function in another language that's been compiled to WASM and put in this host. The second important capability in Go 124 is support for building WASI reactors. So, with Go 124, you can build your WASM module in, you know, reactor mode, which makes it so that it doesn't require reinitialization once it's finished running. So, this is really good for long-running applications or plugins or extensions that you just want available indefinitely. You initialize it once, you leave it running, and you can continue to respond to calls. So, in the example on this slide, we have this add function and an empty main, and then we have this compiler directive WASM export on the add function that'll make it available to the WASM host. And then in our WASM host application, we create a simple program that just imports WASIRO to create a WASM runtime, and there's some boilerplate here that I won't go over, but at line 30, we look for the exported function add. We then tell it to add A and B together on line 32, and then we decode the result on line 33, and we print it on line 34. So, that is the go WASM export compiler directive at work. Then, starting at line 37, we do it again. This time, we add B and C to get a different result, and then we print it on line 38. So, this is the WASI reactor at work. You know, this instance is still alive, no need to reinitialize it, and you can keep state in the WASM plugin, you know, or anything else that you need for, like, a long-running use case. So, we compile our first program, the one with the add function, with the go arc WASM and the go OS WASI preview one flags, and then we add this build mode equals C dash shared so that it compiles to a WASI reactor, and that spits out a WASM file. And then we build our host like we would any other Go application with a go build, and then we run it. And yeah, one plus two equals three, and when we ran it a second time, three plus two equals five. That's great. So, with that, I'm going to pass it to Armand for a talk on service extensions and how you can use this on Google Cloud. Thank you. Thank you, Cameron. Hi, everyone. My name is Armand Rai, and I am a product manager at Google Cloud. Specifically, I am the product manager for service extensions. I'm excited to share with you how you can take advantage of the innovations that Cameron highlighted to run your Go compiled WASM code on GCP at the edge. It's been a great pleasure partnering with the Go team to make this happen, and we're really excited to talk to you about it today. First, let me quickly introduce you to service extensions in case you're not already familiar with it. Service extensions is a GCP product that allows you to run your own lightweight business logic at the edge in a fully serverless runtime that is optimized for both performance and cost. Thank you, WASM. There are four key takeaways that I want to share with you about service extensions. One, it's fully serverless. I already mentioned that. You pay for only what you use. All you have to do is provide the code, and Google Cloud manages the rest. Two, service extensions integrates with multiple cloud networking proxies, offering a common experience across whichever networking proxy you want to use your custom code with. Three, service extensions is committed to open source, leveraging Go, WASM, and the proxy WASM APIs. Finally, service extensions is very developer-focused. It offers a robust local testing experience, including a performance benchmarking kit, but I'll touch more on that in a little bit. Service extensions is built on top of Google Cloud, which means you get to take advantage of GCP's expansive network. It spans over 40 cloud regions, 200 network edge locations, and we have a private backbone with over 2 million miles of fiber and 30 subsea cables. It gives us reach into over 200 countries and territories worldwide with path diversity to ensure both reliability and performance. The best part is you can deploy your code once and run your code as part of this massively distributed edge closer to your end users. There's a lot you can do with service extensions, and we have a ton of samples in a GitHub repository, written in Go, of course, that you can take advantage of. Customers commonly use service extensions for use cases across traffic, security, performance, and observability. For example, with security, you can authenticate client tokens at the edge to help protect and secure content served from our CDN. This helps protect your premium content from being stolen. For traffic management, you can run a bulk redirect engine, offloading this task from your origin. If you own a lot of vanity domains, different language pages, or have a lot of deprecated sites, you can easily manage this redirection with service extensions. All the code does is check to see if a domain is in the list of redirected domains and then redirects the user to the desired destination. To improve performance, you can run experiments by performing A-B testing to see which changes resonate the most with your customers and audiences. Your code can selectively decide which files or which website to serve based on characteristics of the client, and your code can check that for you seamlessly. On the observability side, you can write your own custom logging directly into Google Cloud Monitor and Google Cloud Logging. This is really unique because you can log variables that may not already be logged by GCP, or if your plugin takes custom actions, you can log those as well, and it's all seamlessly flowing into your whatever observability pipelines that you're using today. These are just a few examples. There's a lot more that you can do with service extensions. Please check out the GitHub for a complete list of recipes, but I just wanted to highlight a few that resonate the most with our customers. So far, we've covered what service extensions is, where it runs, and what you can do with it. Now, let me briefly tell you about the developer experience. The main thing I'd like to highlight here is that Google Cloud's networking proxies that service extensions integrates with are all based on Envoy, the open source proxy. What this means is that you as a developer can do a lot more functional testing and benchmarking much earlier in the developer journey. For example, you can run an Envoy proxy locally on your own machine and use the WASM filter as part of Envoy to test your own code without having to actually upload anything to GCP. Additionally, we've created an open source tool which you can run as well that does both functional testing and performance profiling to make sure that your code is working as expected. Making sure that your code is both semantically correct so you can use it for troubleshooting that but also making sure that it's performant so you can get a sense of how this is actually going to run in production before you even have to upload it to Google Cloud. The rest of the journey, you upload your WASM binary to Artifact Registry which in case anyone isn't familiar is Google Cloud's platform for container management or artifacts. You can upload there. And then you can attach it to your networking proxy of choice. For example, our global application load balancer which will then distribute your code to all those regions and locations that I showed in the previous slide. Finally, this product is built on top of Google Cloud's logging and monitoring pipelines. So you have the necessary insights to operate and manage your plugins on an ongoing basis. We have real-time logging so you can see which requests triggered a service extensions plugin. You can see which response codes were served and you can even write your own custom variables into cloud logging like I mentioned before. And then of course from a monitoring perspective you can see how your plugin is performing in production. You can see how much CPU it's using, how much memory. You can create custom alerts around serving certain response codes so that in case anything goes wrong your teams are aware and can jump right into action. To close out my section I will briefly show you a real example of some code written in Go that can be deployed with service extensions. The code is pretty simple. All it does is add and replace some HTTP headers. The full example and many others are on our GitHub but hopefully that should make it easy for you to get started. Grab one of the recipes attach it to our networking proxies and try it out in action on GCP. With that don't just take my word for it. I'd like to welcome Justin Reed Principal Engineer from Shopify up to the stage to talk about how they make all of this come together. Hello folks. Thanks for having me Cameron, Armin. It's a pleasure to be here and it's an honor. Like Cameron said my name is Justin Reed and I'm a Principal Engineer on Shopify's infrastructure team. This is a group of a few hundred individuals who run all of Shopify's infrastructure databases, networking storage, compute, search, streaming, everything. And we run the global platform that Shopify is built on top of. Today I'm going to talk about how Shopify uses service extensions in our Google Cloud load balancers and how we use them on our most highly trafficked traffic endpoints. First up, I just want to talk a little bit about Shopify's global infrastructure. We run in many Google regions around the world. this is just the current map that we use. We aim to serve traffic close to the user's physical location and we also set this pattern up basically to have decent regional failovers between different geographies. We're constantly evaluating the value of these. We monitor all the time the performance, time to first bite, first contentful paint, you name it. And we're able to spin up and down new regions as our buyers grow in certain regions or shrink in certain regions. And so we do this to get the best price performance out of Google. We lean very heavily into Google's global network. We use a layer 7 Google application load balancer as our main entry point into the Google compute network. We put a lot of traffic through this and I will show you here last year, Black Friday, Cyber Monday. These are our public numbers we posted on our press site. We were hit at a max of 284 million requests per minute. We do almost 1.2 million, 1.2 trillion, sorry, edge requests over this weekend and tons of database rights like that. Very large platform. If you're interested, I didn't have the video on this presentation, but if you're interested in an interactive example of what this looks like, visit that website, bfcm.shopify.com on your phone, on your tablet, your laptop. It is a 3D experience the team built where it real-time maps on a 3D globe all the traffic sales of Shopify. during the Black Friday season. Service extensions. Service extensions were a wonderful surprise to us. We used the Google application load balancers to pull traffic into our compute regions, and we worked very closely with Armin over the years to work on things that we would like to see. And so once they started shipping it into GA and the beta, we were first in to try it out. So there's some current examples that we use it for that are really helping us kind of explore the use of them. One example is adding custom logging to cloud logging. We have tons of origins and different applications underneath, and they will decorate their response headers with various information, server timing headers, things like that. And so it helps us having all of that in cloud logging so that we can correlate and debug all the requests as they flow through. We add, append, and modify headers as they come into the network, and that allows us to start tracing. We have a custom OTEL tracing system. It allows us to append tracing headers at the furthest edge as they come into the network. This lets us trace down things like peering agreements between providers in certain regions and then work with our partners to resolve those issues. And then we also normalize headers as they come in as well. Sometimes we use certain headers for various reasons, caching, some segmentation internally, and we like to normalize those at the furthest edge of our network. Here's a couple examples of what that looks like. So as traffic comes in, the bottom example there is that we can add our custom tracing header, and then we have, like I said, internal Grafana-based observability system, and that allows us to add a GCLB, Google Cloud Load Balancer, span into all the spans. And that lets us see what traffic came from the end users network ISP into Google, into our cluster load balancers, and then into the application. And we trace all the way down to database calls in the same UI, all the way down. So this helps us a lot to see what actually is happening to our end users. And then response headers as they come out. We have custom request IDs and various headers for way too many to mention while I'm on stage, but all things that we want the entire system to be aware of as the request goes out. And like I said, this system here ran BFCM last year. So we had all those trillions of requests run through this system. And we're really excited about the future of service extensions. I know Google has very big plans for them as well. But we have other things that we currently do in other systems in our network. Bulk redirects is a great example. Sometimes we just need to service redirects. And Shopify has been around for 20 years. We have a lot of redirects. And so it would be really nice to just pull that out of the origins and up to the edge. We're exploring HMAC validation for secure CDN delivery of private assets. And certain things like custom block lists, user agents, and things like that to protect us against bad actors. And we are also, you know, interested in, you know, the future where we can modify cloud CDN caching segmentation based on headers as well. And we are just keeping pace with Google. Google here is building fast and furious. And so we're very excited about the future. I also want to touch on Go here because this is about Go. And so, yes, service extensions, an incredible use of Go and Wasm. But Go at Shopify is deeply loved. We, as some of you may know, we are a Ruby shop. And Ruby is the language that people reach for first. But Go is in our toolbox probably right after Ruby. And depending on the application, depending on the use case, it is the first chosen language. We use Kubernetes and GKE globally. And so integrating with Kubernetes is, Go is, there's nothing better than Go in order to do that work. Go gives us superpowers internally as well. It's used in some of our largest applications. And we don't, we love, like, the performance it gives us, but also the internal iteration speed it gives us as well. One fun fact here is that the, there is an application internally that serves all of the image, JavaScript, CSS requests. Every one of those for the past five years has gone through a Go application. So this is millions of RPM that travel through a Go application that is incredibly stable. We don't need to worry about it. And when we need to iterate on it is quick and easy. We also love the security, reliability, vulnerability management. Like I said, this image application I talked about is, we bundled in some C Go libraries and being able to track security and vulnerabilities through this system is very important to us as well. And then internally we use it for debugging, performance management. We use PProf a lot and Delve as well. And this is because we run at a very large scale. And so even a 1% performance improvement equals real dollars and real impact to our users as well. And so for us the total cost of ownership win here is substantial. substantial. And we reach for it more and more over time for sure. That's all I have today. So we have enough time for Q&A. I'm going to bring Cameron and Armin back up. And also your feedback here is greatly appreciated on the session. You may find that the stars only select five. So go ahead and hit five stars. It's a little UI bug. We're getting Google to work on it. But five is the one. And thank you. Thank you for joining us today. Thank you. Thank you. Thank you. Thank you. Thank you.