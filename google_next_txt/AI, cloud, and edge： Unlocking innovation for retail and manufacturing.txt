 Good afternoon, everybody. Thanks for joining us today to talk about edge and AI for retail and manufacturing. My name is Will. I'm a product manager with Google Distributed Cloud. I'm going to be joined up here in a little bit by my friend Nishant, also a product manager with Google Distributed Cloud. So what we're going to be talking about today is, of course, edge and AI. So we're going to start with a little bit of a motivator for why this is an area that we're in, why we're so interested in it. We're going to go through a quick overview of Google Distributed Cloud, and then we're going to get into all the things that are new since last year, for those of you that may have come to this session then. I'm going to attempt a live demo in front of you after that before introducing Nishant to come up and talk about some AI solutions that you can run at the edge, and then we'll wrap with a panel with some customers and partners that we brought here today. So based on the fact that you chose to attend this session, I'm guessing each of you is in some way responsible for a place that looks like this. It could be a restaurant, a store, manufacturing facility, a logistics center. And I'm guessing it's not just one of them, it's many of them. Maybe 10, 100, 1,000 in one country, in many countries all across the world. Now, inside each of those locations, there are people. There are people doing the sort of work that your site does. Maybe they are serving customers, ringing up sales. They're preparing food. Maybe they're moving heavy objects from one place to another, building things, fixing things. So for this type of community, what are the things that we do for these people? We give them applications. We give them point-of-sale applications. We give them kitchen or warehouse management applications. Or since it's 2025, AI-based applications, maybe for worker safety, for instance. Now, the key thing that you're going to be thinking about when delivering these applications for your people is where do you put them? Now, this is a cloud conference, so I'm sure that each of you has some experience deploying applications into the cloud. Maybe these are your consumer-facing applications, your corporate applications. And in doing so, you've gotten all the benefits. It's easy. You're using containers. You're using CICD and doing all these things on a day-to-day basis. So your first instinct would be, why not put all of these applications in the cloud as well, your point-of-sale and your management applications? Well, there's some problems that can happen that make that not such a great idea. It could be a Northeaster rolls in and takes out your Internet lines. It could be that the artisanal cheese shop getting built next door has a backhoe, and it rolls over your fiber and cuts off your Internet. Or it could be that your cable or DSL provider decides that that maintenance that they've been putting off for a couple of years is now going to happen at 2 o'clock in the afternoon today. Now, the people in your places and the customers depending on you still need you when the Internet connection goes down. This is sometimes the times that they need you the most. Relief workers are going to need to eat. You're going to need to get aid packages from one place to another. So how are you going to do these things together? You need to deploy these applications to your users, but you need them to be resilient for those times that the Internet connection goes out. Well, you're not going to go back to the olden days, putting a Windows machine back in the back, because you know that doing so means that the next five years are going to be full of you RDPing in, running Windows updates, and doing all these sort of things. You're not going to get anything else done. So it's this sort of design point why we created Google Distributed Cloud. So the aim here is create the cloud-like experience, the cloud-like environment, except it's actually running inside your premises. So let's talk about how it works. Google Distributed Cloud is a cloud service that is physically existing inside your premises. So I call it a cloud service because we've taken care of making all of the operations work on ourselves so that it appears just like any other cloud service to you. So we come in and install it. We maintain it. We monitor it. We do the platform software updates. If a server, if one of the physical servers has an issue, we will detect it. We will send somebody to replace it. All of the same things that we would do in our own public cloud, only, of course, it exists in your environment. Now, because it's a physical thing, it means you can make some choices about the exact configuration that goes into your site. So you can select a size on how many vCPUs you want. You can choose whether it's a single non-redundant machine or a redundant cluster of three machines. You can choose whether you're running AI applications and you want to have an NVIDIA GPU in these machines to accelerate that. Now, I made a big deal about the fact that this platform works even when the internet connection goes down. And the thing that makes that possible is something that we call survivability mode. So as the name suggests, the cluster survives when the tether to the cloud gets disconnected. And the way that that works is all of the Kubernetes control plane that is needed to keep your applications running exist on the local cluster. It's only the management that exists in the cloud. So that tether to the cloud is there for you to make configuration updates and it's there for logs and metrics to go back up to the cloud. So when that connection isn't there, we're simply going to buffer all of the logs and metrics that need to go up to the cloud. So obviously you can't make configuration changes from the cloud. You're not going to see anything in your cloud dashboards. But all the applications serving your users are going to keep on working. And when the connection is reestablished, since everything was buffered, everything is going to catch up and it's going to appear in all these things like it never happened. So that's the quick overview of where we are. Now, for those of you that attended last year, let's talk about some of the things that are new and exciting. And what could be more exciting than compliance? By show of hands, how many of you have people within your company who come asking you about PCI and HIPAA and things like that? Maybe some of you are the people asking about HIPAA and PCI and things like that. So the first thing to talk about here is the fact that we've been working with our own compliance folks and putting the platform through testing of various compliance standards. And we've knocked out SOC, ISO, PCI DSS, and HIPAA. So this is whole platform testing that we do on our side for each of these compliance. But for those of you that do this day after day, you know that compliance isn't something where you take a PCI thing over here and a PCI thing over here and combine them to make a bigger PCI thing. And you know that the entire solution needs to be compliant and audited and things like that. And that's where things can get tricky. So how many of you have some applications that are PCI or HIPAA compliant, but some where you don't want to put in the extra rigor of getting them certified as well? Anyone have this? Perfect. So this is something that we've been talking about with customers and our own compliance folks for a little while of how do we enable customers with these mix of applications to run them without having to duplicate their hardware? Because once you have to deploy your compliant hardware over here, your non-compliant hardware over here, now you have twice as much hardware, it's extra to manage, all of that sort of stuff. And the solution that we're bringing for this is something that we're calling workload isolation. And workload isolation is three things coming together that help with this. One is container sandboxing. So with container sandboxing, we're taking a technology called Gvisor, which is a hypervisor technology that we develop, that sandboxes the containers. It intercepts all the syscalls, so no one is going to break in or out of that container. Another thing is VM sandboxing. We're taking a technology called AppArmor, it's part of Linux, and further wrapping it around those virtual machines for the same reason, create these isolated domains with your applications. And we're putting those together with something that we're calling fully featured secondary networks, the idea here being for different scope applications, you can put them on different networks. For a long time now, VLANs have been an acceptable technology to keep different applications segregated, so we're just doing that, we're taking advantage of that within our platform as well. And each of the networks that your applications sit on are, as the name suggests, fully featured. You can do gateways, DNS forwarding, all the sort of Kubernetes networking features that you may need. So we think all of these three things come together really well to help our customers that are dealing with these mix of scope. So, so far, everything that I've been talking about is very platform level. So how do we scalably deploy these hardware-based platforms in all the places? How do we manage them? But your goal isn't just to get platforms in a lot of places, it's actually to get the applications into all of those places. So we've been working on something called fleet packages. So fleet packages is something that's based on a lot of experience that we have in managing our own application deployments at scale. So Google, of course, for a long time has been deploying applications to our own data centers that you're familiar with, but we also have many edge deployments out there in order to power the YouTube network. We have to put content distribution networks in lots of different countries and lots of different buildings. So in doing those and having to deploy our own applications and not make our own developers go crazy in the process, we've developed a lot of technology on how exactly this works. And as Google, we like to take those learnings and create customer-facing tools, and fleet packages is that. So I think the best way for me to convey what fleet packages does is to walk through an example of how you might use it to solve a problem. So because of the sort of businesses that we're talking about today, one of the common themes is inventory management. So we all have things that are coming in the door, things that are leaving the door. The delta between those is your inventory. It's a real pain to manage. You have to pay people to have clipboards, and you have to audit it, and all this sort of stuff. So wouldn't it be nice if there was an application that just used the cameras you already have to look at the boxes coming in, the boxes going out, and automatically take care of your inventory? Now, I don't know if this exists, but we're going to pretend it does for the purpose of this example. So this application is called Smart Inventory AI, and we are going to deploy this application and subsequent versions of this application to our fleet of 1,000 stores out in the wild. Now, not all of these stores are created equal, and when we deploy our application, we don't want to deploy it to all 1,000 at a time. That would be crazy. What if there was a bug? So instead, you're going to divide them up. So the first thing you're going to do is divide them into some coarse-grained things, like you probably have a lab somewhere where if there's a bug and it breaks something, nobody really cares, to pilot or canary stores where maybe these are corporate-owned stores, maybe they're part of an early adopter program. For some reason, you have some more risk appetite on those, and then you have your production stores, and even within production, you don't want to throw it all out there at the same time. You still want to manage your risk, so you're going to deploy in waves. So each wave is going to be roughly an order of magnitude bigger than the one before it. So the first thing we're going to do as part of fleet packages in rolling out this application is tell the clusters which of these stages that they're in. We're going to do it the Kubernetes way, and we're going to use tags. So we're going to have a key called stage, and we're going to tag it with each of these designations. So once we do that, we're actually going to start putting together the manifest and things for our application. So here is the Kubernetes manifest for this version of the application. The text is small. It's okay. It's just standard Kubernetes stuff. So it's the deployment object. It's a config map that tells that deployment object how to access local cameras and things like that. But the important thing here is we're not going to apply this configuration directly to the cluster. We're going to check this into Git, infrastructure as code, and we're going to use tags to control the versioning of it as it goes. Now, once we check it into Git, we're going to now start interacting with fleet packages to take that code and create a fleet package and deploy it. And the way that you do that is with more YAML on defining what this fleet package is. So walking through this right here, starting at the top, you name the thing, and then you see some configuration that has to do with cloud build. So the cool thing about fleet packages is it uses cloud build to talk to Git. Now, the nice thing about doing that is it means you don't have to hand out your Git credentials to each of your individual sites and manage rotating those and things like that. You only need to have cloud build talk to Git and your clusters talk to cloud build. Then, of course, you link it with your project and you see here that I'm linking this with my stage of lab. So this is a fleet package that I'm going to deploy to the lab. And the big thing this does is it does a controlled rollout. So with a rollout, you can choose to do all at once, one at a time, or N at a time. Since I'm early on in my rollout, I'm going to do one at a time. But then as I progress through my waves, eventually I'm going to get to wave three, in which case I'm going to change it and do ten at a time. It's a bigger wave. I want to get through it faster. So that's kind of the setup for all of this. Let's see if we can switch and do a live show of what it looks like to deploy here. There we go. It's going to take a second to deploy my machine. All right. Let's test it out. All right. I think it's working. All right. So here we can see my fleet packages console. It's part of the config sync part of GKE Enterprise. And I have my smart inventory AI app here. I go in. I can see that I've already deployed version 1.0.1. What I'm going to do is deploy a new version out here, 1.0.2. But I can see here that all the different clusters that it's applied to and so forth. So we can look at 1.2. Here's the definition for it. It's the same stuff that we were just looking at with the other slides. So here's where it picks up the tag for the Git repository. I'm going to do a max concurrency of two since I'm in prod wave one with this example here. All right. Here we go. And here's how I apply it. There we go. So when I kicked this off, I gave this to fleet packages. Fleet packages is turning around and it's giving this to Cloud Build. So if we go over here, we can quickly see... We can see it building. So what's happening is Cloud Build is going out, taking this code, it's compiling it into an OCI image, and it's putting it into the artifact repository. Again, that's handy because it means you only need service account credentials on each of the clusters to talk to Cloud Build to grab this configuration rather than get credentials and things like that. All right. That is now done. We can jump back over to fleet packages. And if we're lucky, there we go. It is in progress. We can see here it picked up the tag of 102. And because I told it to do two at a time, it's processing the first two right now. Once those come back successful, it'll move on, do the next two and the next two after that. So we can see here current version, desired version, it's working its way through. So it's going to take a while. We're not going to sit around watching the whole thing, but over the next, you know, 10 minutes or so, it'll work its way through the full cluster. So that's fleet packages. With that, I'm going to invite Nishant to come up and talk about some AI stuff. Awesome. Thank you. Let's switch to next slide. Thank you very much, Will. My name is Nishant Kohli, the I-BOM product manager for GDC Connected. What you just saw is an amazing platform that we are delivering, but platform is just platform. Without key applications, solutions, platform isn't going to be able to do things for you. I'm going to walk through some use cases, scenarios, some ISV applications that we're integrating with. Of course, they're going to be AI-related. We're in Google Cloud session, so AI has got to be one part of it. As we go through it, hopefully, I'll just use the kick here. As we're focusing on some of the key things for GDC Connected Server product line, these are some verticals that we see highly valuable in the use cases that map to the solution that we're delivering. These fit into retail, manufacturing, financial, public sector. I'm not going to walk through all of the use cases, but generally, if you think about retail, self-checkout seems to be the most prevalent one that we're seeing where AI is going to be able to accelerate the environment and be able to also do other things around loss prevention. I know we'll walk through some of those details as well. In manufacturing, safety is a huge aspect that comes into mind. But then Vision AI also is prevalent in manufacturing, and latency is a big aspect as well. So for those reasons, a lot of those solutions have to run on-prem. They have to run in the environment closer to where the work is happening or wherever the data is being generated. If you think about financial sector or even public sector, there might be security regulations that are prohibiting those workloads to go to the cloud. That's, again, for those reasons, we're going to run that on-prem, and the GDC platform helps with that. So for that, we have several ISVs that we've been working with. Toshiba is one of those. Toshiba has been working with us closely in providing a solution for end customer. That solution is a point-of-sale system. Generally, if you think about any retailer's, point-of-sale is the heart of the store. That's really where all the transactions, everything's happening, the customer monetization, all of that is happening from the point-of-sale outwards. So if we don't have a up-and-running point-of-sale system at all times, it's going to delay. It's going to have an experience that customers don't want. So we're working closely with them, see how we can, A, do this at scale, and why they want to do it with GDC. Now, for Google Distributed Cloud, it is the value of the platform. Being able to roll it out at scale gives our partner the flexibility and the time to develop, focus on their software, the solution that they're looking to build for their customer, in this case, delivering a point-of-sale system. Some stats from them is really, you know, as they work with us, they were able to reduce the training time that takes for their associate by 93%. They were able to transact more on the self-checkout without any failures more times than they normally have seen. So the platform's helping them with the combination of their AI capability, the platform capability, they can do a lot more in those environments than they were able to do before. And as we go through some of those scenarios, they are able to give a better customer experience overall as they deliver that solution. Next, I'll talk about our partner, Standard AI. If you haven't been to the expo in the hardware works, we have the partner working jointly with us and doing a live demo. It's a pretty cool demo. Take a look. But we'll talk about what their capabilities are. So our Standard Division product is what is being powered by Google Distributed Cloud to deliver store analytics. It's really, think about, you know, what e-commerce did to retail. We're trying to do the same thing, but looking at the physical space. How do we take that physical space, digitize that information, and being able to provide that data back to the business near to real time and do something with it? That could be simply you're queuing at the cash register and their queue depth has gone long enough where, hey, maybe call out Nishant that's sitting in the back, you know, racking and stacking, have him come to the front of the store and operate so we can transact faster. So those aspects of the near to real time is critical. You already have technology in your retail stores that's providing that information. It's just locked up in the security cameras. Now, how can you take that, take that information, digitize it, bring it forward? It's some of the key things that we're doing with Standard AI. Again, similar theme you'll see there is the partners looking to deliver a solution to the customer, but they don't want to worry about the platform component. They want to have a consistent way to be able to deliver that to every single location. So as GDC becomes that standard platform for the customer, they know for the hundred, thousand, tens of thousands of locations where that platform sits, they can easily now go deploy their application stack on top of it. Will talked about standardization of delivering our own software stack. He gave you an example of how we roll out our own upgrades to the platform. That same capability can also be used for the applications as well as we go through it. Next, we'll, again, another amazing partner of ours that's in the show floor with us in the expo. So, again, come check us out on the show floor in the hardware works area. Ever seen is, again, Power Ring with Google Distributed Cloud. They're bringing the self-checkout capability. That's one of many things that they're doing with us, but self-checkout is the largest revenue loss that people have. And think about when you're transacting at the checkout, it's not that you're looking to steal stuff. Sometimes it's just errors happen. So they're looking to provide with Vision and GDC how they can reduce those error rates. How can they have their ROI back to you? They generally talk about within six months of deployment, they guarantee that you have the return on investment because the amount of loss that you're seeing at the checkout, they can reduce that by a large sum. Now, not a lot of people can say that guarantee. In six months, we can get your return on investment back. So I think that's a really critical, powerful piece that Ever seen is doing with their Evercheck product line. As you look through beyond that, there have several other things like Vision AI agents that can do a lot more in the store as well. Similar technology that's sitting at the camera at the cash register, that camera is sitting all around the store as well. So maybe shelf management. Other aspects can be also looked through the same capability. The value proposition from there is the same thing. They're looking to see how do they accelerate their application stack, how do they maintain robustness across different locations, industries, verticals, and not have to worry about the platform, the upgrades, the resiliency of the platform. That's all Google's responsibility on that behalf. As we go through some of the key things that we talk about with them as well, is really the ability to deploy this at thousands of locations. So they're up and running close to 100,000 locations as in a store line or a self-checkout station. They're deployed at 85,000 plus stores or 8,500 plus stores across the globe. And it's running 24-7. The key aspect of self-checkout is if the store operations are open, you should be able to self-checkout without having an employee there. So it's really helping the system, helping the business expand and grow and then taking on that ownership and responsibility that a store operation needs. Generally, you know, they're processing close to six petabytes of videos, over 15 million transactions that they're monitoring over the period of time. So just imagine the volume of processing that they have to do and being able to apply those services back to the business. Lastly, I'll talk about our Intenseye solution. Intenseye is really looking to solve that worker safety problem that's out there in the industry. Some of the stats here was astonishing to me where three million lives are cost nearly three trillion dollars to the insurance industry because of workforce safety. So Intenseye's mission in life is really about how they can make the environment safer for the employees, how can they apply the AI use cases so that when you're at a construction site, you're not harmed in any shape or form. So think from those perspective, they're providing a monitoring capability, they're providing assets and things that can actually not only proactively do things, but provide standards where they can actually take safety and make that more of a service that you can then apply back to the customer and provide that capability back to the business. So it's really reducing workforce-related injuries, illness rates is some of the core things that they're focusing on. So really turning safety into a strategy that you can take advantage of, not only prevent, but predict potentially what are the things that may happen in the environment using AI and solutions that they're providing. So these are some general use cases that we've seen, and you see the use cases, I'm not just talking about retail. I'm talking any industries these could be applied to, all the way from your retail industry to manufacturing, wherever there is some sort of a vision capability needed. Vision is just one of many, but vision seems to be one of the key aspects that is generally in any retail shop. You have security cameras, so how can you repurpose, reuse that technology and be able to take that data with Power AI and turn it into a revenue stream, a safety stream, or some sort of regulations that you can prevent and do better for the business and provide a better value back to the customer. So as I mentioned, these are just some examples. You can take advantage of seeing some of these in our show floor as well. On that note, I will introduce our panel and have them walk up on the stage with me. First, I have Naveen Anandaraj. Anandaraj. He's a tech fellow at GPC, Genuine Parts Company. I'm going to say a bunch of Gs and get confused with that. So GPC is one of them. GDC is the product. Next, I have Adam Rice with me. Adam is a business leader from Intenseye. And last, we have Brent Kramer from GEC, German Edge Cloud. So now I have covered the three Gs that we're going to talk about. You guys are in different orders of seating, but that's okay. I think people can see you, but that's fine. I understand you're seeing it the other way on the show floor. But I'll start our questions with the panel. As you see, our panel is made up of many different customers here, different industries. So starting with, you know, our questions around what are some of the key things your business is doing, the challenges that you see, and how is GDC helping you solve those? Hey, thanks, Nishant, for having me. Good afternoon, everyone. Hope everyone is having a great Cloud Next. So like Nishant mentioned, I work for a genuine parts company at GPC, and we are a global services organization for aftermarket automotive and industrial replacement parts. So most of you here are familiar with Napa Auto Parts, which is our automotive business unit in North America. And we have nearly 6,000 stores here in the U.S. And in every store, we have applications and infrastructure which are hosted within our store and support both our selling functions and store operations. Now, we are working on a lot of newer initiatives, and for those initiatives, like some of the features which we are trying to include is efficient price management, efficient inventory management, giving more capabilities for our store associates, and improving efficiency for our store operations. Now, as we look to build these newer initiatives, there are like some challenges that we are looking into with respect to hardware, software, and security. So when it comes to hardware, we want to see how we can minimize the operational heavy lifting of our infrastructure in the stores while still running those operations locally. And when it comes to software platform, how can we have a modern platform that allows us to be more agile? We have better observability and also allows us to natively integrate with the cloud to build more efficient hybrid applications. And lastly, security. And Will spoke about workload isolation, which is really important for us, because we have different types of workloads which runs in our stores, and they have varying levels of data sensitivity and protection needs. So how can we segment these workloads while still running them in a way that we can scale them using a software-defined template? That's great. Adam, same question to you. Cool. Thank you very much, and thank you, Nishant. That was a great Intensi pitch, by the way. Maybe I'll join the company. Good stage set. So a little on Intensi, we use artificial intelligence and computer vision to help organizations see the unseen, help them take a proactive approach to safety and serious injury and fatality management on operations. A little about our business, we were founded in 2018. We've grown incredibly fast in the past seven years. We have 160 customers. We're operating in over 90 countries and have a little over $90 million in funding. So a lot of the problems that we're dealing with as a business are problems around scale. Scale in terms of coming across operations where our customers' infrastructure is prohibitive or coming across use cases or problems that we want to solve that we can't solve with cloud infrastructure. And I'm also happy that we talked about compliance because there's also some very interesting wins with GDPR inference on the edge that we're taking advantage of as well. So when I think about the delightful partnership that we have with GDC, they're helping us scale as a growth business in a way that we weren't able to before. No, thank you. Bern? Thank you. Bern Kramer from German Edge Cloud. What are we doing? We are providing a software solution. If you run a manufacturing line production, you like to have transparency and you want to know what's happening. And what we are providing is a software-based solution to optimize such a production line or productivity. OEE, Overall Equipment Efficiency, for example. And that was always a discussion about do you want to do it on a cloud-based scenario or an edge-based scenario. And what we now see is based on the Google distributed cloud, you can do both. And it's depending on the use case. And that is even for us an important advantage that we can now focus on the use cases, transparency, data exchange, and to give scalability. No, that's great. Now, next, again, you know, without having a conversation about AI, it's not an incomplete question in panel. So maybe, Intensei team, Adam, why don't you tell us what are some of the AI challenges you're solving and how is GDC helping you there? Sure. So I'll speak a little bit about AI as it pertains to safety. So a lot of what we do is we leverage existing CCTV infrastructure and apply our own safety models on top of it. There's a handful of models that we can apply, that we can collect data over the course of days, weeks, months, and understand the trends on a specific facility and understand what risks or what hazards are the most important. And that is enabling businesses to take a proactive approach to understand what's going wrong before it goes wrong. Right now, a lot of the industry is focused on compliance or reporting after an issue happens. Now, when we think about use cases that we were unable to accommodate prior to a GDC partnership is use cases around low latency. So a lot of what we want to be able to do is identify these really meaningful trends, but a lot of these really large enterprises, not only are they looking to improve the general safety profile of their operation, they're looking to acutely decrease what we would call SIFs, which are serious injuries and fatalities. And it oftentimes is a human and it's a vehicle or some heavy machine, and with computer vision, you can actually apply rules where you can look at somebody entering an exclusion zone or somebody's hand entering an area that it shouldn't and a machine turn off. That is a beautiful thing that saves lives and that's something that is really, really difficult to do with cloud infrastructure. So we as a software business, we want to approach these really meaningful use cases and we don't want to have to worry about hardware and this partnership gives us the opportunity to level up in a really, really unique way that allows us to bite off arguably the biggest problems for safety leaders and enterprises and that's where the dollars and cents come from a budgetary standpoint. Now that makes sense. Bernd, I think you've talked to me about AI a lot in the cloud on-prem, so can you shed some light? Yeah. Yeah, if you, is industrial AI still available in brownfield plants? So far not. But the situation is you have ERP systems, you have MES systems, you have structured data, work orders, bill of materials, work instructions, but what about the data like Adam mentioned if there's a visual inspection? How do you collect the unstructured data? Some vibration signals, some temperature deviations and that is what we see is that that structured and unstructured data can now be merged and can be analyzed based on Google distributed cloud capabilities and that is what we see. AI standardized, available in the industrial manufacturing. Yeah, that's great. I mean, back to you, you know, we've been on this journey, you were last year on the stage with us as well, so it's been a great journey so far. Can you walk us through what that's been like and where are you today? Yeah, absolutely. So last year, I had spoken about piloting Google distributed cloud in one of our pilot stores in the US and as part of the piloting and we launched a new and we launched a new containerized workload to run in a hybrid architecture. And for that initial pilot, we used a 3-node GDC cluster and very soon we realized that for us to go further and scale, we would need to optimize for space, cost, and capacity. And so we partnered with your product and engineering team and we launched a new version to run on a single node. And then we repurposed the existing store to run on that one-node cluster. And along with that, we also identified more workloads and use cases over the last year. And the most notable one to call out is our homegrown hybrid payments platform that partly runs on GDC and suppose the payments for our store point-of-sale systems. And now we have a foundational model. We have built a foundational model that runs our entire store software stack on a single node GDC cluster that we have rolled out to about 20 stores and we are looking to scale beyond that. That's great. Adam, similar question to you. I know we're in a different part of that journey with you, but can you share some of your thoughts of where GDC can help? I think it really comes down to those CIF use cases. So identifying a way to provide what I would consider true enterprise value to some of our partners and then realizing that the hardware componentry to that partnership is what's driving it. I think that's a big piece. Nice. Vern, I know we just signed the contract back recently, so we're early, early stages with you, but I know there's a lot of good challenges that you shared with us. Can you share some of those with the crowd? Yeah. What we see, number one, is that if you run a production, again, because we are focused on manufacturing, do you have enough IT specialists? For sure not. That means you have to be focused on use cases, and that means you need a reliable platform for that. And then you need scalability. Scalability means applications which can be, pace by pace, can be added on to a platform. And with the Google distributed platform, or Google distributed cloud, sorry, you have that scalability, you have security, and you have the capability to step by step improve the situation even with industrial AI. That is what we see, and that is the reason why we signed the contract, yes. Thanks. Last question to you, Naveen. You know, as you look next, I know you talked about scaling. What are some of the challenges that you see coming up, and what are the things that you see us growing together with you? Yeah. So the immediate next step, or the need that we have, is to build a model that will allow us to scale rapidly. So today, we know that we can do about one to two stores per night when it comes to rolling out, but we need to scale more aggressively when we consider our vast network of 6,000 plus stores. And when we talk about rollout, there's a lot of activities which happens in our store, starting from hardware installation to networking configuration to app deployments, data restoration, and doing a validation. And with every rollout, we are learning something new, and we are incrementally getting better. We are optimizing for speed, and we are doing things in parallel, and we are automating as much as we can. Parallelly, we are also having conversations with the GDC product team on how we can offload a lot of things as part of pre-staging so that we can minimize the work which we are doing during cutover. The next thing, I think, is more about application deployments and scale, and I think we'll cover this in pretty detail on some of the way on how we can roll out in waves, and we have similar needs on how we can selectively target application work, application work versions to store, or roll it across the entire fleet. But generally, more broadly and longer term, we want to run the right workloads in the right environments, whether it is our, whether it is cloud, whether it is edge, or whether it is our data center. And there is a need to do localized data processing across our edge locations. And today, with GDC, we have a platform, and we'll also mention this, we are bringing cloud-like capabilities to the store, which was not possible before, and also have a platform that allows us to natively integrate with Google Cloud, which helps us to build efficient hybrid applications. Now, that's great. Adam, I know we're talking about some deployments with you and your team as well. Maybe one or two insights you can share that can help the audience, if they're engaging with us, what they could do better. That's a good question. I have written down here in my fun little notepad thinking about what scale out of the box means. Yeah. I think is an important one. So, me as a commercial leader, I dream about the time we have an SOW where there's just a little checkbox for GDC, and they can check the box, and then the contract gets signed, and something gets drop shipped two weeks later, and then it just shows up, and it turns on, and it works. And I can dream that that should be our goal, because that's going to lead to a better user experience in getting our technology turned on sooner rather than later. And I can speak to a personal pain point of IntenseEye. It takes anywhere from eight to 12 weeks to deploy our technology, and nothing hurts more when somebody gets hurt during a deployment period. That's a great point. It stings. It stings big time. So anything we can do to pull that forward has massive value for our collective end customer. No, that's great. I know, Naveen, maybe going off script here, last time we talked about it, you said, hey, the deployment happened, and it took less than two hours in the store. So we're looking to do the same thing. But last part to you, what are the things that you'd like to see better as we go forward as well? What we like to see is standards, standards, standards, because nobody's buying, sorry, technology without return on invest. That means customers are focused on use cases, and for that you need standards to have a faster rollout. You have an update capability and that means you have a future-proof system for upcoming topics like AI. That is what we see, standards, standards, standards. standards.