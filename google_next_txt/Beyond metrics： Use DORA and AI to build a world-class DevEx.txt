 . Hello, hello. How's everyone doing this afternoon? Yeah? All right. Who's here to talk about building a world-class developer experience? Good. You're on the right plane. We're going the right direction. I'm really stoked to be here with you. So, hello everyone. My name is Nathan Harvey. I lead up the Dora team and I'm a developer advocate with Google Cloud. Today, shortly, I'll be joined by Jody Bansal, who's the CEO and founder of Harness. So, we're going to talk about this idea of building a great developer experience. In fact, what we're going to talk about is here on the agenda slide. We're going to start with Dora. How many of you know Dora? All right. Is that the girl with the backpack? Dora? Yeah. We know her. Who do you think of Dora as the Digital Operational Resilience Act? A few of you. Yeah. That's not the Dora we're going to talk about today. But, if you follow the Dora principles, you'll be better with the Digital Operational Resilience Act. Finally, one of my other favorite Doras. Anyone here from at least the state of Indiana or Ohio? No? All right. So, you won't believe me, but this is true. In those two states, at least, they have what they call designated outdoor refreshment areas. And it's kind of like Vegas. You can walk around with an open container. That's what those are. So, that's Dora. That's all you needed to know about Dora. Now, I'm going to give you a lot more about it. And then, we're going to have Jyoti come up and talk about how you can use Harness to really take some of the insights and ideas from Dora and put them into practice. And then, we'll follow that up with a panel discussion. Jyoti and I are going to sit down and have a conversation. There may be time at the end for questions. So, if you've got a question, just, you know, you can jot it down at any point. And, of course, you'll be able to chat with us after the talk as well. So, let's talk about Dora. Dora is a research program that has been running for over a decade now. And Dora's research is really about how do we help technology-driven teams and organizations get better. Get better at what? Well, I like to say we help them get better at getting better. It's really about a journey of continuous improvement. And Dora itself, as you may know, kind of has a center of gravity around software delivery performance. Software delivery performance is one of the things that Dora measures. In fact, many of you will probably know Dora because of its four key software delivery performance metrics. You know, those metrics that ask things like what's your throughput and what's the stability like for your systems. And so, if you haven't seen these metrics before, I'll just walk you through them really quickly. And you can assess yourself right now with me. So, in your mind, I want you to think of one application or service that you work on. Just one application or service. Not all of them across your company, but just pick one that you're pretty familiar with. And I'm going to ask you four questions about that. And in your mind, I just want you to answer not with any precision. I want you to answer just what is the unit of measure for each one of these. And what do I mean by that? I mean, well, we're going to talk about lead time for changes. So, how long does it take when you commit a line of code to your version control system? Because you're all using version control systems, right? Yes, you are. Okay. So, we committed a line of code to a version control system. How long until it lands in production? And I don't need an answer. I just want you to think about it. But I want you to think about it in gross terms. Unit of measure. Is it months? Is it weeks? Is it days? Is it hours? Is it minutes? Like, that's as much precision as you need. So, these four software delivery performance metrics, they start with that. How long does it take for code to go from committed into production? How frequently are you updating production? Those two are our throughput metrics. So, this gives you an idea of how fast you're able to ship changes. Throughput is great. We want to be able to move fast. But, despite what some memes say, we don't want to move fast and break everything. We want those changes to land in a stable environment. We want to know that that deployment worked well. So, we have two other measures on the stability side. The first measure, well, you can see on this slide what it's called. But I don't call it that. I call it the expletive rate. Why? Because, well, you know what happens. You push a change to production and you or someone around you shouts out an expletive. That's a change failure. We have to immediately jump on that, roll that change back or push forward a hot fix. Whatever that takes. And then, the final question. When you do have one of those O expletive moments. And, it's not a question of if. It's a question of when. Because, no system is perfect. But, when those happen, how long does it take you to recover from that? And get your system back to working, happy for your customers again. Now, taken together. Alright, so you have your answers in your head, right? I'm going to quiz you later. It's not really true. I'm not going to quiz you later. But, just keep those answers in mind. And, just a couple of other things really quickly about these four metrics. You can use these four metrics to measure any type of technology that you're responsible for building, delivering to customers. It can be the mainframe application that you're working on. It could be the mobile application that you're working on. It could be, I don't know, a retail front end web application that you're working on. You can use these four measures to assess its software delivery performance. And, the other thing that's really interesting about these. And, the thing that we've learned and revalidated for over a decade now. Everyone thinks of, or has thought of, throughput and stability as trade-offs. I can either be fast and be very unstable. Or, I can slow things down and be really stable. But, what our data tells us is that's not true. These throughput and stability metrics tend to move together. So, either you're fast and stable, or you're slow and unstable. And, so it's really up to you. You pick whichever one you want. I'm not here to judge. You can have better software faster, or worse software slower. It's really your choice. It's really your choice. So, it's really interesting. And, what do you mean if I slow down I'm going to be less stable? Here's one of the best ways I know to, like, really land that idea. Imagine you do something twice a year. How good are you at that thing? Compare that to the thing that you do once a day, or once every two weeks. How good are you at that thing? You're way better. So, if you deploy to production two times a year, I'll tell you what your O expletive rate is. It's 100%. Because, every time you deploy to production, you haven't done that for six months, something is going to go wrong. So, it really is true that if you move faster, you can be more stable. These two things move together. And, a big insight that we have from the research, one of the best ways to get there is to ship smaller changes. Smaller changes that you're constantly shipping, you're moving iteratively. This is the path forward. Now, I want you to come back to those four measures that you put in your head. Because, here's another thing about our research. What our research shows is that software delivery performance is predictive of outcomes. Outcomes like organizational performance and well-being for the people on the team. So, ask yourself, if you were to change the unit of measure for those four things that we just talked about. Maybe they were months. What happens if you change that unit of measure down to weeks? How would that improve your organizational performance? How would that improve the well-being of the people on your teams? And then the question becomes, great, I want to do that. I want to improve my software delivery performance. How? Well, I'll tell you how you don't do it. You don't improve software delivery performance by thinking really hard about your deployment frequency. That's not how you change it. That would be like if we went and told salespeople. Because, you know, it's easy to measure salespeople, right? It's harder to measure us because we're engineers and we do creative work. But salespeople, you know how you can improve retiring your quota? Think really hard about your quota. That doesn't work. In the same way that we can't just think really hard about deploying more frequently. We have to, in our teams and on our organizations, we have to have the right capabilities and conditions in place. And that's what this diagram shows. On the left-hand side, you'll see these capabilities. And they're organized into three broad groups. A climate for learning, fast flow, and fast feedback. We need all three of these conditions on our team in order to drive improvement in those software delivery performance metrics. Now, Dora has been researching these things for over a decade. And now, of course, in 2024, when we released our second to most recent report, AI took up a lot of air in the room. Anyone feeling that? Yeah. All right. So, we are in the era of AI. And we did a deep dive in the 2024 report about artificial intelligence. And you can download that report right here at this QR code. So, if you would all do me a favor and pick up your phone and hold it up. I don't care if you take a picture of the QR code. Just seeing the phones up makes me feel better. So, thank you for that. Thank you for that. So, what did we find about AI in the 2024 report? That's what I'm going to dive into before I bring Jyoti up here. So, first and foremost, organizational priorities. Organizations around the world are prioritizing AI. There's a very small number who have a significant or moderate decrease or even a slight decrease in their prioritization of AI. So, this is at an organizational level we are prioritizing AI. That's great. This is an indicator for me that this is coming from the top down. Top down. But I'll be honest. I'm an engineer. Top down mandates don't always stick well with me. But what we are seeing is that when we talk to practitioners, we're also seeing a huge number of practitioners, over 70% in our survey, that are using AI in their daily tasks. And not just using it, relying on it. I'm relying on AI for a bunch of different tasks. And I know the words are kind of small. Read the top five tasks here. Code writing. Summarizing information. Code explanation. Code optimization. And documentation. Those are the top five things here. And I would wager that 80% of this room, you've done at least one of those things in the past two weeks with AI. At least one of those things. But here's the interesting thing. When we take these two things together, we've got a top down mandate. We've got grassroots or practitioner-led reliance on AI. This is a really strong signal to me that AI is here to stay. We've got those really strong conditions. And I like to contrast that to another technology movement that happened not too long ago. Probably it's recent enough that you all remember it. How many of you remember the time where you and your team were like, man containers are awesome. We have to containerize all of our applications. Probably a lot of you in the room. Yeah, you can raise your hand if you want. I do collect data. So I'm always surveying people. Yeah. So there's lots of people that remember that. For all of you that remember that, how many of you work in an organization where the board, the board of directors was like, man, what's our containerization strategy going to be? Nobody. Nobody. Nobody. But it's happening right now. If your board is meeting this week, I can virtually guarantee they're going to ask, what's our AI strategy? So take these two things together, right? We've got grassroots or practitioner-led adoption of these tools and that executive mandate. AI is here to stay. And we, as practitioners, as leaders, as learners, we need to start to understand how do we get the most out of it. And that's really what we dug into. So let's take a look at some of the things that we found. First and foremost, we ask about productivity. What is your perception of productivity since you started using AI? And you can see here that a large majority of our respondents reported at least a slight increase in productivity. So productivity goes up. That's awesome. There's this other interesting thing, though, when we think about AI. How much do you trust the quality of code that was generated by AI? You can answer that question on your own. You can look at our graphs here and see sort of what is that level of trust. Well, what you can see here is that there's a large proportion, a majority of our respondents report that they trust the code that's generated somewhat, all the way up to a great deal. But about 40% have little or no trust in the code that's generated by AI. And the reality is, if this was 100%, if everybody was up at that great deal, I would be worried for our profession. You should not trust AI 100%. On the other hand, if it was all down at not at all, I would be worried for our profession. You shouldn't trust AI 0%, right? There's some balance here where you should trust it. But you should probably trust it about as much as if you hired a new developer to your team and they gave you some code on day one. You wouldn't blatantly trust that code and ship it to production. You shouldn't do that with AI either. But this also is an indicator that we need good feedback mechanisms on the code that we're generating. Now, let's look a little bit deeper. And in this graph, I'm going to give you a little graph reading explanation here really quickly. I have three graphs like this. I'm only going to explain it once. So, on the y-axis, up and down, we have a bunch of different outcomes. On the x-axis, going across the bottom, we have numbers. That dashed line up the middle is zero. So, we ask this question. If you were to increase your AI adoption by 25%, what would the impact be on other things? And so, the things that are on the right-hand side of that dashed line, those are positive. And the things on the left-hand side, those are negative. So, what we see here is that flow, job satisfaction, and productivity are all increasing when we increase the amount of AI that we have. We see that time doing toilsome work stays about the same. Time, or sorry, burnout and time doing valuable work both go down. Now, just to be very clear, burnout going down, that's a good thing. That's what we want. We want to reduce burnout. Burnout goes down a little bit, maybe. Time doing valuable work also goes down. That's interesting. Maybe Jyoti and I will talk a little bit about that in our conversation. That's sort of the individual impacts. This graph, you can read it the same way, but we're looking out a little bit wider on some of the team or sort of process impacts that you see. And here's what we see. Documentation quality, code quality, code review speed, and approval speed are all improving. That's great. And documentation quality in particular, that's standing out at about 7.5% improvement as you increase your AI adoption. In our Dora research, we've looked at internal documentation quality over the last four years and really seen its powerful impacts. And I know what you're saying. I'm an engineer, not a tech writer. Sorry, your job is also documenting what you're doing. That is important and AI can help you. Now, right in the middle there is cross-functional coordination. It's basically not changed as we have AI. And then the last two, tech debt and code complexity. Those are both going down. And again, a drop in tech debt and a drop in code complexity, those are good things. That's what we want. We want less complex code. We want less technical debt. So, this is really cool. This is really optimistic. If we think about those capabilities and conditions that we need to drive software delivery performance, AI is showing us a lot of optimistic things. Documentation quality, that's one of our capabilities. It's improving. Code review speed, it's improving. All of these are good conditions. So, software delivery performance is going to get better. Right? Yes. Yes. Oh, except that's not what the data says. Sorry. So, this chart you read the same except, interestingly, everything is on the left-hand side of that dashed line, which is zero. And here's what we're seeing. Software delivery throughput goes down. Software delivery stability, those expletive moments, that stability goes down even further. Fascinating. Fascinating. Why is this? I'll tell you what our data says. Actually, it doesn't say anything. We don't know from our data. We only have hypotheses and theories. But I bet you know, or I bet you have some hypotheses and theories. And that's why Jody and I are going to talk as well. So, I think the big thing, really, to take away from this is to remember that if you want to improve software delivery performance, it's not just about shipping more software. It's about creating the right capabilities and conditions for your team. And I guess I'll leave you with this final thought. Like, finding the right thing to work on is a process of finding the constraint. What's holding you back from improving? And I'll ask, how many of you, the thing that's holding you back from improving your software delivery performance is the speed with which you write code? Oh, nobody's raising their hand. Yeah. That's not the constraint. All right. With that, those are some of the findings. I'll talk more about them and how you can learn more about them. But right now, I'd like to invite Jyoti Bansal to the stage to talk to us a little bit about Harness. Hey, thank you, Nathan. Thanks, Jyoti. Here you go. Wow. Fascinating. You know, the finding that we see here, like, you know, what the Dora research showed, AI everywhere. You know, it's real. It's here to stay. You know, it's top of mind. We are all doing it, you know, as developers. But we are not seeing the impact on the delivery performance. You know, the throughput is decreasing. The stability is decreasing. And when we look at, like, why is that happening? You know, let me talk about, like, you know, how we think of that as Harness. You know, Harness is a company based out in San Francisco. We build a next generation DevOps platform, and I'll talk about that. But a big part of what we see is, like, you know, you have the entire process. When you look at the capabilities of what does software engineering mean, you have the code. And code is coming out more and more. And more and more code should come out as we use AI. You know, maybe it's faster. But is that really the bottleneck? Like, you have an entire process that happens after that. Like, you build your code. You secure your code. You know, you have to deploy, harden, optimize. And the entire process goes from, you know, this list of capabilities that you see in the, you know, in the Dora research methodology as well. Unless we, unless we fix that process, you know, there is a bottleneck in that pipe. And now you have more things coming in the, in the, in the front of the pipe. What's going to happen? And that's really, you know, a big part of the hypothesis. You know, we'll, Nathan and I will talk about, you know, what, what is it? Like, you know, we're still, everyone is trying to understand, like, you know, why, you know, developers are coding faster. But we're shipping less software with lesser quality. So what, what happened? It's because the focus is really missing here. And, you know, what we are focused on, Harness, and a quick intro on Harness here, is to how can we bring AI to modernize most of these processes. We're about, you know, we have about a thousand customers, a lot of, you know, enterprises, a lot of engineering teams. So using Harness to kind of streamline each of those capabilities. You know, you see the capability chart in the Dora metrics on how do you do continuous delivery. And if your continuous delivery process is not automated, it doesn't matter, you know, how long, how fast you write code. You know, if you don't have your testing completely done inside your, you know, CI, CD pipelines, whether it's integration testing, load testing, security testing, you get, you're not going to speed up things in the end. And if your confidence is lower on the AI generated code, it will slow it down even further there. So how do you think of AI not just for coding, but how you think AI for, AI assistance for your next generation of the developer tool chain that happens after coding? Yeah, so when I introduce Harness, I look, I introduce Harnesses. We have the platform, the AI platform for everything that happens after coding. A lot of conversation happens for coding. You know, how do you do now, like, you know, more next generation continuous delivery pipelines and CI pipelines and, you know, feature flags, security testing, cost optimization of your infrastructure, you know, bringing, you know, incident response of the next level, you know, resiliency engineering, chaos engineering kind of practices. And can you modernize that one by one? And if you, unless you modernize one by one, you know, all of those different practices, it's really, really hard to keep up. So that's, you know, that's where Harness comes in. We help with, you know, many of these practices, you know, we have a lot of customers with, you know, where the engineering teams could be hundreds of engineers to like tens of thousands of engineers who are modernizing the deployment processes, the build processes, the resiliency testing processes. And unless you do that, you have a bottleneck coming in into the delivery velocity. You know, I'll give a few examples, you know, when we can bring, how we can bring AI to the DevOps and the software delivery tool chain, you know, reducing build times is an example. You know, so Harness is an AI model called test intelligence. And what that AI model does is like, say, if you, as an engineer, you make a hundred line code change, and you have a suite of 10,000 tests or 2,000 tests, you really run all 2,000 tests, or you can pick only 200 out, the relevant ones out of it. And can AI help you do that? So that's how test intelligence AI model does. We can cut down your testing time in CI by, you know, 70%, 80%. So the same build that takes 30 minutes can come down to 10 minutes or six minutes. Right? So that's where the developer productivity boost starts to come in. You know, we have another example is like, you know, fixing vulnerabilities automatically in the CI-CD process. Like, you don't want vulnerabilities to come in from your security team. You want them to be integrated in the lifecycle very quickly. You do the, the developer can do a PR right there and fix it. You know, you dedupe the vulnerabilities, you know, remove all the burden around that. So that, you know, that capability gets modernized. You know, another one around, you know, verifying if the deployment is going to break something. You know, and you can bring an AI model, Harness is an AI model called continuous verification. If you're pulling in data from your monitoring and observability systems, understanding what does normal look like and you're making a code change. You know, it's comparing, you know, whether it's a kinetic deployment fashion or a blue-green deployment fashion, what's the impact is going to be. So you can, you know, reduce the change failure rate right there. You're, you're stopping any change and you can automatically roll back when that happens. So that's, that's an example of where AI can come in and help into, in, in, in a scenario like that. You know, savings, cost savings. A big part of, you know, a burden for a lot of people as you run in the public. Cloud that as engineers, we are also responsible for cost now. Because the cloud cost is our responsibility at the now. And, you know, can you bring AI to optimize your systems to reduce the cloud cost? You know, to get the right configuration of your Kubernetes clusters and how you set up your node pools and, you know, how do you automate the, you know, the shutting down of resources that you don't need? How do you automate the scaling? There's a lot of potential of AI in solving problems that are beyond coding. And that's what, like, you know, harness has been bringing in a, you know, in a, in a, in a, in a, in a real world kind of manner. Some of them completely reliant on LLMs. Some of them do much go beyond LLMs or you have to use, you know, a lot of other models, you know, beyond LLMs to bring it together and solve, solve, solve these problems. And we see that across the board, you know, you know, I'm, you know, I'm sure like, you know, there are, there are folks here who, who, who, who might have used harness. But, you know, some examples, you know, companies like Dish Network or Siemens or United Airlines or JP Morgan Chase and Citibank and, you know, companies that are bringing, you know, harness automated AI models on the, on the DevOps tool chain. And to remove the bottlenecks on the, on the next part of the, of the process here. So, we'll, we'll talk more about it. You know, Nathan, I'm really excited to, to have this discussion on how do we interpret some of the findings, you know, from, from, from your research. Yeah, yeah, very good. So, I, I might start with a question for you though, Jyoti. This talk is about creating an, an amazing developer experience. Yeah. So, what is that? What is this amazing developer experience that we're here to talk about today? What does that look like? That's a great question. You know, we, you know, we talk to a lot of developers around this, you know, I'm, as a developer, I'm always very passionate about it. You know, to me, the amazing developer experience comes down to a few things. Number one is, don't waste time on anything that you don't enjoy doing as a developer. You know, and what do you enjoy doing as a developer? It's normally about problem solving. It's something creativity, you know, the creative part of software engineering. Anything that's non-creative part of software engineering is not fun. You know, it reduces your experience. You know, that's, that's the, like, whether it's like, you know, you are waiting for a compliance review or, you know, someone had, or you're troubleshooting a deployment failure or you're, you know, doing some manual stuff in there. A lot of the toil and the grunt work that happens, that really kills the developer experience. Of course, it kills the developer productivity as well. You know, if you see the data points in many, many organizations, if you ask people, like, your software engineering team, how much time you spend coding? You know, someone who has not done software engineering, they think 95% coding, what does a software engineer do? Like, it's all coding. But in practice, it's like, you know, only about 30, 40% time coding, the rest goes into all the stuff that happens. So, you know, you, if you remove the toil from it, this is, it's a good developer experience. Second is developers like satisfaction of their things, whatever they built, to be out there and solving some problems. Like, if it takes too long, you're dissatisfied, you're unhappy, like, you know, it's like, you know, it's like, okay, I built, I wrote this code, I haven't got any feedback, I don't know if anyone has been using it, how do I improve it? So you need that feedback loop of shipping and bringing the value to your consumers. You know, that's the second part, you know, if you make it faster, you know, you cut down the, you know, all of that loop around it, you know, that's good developer experience. And the third is, you know, third is all about, like, you know, how developers like to operate without interruptions. That's a good part of, like, you know, interruptions, you know, are no developers like it. You know, when I code, I'm not coding as much these days as I used to. You know, I put my headphones, you know, put some good music on and I go in my flow state, you know, mentally, and that's what you want to be. So if you have all kind of interruptions happening, firefights happening, all kind of things happening. And we have given our developers a lot of burden these days, like, you know, developers have to be responsible for operations, have to be responsible for costs, responsible for security, everything shift left means, you know. We've shifted all of it left onto the developer. So that also is a big impact on developer experience. So when I look at broad developer experiences, remove the interruptions, you know, give them the tool chain to, you know, to remove the toil. So all the toil stuff could be automated in the good kind of tools and AI is great to help with those. And create the faster loop. Faster loop is your job satisfaction that you are shipping something that people are using and, you know, how they're using and you're innovating around it. Yeah, I think that is so key. Obviously eliminating toil. Nobody likes toil. And it doesn't actually provide a whole lot of value or at least not lasting value, right? That's why we need to eliminate it. And I think automation is always a step for toil. But we shouldn't overlook the eliminate it first step, right? If you can stop doing that thing, just stop doing it before you try to automate it. I also really like and what really resonates with me is those feedback cycles. You're absolutely right. I need to know. Like I built something. I didn't build it for me. I built it for someone else. Are they using it? Are they using it? And when we have long delays between when I wrote it and when I shipped it, we're going to get right back into those interruptions. We shipped this thing six months later. It didn't quite work. Hey, Jyoti, can you fix this code you wrote six months ago? How's that for an interruption? Like that's not very good. Not very good. Definitely. And it's, you know, now when a lot of conversations happen always about how do you fix the developer experience, right? You know, and I always look at like it's, there is no magic one. Like people think, you know, AI is the magic and it could fix it and anything else. No, nothing is magic is, you have to look at like, you know, it's hundreds of small things. Yes. Like if you really want to improve, it's really hundreds of small things. And you can almost look at like, you know, if you go in our team and say how much time your developers spend on, you know, deployment related stuff, which is like, you know, a deployment is happening and you have to go and troubleshoot it when it fails. Or you have to babysit the deployment and just watch what they be on the call when the deployment is happening. And you start looking at it. You say, okay, 2% of the developer time. 2% sounds small. Yeah. It's 2% time. Like, you know, it's the, or someone said, okay, how much time you spend on, you know, on, because your builds fail too often. And then you start adding up, that's like another 2% of the time. Okay, how much time you spent on approvals, that your approvals are not happening on time. It's like, maybe if you add it up, like, you know, I'm wasting like 2% or 3% of my time, like waiting up on that. You start adding those up, like all of these things add up. And once you start adding them up, like, you know, 30% time is wasted, 40% time is wasted, created a lot of the toil and the burden. So you have to start looking at, okay, how do I take that one thing, you know, bring, maybe I can eliminate it or maybe I can automate it. You know, or, you know, how do you just bring, squash one problem down, another down, another down. And like, you know, you could be much more productive right there. Yeah. Yeah. I also like that because, you know, we want to improve the developer experience. We can't use AI or even snap our fingers and improve developer experience overnight. It's a process, right? And we have to go find what's one thing that we can eliminate today or some savings in that developer experience, toil that we can eliminate. And then I think reinvesting that time back in that development team, I think is really important. You know, same thing applies to, of course, there's a lot of conversation on AI for coding and that is extremely important and we have to do that. How, you know, one of the things, you know, the talk about, like, when I go is like, okay, how much more productive my developers are? You know, how are you hearing about that and, you know, when you look at Dora research, how does that play into, like, you know, is it, you know, how much more code is coming out? Yeah. Yeah. I think this is a really interesting question. Like, how much more productive will our developers be with AI? And, of course, there have been industry studies that answer that question for us. If we give a developer a task and they use AI, they can complete that task 30% faster. Therefore, we're going to get 30% improvements by bringing in AI. But that ignores all of the other things that we've been talking about. It's a little task. Yes, I get that task completed 30% faster, but that's not a real task and it's not something necessarily that I'm even going to ship to production. I think the other thing that's really interesting about this developer productivity question, and I have this conversation a lot. People ask me, Nathan, how much is AI going to improve my developer productivity? And my response is usually along the lines of, how are you measuring developer productivity today? And that's the answer I get. Like, nothing. There is no answer. And so we can't answer that question of how you're going to measure developer productivity. If you don't even have measures of developer productivity, like you haven't even thought about that yet in your environment, I can't tell you how much better AI is going to make it. So let's start with that conversation. And I also think, like you mentioned, the 100 little paper cuts that are really just driving that developer experience in a negative fashion. I also, in my interactions with teams, I've yet to find a team where I spoke to the team, not to the leader, but to the team that's doing the work and said, tell me if you could improve one thing, what would it be to make your developer experience better? Every single team has an answer, knows the answer. So it's not about knowing what to do. They know what to do. They're not getting or investing that time. They're maybe not incentivized to improve that developer experience. So I think it is a real challenge there. Yeah, you know, sometimes you have to take a step back and fix those things. But that is a challenge. Like, you know, we have the, like, I find it so strange that about 80% of the large organizations with a lot, which where a lot of software engineers work, still use things like Jenkins. Like most of the CICD pipelines, for example, are built on Jenkins. It's a 25-year-old technology which not a single developer is happy with. But it's just hard to change and people don't change. And that's what we're driving. Okay, why are you stuck with a 25-year-old technology? That's really sucking all the energy out of your engineers and they're struggling with it. And I think it's time to, you know, right now we're in this, it's a good opportunity that everyone, even at the board level, people are talking about, what's your AI strategy? How do I get developer experience better? How do I get developer productivity better? No one at the CEO level in an organization and the board level cared about developer experience, developer productivity. That is a big conversation, which I feel is a great opportunity for the engineering community to use that. Yes. And like, okay, now is the time to take a step back and fix a whole bunch of these things. Absolutely. And, you know, that's how we improve. Yeah, and I think that, you know, a lot of what we find in Dora also is that culture drives so much of these conversations, the culture of your organization. And there is this truism that it's, there's an old saying, it's old enough that no one knows who said it originally, so you can just cite it as Nathan told you this. It is easier to think, sorry, it is easier to work your way into a new way of thinking than it is to think your way into a new way of working. Right? And so how do you change culture? You change the work that we do, the tools that we have. So actually, I think AI coming in is a real opportunity for us to invest in the culture of our organizations. Right? And AI, to be very clear, AI is not the thing that's going to fix your culture. But what it is going to do is give you opportunities to accelerate certain things. The elimination of toil, the improvement of the developer experience, and to have these conversations, rational conversations about how do we drive productivity, how do we improve that. And we've talked about developer productivity and developer happiness. I want to talk about another developer thing that I think is super important. You know, before we do that, let me mention this culture thing. Yes. It's always been to me, you know, let's say, I always have a controversial take on it. Like, you know, a lot of times I go to people and people say, it's about culture, it's, you know, that's the hard part. So the culture, it's not about the tools, it's the culture. And I tell them, it's actually, you're completely wrong. Unless you fix the tools, you cannot change the culture. No one is going to change. You tell your team, it's okay, we got to automate everything. And you go from your culture of shipping every two weeks to shipping every day. Yeah. And unless you have the tools, how do you expect them to change the culture around it? Exactly. And, you know, people got too wrong into that for too long. Like, you know, it's like every conference, every event, you'll go and people talk about, you got to change the culture first. It's like, really, you got to change the process and the tools first, and then use that as an example to change the culture across the board. Totally. One of my great, my favorite sort of anecdotes around that is, maybe some of you in the room also remember how we used to collaborate on documents. Right? We would write up a document and I wanted your feedback on it. I would attach it to an email and send it to you. And I'd send it to Mavian and I'd send it to Gleb and then all of you would give me results back. And I'd end up with a document that was like, strategic plan, June 2025, underscore final, underscore final V2, underscore, right? Like, that was, and then what did we do? We changed the tools. You have something like Google Docs, where now I write a strategic plan and I share that doc with everyone and you can, we can all collaboratively edit that, comment on it. That's changing the tools to change the collaboration and the culture and the visibility that we have across those work tasks within our jobs. And so, to your point, yes, culture is super important, but culture and tools are inextricably linked, right? Your tools drive and amplify your culture, which then drives and amplifies how you use the tools and what they do. So, you can't change them independently. The only way you can change the culture is to make it easy to do that hard thing. Yes. Like, the hard thing is to change and you have to make it easy. You have to make it easy. Your tool is going to make it easy, the new process, the new automation, the new, you know, some automated migration, all kinds of things. Without it, no one is going to change and they will be like, okay, we're not stuck with this for the next 10 years. And now you have bad stuff and nothing is going to happen to the culture. Yeah. Okay. Well, while we're speaking of culture and those developer things, right, we talked about happiness and productivity. The third one I want to talk about is developer safety, developer safety, right? As a developer or as an operator, if I'm about to push that deploy button, how like confident am I that when I push deploy things are going to go well and I'm not going to get fired, right? And speaking of culture, you shouldn't get fired if things go bad. But I think this also comes back to this idea of compliance and security because this is always a concern. And yes, we're going to shift it left, which great means it's my job now. That's not what I want. So how do you think about compliance and security in a space where we're moving fast? And compliance and security often introduces toil and many compliance and security people will say yes because we want you to think very critically before you ship something. Compliance and security, the only way, you know, that's scalable is to automate that. And, you know, if you, if you, a lot of times that traditionally the compliance and security people will say we have to slow down the developers, they have to do this checks and all. You know, we have, we have, we have a large bank as a customer recently. You know, they had a 200 page, you know, manual for these are all the compliance things you have to do before we ship code in production. And these are like, you know, it's like one of the top three banks in the world, right? So they have to do all of those stuff. Yes. But they were, the compliance team was very frustrated, like, and engineers don't do all the time and we are always chasing them and we are always kind of doing. Engineers are frustrated, like, you know, it takes us like, you know, eight weeks to ship code. Like, you know, once the code is committed, the lead time is eight weeks because you have to go through all these compliance and approvals and all of this stuff. And at some, like, you know, and you have to bring the tool to do the change in culture, right? So they brought in a tool like Harness Automated Pipelines where, you know, they took all, every single compliance and control and automated that in the pipeline. Yes. Including an exception workflow, like what happens when you have to get an exception, what happens you have to get the right approval, you have to get approval in ServiceNow or Jira or whatever it is. Just fully automate that. So now what happens is both sides are very, very happy. The compliance teams are happy, it's like, okay, I don't have to chase a developer at all, ever. You know, it's like everything is in there. All my, if I have to introduce a new compliance check, I can just add it to the automated pipeline and that's everyone will do it. You know, developers are happy that their code is going to production in eight minutes now. Like that same bank that was taking eight weeks or like, you know, eight, nine weeks, that's like eight, if everything works, like most of the times it should not go in production. Some tests will fail, something will fail, et cetera. It should fail. But if everything passes, eight minutes it can go in production. So that's, automation is the key to it. Like, you know, that friction constantly on compliance, security and developer, what developers are building is the biggest, one of the biggest pain in so many teams. And you don't want developer to be scared of that. Like, you know, you want like, okay, everything is in there. I'm going to push my code. You know, everything, if something is not secure, something is not compliant, you know, something should fail. And people will go and fix it. And, you know, you're not waiting two weeks to get the feedback. You're getting like feedback in two minutes. Like, you know, this thing is not passing this kind of thing. I need to fix it. A developer will still be in their flow state of they just recorded that. They know what to do. They will fix it. It will probably take them five minutes, ten minutes to fix it. Right. And that, again, is that faster feedback and we're implementing that better. And I've worked in those environments where you have the 200 page binder of all of the compliance policies. And I'll tell you what, I put that binder on the computer and it didn't change anything. But once we've automated that and written it as code, now I can put that code on the computer and I know that it's compliant. Yeah. And people like the swim lanes. Like, you know, we all, everyone likes, okay, define the rules of the game, automate the swim lanes. And as long as the swim lanes, it's all good. You know, if you violate the swim lane, you need approval from someone, that you automate that workflow of how you do it. And like, you know, who gets involved, how do you do that? And then it's like everything runs smooth. Awesome. Jyoti, we're very close on time, so I'm going to leave you with two questions and then I'll answer them as well. So the first question is, what is the future of software delivery and software development with AI? What does that look like? And then the second thing is, if you could only do one thing to improve developer experience for a team, what would that thing be? Sure. Okay, I'll start with the first one. I do think like, you know, our jobs as software engineers are going to change. That's for, the future is not going to be the same as what the present is. You know, but I don't believe there's a whole debate of like, you know, does the software engineering profession go away? Would AI do everything? I am in the camp of, no, it's not going to happen. Like, you know, I've seen the transition, like, you know, personally from like, you know, I used to be a C programmer. You know, as a C programmer, you do all kinds of things you have to do from like, you know, memory management, you know, you know, all the stuff, garbage collection, all kinds of stuff we'll do, right? And then, you know, I became a Java programmer at some point. And then you will think, oh, you know, 80% of the stuff that I used to do is automatically taken care of in Java. So we don't probably need as many programmers, you know, in the world, we can just do more. That's what happened, right? And look at what happened, like, you know, we had 10x more programmers in the world, you know, and probably since that happened, there will be more. Yes, you still need the C programmers to do some stuff, but you have a lot more productive Java programmers who did a whole bunch of things, much more. You know, AI is going to probably most likely create that, like, you know, we'll have the class of, you know, well-trained software engineers, well-programmers who have to do a lot of things. And then you have the class of people who don't need to go through proper software engineering kind of training there, and they still can program and do things. And you wouldn't, but the amount of software and the amount of work and the pace of innovation will probably just increase. Like, you know, things that, you know, people are like, you know, I can't bring this medical device to the market in two years, and that every new device comes every two years. Maybe it will come in every two months. You know, as humankind, we can, our appetite for innovation is pretty much infinite. Yes. We can see more innovation, more velocity. So that's going to happen. But I would say the software engineers will get much more productive on the, when it comes to the coding side of things. You will have agents help you with all kinds of things. I do worry about what we talked about. Like, you know, if we don't solve this, like, you know, the people still think software engineering is all about coding. Like, coding is, you know, yes, we can automate the coding, but you have all the other stuff that you've got to bring a lot of, you know, I really hope, like, you know, the future of software engineering is engineers are mostly focused on problem solving, creative, creating good solutions, designing great things. You know, AI is helping them with coding. But then you have, like, intelligent automation and a lot of things that can really take care of all kinds of things after that. Like, you know, it's the, from there. So one thing to improve developer experience. One thing, you know, I would, I call it intelligent automation. Off the toil. Like, you know, to me, though, if there's one thing, it's like, do automation of all, and use intelligence and AI to do automation. But toil reduction is really the single most important thing that improves developer experience in my mind. All right. What would be your answer to that? So the one thing to improve developer experience, I think, is to have that conversation to identify what toil are we going to attack next? And let's eliminate that. What friction point are we going to address? I think that's really important. And I think the future of software development with AI, I think I stated the case early in the presentation, AI is here to stay. And I think that we, as humans, are adaptable and resilient. And we are going to learn how to work with these new agents with this new capability that is an amplifier and an accelerant. With that, we are just about out of time. I want to thank you all for coming and encourage you to continue your learning journey. Harness is here for the rest of the conference. We've got a happy hour that's coming up immediately, in a little bit. I think it's in an hour or so following this. So hopefully you'll be able to join us there. You can visit Harness. There are a lot of events going on, but that's the most fun happy hour if you want to go. There we go. There you go. And then tomorrow, also down in the expo, Dora has a community booth. So you can join that there. And tomorrow I'm also co-presenting with another colleague on measuring the impact of code assistance with Gemini Code Assist, where we're going to dig into, again, some of the Dora findings, but also some of the tools that Gemini Code Assist ships with to allow you to measure and monitor how that's being used within your software development lifecycle. And then two other things before you leave. First, you cannot do this alone. You cannot improve your developer experience alone. If some of what you heard today resonates with you, I encourage you to join the Dora community. This is thousands of people around the world trying to put this research and these findings into practice. We're learning from each other every single day, sharing insights, pointing out pitfalls to hopefully help you avoid those things. And then finally, thank you again for coming. Please do take a moment to open up your app. Leave us a little bit of feedback. We really appreciate it. Jyoti, it's been really great. Thank you so much. Thank you. Thank you, everyone. Thank you, everyone. Thank you.