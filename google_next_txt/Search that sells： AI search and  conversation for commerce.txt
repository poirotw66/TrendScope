 Hello, everyone. I'm Shrey Gulati, and I'm a product manager for Vertex Search for Commerce. And I'm very excited to be here to talk to you about our session, Search That Sells, which is AI-powered search and conversation for commerce. Within our suite of products, we bring the best of Google AI to drive personalized product discovery for your applications and your website, especially fine-tuned for e-commerce. We offer Search and Browse, which brings Google quality search to your shoppers to help them on their product discovery journey. We also offer recommendations, which is personalization to find the next product for your shopper, no matter where they are on their shopping journey. We also bring Vision, which is multimodal search. And we're bringing a whole bunch of improvements to that, which I'm going to talk about today. And last but not least is Conversational, which is our new launch at Cloud Next 25. Vertex Search for Commerce has a wide range of products in its suite, all of them bringing the best of Google AI to drive personalized product discovery on your website or application for your shoppers. We bring Search and Browse, which is Google quality search, fine-tuned for e-commerce, helping your shoppers on their product discovery journey. We also have recommendations, which is personalization to find the next best product anywhere across shopping journeys. We have Vision, which is multimodal search. And I'm going to be talking about a whole bunch of improvements we're making to it over the year. But last and not the least, we're announcing the launch of Conversational Commerce at Cloud Next 25 in preview mode. And I'm very excited to talk a lot more about it in detail. Before we jump into what we built, I want to talk about why we went down this journey. We saw a huge change in how users are now interacting with Google shopping and how they're finding new products or how they're searching in their queries. We saw customers are now searching for more inspirational or more need-based queries instead of just product queries like they used to in the past. They're asking questions like, what should I wear to a wedding instead of just searching for a dress? They're asking questions like, how do I build a roof deck instead of just searching for products? Not just that, they're mixing multiple intents into one query. For example, instead of just saying electric bikes, they're saying electric bikes with the mileage and with the commute. And all of those intents are being smooshed into one query. And they expect us to not just understand, but also show them the best personalized products for these complex queries. For this, we launched Vertex Search Conversational Commerce. And we'd have to talk about this in the way of looking across different queries and how all of them come together to power these suite of products. How we've stacked them up is on a level of complexity and ambiguity. So at the very bottom, you see is where a user knows exactly what they want. For example, Taylor Swift Golden Globe dress. This is a specific product. This was already offered with Vertex Search. We know we're the industry leaders. And we already do a great job when a user knows what they want to bring that best product for them. We use Google Shopping as a relevant stack. And we use a whole bunch of our models to enhance Search AI to understand what a user means when they say Taylor Swift Golden Globe dress and show the most relevant products. And for this, it's why we launched conversational commerce. The best way to think about this is to look at different queries, scale across complexity and ambiguity. At the very bottom, you see a user knows exactly what they want. Taylor Swift Golden Globe dress. It's a specific dress they had in mind. And we use our relevance from Google Shopping. And we bring Vertex Search models on top of it to best understand users' intent and map it to the best product in your catalog for that. We already knew we're the industry leaders for that. But we wanted to go beyond and handle even more complex queries. So next are queries where the user kind of knows what they want. And they are product queries, but they have an overwhelming amount of results that they see. For example, dress, snacks, and mattresses. When users search for these on your website, they'll probably get thousands of results. So how do we help them navigate from these thousands of overwhelming results to a few that they can manage? And how do we guide them through that journey is why we launched conversational product filtering. And that's what you see in action here. When a user searches for a query like coffee table and they get 3,500 results for it, the AI decides what's the best question to ask them. And a series of questions to guide them from these 3,500 to under 15 to 20 results in, let's say, 3 to 4 turns. And so this one-sided conversation is what we launched with Bed Bath & Beyond in Q4. We experimented and they saw a 5% revenue per visit jump over and above what access search. And this was at 100x ROI. This is now GA and is already being implemented by lots of other retailers. So we're going to go to the next slide. Next, we wanted to go even further and address some of those complex queries I talked about earlier, like what wine pairs were with fish. Or like I mentioned, what do I wear to a wedding? And these in their very nature aren't really product queries. Here the customer has a need or has shown some vague intent but hasn't really mentioned a product. So how do we solve for those? That is conversational commerce, which is now available in preview. I'm going to show it to you live in action and then talk a bit more about what we did there. So here you see a user starting their journey on search like they do, but they enter a query like, I need a dress for a summer party. Again, not really a product query but an intent. Our models detect what the user wanted and they first of all ask a follow-up question to narrow that intent. They show a personalized list of products. But then more importantly, they understand and rephrase that query to best match what a garden party might mean on your catalog and to show those products to them. So let's look at it live in action. What we'll see is a user starting their journey as they do on a search bar. And they search for something like, I need a dress for a summer party. Now this is where our agent understands that it is a conversational query. So it pops up with a conversational bar because it is a complex ambiguous query. What it does is it asks a follow-up question to narrow down the intent. But it also shows a set of personalized results for the first query that was made. Now we're not just searching for a summer party dress or a garden party dress. You don't need to have that in your catalog. What the LLM is doing is it's rephrasing the query and matching it to your catalog. We also allow users now to upload an image and find the closest product in your catalog for that image search. We'll allow summarization. We'll allow reviews. All of those helping your users decide to convert with the best information from your catalog. At this stage, when it's checking out, the agent decides it doesn't need to talk so it goes in the backend. But as soon as the product was added to cart, it comes up with a set of the best personalized recommendations. Throughout this journey, we've helped them through product discovery. But now let's look at some maybe ancillary shopping journeys, which are not exactly what we usually think about when we see search. Now the user found a product they liked, but it wasn't shipping in time. We know who they are. We know where they are, their sizes, their location. So the personalization part of this agent will say, all right, if it's not shipping to you in time, this is the closest store where you can go pick up. Coming soon now, what we're working on next on the roadmap is how do we help them on this journey in store too. This is where the agent can now connect to your own agents, for example, an in-store locator and say, find the product in this aisle. We'll also be bringing AR for the users to be able to interact live in store and ask questions about the product just through their lens. And in this case, they can further ask any other question or talk to the in-store agents or just have a fully seamless digital to in-store experience all through the conversational agent. So let's quickly recap what we saw. First of all, conversational commerce, as we build it, our core principle has been to be very ROI focused. We know ROI is an important, the most important part for you as a commerce partner. So we decide through intent detection when to chat versus not to chat to avoid including extra LLM cost or adding a friction for a customer when they don't need to. Second, we're handling way more complex intents like you saw, which might not even be mapped or as metadata or tags to your catalog. And throughout the journey, the agent will retain context on what is it that you were shopping for. We're always product first. We know your customers are here to shop, not just to chat. So for every query, we will show the best set of product personalized results across all products. We especially built this for commerce, not just another chat bot which you see across retail. What that means is it's not just when to talk, but also what to talk. We have crafted these conversations in partnership with a lot of retailers to be very focused, not transactional, but still focus on what the user needs to transact instead of walls of text which they would otherwise get through LLMs. And lastly, what's very, very important to us is AI safety and responsibility because we know this will be your users interacting on your websites with this agent. So we've built a whole steel of multiple layers of safety, but we also provide you the control to restrict anything which might be against your policy. So that's what we launched in preview, and you can now go sign up and try it out on your catalog post-next 25. And I want to quickly walk you through what is coming very soon over the next couple of quarters on our roadmap. We'll be taking the next step or the next evolution from multimodal. We will not just be able to search for an image, but you'll be able to talk about it. You'll be able to ask questions where the model reasons and understands your queries. So in this case, it's not just searching for your address, but saying if this is what I'm wearing, what should my partner wear? And the multimodal agent will be able to understand, reason, and show you the best set of results for your query. Taking it a step further, we'll be bringing virtual try-ons available to you out of the box. The users can upload images or in some cases if you prefer avatars, and they can modify those body types. They can try out different clothes. All of this experience which they would have done in store in a fitting room available at their homes. The applications for this tech go beyond fashion. It's about modifying an image and interacting with it, and we see use cases across industries, and we're already piloting some of those with our partners. Going after multimodal, we'll also be launching a shopping research agent very soon, which is for those high consideration queries where the user is still deciding what they would buy. So for, let's say, a stroller, or in this case, we're looking at an example for a noise-canceling headphone, where they can first of all, of course, make their query very complex, like noise-canceling, $300, battery life, wear them at work, comfort level. But not just that, the research agent will go across websites which will be grounded in Google search and bring that information in a consumable format to your shopper for all the parameters that matter to them, and not just that, compare these products in depth side by side. All of this to help your users decide the best fit product for them in your catalog. And we're not just bringing our own agents. The conversational commerce agent was our first agent that we launched, but we're now also allowing you to connect multiple agents to offer a seamless journey for your users. We will start with an orchestration or intent detection agent, which will understand what the user wanted, and call on the relevant agent, for example, the shopping or the customer support agent, or any other which might be applicable. And it's not just Google 1P agents. Very soon, you'll be able to bring your own or third-party agents and connect all of them to give that one seamless experience. So thank you for joining us at Cloud Next 25. We're excited about the launches and the future to come. Hi everybody! We have time for wild sorrisdle inside Los Angeles Can.