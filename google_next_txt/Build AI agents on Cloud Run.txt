 Welcome, welcome, welcome to Build AI Agents on Cloud Run. I am Steren. I am the Group Product Manager for Cloud Run at Google Cloud. Today we'll be joined by Harrison, the co-founder and CEO of LangChain, Vidze, Senior Developer Relations Engineer at Google Cloud, and Harjo, CEO of CodeRabbit. Today we'll first talk about AI Agents. What are they? Then we'll look at how Cloud Run can be used to build your AI Agents. Vidze will go through a demo of actually building an AI Agent. We'll then talk about hosting tools that AI Agents can use on Cloud Run. And finally, we'll hear about the story from CodeRabbit. Let me give it to Harrison for AI Agents. Awesome. Thank you. So, if we're talking about Agents on Cloud Run, let's start by talking about Agents. And let's start first by talking about LLMs. LLMs are great. You can pass in some text or some other information as well, and you get back some text. That's fantastic. We think LLMs are awesome. We think they're even more awesome when they're combined with other sources of data and computation. And so one of the ways that people have been doing this for a while now is through things like retrieval augmented generation, which we call basically a chain. And this is where the name LangChain comes from, Language Chain. And so you do a series of predetermined steps. A few years ago, LLMs started supporting this thing called tool calling. This is where it doesn't actually call the tool, but it generates a payload for the tool, which you can then call. This gave rise to another paradigm of building applications, which most people call Agents. And when people say Agents, they generally think of this. They think of running an LLM in a loop. It generates the payload of a tool to call. You then go and call that tool, and then you go back to the LLM, and you ask it to continue until it decides that it's done. So this is really simple. This is really general. It's easy to get started with. But these types of more simplistic agents can often suffer from reliability issues. A lot of these issues have to do with a failure to communicate properly to the LLM what you want it to do. This can be in the form of missing instructions or vague instructions or missing tools or bad tool descriptions or bad returns from the tools. And so there's a variety of issues. But the long and short of it is that this simplistic kind of like tool looping thing, while it's general and can be really cool, it often suffers from reliability issues. And so we were thinking of ways to bridge that gap, to bridge the gap between the reliability of chains, which is great, but they're not flexible, with the flexibility of these tool calling kind of like Agents. The intuition that we came up with as we were talking to a bunch of developers who were building and putting into production Agents systems was that part of the control flow of the application is predetermined, set, in code. So part of it is a workflow, is a chain. And part of it has this LLM logic in it that decides the control flow. And so it has a combination of both of these things. And you can represent this, or you can think of this, either visually or conceptually, as a sort of graph, where you have different nodes which do different operations. And some of these operations are code, and some of these operations are LLM calls. And they're connected through kind of like logic, which can be represented as edges. And so that gave rise to the adjunct framework that we launched about a year and a half ago called Lengraph, which we think bridges the gap between these reliable workflows and these more interesting and flexible agents and produces a pretty flexible but reliable orchestration framework for building these types of applications. And so we launched this about a year and a half ago, and we've seen a lot of pretty compelling success so far. So what exactly is Lengraph? Lengraph is a low-level orchestration framework to build agentic types of applications. There's a few different focuses of Lengraph, and I want to call those out and then go through them in detail. First is controllability. So you can do anything you want in Lengraph. There's no hidden prompts. There's no hidden cognitive architectures. Second is streaming. We want to show what happens at each step throughout the process. This is really important for UXs of applications. Third is persistence. This can enable memory as well as fault tolerance, and this is really important for longer-running agent applications. And then fourth is human-in-the-loop interaction patterns. So we see human-in-the-loop being an important way to keep the agent reliable and on track. So talking about a few of them, let's start with controllability. The idea of Lengraph is that you express your agent as a graph. You have nodes. You have edges. You can branch out. You can come back in. You can conditionally go from one node to the next. All of this can be represented as a graph. Some of these nodes can be LLMs that can produce this type of agentic logic. Some of them can just be code. The way that these nodes work is they all operate on a shared state. So when you define your graph, you define the state of an agent. So a really simple state would just be a list of messages. This would be like a chatbot. A chatbot's just represented by a list of messages. And each node, as it goes along, appends to that list of messages. And so it could append an AI message with a tool call. And then you could go to another node, which executes that tool. That appends a tool call result. Then you go back to the AI. It appends an AI message. And then you go to the end. It's updating the shared state. These nodes can contain arbitrary logic. They can be anything you want. We have Python and TypeScript SDKs. So they can just be Python and TypeScript code. Or they can be LLM calls. Or they can be a call to a vector database. They can be anything you want. They can also be subgraphs. So one of the really cool things about LLM graph is you can create these graphs and then nest them hierarchically to create these more complex applications. So this is a pretty common pattern that we see. You create a graph that does one thing, and you use it as a subgraph in a larger graph, and you keep on doing that. This enables a lot of multi-agent patterns. There's a bunch of different ones that we've seen kind of, like, come up over the past year. The most common one that is actually in production is actually the custom one. And the reason that is is because one of the most important things about multi-agent applications is the communication pattern. How do they communicate? What information is passing from one agent to the next agent? What's the order that these agents are getting called? And while things like network or supervisor or swarm can be really easy to get started with off the shelf, when it comes to putting reliable agents in production, you want to have really, really good control over how these agents are communicating. And so we see people often building towards more custom types of architectures. A second thing that we really emphasize with LandGraph is streaming. So you can stream a variety of things. You can stream back tokens from LLMs. You can stream back progress as you go throughout the app. Or you can stream custom things from inside a node. Why is this important? The most interesting area for building with agents right now, I think, is in the UI UX layer. And a really important part of that is showing the user what's happening as the agent is running. This is really important because if you don't do that, the user might not think that anything's going on, especially for these long-running agents. And so being able to show, hey, I'm doing a planning step. These are the documents I retrieved. Now I'm starting to write your answer. This is the first token. Second one. We see that being really important for delivering delightful user experiences. Persistence is a core concept of LandGraph, and I think this is one of the big differentiators. So I mentioned this state. The way that this persistence works is we write this state as a checkpoint to a database after we invoke each node. What that enables is this type of fault tolerance. So if you're running this agent and one node executes, you write it to the database. You go to the next node. You read from that database. That node errors for whatever reason. Maybe the connection to the LLM goes down. Maybe there's a connectivity issue to a tool. Whatever happens, there's some error. You can resume from that point in time by just picking up from that persistence. This persistence also enables memory. One of the types of memories that enables is short-term memory. This is just basically adding to this persistence layer, adding to this, we call it a checkpointer. You can append messages. This can be AI messages and tool messages, and then it can be a human message when the conversation comes in. And then you can just continue in that thread, and that enables short-term memory. Not just for messages, by the way. If your state contains other information like documents or some transformed notion of the conversation like a summary, you can just persist that automatically out of the box, get production-ready short-term memory. We also have support for long-term memory. So the way that the checkpointer works is it's for a given thread or session in a conversation, and you keep on building the top of that. Long-term memory works across threads. So you define what we call a store, and then you can store things there that you can update from one thread and then access in another thread. And so this performs or this allows for long-term memory between threads. And then the last thing I want to call out is human-in-the-loop. I think human-in-the-loop is one of the most undervalued things in agent applications. Agents aren't amazingly reliable right now, and so as you try to have them do longer and longer time horizon tasks, you will probably want points in that process where you check their work or where you give them feedback or where you let them ask you for help for a clarifying question. And so in order to do this, you need a way to basically pause the agent, wait indefinitely for the user's feedback, approval, edits. And then when that happens, it could be a minute later, an hour later, a day later. When that happens, you want to go on from there, pick up from that state, and continue with the exact same state you were before. And that's all powered by this persistence layer. Remember, I said we write these checkpoints to the database. The human can edit those checkpoints as well in the database, and that's what enables this human-in-the-loop ability. So those are the core pillars of Langraph, and with that, I want to hand it off to Saren to talk about Cloud Run for AI agents. Thanks, Harrison. Thanks a lot. So great to hear about AI agents from the person who actually created Langraph. All right, now let's take a look at a typical architecture for AI agents. These are the high-level building blocks. We'll go into more details later. But first, there is often a user who needs to interact with that agent via multiple request responses. We've mentioned human-in-the-loop, and more importantly, it's very important for that agent to be able to stream back responses to the user, as Harrison mentioned. But then the agent itself is composed of a serving and orchestration piece which will leverage a few other pieces. The first one is obviously a GNAI model to add the reasoning capability to the agent. The second one, as Harrison mentioned, is short-term and long-term memory. Then the orchestration piece might need to retrieve data or just context from databases. And finally, the orchestrator will need to leverage tools to achieve its tasks. So what are the requirements for an AI agent's runtime? Well, it has to scale. It has to be able to handle multiple concurrent users using the agent at the same time. It has to be cost-efficient. It has to have good latency performance and high reliability. It has to have a great developer experience for the developer who is actually building the agent. Of course, it has to integrate with LLM models and be able to call tools and reach out to databases. In my opinion, a runtime should be flexible to allow you to use your favorite language or your favorite AI framework. And finally, very important, it has to support streaming out of the box so that responses can be streamed as fast as possible to the end user. That matters a lot for a great user experience. And guess what? Cloud Run checks all of those boxes. Cloud Run is a serverless runtime to run your AI agent on top of Google's scalable infrastructure. It scales via automatic, on-demand, rapid scaling of Cloud Run instances. It is very cost-efficient, literally pay-per-use. You only pay when the agent code is running. It has a pay-per-use pricing model, and notably, it has no flat fee, and when you don't use it, it scales down to zero. You just don't pay for it. There's no pre-provisioning of any capacity to do. It is fully managed. Google really takes care of a lot for you. Notably, in case you didn't know, Cloud Run has built-in zonal redundancy, and it is enterprise-ready in terms of security, networking, and compliance. Developers love Cloud Run. It has top satisfaction numbers and usability scores of Google Cloud. And agent builders can go from deploying their agent sources to getting a URL in under a minute in one command. So Cloud Run services have this nice property that they have built-in credentials enabling you to call Gemini API and other APIs without any API key or having to worry about credentials. You can run any AI agent framework in any language, Python, JavaScript. And finally, Cloud Run is the only serverless runtime which supports out-of-the-box HTTPS endpoint with streaming, HTTP chunk transfer encoding, web sockets, or even HTTP2. So now, let's take a look again at this architecture diagram, but let's see how you can do it yourself on Google Cloud by using Cloud Run. So first, a Cloud Run service takes care of serving and orchestration. It runs the AI agent framework like LandGraph or the newly released agent development kit, ADK. It will call a Gen.AI model, so the Gemini API or Vertex AI endpoints or even your own fine-tuned open model that you serve itself in a different Cloud Run service with GPUs. In terms of memory, you can use Firestore, which in my opinion is a great serverless NoSQL database and you have also Memorystore Redis for maybe more short-term memory. If you want to do a retrieval augmented generation or just fetch some data, Google Cloud offers Cloud SQL for PostgreSQL or AlloyDB for PostgreSQL, which both provide the PG vector plugin to do vector search. And finally, tools. We will go back to these a bit later. Now, let's actually see in action from Vidze how to actually build an AI agent on Cloud Run. So, my name is Vidze. I'm an engineer in the Cloud Run team and I'm trying to remember my password. Yes, it worked. Okay, so, when I was a kid, I really liked playing SimCity 2000. Right? More fans in the audience. That's great. So, today, I like to pretend that I am a support agent for my online store. So, we can go to the demo and see that store. So, here it is. This is my bridge for navigating all my customer interactions. I can manage them all from my central dashboard. And I made it very real, right? So, there is a lot of different systems I can interact with. There's orders. There is all my customers. You might recognize a few of them. There's payments and shipments, invoices. And I wanted to make this really realistic, right? So, all those systems are not linked. If I want to make links, I have to copy and paste IDs. It's just like the real world. I also have a long list of my standard operating procedures that I have to follow to correctly handle my support cases. So, I'm going to take you on a tour and we're going to solve a few of those support cases together. So, let's take a look at this first one. There's a customer who has trouble finding the room category. Seems to be having trouble navigating the side. Let's look at the... Who would this be? Ah, okay. Got it. Yeah. Well, let's type a response. Or maybe I'll fire up my reply agent, which will help me draft the response. So, the agent starts and Gemini says, okay, we'll review the case and draft the reply. So, it uses a tool. We'll get the conversation history. Right? It's right there. And the link to entities because, well, the customer was linked to this case. There, he knows the name of the customer. It will help Gemini come up with a good response. We'll also retrieve a few notes that other agents might have made on this case. And then, we'll draft the reply. So, let's go back and see. Filter by Ramo. This looks good. But I know that we don't have a good filter there. So, then search. Right? This is human in the loop. Then search for, oh, may your search. Typing is very hard. If 600 people are watching you. Search for Ram. Good. Okay, this is good enough. Reply. And I will close this case. There you go. So, another request for dark mode. I'm just gonna quickly draft a reply. So, this is Edward. Edward Scissorhands. He wants dark mode. You're all like, this is a corporate event. He's not supposed to be funny. Right? Like, it's okay. Reply that. Close the gaze out. There we go. Excellent service. Okay, now let's start linking some entities. Right? So, here's someone who's very happy with this recent order. So, we'll go copy this customer ID and then go to the orders. Search for it. Find the order. View the order. Okay. Phaser replica. Communicator replica. Copy the order ID. Go back to my case. You know how this workflow goes, right? This is just like the real world. I go here. Link my order ID. Link entity. And now I can draft a reply. Go back to the case details. Live long and prosper. Okay. Awesome. Let's take a, oh, I should close the case, of course. Close the case. There you go. Now, let's look at a bit more difficult example, right? Payment failed for finger traps order. Use Lumen credit card. Miss Wang. Admin for Mr. Milchick. Okay. So, there we go. So, I don't want to link all those entities, right? Because it's a shipment. It's a payment. I will need to navigate all these systems and copying over IDs and that's going to be very boring for you. I don't want to, like, have you waiting on that. So, I'll invoke my case context agent and this agent will look at my conversation history, get all the linked entities that are currently linked and then I'll reason about what are relevant entities to link to this case so I can resolve it. And look, it also links a standard operating procedure because I have a operating procedure to investigate failed payment. So, that's also linked. Now, let's go back to the case. All the related entities are there including the standard operating procedure which is a lot of text, right? I'm not going to read all this and figure out what I need to do. Well, actually, I should read it, right? But this is like terms and services. I'll invoke my standard operating procedure agent. We'll look at the case context and we'll know that I can execute this SOP, right? It will retry the payment that will be successful. Update the order status. It's now awaiting fulfillment. Well, that's great. So, we'll be happy. Draft the reply and there we go. So, the draft is ready. Again, human in the loop. I want this to be a bit more formal, right? Those kinds of regards. Wait, sir. Reply. And close that one out, too. I can go on with this all day. But let's go back to the slides for a bit so I can add a bit more context. Did you like this? Great. So, my application architecture. I used LandGraph for building agents. I really liked the library. It was a great experience. I wrapped it into FastAPI, my web framework. And look, I am a Go developer originally, but Python is my destiny. So, it's just how it is, right? Everyone uses Python. The AI happened. Now, I am a Python developer. And thanks to Gemini 2.5, I'm a very capable Python developer. It's really good at vibe coding. Like, I vibe coded most of this demo. It was really great. So, finally, I packaged my Python app into a container image with a Docker file. And that's great because now I have a container image with my web app in it and I can deploy it to Cloud Run. So, I deploy my container image to my Cloud Run service and I can get a web endpoint back and my users can send requests. And now I want to show how I deploy a change to my app. Because, oh, I totally forgot to show that this is a real app running in Cloud Run. This is my service overview. When I click on this link, I'll get to the same side that I just showed, right? But now let's make a change to this, right? This is my local Vizal Studio IDE. I'll add a little paragraph to the homepage. Hello, Cloud Next. And then I'll save this file. Like the number of times that I've done a demo like this and forgot to save the file. And that's really awkward because then you deploy the change and then, oh, it's live and then it's not live. Well, not today. G Cloud, I go to my terminal, do G Cloud, run, deploy, enter. I'll confirm the source directory, this is the source directory, and the service name, service agent. And now the build kicks off. And we'll watch this screen for about five minutes. No, we'll go back to the slide and we'll explain what happens. So, let's break down what G Cloud run deploy does for you. It starts by uploading the local directory to cloud storage. And then it starts a build in cloud build. So, these are all commands that G Cloud is running for you, right? This is not something you type yourself. I just did G Cloud run deploy, enter. And then this build will look at the source code and we'll see, like, is there a Docker file? If there is, it will do a Docker build. If there isn't, it will run pack build using the Google Cloud build packs. And build packs, the Google Cloud build packs is the open source version of the technology that we, that powered App Engine since 2008. So, we're now using that in Cloud Run. Finally, you have a container image. It's pushed to artifact registry, which is the container registry on Google Cloud, which creates a new version in Cloud Run. Because it creates a new version, I can easily roll back later if I want. That's nice. And finally, it will migrate all the traffic to the new revision as soon as it is healthy and scaled up. And, this is what, this workflow is what sold me on Cloud Run in 2019. Right? I was right there in the audience listening to Starin giving, like, launching Cloud Run. And I thought, wow, this is awesome. This is Docker Run on my local host, G Cloud Run Deploy, and I have a running web service. And that's also why it's great for running agents. So, I'm going to hand it back over to Starin and he will explain more about agent tools. Thanks. Thank you. All right. So, you've seen how you can deploy the serving and orchestrating parts of your AI agent. Pretty straightforward using LandGraph and G Cloud Run Deploy. Now, we'll go into a bit more advanced topic by looking at the tools that the AI agent can use. So, first off, what are the tools that we see agents using? These are the ones I could think of and have observed from our customers. So, very basic tools. Like, you don't want an LLM to do math or a time conversion. Right? You want to give that to a function that does that precisely. Then, as you've seen in the demo, you need some database access and to do retrieval augmented generation, which is about passing more context to your prompt, but also to just query entities in your database. The tools can also simply be APIs, first-party APIs, third-party APIs. read most commonly, but sometimes also write access to those APIs. The tool might be an image generation API or even just chart generation. Maybe you want to return some charts to your end users. And finally, two tools we will go deeper into are browser and web search tools and code sandbox tools. So, let's talk about these. browser tool. So, this is a bit more advanced, but just so you know, because Cloud Run can run arbitrary containers, you can actually put Chromium in that container running on Cloud Run. And then you can give that Chromium to your AI agent so that this AI agent navigates the web. And the nice thing, again, because Cloud Run has streaming support of the box, you can stream all of this back to the agent or to the end user. So, how do you build a browser tool on Cloud Run? Well, there are roughly two strategies. One is to only use Chrome and something called headless Chrome. And here, you can use high-level API libraries like Puppeteer or Playwright to control Chrome. so you can tell it, go to that page, click at that location. Or you can go a little bit down lower level and use the Chrome DevTool protocol, which exposes basically all of the Chrome features programmatically. It's a very stable API, and that's really the API that Chrome DevTools uses itself. So here, the agent would control the mouse click and get back maybe the text, the DOM, or just the pixels. But there is another strategy, which is just because, again, Cloud Run runs container, you can run a full desktop OS on Cloud Run and stream back the screen via web sockets. So that's what you can do, that that desktop OS can have Chromium app installed that then the agent can navigate the OS, navigate the Chromium app like a human would do, and then get back the pixel of the desktop OS. Another kind of tool that you can build on top of Cloud Run is a code sandbox tool. And here, the same way Cloud Run can run Chrome, and because it runs any container, Cloud Run can actually execute any code, any programming language, because anything can run in a container. But here, the key is to understand that Cloud Run, out of the box, for everyone, provides two layers of sandboxing around your containers. So, your code, when you deploy to Cloud Run, you cannot escape that sandbox. But that also means that you can run untrusted code on Cloud Run securely. On top of that, you should really also restrict the IAM permissions that you give to the Cloud Run service on which you run untrusted code. you really want to give no IAM permission if you run untrusted code. And finally, you might want to restrict internet egress so that the untrusted code cannot make any calls to the internet. It is sandboxed from the networking perspective, from a security perspective, and from a compute perspective with those two layers of sandboxing. Cloud Run uses a micro VM technology and another layer of sandboxing. So how to do that? If you want to do asynchronous sandboxed code execution, you would use a Cloud Run job. So here, you would upload a code to execute to Cloud Storage. You would execute a Cloud Run job that would load the code, potentially install the dependencies, execute it, and store back the result into Cloud Storage or wherever. But if you want to do a synchronous code execution where the code is actually passed on the fly, then you really should use a Cloud Run service which has a one-hour timeout, so you can execute that code for up to one hour. And in that case, you should set concurrency to one because you only want one instance, each container instance of that Cloud Run service to only process one code at a time. In that case, you can even retrieve the code to execute as part of the request body, potentially. Then you execute the code and you return the result in the response. And then you can clean up that by just terminating the container instance. So those are advanced tools you can actually build also on top of Cloud Run. And to hear about that specifically, let me invite Harjot from CodeRabbit to tell you how they built an AI-powered code review agent on Cloud Run. Thank you. Thank you. Thank you. That's great. Thanks. All right. Thanks for having me here. So first of all, quick show of hands, like who here has heard about CodeRabbit or used CodeRabbit? That's nice. Thank you. Yeah. So CodeRabbit is a startup that uses generative AI to do code reviews, essentially looking at code quality, code security for users which are on Git platforms like GitHub, GitLab, Azure DevOps, Bitbucket, and so on, and provide really human-like feedback when it comes to best practices, consistency, and so on. So the company was founded a couple of years back but has seen rapid adoption and kind of like nonlinear growth and is currently like used by thousands of organizations and tens of thousands of developers on a daily basis. So let's look at the motivation like why CodeRabbit and why now, right? So one of the things that has been taking off recently as you all know like with the tools like Cursor, Windsurf, Copilot, and so on, developers are actually generating a lot more code. Like there's something called wide coding and we just saw a demo of that which has taken off. So you're starting with a prompt and generating like large blobs of code and large change sets, right? And in many cases the developers themselves are now becoming a reviewer. Like the code is written by AI and the role of developer has become kind of the reviewer role. And as a result, what we have seen the bottleneck in the SDLC has started shifting towards the code review process, towards the CICD quality checks and so on. And that's where CodeRabbit has seen a lot of success in terms of providing like an AI native quality gate through which all the developers have to go through. And it's a very team-based product unlike the IDE which is very developer-centric like CodeRabbit is a very team-native kind of an experience, right? But let's look at CodeRabbit architecture a little bit under the hood, right? So unlike most of the AI applications which are very chat-driven like where the user is driving the workflow, CodeRabbit is pretty autonomous. Like it's a multi-agent, multi-step kind of a workflow with zero activation energy. In fact, like it triggers automatically whenever some developer opens a pull request. Like pull request is like what developers open when they want to ship changes into and merge those into the main line in Git or something, right? So whenever a pull request gets opened in one of these Git platforms, it kind of triggers the Cloud Run workflow. So we are hosted on Cloud Run and we'll talk more about like how we're using it. But that workflow is pretty interesting. Like we do a bunch of things there. Like one is like there's a lot of context enrichment that we do. So we are like not just looking at the diff of changes within the pull request. But there's a lot more context we pull in from other sources proactively. This is like not agentic. This is more like RAG. There's like a lot of vector store, vector search. There is like static analysis tools that we are running in a sandbox where we bring in output from tool failures in CI, CD. We are also like bringing in context from issue systems like JIRA and linear and so on into the review process, right? After the context is prepared, so we use reasoning models like O1, O3 mini, 3.7, so on it. And in fact, like we're one of the largest users of the reasoning models in the world actually. And we use these reasoning models to run the code review process where it's like we have to understand all the changes like the branching, the control flow and so on. And also the agentic workflow that pulls in additional context that we have not provided earlier. Like this is one of the things we have seen with the AI. Like even though the context windows are growing larger, you still cannot stuff everything there, right? So we try our best to like give AI some relevant context up front, like even code snippets from code graph. Like we do all those code analysis and so on. But we also give AI like agency to go and look for information that's missing in order to perform a code review. Like that's where the AI agentic step is and it goes and looks for the information, right? And the way we have done it is very interestingly on Cloud Run. Like so we found Cloud Run to be really a good fit for this kind of a use case where we are running untrusted code in a sandbox. And when it comes to like running untrusted code, like many things we are doing. One is like running tools. Like a lot of our customers use ESLint and sometimes they use ESLint with their own custom plugins, right? And which is like again untrusted code. They are running RuboCorp with like untrusted plugins, right? So those static analysis tools, we are kind of using the user's configuration to run those tools in the sandbox in Cloud Run during the PR review, just like how you do CICD, and use that context during the review. And the other thing we do is also very interesting. Like let's say we want to like look at changes not just in the pull request. we also want to like understand the impact of the changes which are in the rest of the code base. So one of the things we do is like we kind of clone the repository and we let the AI run an agentic flow where it kind of generates shell scripts to navigate the code just like a human does. So AI generates shell scripts to let's say it can use a cat command to read another file which was not in the PR, right? It can run a rip grab query to search for patterns in the code. It can run AST grab queries as well to bring in functions that we did not provide in the earlier context, right? And all that is happening in the sandbox environment, right? And unlike what Stephen said, we actually allow egress, we actually allow outside access because a lot of the things we allow in this sandbox is like in our chat, like users can even say open a GitHub issue. So the agent can go to GitHub using GitHub CLI to open an issue, right? So those kind of functionalities also exist. But we locked down all the internal access. So this is kind of a different design on the sandbox side. And this is kind of like one of the screenshots from one of the review comments we had from a real user in open source and what it looked like. Like we have this verification agent bringing in additional information. And you can see that in the thinking chain, the AI at some point decided that, okay, I'm seeing this change, but it looks like it might have some impact in the other parts of the code base. So it goes and runs this kind of grep calls. And it comes up with some comment and flags a problem. Like if it's there in the remaining code base. It's kind of like tool calling, but we kind of like leverage a lot more shell scripts, AI generated shell scripts to review the code. And that's what like has really helped CodeRabbit differentiate a lot and get a lot of adoption and love from the developers. On the Cloud Run side, like we are running, like our infrastructure has been pretty much growing. Like we are running like pretty, like 32 GB instances. By the way, this is a whole screenshot now. So around like we're running 100 to 200 instances or more than that now during peak hours. 32 GB RAM, 8B CPU. And we are like pretty much maxed out on CPU, network bandwidth and the memory on these instances. But so far, as Staden said, like it's scaling really well for us and we don't foresee like hitting a wall anytime soon on this architecture. And thank you for having me here, but I'm going to pass on to Staden for conclusion. Thanks. Thank you. Should we call our speakers back on stage? They were so awesome. Please join me. Join me. All right. So let's summarize what we've talked about today. First, AI agents, they are a new type of architecture, but, you know, not so new in the end. Like we've decomposed how they are built and if you know how to do a web application, you will see you can evolve the architecture to what you know from and build an AI agent. So AI agents, the key piece is that they leverage a model to reason. That model will set goals, plans and responses. It retrieves context and stores memory. And most important, this agent will orchestrate tasks and use tools. And why is Cloud Run a good fit for your AI agent? So for this orchestration piece, it can run any AI agentic framework, notably LANGraph. It can call Gemini API, vector databases, has access to memory databases, and it can stream things via built-in WebSocket support. You can also host your agent tools on Cloud Run. Of course, you can host APIs on Cloud Run, but you can even do web browsing or, as we've seen with CodeRabbit, code execution in a secured way. And finally, you can even do AI inference on Cloud Run by leveraging our now generally available Cloud Run GPUs, which happen to be on-demand, scale-to-zero, fast-starting GPUs to run your open models or fine-tuned versions of open models. So you can see how Cloud Run can support all the pieces you need to build an AI agent. We thank you for joining us today, and thanks to our awesome speakers. Thank you. Thank you. APPLAUSE Thank you.