 . JOANA CARRASQUEIRA- Hello. Good morning, everyone. Welcome to our session about models from research to production with Google DeepMind. My name is Joana Carrasqueira. I lead the developer relations team at Google DeepMind, and I'm joined here today by Luciano Page, who's our technical lead for Gemini APIs and Google AI Studio. We're going to have a very interactive session. We really wanted this to be very hands-on so you can see some of the tools, some of the playgrounds, some of the new innovations that are bringing to market almost every month, almost every other week. Google's shipping a lot. And so we wanted this session to be catered to you. But if there's any examples, any problems, any questions, anything that we don't touch upon during this session, feel free to come and meet us at our Google for DeepMind developers. We are in the exhibition hall. We have team members that are around, always happy and ready to answer any questions that you may have for us. So like I said, this session, we're going to give you a quick walkthrough of some of our most recent innovations from model research to production, hands-on, and then we would love to continue these conversations after this session. And for those who don't know, and I'm sure that most of you here have heard of Google DeepMind, but for those who might not know, Google DeepMind is the AI research lab within Alphabet that is truly dedicated to building AI responsibly to advance humanity and science. And this is truly rooted in our mission. And it's something that we truly believe in. AI has to be developed responsibly for the greater good of humanity. Because AI is a very powerful tool and it can boost productivity, it can enhance creativity. But what we really want to make is AI helpful for everyone. Making AI helpful for developers, for industry experts, for the general public. Everyone should be able to harness the power of AI. And like I said, this is deeply rooted in our mission. It's something that we truly believe in. Because we want to go beyond productivity. We want to think and explore the barriers of AI. What we can do together to help tackle some of the most challenging problems that humanity has. And so whether if you are a solopreneur trying to automate your business. If you are a business decision leader or a key leader in your industry and you're trying to find new solutions to increase productivity or make better decisions with data-driven information. Or if you're an expert in a different type of industry that is trying to stay ahead of the curve. Google has end-to-end solutions to help you build scale with us. We are your best partner in AI solutions and we have these very robust ecosystems of tools. And we've been an industry pioneer for many, many years. All our innovations are the result of deep investments in AI research and technology. And we've been pioneering these technologies for many years. From the moment we open source TensorFlow in 2015 to the Transformers technology in 2017 which powers most of the Gen AI innovations and technologies that we have nowadays. To the release of AlphaFold, Lambda and now more recently, Gemini. Gemini is our most ambitious project yet. And it's like I said, one of our, it's our foundational model built from the ground up to be multi-model and also very good at API integrations. We are in the Gemini area. Who here uses Gemini? Can I see some hands? Okay, okay. Right audience. So, and like I said, our most ambitious project yet. This is a result of a true collaboration across Google, across collaboration within DeepMind, many different teams. They work tirelessly to bring all these new innovations to you, to put it in your hands so you can build and scale with Google. And it's really easy to get started with the Gemini APIs. You literally just have to get an API key in Google AI Studio and you're ready to go. We're going to do some quick demos so you can see just a wide range of use cases so you can see how you can get started as well. And our Gemini models built from the ground up for multi-modality. And they really reason seamlessly across text, image, audio, video, and coding. They're also really good at understanding nuanced information, but also reasoning math, in physics. And they are really able to process very vast amount of data. And they come in all different types of flavors. We have models for all the different use cases that you may want to use them for. Starting with Gemini 2.5 Pro, which is our best model for general performance across a wide range of tasks. It also comes with our biggest context window of up to 2 million tokens. And it's really the longest of any large scale foundation model. Then we have our Gemini Flash models, which are really good models with low latency, but also built for very good agentic experiences. The Flashlight models, which are really cheap and cost effective, which is something that developers typically give us really good feedback on. And Gemini Nano, which is more targeted for on-device tasks. And like I was saying, the long context window truly, really matters. Because it allows the models to process and retain information, but also it enables them to process very complex tasks and also analyze very large data sets. So this is really a true differentiator and something that really, really benefits developers. And just so you have a more clear understanding of what 2 million means, it's literally two hours of video, 22 hours of audio. It's about 60,000 lines of code. So it's really, really, really long, and it enables you to do a very wide multitude of tasks. It represents, like, all the text messages that you will send in a lifetime. It will represent all the emails that you've sent in the last year. So just so you understand, like, how important this really is. And it's something that we're really proud of to be able to provide you with this type of technologies. Another thing that we're really proud of is how much the team has been shipping. And if you follow us on social media, you might have seen that we're shipping quite a lot. So just to highlight some of the most recent launches that the team brought to market. And very quickly, who uses Gemma in the audience? We had some quite Gemini users. Okay. Not so much of Gemma. But have you heard of Gemma before? Okay. Okay. I see some heads. Some nodding heads. So if I gave you a quick overview of our Gemini model family, we also have the Gemma Open Models family. Because we believe that true progress doesn't really happen in isolation. It's all about collaboration. And that's why we also developed the Gemma Open Models family based on the same research technology that our Gemini models is powered with. And it's a family of lightweight models designed specifically for you, for the community, so you can have more control of what you can do. And this commitment to open development, like I mentioned before, is something that is truly rooted in our design philosophies, in our mission, in our approach since the moment that we open sourced TensorFlow in 2015. And Gemma 3, which we've recently launched, is our most powerful model. It's very fast and more capable than the previous generations. And just like the Gemini family, the Gemma Open Models family also comes with different sizes. And we have the 1, 4, 12, and 27 billion variants, which are ready to run anywhere, from your smartphone to cloud. And so, in our model, these models, they offer different capabilities depending on the problem that you're trying to solve. And especially the 4, 12, and the 27, they are multi-model and multilingual as well. But going back to the Gemini ecosystem. Within our Gemini ecosystem, like I said, we have different flavors for all different use cases from consumers, and you've probably have used the Gemini app. For developers, our playground is Google AI Studio, and that's what we're going to demo here today. And then for businesses and enterprises, our Vertex AI solutions. So, like I was saying, Google is your end-to-end power, end-to-end partner, regardless of what you're trying to do with AI. For the purpose of this session, we're going to focus on the developer segment, and we're going to showcase some examples in Google AI Studio, which is the simplest way for you to experiment with our models. It has a very UI. It's very friendly to use. The interface is super easy, so I'll show you in a second. And you can create, test, and save your prompts. And now you can get access to the latest models also for image and video generation. So, if we go and change to the laptop number one, I will give you a quick demo here in AI Studio. So, basically, there's many things that you can do. This is exactly what I was saying, super easy to use. And you can interact with AI Studio, with your playground, just in plain English. You can use it as more of a, like, text conversation, so back and forth. So, those are, like, the easiest prompts. But also, you can add system instructions. So, I already have some system instructions here prepared for you today. So, I'm going to just quickly change the model. So, I'm going to call Gemini 2.0 Flash. I'm going to give it some instructions. And I'm going to say to the model, you are a helpful cooking assistant, specializing in low-carb ingredients. Your task is to analyze photos of ingredients and suggest recipes based on them. Always stick to recipe suggestions and cooking advice. If asked about anything unrelated to cooking or nutrition, politely redirect the conversation back to recipe ideas. Provide recipes in a clear step-by-step format. And now, I'm going to give the model just a quick picture of a fridge. And let me ask you to analyze the picture, list the ingredients. And since I'm Portuguese and it's early in the morning, let's ask it for a breakfast recipe. And give me a Portuguese style breakfast recipe. There we go. And now, let's see what the model comes up with. Okay. Here's a breakdown of the ingredients. Okay. So, it literally understood what was being shown to it. It listed everything. And now, it gives me a recipe with a Portuguese omelet. Okay. I'm not too disappointed about that. Actually, I like it quite a lot. So, that's really good. And you can keep going and ask it to do more things. So, just a quick, very, a very basic chat interaction. And I just wanted to quickly show you something else before we go into the more advanced. So, I'm going to show you a video of view that I produced earlier this morning. But it's actually not loading right now. So, that's fine. So, that's fine. And I am just quickly going to go into the more advanced. So, I'm going to show you a video of view that I produced earlier this morning. But it's actually not loading right now. So, that's fine. And I am just quickly going to go into my drive. No, I'm not going into my drive. It's fine. So, I'm not going into my drive. So, I'm going to go into my drive. Anyway, that's fine. If you can go back to the slides. I'm going into my drive. I don't know. That's fine. If you can go back to the slides, I'll show it to you in a second. I'll share it with the production. I basically asked the view model to create an orange cat named John going to space, and they produced some really cute videos, so we'll show it in a second. So basically, AI Studio is your playground, is the best place for you to get started. Simple prompt interactions, text like you can upload some files to also give additional context to the models. You can pick and choose the model that best suits your use cases, and then you can literally give some system instructions, and from there, the world is your Easter. I'm going to call Luciano on stage, who's going to show us some of the more advanced features, some of my favorites that include some YouTube links, and you can actually interact with Gemini, and Luciano is going to show you all of those. Thank you, Luciano. Thank you, Joanna. Some applause for Joanna, please. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. And basically, you have some options of how you can interact with the model. For example, you can send an image. You can send like one PNG, one JPEG image. The VO model will handle that as one initial frame for a video, and it will create the additional frames to make this animation. So, normally, as we are talking about high-quality video generations, it takes a while, like something between 40 to 60 seconds, depending on how complex it is. But once it is done, you have it generated straight on your screen, and you can download the video, or you can regenerate changing the aspect ratio, or how long you want the video. Like, for example, by default, there you go. So, I sent basically this image as reference, and VO animated that thinking on a kind of context of what would be the next frames on this video. So, that's exactly how that works. It basically creates only the video, so if some of you are thinking how we can embed audio or narration or something like that, it should be some post-video creation step. Yesterday, we announced that the Lyria model, has any of you heard about Lyria? Yeah, so, basically, on Thomas Curian's keynote, there was this demo where they created one video about Google Cloud Next. They used VO to create the animation or the video itself. Then they used Lyria to have some music, some track created with the context of the video, and then they merged it together to have, like, a real video with image and audio. So, it is something pretty cool to be tested. Well, moving forward here, on the right side of the AI Studio, you can see pretty much all the models available. So, you have the Gemini 2.0 Flash as our official GA model, but also, you have the 2.5 Pro in preview. You have one variant of 2.5 Flash capable of creating images. And Joanna mentioned that Gemma, and we heard many of you sharing some struggles to run local models with needs of accelerators, like GPUs, or high-memory instances, and things like that. So, to make it easier to folks to experiment with Gemma, now you have all the Gemma 3 models available on AI Studio as well. So, you can do initial experiments using Gemma to see how the quality is, what's the average response time, the latency of generations, et cetera. And then, if it is cool for you, if you want that, you can move forward and deploy it on Google Cloud, on your own servers, or something like that. Okay. But thinking about the developer's experience on the AI Studio, one of the cool things we have available on the API and available for you folks here as well is the idea of structured outputs. So, how many of you work on a place with, like, one ecosystem of APIs? Nice. So, basically, a common struggle every time we add a new functionality or a new API to our ecosystem is how to ensure that the outputs or the outcomes of the new API will be with the proper typing or the proper structure as the APIs you already have to avoid breaking experiences. So, what we can do with Gemma 3. So, what we can do with Gemma is we have this idea of structured outputs, and basically what you can do is create one open API schema explaining what's the format or what's the way you want your answer. And here, I'm going to use a very simple object type. And having it enabled, anything you request to Gemma will be formatted with this structure. So, for example, I'm as hungry as Joanna, and I'm thinking about breakfast as well. So, I will ask Gemma to generate a chocolate cookie recipe. So, what's going to do is, oops, let me rephrase that then. Generate for me chocolate cookie recipe including ingredients and key directions. Okay, now I have it. Thank you, Gemini. So, basically what we can do here is speaking about any kind of experience like coding execution or digesting some user information to prepare for further APIs interaction. You can define how the answer comes, what are the key fields and the key structure for the response. So, this way you can plug Gemini interactions on your own pipeline or your own already existing pipeline. So, it reduces all the friction you may have to include new AI features to your world. One of the things we do here to connect your experiences running here with your real coding experiences outside of the AI studio is the get code capability. So, with get code, we pretty much gives you how we can replicate the interaction you just had with Gemini via AI studio on any of the possible ways of interacting with the API. So, for example, that would be the rest kind of interaction. And here you are pointing what's the response type you want. So, basically one JSON on a certain open API schema. You can point to the Python SDK. And then you can have clearly here how you can define the response type and how you can point to the schema you want to use. So, it is all there. And, again, even for JavaScript, for JavaScript folks, you have the ability to do the same as well. And it is pretty much available for everything that is in preview or GA here on the AI studio. Okay? So, moving forward, another common struggle for people experimenting with LLMs is normally when we need some specific information out of the prompt, especially when we are talking about prompting strings. Like, for example, how many of you have seen memes on Internet of LLMs struggling to counting letters on words? So, how many R letters are in the word strawberry? I guess that's the most famous one. Something very nice that the Gemini API does for you is what we call code execution. How code execution works? So, we realize that relying only on the LLM generative capability may fall short for some very specific scenarios. So, with code execution, what we do is when you send something to the API, the API will interpret the request and create a Python code to solve that problem. So, it's not going to be programmatically. So, it's not going to be an LLM generation. It will be a real code execution. And the LLM response or the API response will be based on the code execution results. So, for example, here I chose 2.0 flash again. I enabled the code execution. And, for example, if I ask how many letters are in the word strawberry. So, what's going to happen? Oops. Let me do that again. And the tricky thing is it is enabled as a tool for the API, but your prompt must direct in some way that the API must use this tool to avoid having the tool using for any kind of interaction. Like, for example, if you say hi to the model, there is no reason to have a real code being generated and executed. So, let me rephrase here. Calculate for me how many R letters are in the word strawberry. Okay. Let me try again. Calculate for me in Python how many R letters are in the word strawberry. Okay. Now, I have it. So, basically, you can see on the AI Studio, like, to make it easier for you folks, what's basically the Python code? It uses it to do the code execution. You have the code execution result. So, pretty much what would be the result if I run this Python code manually. And then I have the answer composed of the model generative capability to create text and also using the Python code as reference. Something nice about the code execution is the ability to use other Python models. For example, if I ask the model to generate for me one matplotlib. Does everybody here know matplotlib? Okay. So, pretty much for those not familiar with Python, it's basically one library to create charts. So, it makes it easier to write code and generate image of charts. Okay. So, generate for me one matplot chart including the top ten U.S. states by population. And I will enable code execution again. Okay. Look at for Wikipedia. And here, it's a nice thing to be seen because basically what the model does is it creates a Python code on the best way the model can create a Python code. But the accessory VM, we run the code execution, the generated code for the code execution too. It has a limited number of Python models and it still doesn't have the ability to pip install new things. So, what happens is if the first version of the code is not able to be run for any reason or it fails, what we do on the API is basically try generating new codes. So, without you folks need to have the broken experience and trying again. And then it is fixed. So, here, I have a chart, a pretty matplotlib kind of chart with the light blue bars. And another thing I can do is I can interact with this result. So, for example, I can tell the API, please use different colors for each bar in the chart. And what the model will do is it will adapt this code for this request and will regenerate the chart for me. So, it makes simple to have even some sorts at some degree some data science kind of experiments using the Gemini API, but not relying only on the LLM generation capabilities, but also counting on the real code execution. Okay? The next cool thing here is has anyone heard about the support of YouTube links with the Gemini API? Okay. So, basically, we got a lot of feedbacks from developers everywhere that a common struggle is how we can use information that may be even your own information available on YouTube videos like podcasts or educational videos or stuff like that. And connect that with the Gemini and get some insights or some information out of it. So, we created this idea of what we call YouTube link support. So, I'm getting here a synaptic video from a recap from the yesterday Thomas Kurian keynote. And here on the AI studio, hitting on the plus button inside of the prompt, a text box, I have this option of YouTube video. So, here I can simply paste the link. It will be included on my prompt. And I can simply ask this for me in a bullet list format all the announcements mentioned on this video. And what's going on in the back end is we are working together with the YouTube engineering folks. We are generating, like, almost in real time embeddings for this video. And we can collect semantic information out of this video and have on your own interactions. So, while it is processing, the same way we see for, like, the structured outputs, you can replicate the same experience using the API. So, you have a particular way of sending those information, and that's it. Now I have the results. So, Thomas Kurian mentioned a lot of new stuff, like the JKE, inference gateway, the VLNM's on TPUs, and Gamma 4 on Vertex AI, a lot of stuff. I have all listed here. And I can keep interacting on this, like, for example, if it is one educational video, you can not only get the insights from the video, but you can also use the Gemini knowledge to extend the kind of insights you have. So, for example, if it is one video from my university course, I can get the insights, I can get ideas. But also, I can ask the model to base it on those information from these videos, what would be good books to have me learning more about that? So, we extend the ability of extracting information from a video only, but we can connect them with even more knowledge from Gemini. Okay. Are we good up to here? Nice. So, have you guys heard about Google search? Nice. Okay. So, and talking about LLM experiments, how many of you are working with RAG or any sort of external data sources to complement the LLM experience? And how many of you really like doing that? Because that's really complex. You really like? Oh, my God. Okay. What you have here, using both the Gemini API and the Google search ability, is what we call Google search ability. It's what we call grounding with Google search. What does that mean? It is pretty much similar to a regular RAG architecture. But basically, what we do on the back end is we do a Google search with that question. The results from Google search, we use as context for the model, and then the model answers us back using this information. It is very useful for two key reasons. First, we are extending the model knowledge for more stuff available on Google search. And second, all LLMs has what we call the knowledge cutoff dates, or pretty much what's the maximum date where models have information. For, for example, Gemini 2.5 Pro, it is trained up to January 2025. It means that anything that happened after that, the model doesn't know about that out of the box. So, for example, I'm from Brazil. I love soccer. So, if I ask the model, what was the score of the latest Brazil versus Argentina soccer match? So, what happens here? As the model has this cutoff date by the end of last year or start of this year, it doesn't know that there was a very sad match that happened last month. So, what we can do here? We can enable the groundwork of Google search. And if I ask the same question, the model will do this search. Like, in the back end, you don't have explicit efforts to do that. And it will know that last month, there was this match where it was 4 to 1 to Argentina. Really, really sad. And also, here on the AI Studio, but also with the API, you get all the metadata that base this information. So, if you have, like, a multi-agent scenario where you have an agent responsible to check the accuracy or to avoid made-up information, you can use this metadata to confirm from where the information came from. Right? Cool. So, we talked about JavaScript a few times here. How many of you have heard about the P5 framework? The P5 framework is a nice kind of educative or gaming-related framework where you can create games or interactive apps really fast using all the methods and all the capabilities of this open source framework. And before I do this next demo, how many of you have tried in the past creating code with Gemini? Was that code good? You can be honest with that. Okay. So, we heard you all. And we did a huge effort on the Gemini 2.5 Pro to make it very good on coding. So, since we launched it a few weeks ago, we tried to approach the two best sources of opinions in our perspective, which are the academic benchmarks and the feedbacks from developers. So, Gemini 2.5 Pro is leading most of the coding benchmarks for now, which is really good. And we have been talking and partnering with people like from Cursor, from Witserve, from GitHub Copilot. The model is available on those environments, and the feedbacks are being great. So, what you can do now with the Gemini 2.5 Pro is using both the quality of the model out of the box, but also its reasoning capabilities, you can have very good code on the first interaction. And also, it makes it easier to keep interacting or vibe coding, as everybody is calling that now, to keep enhancing, adding new functionality to the code and run it really fast. Like, for example, here I'm going to use the newest Gemini 2.5 Pro, and I will ask the model, create for me a Pac-Man-like game using the P5.js framework. So, I can start, like, very simple like that. And here on the AI Studio, you can see the reasoning process, or basically all the self-conversation the model has with itself, thinking on possibilities, on the best next approach, if the result was not good, what it would be doing to enhance that. And once the reasoning process finishes, you're going to have the final answer ready for you. The final answer may be, like, one paper analysis, it may be a comparative analysis between quarter range of big banks or the companies we invest on, but on this case, it is a code. So, what we can do here is, once the code is fully generated, and as you can see, it generates for me two files. The first one is one HTML, if I want to run the app myself, or I'll go straight to the sketch.js file, which has the real game implemented. So, what I'm going to do is, there is this website called p5.js.org, which has a live editor, which makes it easier to test P5 codes. And let me see if it finishes. Yeah, it's finished. So, I can simply copy it here, and it can work out of the box, or it may have small needs, but we can keep interacting with the model back and forth and have the thing done. So, I simply pasted the code here, and I have my Pac-Man game ready here. Okay? What's nice about that, but for the sake of time, we can go through of that, the model itself recommends what would be the nice next features or optimization for this code. So, I can keep incrementing this result, saying, please implement for me all the features you suggested. Something simple as that. And then, I will keep having new versions of these codes, and I keep testing online as I did, or I may be testing inside my environment or something like that. Right? So, I hope you enjoyed those experiments. Thank you so much for your time. I'd like to invite Joanna to come back on stage, please. And could you please go back to the slides? Nice. And while Luciano gets the video and we try to end this session with some really cute cats for you all going to Mars, we just wanted to thank you for joining us today. Oh, great. I hope that you enjoyed some of the cool features that AI Studio allows you to test and try. Like we said at the beginning of the session, it truly is the best playground for you to test our models. We always have new models in experimental mode in preview. We also have a lot of new innovations that are coming to you. And so, AI Studio is the best place for you to get started. These are some of the features that we showcase today. And then, if you... I think I got that. You got that? Oh. Could you please put the notebook 2 again, please? Okay. Let's go back to laptop number 2. Okay. So, this was just to showcase Vio. So, Luciano showed you how you can animate to one image. And this was literally just to say just a quick prompt that you can create. Playful orange cat named John, which I actually own. I love my little John. And so, it was just to show you that one of the best capabilities of Vio is that it allows you to animate images. It allows you to create prompts and generate videos as well. But then, at the same time, it also allows you to edit. And that's something really cool. So, you can go back and forth and ask the model to add some party hats to the cats, add some sunglasses, remove the sunglasses. And it's really nice to see how it does all those different editing capabilities, which is something really unique to us. This was literally launched in GA yesterday. We also have some cookbook examples that are really cool. So, after this session, you can either go to our cookbook on GitHub and try it. But also, if you want to try it here at the conference and speak with the team, we are at the Gemini. We are at the Gemini play space. And there we go. And our cute cats say thank you for joining us today. We are at the Gemini play space where you can interact with some of these cool technologies, the cookbooks, but also at the Gemma model garden. So, if you have any questions for the team, we can go back to the slides just to end this session. And also, if you are a part of a startup, if you have a business idea, if you want to prototype some ideas and test them with us, see if it's feasible, we are very happy to help you. We will also run many hackathons, many hands-on sessions, and demo days for startups. So, literally, we can help you tackle some of these big challenges that you may have. You can get started with our examples on GitHub. Great place to start. And it's time to build. Thank you all so much for joining us today. Thank you, folks. Here's to Luciano. And we'll be available throughout the conference at the developers booth in the community space. So, thank you so much. Thank you all. Thank you. Thank you, guys. Thank you very much so much for joining us today. Thank you. Thank you. Thank you, guys. Thank you. Thank you.