 Hey, everyone. How's it going? Nice to see a nice crowd here, especially during happy hour. So my name's Alicia. I'm a product manager on Google Cloud, and I'm going to be starting us off, and I'm going to be joined by Sean and Hugh, also from Google Cloud. So to get started, just a quick look at our agenda. So I'm going to start us off with an introduction about some concepts around Earth observation and geospatial data. Sean's going to show us how to enrich data that you have in BigQuery with Earth observation data using Python and user-defined functions. And Hugh is going to introduce us to a new capability in BigQuery that brings raster analytics from Earth Engine to BigQuery that's actually just announced in public preview today. So everyone here is familiar with BigQuery. It's a tool that you know and love. You probably use it every day. You may also be familiar with some of the geospatial capabilities that BigQuery has with the geography data type and ST functions. But what's new here is the inclusion of Earth observation data, and you're probably familiar with Earth observation data, but maybe you don't know. This is what you see in Google Maps satellite view or if you've ever explored the world with Google Earth. So to start, I'm going to answer a question that you may be having right now, which is, what is geospatial? It's a broad term that's used to describe a component of where in data. The most common example that most people are aware of is an address. And addresses can be represented as text. You can think like 1, 2, 3, 4 Sesame Street, Las Vegas. And that text can be useful on its own. But geocoding that address and adding the latitude and longitude and gives it context to where it is on Earth makes it geospatial. Generally, geospatial comes in two broad formats, vector and raster. You can think of vector data as something that you see on the map that represents a location or a shape. So think like latitude and longitude, a building outline, a point of interest, or a road. Raster data is a little different. Raster data is actually imagery. And in the case of what we're talking about today, that imagery is satellite imagery, and it's derived data sets. So you may be asking, why is this coming up now? I've gone my whole career with never hearing the words geospatial, vector, raster. And basically the answer is that there's an awakening happening in our customers around the need for geospatial data and how it can help their business decisions. All right, so as I mentioned earlier, BigQuery has long had the capability to process vector data with the geography data type and ST functions. And you may have seen demonstrations like this in the past that show BigQuery's built-in geospatial processing for vector data. This capability enables performant queries of tables with vector geometries and answer geographic questions. So, for example, this query asks how many parcels in Texas have impervious surface. Moving on to raster data, let's talk briefly about the Earth observation data and how it's used. So at a high level, Earth observation data includes information about the Earth's surface, water, and atmosphere measured by remote sensing platforms like satellites or aerial sensors. And you're likely familiar with this type of Earth observation data from satellite imagery in the visible spectra, which is what you see here on the left. And looking at these pictures of the Earth on their own can be amazing, but the really powerful insights come from what we can derive from them. So they help us assess and monitor the changes in the natural and human-made world. And, for example, we see here a derived data set showing where palm oil and rubber are being grown in 2020. So Earth observation-derived data sets, they are created through tools such as Google Earth Engine, which is also another product available on Google Cloud. And Earth Engine is combining petabyte-scale catalog of satellite imagery with planetary-scale analysis capabilities. And it's used by scientists and researchers and developers to create derived data sets from Earth observation that show the changes, trends, and quantify the differences on the Earth's surface. And to give you an idea of what these derived data sets are, here's a few common examples that are used by our customers to help answer questions about their business or enrich their data. And the important thing to note here is that this is only a small number and a few examples. There's many, many, many more in the Earth Engine data catalog. In fact, we have over 90 petabytes and growing daily of data. So now that we've talked about geospatial data, Earth observation data, and how those derived data sets are created, let's talk about a more specific example of how Earth observation data can be transformed into insights that help us monitor the Earth. So this is an example of a raster showing the probability that oil palm is being grown in an area in Indonesia. This raster was created from public satellite image sources by training a vertex model to predict the probability of oil palm in each pixel from the satellite. And the result is probability values from zero to one. The darker green here indicates higher probability of where palm was growing in the year 2023. And what we see along the bottom, this is showing an area successively zoomed in, so you can see that the area of dark green in the middle is being correctly identified as palm, and we're looking at it with high-resolution imagery. All right, so now I'm going to provide a little bit of context. And so let's look at what Earth observation drive data can help in a business case. So to do that, I'm going to briefly introduce you to the European Union's deforestation regulation, which goes into effect at the end of this year. And very briefly, it says that any of the seven commodities under the regulation, which are palm, soy, rubber, coffee, cocoa, wood, and beef, or products made from those commodities, must have a due diligence statement that proves they were not produced on land deforested after December of 2020. So this is inherently a geospatial problem and something where Earth observation and these derived rasters can help. So getting to a real-world example, so organizations that are bringing products into the EU that contain one of these seven commodities need to do due diligence to understand the risk of deforestation for the location where that commodity was produced. One of the challenges is that organizations may be several steps away from the production and supplied with a polygon of an area, but not know if that polygon actually represents a place where that commodity was or could be produced. And they want a way to be able to verify that this area was actually producing the commodity as well as understand if it was deforested after December of 2020. So in this example we're going to walk through, this is a hypothetical polygon representing a production field that was supplied to an organization and this organization wants to confirm that the polygon represents an area where the commodity they're receiving is grown and also understand the risk of deforestation. So using layers like the Forest Data Partnership's community commodity maps, you can start to build an understanding of what happened on the landscape and also check the likelihood of the crop that they claim to be growing there is actually present. So here we're showing the Forest Data Partnership palm probability layer, and these all layers are all available in Earth Engine, I should say. And what this is showing is a very low likelihood of palm being present in this polygon in 2023. And there's a few pixels that indicate potential presence of palm, but these are a very small amount, and they could be either true intercroppings or potentially some model noise as well. So the conclusion from this is that there's a low likelihood that this polygon had palm growing in it in 2023. You can do a similar thing for rubber. Here this is the Forest Data Partnership rubber probability layer. Again, like palm, it's showing a very low probability that rubber was growing within this polygon. So here's where it gets interesting. This is the Forest Data Partnership layer for cocoa probability. And as you can see, the majority of the polygon indicates a high probability of cocoa. So the reasonable conclusion from this is that there's a high probability of cocoa being grown here in 2023. So if your supplier says that this polygon represents cocoa and that's what you're buying from them, that's great news. That means that you can feel confident that that's probably correct. However, if they said they were providing something else from this polygon, that's when you should potentially be asking some questions of your suppliers. So at this point, we have signals that this polygon has low probability of palm or rubber growing in it, has high probability that cocoa was being grown in 2023. But however, we still need to understand and demonstrate that how risk was assessed to show that this area was not deforested after 2020 to grow these crops. So this is where a layer like forest persistence can help. And this layer indicates the persistence of forest cover between 1984 and 2020, which coincides with the time the Landsat 5 mission started through the baseline of the EUDR. So simply put, this layer shows for each forest cover pixel if there has been a disturbance to that cover in the last 40 years. So pixel values with high scores or those closer to one indicate that areas of forest cover that have not had disturbance in the observed time period and indicate that there is a higher likelihood of there being older or primary forest in that location. So as we can see here, there is a significant number of high-scoring forest persistence pixels within this. And so we have an indication that this polygon, while it is likely growing cocoa, may have also had some deforestation activities happen to it as well. And so this is another indicator that you may want to go and invest some more resources like going to the field or investigating other sources to further verify the risk. And the other thing I wanted to point out was that this method that we just walked through is following what's called the convergence of evidence method, which is developed with the Forest Data Partnership and the UNFAO, which basically means bringing together multiple data sources and business logic to make a decision about risk. And there's a link on the bottom of the slide here, and you can go read more about it at that location. So now that we've seen kind of the method, when we start thinking about scaling it, you're definitely not going to want to do this for every one of your polygons in a large supply chain. So now I'm going to have Sean come up and talk a little bit more about how to scale this. Thank you. Thank you. Thank you, Alicia. Wonderful. So we heard this term a lot in data analytics. It's enrichment. And in this case, we are trying to enrich it from these raster data sources. So I wanted to kind of start with where we are so that when I can bring up Hugh, we can show you where we're going with what we launched today. And really, this has historically been done two main ways. How do you bring that raster data to your tables? The first is using Python. And sometimes that is fine. Sometimes using a Jupyter notebook, running in Colab or something like that, it's not that big of a question that you're actually trying to ask. And so that's a perfectly valid approach. Another approach is to actually build a cloud function on Cloud Run and make a UDF that calls that. And we'll show examples of both of these. So what's really nice about the Python API? So first of all, it's great for prototyping. You can get into a notebook, use AI generation in the code cell, not that I would ever do that, and go quickly and quickly build these prototypes. Other things is that when you're looking at this data, Python has an amazing ecosystem of data science tools, you know, Pandas data frames, geodata frames, et cetera. And so the questions aren't that big, and you can actually bring them into the memory of the notebook. It's actually a great approach. So I thought I'd walk through an example of how this works. This is where we're going to be looking at deforestation in the Republic of the Congo. Our partner, Cardo, you'll hear their name a lot during this talk. They maintain one of the BigQuery public data sets for us, and it's the Overture Maps. So this is a foundation of open source maps pulling from OpenStreetMap and a number of other providers, which Cardo upstates for us every single month. So it's a great place to go to find vector data in the BigQuery public data catalog. And specifically here, what I'm going to do is just do a query to go get the region of the Congo using its digraph. So in my notebook, and by the way, there's three notebooks that accompany this session. There was a link in the very beginning, but we'll also have it on the final slide that we keep up. So there'll be three notebooks for you to try. So when I run this first cell, you can see that I pull back these geometries. So this was that selection, pulling back the 12 different departments of the Republic of Congo. Now, I want to ask a question about deforestation. So my goal is to utilize a layer that tells me about land cover. One of those is provided by the European Space Agency. It's in the Earth Engine catalog. That's a 10-meter product that has basically every single pixel has a land type, water, shrubs, forest, et cetera. So this is the layer that I'm going to use from Earth Engine. Now, we have a very robust Earth Engine API for Python. It just went to version one this year, and we're already up to 1.59. That was actually a pretty major release for us. It's been around since 2013, but we've actually increased really stability and typing with it this year. And so the first thing to do is to import our requirements. So we're going to import EE. So this allows us to actually interact with Earth Engine from a Python environment. Then we have a couple of other imports here as well. So when I'm talking in a notebook, I'm actually creating a session on the server in Earth Engine. So I'm starting to build my script. I'm starting to build the computation that I'm asking Earth Engine for. The first thing that I want to do is define an image. And we do this with EE.image, and then we provide a string that points to that catalog location. Now, for the purposes of demonstrating this in a folium map so we can actually visualize it inside of the Python environment, we're going to get the XYZ tile format, or ZXY tile format, to actually pull map tiles into our notebook as well. So we'll do that. We'll just create a new tile layer, and then we're going to add it to the map. So now my map, and if you're running it in the notebook, you'll see you have those vector geometries from BigQuery, and then you have tiles that are representing the land cover. And you can visually see, in the southern part of the country, significant deforestation just visually, right? You've got a forest class, and you've got a urban or barren class. And so what we really want to do, though, is to quantify that. And now this is where it gets kind of tricky. I have a geodata frame from BigQuery locally that I just showed on that map, but now I have to push that up to Earth Engine. I have to create a server-side representation of it. And so this is kind of gross, but I have to iterate through each row of the geodata frame, create a geometry on Earth Engine, and then put that into what we call a feature collection. So this works okay with 13 polygons. This doesn't work really well when you get into, you know, hundreds of millions of polygons, right? It would not be an appropriate way to do this. But this works for this scale, and I'm going to push this over there. And what I'm ultimately trying to do is the concept of zonal statistics. Basically, for every geometry that I have in this feature collection, I want to pull back a statistic from that image. And in this case, I'm looking for the mean. I'm just trying to understand what is the percentage of forest, like, specifically. So how many pixels inside of the polygon are forest versus not forest? So we do that. We create a filter to say, I want the image specifically where it's equal to 10 because 10 is the value for forest. And then we go through and we do a reduced regions call. So the reduced region call takes the input of our collection, which is our features, and then we're telling it we want specifically the mean. And we're also saying, this is a 10-meter input. This is a pretty big country. We can actually put this down at one kilometer. So we can adjust the scale to reduce our cost of our processing. And in this case, at this level, it's an approximation anyway, so that's fine. Now, another thing that we launched about three years ago was a REST API for high-volume retrieval of data from Earth Engine. Now, this allows you to make up to 32 megabit calls, and it's great if you're trying to compute pixels or just pull down pixels for machine learning purposes if you need a bunch of imagery data. And we also, when you retrieve a feature collection, we actually allow you to pull that back as computed features, which allows you to actually paginate automatically and pull down all the geometries. And when you're dealing with very large collections, this is a lifesaver. So the next thing that we're going to do is use this compute features call to return it with a specific file format. And the really nice thing is that you don't have to do this. We actually will return it and build it in the Python client as a Geopanda's geodata frame as one of the formats. So the result of this is our map, and now you can see our geodata frame has been enriched. We have a new column in here called mean, and that I can use to build a chloropleth map that you see here that kind of quantifies what we saw previously. We've got some of the departments have high forests, so they're in green, and then we've got the ones that are shown here in red. Okay, so that, like I said, works. Now, if I want to scale that and not run in a notebook environment, we've seen a lot of people go through the process of building custom UDFs that interact between both services. This can be great for simple functions like get participation, or it can be very complicated workflows that you want to chain a number of these together. So this uses our ability to use remote functions, which I have, I can write this in Python, I can write it in Node.js, we have both libraries for Earth Engine, and basically for each one of these, I can, in a SQL statement, call that function, which calls the cloud run function to run execute my code. Our partner, Cardo, has done this. They've built extensions into their web GIS tool for very specific purposes. I want to get elevation, I want to get NDVI, which is a vegetation index, or I want to get precipitation. So there's Earth Engine code packaged into each one of these, and they're curated to go to specific data sets for you. My colleague, Encore, I don't know if he's in the room today, published a blog about this about two years ago about how to deploy a function. It looks very similar to what we just did in the Python environment. We're going to import Earth Engine. We're going to, because it's cloud function, we have to put in the appropriate scopes and credentials, and now I'm writing my Earth Engine code, and for every geometry that gets pushed into this function, it's going to run through this, and it's going to calculate NDVI calculation for that particular area. Okay? And then when I want to call that, all I need to do is call the specific function in this call case get NDVI with the geometry from the table that I have, and I want to return a specific year, a specific month, in this case, the month of July in 2024. When all of that starts to get really, really complicated, it can be very helpful to have a low-code tool like Cardo Workflows to build that. So Cardo has this great tool. You just drag and drop. So you start with a table. You say, I want to go get elevation. I want to go get precipitation. You can chain all of these together, and then it will actually write the spatial SQL for you underneath, and then you can, you know, you can productionize this. You can put it into Cloud Scheduler so that it can run automatically. It can respond to a PubSub event, for instance. Great. So that is what it looks like today. Now I'm very excited to bring up Hugh, and he can talk to us about what it looks like starting today. Thanks, Sean. Hello, everybody. I'm Hugh. I'm an engineer on the Earth Engine platform team. So we've covered a lot of ground already. Let's do a quick recap. So Alicia introduced us to the world of geospatial rasters. There's a ton of data there, petabytes and petabytes, describing our planet from simple things like temperatures and elevations to reams of visual data from satellite sensors to derived properties like predictions for land cover, land use, fire risk, flood risk, even weather forecasts are available. She also gave us a really good example of analysts trying to validate their supply chains using Earth Engine, examining the plots of origin for compliance with EUDR's deforestation regulations. And Sean showed us how tools like Python and remote functions can tap into that trove of data and annotate BigQuery tables that describe the places that we care about with insights derived from Earth Engine rasters using Earth Engine analytic functions like reduce regions. As powerful as those tools are, I think Sean would be the first to admit there is a learning curve and they're not quite as simple as one would hope. But what if it were easy? What if you could harness Earth Engine's raster analytics and use its data catalog with the familiarity of SQL without ever using, leaving the BigQuery editor? Well, today we're announcing the public preview of ST region stats which is a function in native SQL that lets you do just that. So let's go back to Sean's example with the tree cover for regions of the Republic of Congo. Can we express that in a single query? This is what it looks like with ST region stats. So let's break that down. Running a query on just the from clause here, we can see the table identifies the regions and includes these geometry or geography objects called geometry in the column name that outline our region of interest. you can think of these regions as like a lasso that you're throwing into a field of pixels and computing statistics on the enclosed region. Okay? So we pass that geometry to ST region stats. We also pass it through to our result set so that we can display the results on a map. The next argument identifies the raster from which we're going to draw these pixel values. In this case, the EE colon at the beginning indicates that it's an Earth engine raster, but you can also use cloud-optimized geotiffs or COGS that you have stored in Google Cloud Storage pockets. In this case, it's the same image Sean used, 10 meter pixel resolution. We're going to use the map band. I think it's the only band in this image. And we're going to identify the one value within that band 10 that identifies forest cover. This is kind of an interesting case of ST region stats because basically we want to transform these arbitrary category values into a Boolean so that we can take a mean and get a meaningful tree cover fraction for our region. Okay? And then the last argument allows us to specify a scale for the analysis. As Sean alluded to, you can conduct these analyses at different scales to control execution time, associated costs, and playing that off against the precision of the results. In this case, if you ran this without these options, it would run at the 10 meter resolution. You'd be summing some 4 billion pixels, I think, over the area of the Republic of Congo and that would take a couple of minutes to execute. Whereas at one kilometer scale, it runs in a few seconds. And what do we get? Well, we get exactly the same results Sean did because it's the same computation infrastructure under the hood. Earth Engine's doing the work. Let's go back to a more detailed example and reconsider Alicia's case where we want to analyze our supply chain and identify any originating plots where we might have to investigate whether there's compliance issues with EUDR. How can we phrase that as a SQL query using ST region stats? Here's a query that does that. The general approach here is to sum up the area of commodities in 2023 and compare that to the forested region in 2020. and if the commodities in 2023 are greater than the non-forested area in 2020 then there's a good likelihood that some of that commodity area came at the expense of the forest. So again, we start with our plots and our geometry and then, oh, forgot to mention, we're using pipe SQL syntax here. If you're not familiar, that's something worth trying out. I chose to use it here despite it might not being familiar to everyone because it does allow you to fit it on the page and it runs nicely from top to bottom. Pipe syntax is very linear. So, after we've selected our plot, we're going to use the extend operator to add five new columns to our data set. The first is the plot area. We're using one of those built-in ST functions, ST area, to add that. And then we have four calls to ST region stats to add columns describing the commodities in 2023 and the forest in 2020. Looking at the forest example, that's the same layer that Alicia demonstrated. Again, this is a Earth Engine image asset provided by Forest Data Partnership. We're using its value column or value layer. in this case, I believe value is the only band in this raster, so we could actually omit this argument. But then the interesting thing we're doing here is we're setting a threshold. So, we've cast our lasso out, captured a range of pixels, but we're going to say we're only interested in those pixels that meet our probability threshold or score threshold. If we run the query on just the lines we've described so far, you can see how ST region stats has annotated our source table with additional columns. So, ST region stats actually returns a struct that includes seven additional columns. They are the count, the number of pixels, which is a good one to pay attention to to make sure you're doing an analysis at an appropriate scale. These are small plots, so the counts are relatively small. Min, max, standard deviation, sum, and mean all describe the values from the pixels. You can use the minimum and max to do a little sanity check that your threshold was applied appropriately. And then the last column is the area of the pixels that matched our filtering criteria. We repeat this for the three commodity layers. Call extend again to sum up our results, totaling our commodities and computing the forest area in 2020, or non-forest area in 2020. 2020. And what do we get? Well, we get some plots that we need to look at. So, the first plot in particular showed 57% forested area in 2020 and maybe fully 100% commodities in 2023. the commodity layers are calculated independently. So, depending on the thresholds that you set, multiple commodities can claim the same pixels, so you can add up to more than one here. It would be a simple extension to this query to add a link to a visualization that would allow you to inspect visually high-resolution imagery from the two time periods. That would look something like this. As we saw in Alicia's example, this is a time-consuming process requiring trained individuals, but using ST region stats really allows you to identify just the problematic cases for this further manual inspection. So, if you've got thousands or tens of thousands of plots in your supply chain, ST region stats could really streamline that process. Now that you understand how ST region stats works, you might wonder what other rasters are out there, how else can you use this function? A great place to start is BigQuery's analytics hub, if you go there and search the listings for Earth Engine or for climate and environment, you'll find a number of examples. I think there's 20 if you search for Earth Engine, each of which is annotated with a working SQL example where you can see how ST region stats is employed in different ways for different data sets. and I think that's all we have and we have time for questions so I'll invite Sean and Alicia back up. in general. Thank you very much. relação to H amigo kelkaprel Fireman Pacific