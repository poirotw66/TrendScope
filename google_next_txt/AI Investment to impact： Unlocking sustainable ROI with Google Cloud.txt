 . At Google, we are thrilled today to have all of you here to be involved in what are most critical conversations where every single leader within your organization will have, which is getting the realizations of value from AI. I'm Bruce Warner, Cloud FinOps lead here at Google. We're not just talking about adopting AI, but we're talking about achieving and realizing real, measurable business value from your Cloud AI investments. This is where Cloud FinOps comes into play, using those core financial governance best practices to ensure that you have proper visibility and control over your AI investments. It doesn't become a runaway cost center, and you achieve predictable, controlled, realized value from these investments. So with a show of hands, how many of you have AI running in production in your respective organizations today? Wow, quite a few of you. All right, now how many of you who raised your hands are able to quantify the business value that you're achieving from those? Let's see a show of those hands. All right, a few less. Now I'd like to pass on to our panelists for quick introductions. Brent, why don't you start? Hi, Brent Eubanks, FinOps architect at Wayfair. I've been doing FinOps work for about 12 years now, active member in the FinOps.org community as well. Hi, my name is Leslie Nolan. First, thank you Google for giving us this opportunity. I think Brent and I have now done this road show at least four or five times. CME is getting closer to getting on their AI journey versus Wayfair. That has launched data production and doing lots of great things. But I've spent 14 years at the CME group. We signed our Google partnership back in November of 2021, and I was fortunate enough to be asked to lead our FinOps practice, which is a very close collaboration though with our technology team and hopefully a few of my CME colleagues made it here to find this very difficult ballroom, as well as with the business. So it's always exciting to present on FinOps, maybe a little with Gen.AI, and hopefully we can have a little bit of fun even though this is a big scary room. Handing it over to you, Ivan. Hi, Ivan Boyer. I run the Cloud Center of the Excellence at Palo Alto Networks. That mainly has to do with infrastructure, cloud ops, and the technical side of the FinOps. I am fortunate enough to have enough engineers who can actually build a lot of FinOps tooling for our company, which actually help us make sure we don't run away with a cost. Maybe. Maybe. All right, so let's set the stage. Now we've all seen the hype cycle with generative AI. First came wave one with excitement, followed closely by wave two with early adoption. Now as you've seen in today's keynote and many, many of the other sessions, we are quickly moving into phase three or productionization. And this is where the rubber meets the road financially. It's no longer about experimenting, but about quantifying business value, understanding total cost of ownership, establishing effective controls, and on an ongoing basis, optimizing these costs to ensure that as models change, as economics change, you continue to achieve value. So we see organizations going through a set of distinct phases as they innovate and adopt generative AI technology, from initial concepts to prototypes and MVPs to full-fledged products. And at each stage, FinOps is important and critically important as you get to production stages. Right, and at each of these stages, you need to be looking for the business alignment. How are you actually measuring the value? And how are you maturing your organizations in embracing FinOps to drive that discipline? As such, we need an effective FinOps strategy for AI. And that requires three core pillars to this conversation. First one is business alignment. Second, value measurement. Third is FinOps adoptions. In the business alignment phase, this is about ensuring that your AI investments is actually directly supporting your business objectives, right? So it's not just about having the AI capabilities. It's about having the right AI capabilities that drive tangible business outcomes. Think of this as their building bridge between your technology investment and your business value creations. Second part is value measurement. Once you have that alignment, how are you actually measuring those KPIs and metrics to making sure that you're getting your return on your AI investments? And that's about moving beyond just your traditional IT metrics to measure those business impact. And are we reducing the time to market? Are we improving customer satisfactions? Are we increasing operational efficiencies? So then, you bring that all together with embracing the FinOps practices, which is the third pillar in operationalizing these core principles across your organizations. And this means building a cost-conscious culture. This also means maintaining that agility while innovating in your respective organizations and find that sweet spot between the technology and your AI investments. So let's double click on that and on the business alignment pillar. So what that means is a successful AI strategy isn't just about picking the right and the coolest technology, right? It is a two-pronged approach. So you're starting with a top-down approach, which is defining what your corporate strategy is and your priorities. These priorities should then be informed into the specific AI themes or domain area. And then from a bottom-up perspective, you're identifying those individual use cases. And those use cases then create a more impactful sort of feedback loop to ensure that you have that strong alignment. And it's crucial in building that robust prioritization system, focusing on the use cases with the highest technical visibility and significant business impact. Okay. So this is where our first panel question comes in. So we've talked about this tops-down and bottoms-up approach. Leslie, Brent, and Yvonne, could you share how your organizations are ensuring that AI initiatives aren't just interesting projects, but are actually directly tied to producing tangible business outcomes and ultimately a positive ROI for the shareholder? What does that strategic process, alignment process, look like in your businesses? Leslie, why don't you start? Sure. Absolutely. So I would say today our AI journey is more focused on the efficiency side, right? And so that is a core goal within CME. We're looking at four different areas today and how we could apply Gen AI techniques and technology to different areas to enable those efficiencies. So one is sales collaboration and embedding it within our sales organization. We heard from the gentleman today from Salesforce, right? We are very engaged in how do you create the Salesforce agents and do different things from a sales perspective. We're also looking at Duet AI and how it applies within Workspace to create greater productivity for all of our employees. We are, as Thomas announced, right, a big user, I guess user is the right word, of code or Gemini or the code assist for developers and technology. And then lastly, in an early stage, we're looking at how we can apply it to Looker and our analytics. So, you know, from a top-down perspective, it's driven very much by our CIO who spoke earlier today as well as our chief commercial officer. So it is a top priority of the CME. And then you're exactly right. We have something like 50-plus use cases, which probably doesn't seem like a lot for many of you, but for the CME, that's quite a bit. And we're continuing to continue to analyze it. From a CME side, though, you know, we're still probably in the early part of that curve because for us the data integrity is so critical, right? We are a conservative organization. If any of you are playing in the markets right now, you're probably hedging. Hopefully you're hedging with the CME group today. And so data is everything, and our clients' data and that trust and integrity. So we are conservative. It's very risk-based. It's not quite at the business value ROI just yet. That is the goal. But it's more how does it help us operate, internalize, do work in a much more efficient manner. So, Leslie, let's double click on that, right? In order to drive those operational efficiencies through those use cases, we talk about business alignment. So, and business alignment is beyond just the business team, technology, and finance. So how are you drawing that cross-functional alignment together to ensure that you are maximizing that value and that investment metrics? Absolutely. So it is similar to FinOps, and that's what Bruce is, you know, for us, GenAI is just another workload resource, right? So, but we do have, I will call it, you know, I hate to use the word committee because I'm sure all of you are thinking of bureaucracy and process and admin. But there is a group of individuals mainly comprised of our compliance, legal, cyber, I participate. And that's where we're really trying to understand what it means from our data to get comfortable with this. And then in parallel, we're pushing on the business cases that have the metrics that should have quantified benefits with it. But right now where we are is really getting comfortable with if we share information in Salesforce, where does that information go? What does it mean? How do we take advantage of it? So right now it's, there is a group. They're setting the framework. They're putting together the guidelines and the policies for which our employee base can then go use it. Thank you, Leslie. Brent, love to hear from you on this question. Yeah, it's multifaceted here at Wayfair. So we have AI that's in production for like search recommendation systems. It's things like visual search. So looking at a room and you have a certain couch or rug of a certain color and you want to see things that match. And it's like a shoppable room scene that is generated using color codes. Another one is like customer service. You know, how do you enable the customer service? And then we have operational maturity under tech spend management as well. And then the last is to deliver those customer service and those new revenue generating goals. We have developer experience in productivity. So we measure each of these. And so as we're looking at, well, who's the intended audience for the shoppable image? It's the customers. Are they happier? Are they clicking faster? Are they buying more? Or customer service, net promoter score, satisfaction to get time to resolution. And then on the engineers, the amount of code generated versus code committed and how many users are actively using it. Or if you're comparing different models or different providers, those are some of the metrics where we don't allow investment unless you already have those. So by making the teams say, this is what we want to build, everyone's trying to experiment. But if you're going to deploy and then scale and iterate any meaningful investment, you're going to have to be able to answer those questions. So those are some of the metrics and some of the things that probably follow up on a little bit. So do you try to apply a consistent metric across use cases or define a metric aligned to an individual use case? So there's like three or four perspectives. So it's relevant to the context of the workload. So you have like the intended user audience who the user story is for, and then the engineering team that built it. And then the business impact. Are we going to invest X people, staff, licenses, infrastructure to get what back? Is it time? Is it moving things faster? So there's all sorts of like dimensions of value. So it really depends on the use case and what we're trying to get out of it. It's multiple dimensions. It's not just one metric. Thank you. Ivan, how about you? Well, I think like everyone, we're focusing on embedding AI into all the core business functions. You know, product development, threat detection, automating the stock teams. But for each of these, like you asked, we have different KPIs because they basically are trying to measure different outcomes. From the KPIs to the outcomes, we define like a feedback loop, which actually helps us fine tune the KPIs because the challenge is finding those direct KPIs, as we're going to discuss later on too about. So really to fine tune those KPIs is probably the most challenging part of the whole process. I think Ivan, just regarding those KPIs, right? Just playing the devil advocate. What if, you know, how do you handle the situation when those KPIs aren't meeting your initial sort of return on investment needs? Yeah, so I'm a technologist. I love that question. I like to play with things around, right? So I'll give you an example. So when Palo Alto started embracing the latest large language models, we started with the AI Summit. AI Summit lasted for about three months and pretty much involved entire engineering organizations. It's pretty operationally expensive for us. It was driven by top down. There was interlock. It was driven by our CEO. There was interlock between all the business teams and some engineering to kind of get some outcomes. After three months, which was, you know, once or twice weekly we would meet together, this AI Summit, we had about 760 submissions for the ideas how we can use AI in Palo Alto networks. From these 760 ideas, we came down to about 14. And from those 14, they were debated at the leadership level. We ended up with only two back in the day that we implemented in the product. So how did we figure this out? Well, we looked into three major things. Number one is like, what are the core things that this potential idea can help with? So, for instance, is there a problem? Do we really need an AI to solve the problem? The second one had to do with a pivot and terminate. I'll tell you a second more about it. And the last one was a post-mortem. So what kind of questions can we ask as we go through the intake of the AI project so that we can shortcut some of the decisions and get to that faster? As far as the pivot and terminate, I'll give you a concrete example where we had an idea that we're going to use an LLM to create rules for a SOC operation. So basically, our alerting and what's going on within the enterprise. Those models didn't work for us. This was back in the days, maybe even today, they didn't work for us. But we didn't kill the project. We decided, let's use the same thing, but let's actually enhance our current rules to get to the better outcomes, to enrich them, to deduplicate them. And that was actually super successful. Great. Thanks very much. Appreciate everyone's thoughtful answers here. So this slide highlights the importance of measuring value, not only when you get to production, but at every stage throughout the AI lifecycle. And it's not important to just launch an AI-powered product. You need to continuously monitor. And Ivan, you gave some great examples of how that can change over time. So you need to have ongoing inspection processes to manage performance and operational metrics. So notice the different types of metrics here. Right? The business KPIs are your high-low indicators. Revenue, cost, savings, customer satisfactions, time to value, risk reduction, security improvement, and even sustainability. The second type is the cost KPIs. And those are more focused on the expense side. Right? So those are implementation, training, cloud and software costs. And then the third bucket is the adoption and operational metrics, which tell you how effective is your AI solution being used. User engagement, qualitative feedback, you know, thumbs up, thumbs down. Operational improvement, like reduction in handle times or increase in the click-through rates. So this is a great framework, but it needs to be put to the test. So let's get concrete. I'd like to go back to our panel again. Could each of you share a specific AI use case your organization is developing? Leslie, why don't we start with you again? Wasn't sure. You know, keeping you on your toes. Yeah. Thank you. So I'm going to use CodeAssist, right? Because it's a very exciting use case for CME as well as for Google and others, right? We have a group of 10 folks that meet weekly right now. And so we are doing a very probably more time and effort is going into this because it's kind of a big foundation kind of use case for us to get this kicked off. So some of the different like metrics or cost KPIs we're looking at, right, is the velocity of outputs, accepted rate of the code, training time, total usage time, time saved, satisfaction of the output. So we're looking at the evaluation of the by the users. A number like the elimination or the number of production bugs we have. Now, all those metrics are very specific to a developer. But that, again, gets back into the efficiency. And then the goal will be we can start to apply those are can we deploy products faster to get to revenue sooner? So we're slowly getting closer and closer. But the business value, again, for us is always going to come back to that efficiency. We're trying to get our developers today who can maybe code 1x. Maybe they can get to 2x. But could they get to 10x? And how do we apply Gen.ai and give them that power so that they can actually go work on a lot more things? And we don't, I mean, I'm sure like many of you, head count is always difficult to find. If anything, head count always seems to be shrinking versus expanding, right? So the more we can get people to do, the more product and closer to revenue we can get. So some of those metrics are, you know, very straightforward in terms of measurement, in terms of time to deploy. Others are much more subjective, like satisfaction. In either case, how do you go about establishing a baseline? What does good look like? Yeah, good is hard, right? And you've got to be able to always innovate on what good means. But, you know, from our perspective, it's surveys. It's working directly with the developers. It's historic. It's whatever piece of information we can put together. Now, again, in this case, and certainly we won't apply this to all use cases, but we are trying to do that level to define good so that good now exists as we go forward, right? Because if you can put the time and the effort into kind of your first couple foundation use cases, then you can apply it to almost anything. So that's how we're trying to define good. But it goes back to the feedback loop that Ivan talked about, right? You have to be willing to adapt and modify as you learn new things, you see new things, and continuing to define what good or that success is. Thank you, Leslie. Brent, how about you? Yeah, so we have that. It's like the, you know, shop by visual, right? You got a couple million products, and you're like, oh, I got this thing in my head. What do I do? Well, there's a second part of that that's really important, and it's the image metadata extraction. So the Gen.ai that is happening is go in and look at this image. So it's using an image pull to populate metadata and send out a JSON structure. And so that's what really drives all this data interchange, and then it powers that app on the back end. So that Gen.ai is saying, well, here's how much we forecasted it to be using the 1.5 Pro model, and then 2.0 Flash comes out, and then that's much cheaper. And if we want to custom and fine tune it, like if the only job is to pull the color hex code off of a sofa or rug or something, we could probably fine tune that and train it, run it on small parameter count models, and use our existing CUDS, and we don't have a capacity issue. So there's a lot of like engineering operations and strategy just going into, hey, what toolkit are you guys even able to use at the company? And then as FinOps, we kind of bring in advisory tactics and steps of like, hey, are you doing this? Or what about that? Or when that starts processing, it depends if it's powering production revenue generating at scale. They're probably not going to iterate instantly as soon as the new 2.0 model comes out, right? Because if you change the model, there's a lot of other things. You got auto judge and quality and reinforcement learning, and it's just risk, right? So at what expense is it worth to execute? Yeah, there's a little bit of cost savings. Or do we need to do that though? Is it going to be a risk to the revenue in relation? So these are some of the things that are, you know, as the business is getting more in tuned. If you lay down a good CICD automation pipeline, and you don't have your engineers hard coding the model, name and version. If you let the engineers build and say, hey, pick for me based on an auto judge thing, whether that's an API gateway or a proxy, or semantic routing for the workload with like threshold optimizations. Do you want to have these intelligent systems in charge of picking the best models that you allow it to or approved? So it kind of comes back to you kind of need to know what the technology is the engineers have available to use. And then is it prod? Is it revenue facing or not? You know, the appetite of risk. And so I think those are some of the bigger things I just want to make sure you call out too. So Brent, especially coming from an online retailer commerce platform perspective, how are you isolating those, the impact of the AI from other factors that might influence, right? Those metrics, right? Are you using A, B testings or any other methods? Yeah. So in the opening keynote, he said the cost of intelligence has come down 24x. And so that's exactly what we're doing is we're looking at these workloads saying, hey, the cost of intelligence is coming down, meaning to get 99% accuracy of the right hex color code off of that image, you know, let's say there's a million of them, it has to get it right 99% of the time. Well, it required a pro based model eight months ago. Will it use Gemma three open source in six months from now? Somewhere in there, right? So as we're looking at that, you have the test of either who wants to do the eval? Do you have a golden set? So these are all the ones we've already had the way we used to do it before, you know, with a lot of human evals. So there's like a golden training set, how good does it do? If that's good enough, then it goes through a review or reflection or react process. So the evaluation models combined with the technology advancements of the vision models themselves, both are accelerating. So that's the thought process too, is it's innovating so fast that how often are we going to go through like a cloud architecture review board, right? So it's spawning new questions. And then if you're looking at it, when do you plan that into the engineering sprint, right? So they have so many other things to do and not enough time and people. If your engineering sprint is delivering great, right? So there's always that trade off of other competing priorities. And then the last would be just the developer metrics. Which tool do they like? Are they feeling more productive? Are they using the tool every day? As they use the tool, is it self-learning? And that's the biggest thing is this memory. So as the memory is coming in or the fine tuning of your own models, that's really where I'm seeing more emergent advancements. And it's going to benefit everyone in the company if you can share and repurpose those things. Because a lot of the departments, everyone's kind of off doing their own thing. The biggest value is when we lay down good patterns or reusability. And so that's really good in like our documentation and having the documentation AI searchable now too. So we've invested in those other systems to help the knowledge flow around the company as well. Thank you, Brent. Ivan, how about you? Yeah, so Brent kind of alluded to some of them. For us, enhancing the product development productivity was probably the most important use case. Because we do believe that's kind of the current that lifts all the boats, right? In a way. And so on that note, we actually build a code co-pilots, but specific to the products that we are building. So those co-pilots, they actually understand the entire code base. Then they understand the PM requirements, the product management requirements. They understand the certain ways how the code needs to be written for the product. I mean, you can buy some of these off the shelf today, but this goes much deeper, right? So imagine when a new engineer developer comes to develop a product. In most cases, you know, it's going to take him or her a lot of time, maybe never, to understand the entire code base. Well, with what we build, actually, we get immediate benefits into, you know, this engineer can become productive immediately, can start writing a bootstrap in the code. The engineer potentially can write the code that he wouldn't think of because the co-pilot saw that somewhere before in a big, humongous code base and suggest a way to do it. Code reviews, you know, usually when you do code reviews, you need to have an engineer who's been there for a while, that understands the whole code. That's kind of luxury sometimes. So with these co-pilots, we can do it much quicker and then look into sort of potentially how many bugs were created by the code that was reviewed by the AI. So for us, that makes the most sense. So one of the metrics that tends to come up is the number of false positives. So how do you approach that and what role does your FinOps team play in kind of optimizing the balance? False positive, that's my life. So we benefited from the fact that our entire FinOps ecosystem was built in-house. So we kind of had engineers, they helped us with that back in the days. They weren't tooling, so we did it all ourselves. So we can highly customize our tools towards, you know, whatever we want to. So to give you an idea, today when we develop a new feature, whether it's incremental or the whole feature, we can actually attach our tooling, FinOps tooling to it. And we can see as the developer develops a feature, how much it's going to cost. And in the case it's going to cost a lot, we can decide, well, maybe that is not the best tooling you're using. Maybe you want to use something that will provide a better cost, but not as performance. There's always this fight between obviously performance, quality, and cost, and you've got to choose. Some of the features you don't have a choice, some of the features you do. Sometimes we use that FinOps to find bugs. I'm not saying this is the right way to do it, but I'll give you an idea. So yeah, the engineer can write a big query, which goes through a tremendous amount of data, which is going to cost us a lot of money. We can immediately duck that bug, but it's cheaper to find it before it goes to the customer, to the GA, right? So really, you know, that cost modeling we can do in the staging environment really helped us a lot with the new feature development. Well, thank you. So at Google, we certainly had a vantage point at the intersection of AI and FinOps. And this perspective has given us invaluable insights into what truly drive successful AI transformation. And being as a thought leader in this space, we have developed a comprehensive Cloud FinOps for AI framework. And we focus on enabling AI adoptions, allocating costs accurately, optimizing the models, understanding the various pricing structure, and reporting on those both costs and value. This provides the transparencies needed for those informed decision making. And this is the core of our framework. And we identified the five key pillars for applying FinOps for AI. First, starting with the GNI enablement. First, what that really means is making sure your teams have the right skill sets and the resources to effectively using those Gen AI models. Second, cost allocation is beyond just the LN model inferencing cost. It's also considering all the shared services costs that surround your AI solutions. Third, model optimization. This is continuously finding ways to reduce the cost of your running time of your Gen AI models without sacrificing the performance and the accuracy. Fourth, is the pricing structure. And understanding those different pricing structures for Gen AI services and choosing the most cost effective model. Because there's a lot more out there. You can use Flash. You can use 2.5 Pro now. So, understanding which model fits your needs is very important. And then finally, value reporting. Creating that clear and consistent reports that show both the cost and the value generated by your Gen AI investment. And it's important to call it that these principles apply across different modalities. Right? Everything right now is multimodal. So, where there's text, code, image, video, audio music, then dialogues, and so forth as well. So, Eric has just outlined the five key pillars for the Gen AI framework. Now, back to our panel. So, from your real world experience, which of these pillars present the most significant challenge in your organization? And most importantly, what practical strategies have you employed to overcome those challenges for adoption? So, Leslie, back to you. Thank you. So, I'm going to focus in on cost allocation. And the reason why I do that is it's no different than your FinOps practice. Right? So, again, if you go back to kind of what I said before, as we're right now viewing Gen AI, it's another resource, it's another workload. Right? So, how do you best do your optimization? You have to have the cost allocation. Right? And Gen AI is no different. Because, right, all they're doing is taking different forms of data. Right? So, at the CME, to give you a feel for what we've seen in the last couple weeks, to say, we have on a daily basis right now probably close to 60 billion messages that are coming through. And that is the data that we then can sort through and apply all different models and different things to. So, the question becomes, who are the users of that data? Right? So, one of the, I would say, the challenges and the hurdles that we have in our FinOps practice in general, and now you'll add Gen AI on top, is how do you allocate that? Who's using it? Who's querying it? What are the names of those data sources? And so, cost allocation to have a successful FinOps practice, in my opinion, is just one of those critical, fundamental, foundational things that we all struggle at as a team. And, you know, this will continue to challenge us in the Gen AI world as well. So, Leslie, what would you give advice to someone who's just starting to grapple around this cost allocation sort of framework? Yeah, stick to the core principles and the foundation that you have within your FinOps practice, right? Like, so, tagging is critical, right? Starting a FinOps practice from the beginning, right? It's much easier when you've built everything native, like these guys have. Well, I'm assuming almost native. You know, CME is in the midst of a major migration, right? So, we're trying to go back and do it. But, from the beginning, the tagging, the approach, you know, having those core principles, the education. I mean, the first pillar you have there is, I mean, we're all trying to upskill as quickly as possible. And the technology is moving. And it's hard to always be on top of it. But you always want to fall back to what that key framework you've built out in your FinOps. And, you know, that's where having the right team, the right collaboration, all those core principles that for any of you working within FinOps, and hopefully most of you are familiar with that term, you know, you are trying to apply now to the Gen.AI world that we're all operating in. Okay, Brent. So, at CME, cost allocations, the top challenge. How about at Wayfair? I think the piece that I'm mostly focusing on is the impact on forecasting of Gen.AI optimization. So, working in the FinOps group for the last year, we produced a couple of series of papers on how to cost it, what it costs drivers. But then the last paper just recently published was optimizing these Gen.AI workloads and its effect. Because if it's 24 times cheaper, or if it costs 80% less if you just change this one line of code to go to this new model, that's really where we're trying to provide an option for. Are you using a third party provider and you have to go do like terms? And, you know, you got to get more into the token usage. And you got to do various techniques for how that works from a contract, capacity, provision throughput, how you purchase it. And then there's things to do on the input and the output. There's not going to go into detail on those, but it's in the paper. And then if you're going with a cloud vendor where you're consuming like the Gemini models, or you're rolling your own with any of the 300 models in the model garden that they mentioned, there's tactics in each layer of like the core stack. And then there's things that are at the infrastructure selection and provision throughput. So we want to make sure that our rate card that we're giving the engineers, they shouldn't need an ML degree just to produce a quote, right? And so if we layer it in like these, hey, this tactic, if you, why did you use streaming? It's a batch job. If you use batch, the cost is instantly cut in half. Oh, I didn't know that, right? So there's an awareness and education mostly. So a lot of it is around awareness and education because these developers have to do so much. And they're upskilling a lot and it's changing a lot and they're not pricing experts. So us and FinOps, we're trying to make sure that we have really good, clear, up to date, simple rate cards that they can plop in. And here's the quantity on this model. Here's how many I'm going to do. And then here's my next model. I'm, I'm, I'm thinking I might use and push it out. And now they can at least have some impact and then they can play with scale, right? Or as we come in with a better tactic, hey, did you think of this? Right? So that's been very clear. And it's kind of the thing I'm focusing on because it unlocks, let's say someone had a, you know, couple hundred grand a month in something. Well, now if it's only going to cost seven, all these other projects could now be funded. We don't want to lock up that budget in the forecast plan, right? We want to get all our AI projects funded. So if we set a precedent early in the company that we seeing the price trends, keep up with benchmarks, watch the cost of quality. And if you know your quality score, you don't need Einstein to tell you if the chat was positive or negative, right? So don't overbuy your intelligence by defaulting to the most expensive models. Use just enough intelligence and which models can do it in that era of time. When we go back, you imagine a thousand engineers. Well, now all the projects are rolling up. So when you're going to, you know, deliver a board presentation, hey, this is what Wafer is going to deliver this year for AI across the entire company, right? You've got software base, you've got infrastructure, you've got core AI. It kind of spans broad use and focused use. So there's forecasting issues at the engineer level we're trying to educate, but then also how much is the right amount to spend on Gen AI? Well, you can't be off by 10x, right? And so we're all trying to figure that out right now. So as we're seeing the effect of, wow, this is really early days and we're seeing price trends, we're seeing optimizations just starting. I think those are the things that are top of mind for us. So by the way, if you're interested in some of the things that Brent's talking about, Brent and I will be doing a solution talk tomorrow that we're going to go into even more detail on this. So stay tuned. Ivan, so how about you? Yeah, so I think for us the value reporting was probably the most challenging and that's because there's really no, well, it's very hard to have entirely direct ROI calculations to figure out if your investment work out. You know, direct ROI calculations, they miss on some intangibles. For instance, like if I have a developer to developer productivity copilots we build, fix the bug and so the code works better, how many security issues did you fix with that? It's very hard to quantify. So you have to depend on some of these proxy metrics and then, you know, collect the feedback about them and keep reporting them up to find which are the ones that you want to fine tune. I kind of mentioned it earlier on and then use those to kind of figure out what's really the ROI for AI investments because it misses on all the intangibles. And also, AI projects often take some time to basically show the ROI on it, right? It's not immediate. It might take some time. So how do you figure it out? How do you kill the project when you don't really know what's going on? Thank you. Yeah. Yeah. That's the, go ahead. Okay, so I see we're about six minutes left, so we're going to try to get through the rest of our content here. This slide covers the CloudFront door process, which is a centralized process for managing both the technical evaluation and the financial evaluation processes associated with the development of any new workload. What we find is that in that many businesses, those processes are quite distinctly detached from one another and lead to disconnects down the road, such as financial expectations, impacts in the forecast and the plan. And by creating a tightly coupled process where for a new given proposed workload, there is a consistent process to measure cost expectations, evaluate optional models. It provides for a much more rigorous process and ultimately a tighter integration with forecast and planning processes. So the first step here, right, is assess your genetic readiness, and that is through the genetic framework I was talking about earlier. Second, you want to able to prioritize those use cases. And so through the CloudFront door process, we want to assess the financial viability and the genetic suitability. So when we think about the genetic use cases, oftentimes the TCO analysis is important, but more importantly, it's also understand what those value, right? And then once you funnel through those use cases, you then come up with a two-by-two matrix, which allow you to understand what's the impact versus the effort, and which is oftentimes a good tool for your prioritization process. So as AI adoption scales, and as we build this, you know, robust intake process, which become even more essential, how have you structured your AI use cases through this onboarding process? What are the primary hurdles you're facing in ensuring that the efficient governance and scalabilities, and how does this CloudFront door concept resonate with your approach? So, maybe we can start with Leslie again from CME. Sure. So we are not necessarily creating a new CloudFront door where we are at this point, but we do essentially, I mean, front door is kind of a naughty word, right? I mean, we're all trying to move away from that in the new product and operating model. But we are trying to build this all into our operating model, right? So as teams come forward, they need to build an epic hypothesis. That epic hypothesis must have business outcome. What is success? What does that look like? Then you move on to the lean business case. So for us, it's building it into the operating model and using the same processes that we have in place, just adding Gen.AI as a work load resource. I mean, they need to identify all the same key things. So we have taken it more from that approach, Eric. Thanks, Leslie. How about you, Brent? Yeah, it's interesting. I think it depends on where it's coming from. So from, let's call it like senior tech leadership and finance, there's only so much money. So what's the right number? How do you even get to the right number, right? And then how is it distributed against the way you work today? Is it labor efficiencies? Is it code and developer experience improvements? Is it operational maturity, right? Are we targeting suppliers? Are we targeting customers or both? So there's a lot of different aspects where they know how they operate today and all of it is great. AI is going in everything. It's going in your SaaS. It's going in your tools. It's coming in your cloud providers. So as it trickles in, the innovation is coming in from top down to say, hey, these are the things I think are going to move the needle, right? And then at the bottom where we have the engineers and they have these ideas, they can code them into pilots or into working MVPs in days. And I think that unlock is still yet to be felt. We are at a point now where you can speak and an app just shows up with a working demo. So as an architect, I think that to me is like the most fundamental piece of being able to explain and show. And it can accelerate the time to value bottom up and top down. So as it comes together, I think that's the piece that this front door aspect is great. So you got 300 projects. Typically, management is going to scatter plot these out, right? Operational maturity, customer service, revenue generating, right? So they're going to scatter plot those out. And like you say, big bets, don't do these, right? So it really helps folks visualize how much stuff do we need to kind of prioritize and sort out. So it's a little bit better in the strategy planning cycle. So that's kind of what I like. Thank you, Brent. Real quick, lightning round, Ivan. Lightning round. We embrace the Cloud Front Door concept heavily. We did it mainly by doing two things. We created a model team which intakes all the AI projects. And that benefits from seeing what is happening in a company. If you can combine projects together and create some other value out of it. And secondly, we created the kind of a common pipeline, CSD pipeline, that all the AI projects go through, which allow us first to isolate and make sure the security, you know, top notch for everything that we built. And secondly, use the latest, greatest tooling that we already established in architecture. And so we know that everyone in the company has to figure this out. We can figure all that for them. All right. So let's just wrap up. So what are the critical next steps? One is we talk about the executive alignment, ensuring that your leadership and as every single leader in this room today, right, that you have those strategic goals and aligned to your expected outcome. Second, value discovery. So we do a number of workshops with our customers to identify opportunities and solutions. Third, create that roadmap, right? It's very important to link your cloud AI transformation adoption strategy to your business impact. And finally, capturing the delivery of those solutions and capturing those value. Well, we want to thank you for attending today. Feel free to scan this QR code to download our latest white paper on Cloud FinOps for generative AI. And if you want to have some questions, we'll be on the side after the presentation today. So thanks for your time. Thank you. Thank you. Thank you. Thank you.