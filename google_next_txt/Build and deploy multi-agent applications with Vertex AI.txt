 Welcome and thank you for showing up at 8.30 a.m. on day three of the conference. We really appreciate it. I hope you guys have had enough caffeine and you're energized to talk about how to build agents on Vertex AI. In this session, we're going to talk about how to build and deploy multi-agent applications on Vertex AI. My name is Anand Dair. I'm a product manager at Google Cloud. I'll be joined by my colleague Derek Egan, also product manager at Google Cloud. And we're super excited to have some industry leaders joining us this morning. We have Brian Goodman, director of AI at Ford. And from Revionics, we have Akriti Bhargava, VP of engineering and AI, and Alex Braylon, director of AI. In terms of the agenda, we'll start by giving you an overview of Vertex AI agent builder. And we will double click on three specific components, the agent development kit, agent engine, and agent garden. After that, we'll hear from our friends at Ford and Revionics about the agents that they've built using the tools and services from Vertex AI agent builder. And there are two demos in there as well. So it should be fun. Let's get into it. First, let's actually define what is an AI agent. These gentlemen here are AI agents, if you remember from the movie The Matrix. And it's actually, it blows my mind that The Matrix at this point, the first movie, is more than 25 years old. Can you believe that? And if you haven't seen The Matrix, you must. Timeless classic. But coming back to actual AI agents, if you speak to someone from marketing, and no offense to anyone from marketing, they label any generative AI application as an agentic application. Because these days, agents are the topic du jour. So any generative AI application gets labeled as an agentic application. But we are building a platform for others to build agents. So we had to get a little bit more specific in the definition of an AI agent. So let's get into that. First, let's actually quickly look at the evolution of generative AI leading up to agents, because it provides good context. First, of course, we started about a little over two years back with just LLMs and prompts. You send it a prompted response. Then we realized, you know what? If only these LLMs could be grounded in very relevant real-world information, they'd be so much more useful. So that's why we had RAG and grounding and became near ubiquitous in generative AI use cases. And then, I think around mid-2023, we realized that these agents or these LLMs are actually capable of using other services as tools. And what I mean by that is, if you give the LLM definition of the APIs and functions used to invoke other services, and you have to give them detailed definitions of the APIs and functions, and then you give the LLM a task, the LLM can actually tell you for that task which API to call with what parameters. And that was critical, because now these LLMs enable you to build applications where you can actually do things in the real world, such as, I don't know, book a flight and so forth. So that's where things started to become agentic, because LLMs can actually do things in the real world. And then we realized, and this is, I think, like late 2023, that these LLMs are able to reason. So given a task, they're able to break that task down into the subtasks and come up with a plan to achieve that task or goal. And that gave rise to what we sometimes refer to as the agentic loop or agentic orchestration, where we now use an LLM to come up with a plan and execute the steps of the plan. And with that, we had full-blown agents. And then we realized, hey, why stop with one agent? If we have multiple agents, bunch of specialized agents all working together, it's even better. You can build more sophisticated applications. So that's where we're at today, the world of multi-agent applications. So with that context, let's actually further get crisper about the key ingredients for an agentic application. Of course, the main ingredient, the LLMs. We're using the LLMs for their ability to reason and come up with a plan for achieving a given goal, given a set of agentic definitions and tools. And of course, we have the tools that we provide agents. And as I mentioned, tools are specific APIs and functions that are available to use external services. Then we have the definitions of the agents. And for the most part, the definition of an agent is authored in natural language where you specify the goals, the persona, and detailed instructions about how the agent should behave, what it should do, what it should not do. And you also specify what tools are available to the agent. And then we have a very critical ingredient which, for lack of a better term, we call agentic orchestration. And honestly, if someone has a suggestion for a better term, please talk to us after this presentation. But agentic orchestration is this body of code which has the logic to call the LLM. It's given a goal or a task, calls the LLM and says, hey, give me a plan. Here are my agents and tools. Then it parses the plan that the LLM returns and executes the steps of the plan. And every step of the way, you execute one step and then you call the LLM again and say, hey, this is, I performed step one. This was the result. What do we do next? Because sometimes the plan would need to be adjusted based on what happens. And this orchestration code is also responsible for the actual tool invocation and also maintaining intermediate state and what we call agentic memory. Because with these agents, these are not single short tasks. These are multi-step tasks. And also increasingly, people have long interactions with agents. So this orchestration code is also responsible for maintaining intermediate state that is pertinent for achieving the goals. So these are the key ingredients, typically, for an agentic application. There's a lot of stuff. Building agents and deploying them in production is hard. It's not trivially easy. And we aim to make it easy for you. Our goal is to deliver the industry's most comprehensive enterprise-ready agent development platform. And we call that the Vertex AI agent builder. And let's take a 10,000 or 100,000 foot view of the Vertex AI agent builder. It consists of three key pillars. First pillar, no surprises, the models. Like, duh, you need the models. And over here, I have to give a shout out to Gemini 2.5 Pro, which has amazing reasoning capabilities for agentic use cases. But we also have a model garden where we curate a bunch of high-quality LLMs like the recent Lama 4 LLMs are available in our model garden. The next pillar is a set of tools and services we provide to developers to build and deploy agents. And this pillar is what we're going to focus on for the majority of this presentation. It consists of the agent development kit, agent engine, and agent garden. And the third pillar I want to highlight are the set of tools and services to connect your agents to what we call the enterprise context. Your enterprise data, whether it's structured or unstructured, your enterprise APIs, your APIs running in your backend services, as well as SaaS applications that you have running within your organization, the agents you build are ultimately going to be useful if they can tap into this full enterprise context. So we provide a set of tools and services for you to connect the agents you build to your enterprise context. And all of this is wrapped up in full enterprise, comprehensive goodness with security, governance, and safety. And I do want to highlight that the agentics, applications, and services that we provide out of the box in Google Cloud are built on top of our framework. No surprises. We use our platform. With that, let's dive into the agent development kit. It is open source. We open sourced it about two days ago. We aim to grow a community around it. Currently available via Python SDK and with more languages on the roadmap. Agent development kit itself, it's a pretty broad topic. And in the interest of time, we cannot give you a full tutorial on the agent development kit. I'll have to refer you to the documentation. I want to touch upon the most sort of distinguishing capabilities of the agent development kit. Let's start with the most important one. It helps you build multi-agent applications, and it handles what I refer to as orchestration across these multiple agents. So it actually handles the full plan and executes the plan. And not only that, this comes with a nice client-side visual UI where you can actually observe in full detail what sub-steps were executed given a particular task or goal. And this GIF here is actually a screen capture of the UI that's available with ADK. And as you can see in this case, there's a root agent that transferred control to a flight agent. It used a particular tool and the flow of control went back. Full observability into what's happening. And it handles all the orchestration, memory, and so forth for you. Another aspect I want to highlight is the ability to interleave deterministic logic with LLM-driven reasoning. Why is this important? LLMs are really remarkable with their reasoning capabilities, and they'll only improve. But when you go from, say, demo to production, there are all these 101 edge cases, highly nuanced situations that the LLM's reasoning may not get right out of the box on its own. What do you do in these situations? These situations, if you can inject deterministic logic that is seamlessly interleaved with usage of LLM reasoning for executing the plan, then you can address all of those edge conditions and nuanced situations. That is something we aim to provide with the agent development kit. And I'll be the first to say that by no means have we solved this problem. There's a lot to do. But we're taking very meaningful first steps in that direction with the agent development kit. Sorry. It has built-in support for multiple tools, including tools that actually are async tools or long-running tools, as well as built-in support for building human-in-the-loop agentic applications, which is actually fairly common. I want to highlight that agent development kit is not... It's actually built to be model-agnostic. While we recommend, you know, the out-of-the-box default are our Gemini models, and you'll see amazing results with our Gemini models, agent development kit itself is built to work with multiple LLMs. You can use your LLM of choice. And the last thing I want to highlight is a topic which is very much top of mind the past couple weeks for folks. Agent development kit has support for the MCP protocol, so you can build a multi-agent application where you can have agents talk to fully remote agents via MCP for complex agentic interactions. All right. So that's a lot of functionality, and you must be thinking, oh, man, that must be a fairly complex SDK to use. The reality is it's not. It's actually very elegant, simple to use. We focused a lot on reducing boilerplate code. In most cases, when you're defining an agent, you will spend most of your time in your natural language description of the profiles and goals and instructions for the agent. That's where you're going to spend most of your time. You'll have minimal code around that to write. So if you have an agent, you give it a list of tools, and in most multi-agent applications, you'll define an agent, and then you'll specify a list of sub-agents that it has available to it. In most cases, honestly, very minimal boilerplate code, very elegant SDK, and you've received amazing feedback. So with this SDK, you can build multiple types of applications. You can certainly build sort of the most common type of agentic application where customers interact or users interact with an agent via like a chat interface. You can also build offline processing or background processing agents. And this is really compelling. It's really cool. You can build agents that users can interact with with just a back-and-forth, what we call, bi-directional audio-video streaming format. Just have an audio-video back-and-forth with the agent. And the best part is to build such agents, this back-and-forth audio-video bi-directional streaming, you get that capability out of the box. You do not have to write any extra code to get that working. Really proud of this. Amazing feedback. And if you folks remember the keynote from two days back, there was this demo where the person shows the agent, hey, here are some flowers I purchased. Can you give me recommendations for potting soil or something like that? The person was actually talking to the agent. That's an application built, legitimately built and working. There was a live demo on ADK with Gemini. So we're super excited about this. And last piece I want to highlight before I turn things over to Derek. I want to talk about, I want to highlight a few key tools and services we offer for you to connect your agents to your enterprise context. To connect agents to your private APIs, we recommend using the Apigee API hub. To connect agents to 100-plus SaaS applications that might be running in your enterprise, we recommend using GCP integration connectors to connect your agents to these services. If you have business workflows that are running, we recommend using GCP application integration. And to connect your agents to databases, we recommend using a sister open-source project called the Gen AI Toolbox. With that, I'll turn things over to Derek. Thanks, Anand. As you can tell, we consulted the same agent when deciding what to wear today. All right. We talked about the agent development kit and the agent builder. Let's double-click into a demonstration of a data science agent. This data science agent is going to help us with a Kaggle competition where we're going to predict sticker sales. We'll use the local UI that Anand just introduced and look at the definitions. We can see this agent has a couple of different sub-agents and several tools that will get data, analyze it, make predictions, and help us understand some next steps. We can look at the definition of individual agents, the tools used, the model used, and its sub-agents. So now that we understand the context of the agent in use here, let's actually dive into a conversation and see what it can do. First, let's plot sales over time, and you can see the agent taking multiple steps and streaming its thought process back to us, and then ultimately giving us this line graph with weekly spikes in sales, which the agent recognizes and calls out in its insights. Now let's go inspect some of the individual steps that the agent is taking. First, the agent calls the database agent and asks it to summarize the sales per date. Then the supervisor passes this summarized data to a data science agent and says take the input data from the database agent and create a line chart that's going to show us the daily sales. The data science agent generates this chart and the insights that are shown below in this explanation. Let's continue to explore the data and ask it to plot sales per product. You can see the database agent recognizes the proper table, and the data science agent generates this bar chart. So let's move beyond our conversation and go into making predictions by first training a model. So this data science agent gives us a few options that we can pursue in order to train a model to make predictions. We can look at each of these options, weigh the pros and cons, and then say let's go with version 3. The agent then makes a detailed plan with the specific BQML code that will be used to train a model along with very specific parameters. We take a look at the code. It looks good. And we say let's go. Now, the agent executes this plan and discovers, hey, I've made a mistake and is able to self-reflect and say the max order value that I recommended of 6 was too high. We didn't make an adjustment. The maximum order value should be 5. So let's make that adjustment. It makes the adjustment to the code that's written for BQML. We can take another look and ask it to proceed with the updated plan. Now, an ARIMA model is trained without me writing a single line of code that can be used to now predict future sticker sales for this Kaggle competition. So let's take this model and put it against the evaluation data set from the Kaggle competition and see how accurate it is. The agent will take the evaluation data set, make predictions with the model that was just trained, and store that data in a table in BigQuery. Once that table's been generated, we can go over to BigQuery and see the table. We can see the number of rows in this data set, and we can export this data to a CSV file, which is what's required for the Kaggle competition leaderboard to understand how we're performing versus all of the other submissions. So we'll head on over to Kaggle. We'll upload our CSV, and the leaderboard will give us a score, and we can see the public may of a .12. So we've trained a model. We've understood data. We have a submission. Let's look at the multimodal live API with the bidirectional support in the ADK to take a look at what we've done so far and see if we can get help from an agent on some next steps that we can take to improve the model. In the last part, we will show the live data science agent that we integrated into our custom frontend. The demo highlights how we can use the multimodal live API to enable live interactions with our DS agent, including voice chat and live screen sharing. Let's dive right in. Do you see my screen? Yes, I can see your screen. It shows the Kaggle forecasting sticker sales competition submissions page. Is there anything specific you'd like me to do or tell you about what I see? I've trained an ARIMA Plus model in BQML and I submitted this to the Kaggle forecasting sticker sales competition. Do you see the private score that we achieved with this model? I see that the submission, submission underscore v3 dot csv has a private score of 0.15005. Is there anything specific you'd like to do with this information or any questions you have about it? I do have a few questions because now this score is in the top 40% of the submissions. However, if you can see here, can you tell me what you see as the highest score? On the private leaderboard, I see that the highest score is 0.04450. Exactly. Yes. And now I would like you to kind of help me come up with a plan of what steps I can take to optimize this further. Okay, I can help with that. Since you've already implemented an ARIMA Plus model with holiday effects and you haven't done any feature engineering yet, that's a great place to start. Here's a plan to optimize your results further. One, feature engineering, date-related features, extract features like day of the week, month, quarter, year, and day of the year from the date variable. Lag sales data to count. Yeah, I understand. No, that's fine. Thank you very much. Excellent. So this agent has helped us with a simple screen share and a conversation, analyze the submission into Kaggle and some concrete next steps with some feature engineering to continue to improve the agent and climb the leaderboard in this Kaggle competition. So that's similar to what you might do when you're defining an agent. You'll iterate, you'll improve the quality, and once you're ready to take it into production and use it at a much larger scale, that's where the agent engine comes into play. The agent engine is our fully managed runtime that helps you dramatically simplify a path from development to deployment of agents. Let's dive in to how this works. If you look at the right side of this slide, that's where all the agent definition takes place. In an agent framework like the agent development kit or a third-party framework like Lane Graph or Crew AI, that's where you're going to define an agent. Those agents connects to the proper tools and models to get your work done. And once you've defined an agent that you're happy with, you move it into production, which is in the middle of the slide here. The agent engine essentially provisions a fully managed runtime that you can invoke to use your agent in your application and have CRUD access to the agents that you deploy into production. There are multiple features that come with the agent engine to help you scale out of the box for agents in production. A key feature in the bottom middle of this slide is observability to understand tracing, logging, and monitoring for you to understand the agent behavior to help you iterate in production. And that's through an open telemetry integration. Moving to the left side of this slide, you can call an agent deployed onto the agent engine from any endpoint. It could be your application. It could be your website, wherever you'd like to invoke it. If your company is using agent space for your employees to consume and interact with agents on a day-to-day basis, you can take this custom agent that you've defined and deployed onto the agent engine and register it within agent space. If we move beyond monitoring and deployment, we can see that the agent engine also helps us measure and improve quality. First, the agent engine is integrated with the Vertex AI evaluation suite. This helps us measure the quality of an agent and its responses versus a specific data set, a golden data set that we've prepared, or through an automatic rater from an LLM to help you understand how your agent is performing. Next, the agent engine helps with critical context management. For sessions, it retains memory of an individual conversation between an agent and a user, and the managed memory helps store summaries of those individual sessions that gives context to an agent for a specific user so that agent can personalize and improve interactions with an individual user. Finally, the example store in the agent engine is something that you can use if you run into edge cases where the agent is not performing as you'd expect, and you can save an example to the example store, which will be used to improve future interactions with that agent. Okay, that's enough talking. Let's see if we can show you a little bit of how this will work. Very soon, in the agent engine console within Vertex AI. Within the agent engine console, we'll be able to see all of our deployed agents, and we'll take a look first at our data science agent. We can monitor performance, including queries per second, the request count, and utilization. We can review past sessions with agents and users. We'll dive into the first one, and we can see a playground with a history of the prompts from users and the agent responses, including our data science agent that we just looked at. We can ask another question, like plotting the sales per product, and you can interact with the agent here to understand the bar chart again of the sales per product. We can then inspect through a full trace the individual steps that the agent took, the subagents and tools that were invoked per step, and you can inspect the inputs and outputs of the tools and the subagents and understand how the agent came to its conclusion and the output. To improve performance, we can save an individual interaction to the example store and make a correction, like this function call, for the expected outcome that we would like from this agent. We save it to the example store, which is then automatically saved to the agent and used to improve future interactions with few-shot learning. We can look at the public URL, well, the URL that we can call to invoke this API and some settings, and then we can look at the trace, which will give us detailed traces per LLM call, including the latency and the token usage. Okay, so that's a little bit about what's coming soon within the Agent Engine console. If you're curious about how do I build an agent or how does this work, I'd invite you to go over to the Agent Garden, right now available in the Google Cloud console, and it's a set of samples and tools that will help you discover and learn how you can build an agent today. And for that, let's switch over to a demo. Okay. Okay. All right. So the first thing I'd like to show you is if you come into the Google Cloud console within the Vertex CI section, look at the agent builder and come into the Agent Garden. You'll find samples of different agent implementations and tools that can be used within the agent that you're building. This data science agent sample is the source code that we've been showing you today for the data science interaction. You can see its capabilities, some of the use cases that our private preview customers have extended this sample for their individual use cases, and again, the topology that we've been looking at. If you open up GitHub, you'll have access to the source code that you can copy to your own repo and get started. I'll call out this customer service agent as well. This powered the flower demo. It was the base that powered the flower demo in the keynote that Anand had mentioned in TK's keynote a couple days ago. Now, moving down to tools, you can see a sampling of the tools that are available when you're defining agents in the ADK. Let's take a look at the custom APIs that I've registered in my API hub. Just with a few lines of code, I can enable that private API to do work within an agent that I define in the ADK along with specific documentation links to help me learn more. I can also search for specific use cases that I'm interested in, and we can find one of the common enterprise applications that I can quickly connect to my agent through the agent development kit and specific links, again, that I can follow to define the agent with this set of tools. So the Agent Garden is a set of samples and tools to help you discover and accelerate your agent development, and it's available right now. That's the end of the demo. So we've looked at the Agent Builder. The Agent Development Kit helps you define an agent. The Agent Engine is what you use to deploy it and run it into production, and the Garden is what you use to go learn and discover and get started today. I'll turn it over to some customers of ours who've used some of these products to build agents for their use cases. I'll turn it over to Brian Goodman, Director of AI at Ford. Thanks, Derek. Okay. Well, at Ford, we're really proud to have millions of customers of our products and services, and sometimes those customers have questions and need to find information. And so about a year ago, we embarked on improving, greatly improving, the ability for customers to do exactly that. And we started out with what I can't believe is now, I'll call it legacy technology, and that's retrieval augmented generation. And we enabled our owner's manual sites, our online owner's manuals, with LLMs combined with retrieval augmented generation. And I think the result has been quite good. We're very happy with that. But that, we've now moved on, and that's now going to be an example that I'm going to use today of just one of the tools that our customer assistant agents have access to. So it's not just looking at owner's manuals. Our Ford.com website has over 17,000 pages. There's a lot of information out there for customers. Everything about service, towing guides, towing calculators, just all kinds of information. And we find that it's actually overwhelming. So where do I go to find information about something like, how do I change my windshield wipers? And there are specification guides, there are service guides, owner's manuals, all kinds of things. So we found that with building agents, we can make that whole process much easier and much more pleasant. So building on the success of the owner's manuals, we've started to use the agent development kit and then deploy agents using Agent Engine. And I'll show you a quick example of that. So an overview of what we're doing is, again, using both ADK and Agent Engine. We're building agents that have access to all of these tools. And just, this is an example list on the right. And as I already mentioned, things like, say, towing calculator or the warranty and service guides. So let's get right into a prerecorded demo. And, of course, the UI that I'm showing is the developer's UI. It's in the agent development kit. What the customer would have as a user interface would be quite different. But I think this is more interesting for today. you get to see a little bit more behind the curtains what's happening and how the agent is working. So if we start out saying, well, I have some questions about the Ford F-150 truck. The agent says it's happy to help with that. And then I can ask it, well, what if I need a truck that seats five comfortably, can tow 10,000 pounds, I'd like leather seats, I want to be able to drive well in snow and mud, I have a certain budget. And, of course, the agent comes up with a nice plan. It uses reasoning. And, of course, the nice new or newer large language models are quite good now at reasoning and coming up with a good plan. And so we can see it does exactly that. It figures out what tools it needs to use. And through this UI, we can see exactly what is happening throughout in these steps. and it also, we can control things like should we have the agent execute sequentially or in parallel? Because one might be better for scaling, the other one might be better for response time. And as we go through, we can see, again, how it's thinking, what the results are, and it's very easy with these tools to observe what's happening, to test, and really have fairly robust development practices. So I'm going to ask a question now about seeing the wheel design. And in this case, it's a fail. I wanted to see some pictures of the wheel design. I didn't get that. So we'll try it again. And we can see why it works or why it does not work. I think in this case, it was looking at the wrong source. So when it went to a much better source, it immediately found a picture of what I wanted. And in just a second, it's going to go and I'll show some of the tools that Derek and Anand showed as well. Really nice capabilities for traceability and seeing what's happening throughout, being able to inspect many of the intermediate steps, the intermediate prompts that are generated, and just excellent capabilities in that regard. So now when we start to inspect, we can see the graph and figure out, okay, what tools was it considering? What tools did it step through? And then we can inspect further what were the specific prompts? So again, we can see exactly what was happening and it makes the whole process really nice, really slick. So with that, I'm ready to welcome up the team from Revionics. I think we have Akriti and Alex. Hi, everyone. So my colleague Alex and I are going to talk to you guys about what we've been doing at Revionics. We're building a multi-agent pricing system and we're going to do a quick demo. But before we do a quick demo, I want to talk about three things. I want to talk about who we are and what we're doing. I want to talk about what the pricing agent system is and the value delivered. Right? So just a little bit about Revionics. We are the worldwide leader in retail pricing. What does that mean? We help our customers that are retailers, price products, predict demand and price products. We optimize over $500 billion in revenue and work with top retailers globally. So just getting to the meat of it, right? What is the pricing agent? Alex is going to talk about it a little bit, but what we do is we're going to automate the entire pricing workflow. The second thing is Revionics has been doing predictive AI for the last 20 years and we've coupled that with Gen AI to elevate our price recommendation outputs. The third thing is we've built it using Google's ADK and deployed it on Cloud Run soon to deploy on Agent Engine. I want to talk a little bit about the pillars because I think we were thinking about four pillars when we built out our multi-agent system. The first one is performance. We want to make sure that the results retrieved for the user are quick. The second one is accuracy, right? We want to make sure that the results retrieved are correct, the transfer of agents is correct, and the tools that are being called are correct. The third one is transparency, and you'll see this a little bit in the demo, is we ask the user for confirmation before we take an action. We want to make sure we're doing the right thing. And I think the last one is modularity. I think we're still figuring this out, right? I think multi-agent architecture is pretty new. We're figuring it out by trial and error, but I think the main thing we're thinking about when we talk about modularity is how do we use different agents within different aspects of our product and our Gen AI experiences. And lastly, why it works, right? So we build tools with robust responses, so if a user asks something and doesn't give enough information, the agent can respond back with like, hey, can you give me this additional information? Abundance of examples, that makes sense. I think the third big one is just the nature of what we do. We have to pass data between multiple agents, and so we've done that through artifacts rather than the context. I think the fourth big thing which I think Anand also spoke about is just the multi-agent transfer and planning has become super easy with Google's ADK. And lastly, I think we wanted to reduce hallucination by constraining what the LLM can use. And what that means is, so for example, the data agent. We haven't told the LLM, hey man, this is our entire data schema. We've given it small goals in data sets so that we can maintain accuracy and the four pillars that we've talked about. And, you know, value delivers. We aren't just building this because it's fun. It actually delivers value to not only, you know, our customers, but also us. So let's talk a little bit about our customers' retailers, right? So merchandising and pricing teams are small. We want to help them scale. We want to help them do a lot more in a short period of time and increase their operational efficiency, and that's what this helps us do. The third one is quick data-driven decision making. So when they're in our tool, they might be going to different dashboards or to Excel to kind of do some data crunching. We've helped them kind of just be within our tool. We have a conversational analytics text-to-SQL agent that we help them kind of just get natural language to data pretty quickly. And the fourth one is improved user experience, and I think this is super important because every person who uses our tool will have a different workflow that they want to automate, and what this helps them do is actually figure out their own workflow while staying inside the entire tool. And the benefit for Rebionics. We have increased our user stickiness by helping them do everything from one window, which then increases trust and then thereby adoption. The second one is reduce time to market, right? So we've built a lot on the back, and everyone knows there's tech debt, everyone knows front-end, back-end. It's hard to ship a feature quickly, and based on how quickly we're moving on the back-end, this helps us reduce time to market for our new features. And the last one, I think, is the most important one, which is revenue growth through new TAM. So what the pricing agent is going to help us do is going to help us unlock new markets just based on the, you know, the functionality of it. Cool. So now I'm going to call Alex up on stage to do the demo. But before he does that, he's going to talk a little bit about the agent architecture. Yes. Thank you. Oh, great. Thank you, Akadi. I'm Alex. I'm the head of AI at Rebionics. And I want to talk a little bit before getting to the demo about the overall architecture. So it is a multi-agent system. It does not engage in intense Kung Fu battles with the last remnants of humanity. It does something even cooler. It helps retailers price their products. So here's the org chart over here. You see there's a pricing agent. That is the orchestrator. There are three child agents, the data agent, the action agent, and the help agent. And there are a lot of other cool things that I'm going to talk about, but I'm going to do it in the midst of giving the demo. So why don't we switch over to that right now? Great. Yes, feast your eyes on the Rebionics landing page. This is where retailers, their merchant teams, their pricing teams come to set price strategies and see the impact of those. So I'm going to show the agent here. This is the agent that helps them do that. Let's say hi. Make sure it's online. This is a live demo. So good. There it is. Now let's go through a very simple example of what a retailer might want to do. So all retailers care about staying competitive in the market, right? So I'm going to ask it this question or give it a command, really. Can you please find the products where my competitors were cheaper than me and lower the prices on those products? So let's see what it did. Expand. So the first thing that happens is the pricing agent, the orchestrator, it does a bunch of planning. And so what you can see here is it realizes that in order to do this, it requires the data agent, data retrieval, and also action execution. So it says, let's transfer first to the data agent. The data agent is our specialized natural language to SQL agent. It takes a request from the user in English, in this case, and it translates it into SQL, executes it against BigQuery, and then gets data back. A nice thing that it does, as Alkadi mentioned, is that it's transparent. It explains, this is what I'm going to do in case there's ambiguity in the question from the user. And so here we go. Here's the data that came back. In this case, you can see the retailer's price and the average competitor price for all these products. And so, oh, and then it asks for confirmation, and then it says, I can transfer you to the action agent. The action agent calls our tools, our internal APIs that do a bunch of things. So before I do this, I want to explain a few things that are cool about what's about to happen. In this example here, the data that is retrieved by the SQL, text-to-SQL agent, is pretty small. But in a lot of cases, it could be very large, and you don't want to pass all of a humongous amount of data directly into the prompt of an LLM. So what we use is in-memory artifacts that the ADK provides. So this is a link to a data file that's in memory that then is processed by the internal tool that I'm about to call. All right. And the other thing is updatable context. So there's metadata like the names of the columns and stuff that also get passed to the new system prompt of the data agent. I'm sorry, the action agent. So here's what happened. The action agent just consumed a payload and called our internal API. Our API does a bunch of things. It applies constraints, changing prices, and then calculates an impact to financial metrics like revenue and profit based on elasticity, numbers that are estimated, and advanced by our demand modeling. And so, you know, I'm a retailer and I'm seeing, okay, like, what happened here? For example, this Tamanishiki super premium rice that was $44 at my competitor. If we bring it down to $43.55, it's got this impact. Let's say, I can live with that. In fact, I could even make it even, let's undercut our competitor even more, bring it down 2%, and then it generates an awkward silence. And then here we go. The results, even cheaper price, and let's say I'm the retailer, I'm looking at the impact. Yeah, I can live with that. Let's go. Let's compete. So the last thing I'm going to show is a transfer to the help agent. Now, I'm going to ask it this question. Tell me what ending numbers are. Most of the time, if you work in retail, you know what ending numbers are, but for the sake of this demo, I'm going to pretend I'm a new hire, and my boss told me to always apply ending numbers, but I don't know what they are, so I'm going to ask the Revionics agent, hey, help me out. Explain ending numbers to me. And so it transfers to the RAG agent, retrieval augmented generation, that looks at our documentation, our user guide, and comes back with an answer explaining what ending numbers are, and this has some psychological impact on the consumer. I'm going to assume that that's great, and then I say, do you have the capability to apply it? Again, for the, sake of this demo, I've actually not given our action agent the ability to apply ending numbers, just to show that we've prevented it from hallucinating and pretending that it can do things that it actually cannot do. And so it says, I cannot, I do not have the capability to apply ending numbers. Shall I just send you an email? Well, so I say, fine, I can live without ending numbers, but just send me, just send me the prices, and done. So in that very brief interaction, I've helped my retailer lower prices below competitors, and that's time to information, time to action that's saved by this multi. And automate their workflow, right? And automate it, yeah. Thank you everyone for coming. That's time, but we're excited to see what you build with the agent builder. Thank you. Thank you. Thank you.