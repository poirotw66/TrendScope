 My name is Ben Gittenstein. We'll go through the intros in just a second. We're going to be talking about efficiently scaling storage for AI and analytics workloads with hyperdisk. We're going to be talking a lot about block storage. So hopefully in the right spot. My name is Ben Gittenstein. I am the product lead for hyperdisk and we'll talk all about what hyperdisk is and how it's useful and how you can use it. I'm joined today by Ragnar and Pierre who are two amazing customers. We're going to talk to us all about how they use hyperdisk to power their next generation AI and analytics workloads. Really excited to have you all here today. We're going to try to have room for questions at the end. But if not, I will definitely stick around down here. So if I don't get a chance to answer your question in the audience today, I'd love to connect with you afterwards one on one. A little bit of an overview of how we're going to move through this, what this presentation is. We're going to start with hyperdisk itself. What is it? How do you use it? What have we done lately in hyperdisk? Then we'll take a little bit of a detour and we're going to talk about how AI and AI workloads and large scale analytics workloads are changing the way customers provision, use and manage block storage in the cloud. So we're going to talk about some different approaches that we're seeing customers adopt. Then we'll hear from Pierre and Ragnar. Pierre is going to talk to us about what they do at Sentry and also how they are unifying a diverse set of data sets into one unified data layer using block storage, which is hopefully pretty exciting. And then Ragnar is going to talk to us about what they're doing at HRT, how it's in river trading, and how they use storage pools along with a bunch of other compute products here at Google to build dynamic infrastructure that can go up and down at will and also reach really high scale. And then finally I'll come back and like any good product manager talk to you about what's next and where we're going and get you excited about the future of hyperdisk. So with that, I'm going to start talking about hyperdisk itself. So quickly, just a show of hands. How many folks here have already heard of hyperdisk? Oh, great. Most of the room. How many folks here have tried hyperdisk or used it in some way? Okay, a little bit less than I would expect all the Googlers to have raised their hands. We'll talk about that later. So really quickly, we're going to talk through over the next couple of pages, what is hyperdisk? Why did we build it? How can you use it? And then we'll get into what's coming next. Think of hyperdisk as, first of all, it is the next generation block storage product from Google Cloud. If you are building a workload using any fourth generation compute of any kind, you will be using hyperdisk. It is the evolution of persistent disk. We love persistent disk. It's not going anywhere. But if you are building on our next generation compute products, you will be using hyperdisk. What makes hyperdisk unique? There's really three things that makes hyperdisk unique and valuable. First, it's workload optimized. It's workload optimized, which really just means that you can independently tune compute, excuse me, capacity and performance at the volume level with hyperdisk. So we'll get into this when we look through the different types of hyperdisk. But you can, on a hyperdisk balanced volume, you can set the capacity, the IOPS, and the throughput you want at each volume level, which lets you tune the storage to the workload. The second thing that makes hyperdisk unique is that it's very, very efficient. So with pooling, which is a concept we've deeply and broadly adopted here in hyperdisk, you can thinly provision resources, which means that you can buy resources at the pool level for your workload. And then you can hand out significantly more resources than that at the volume level. And handing out a volume, provisioning a volume inside of a storage pool does not consume any resources. As a result, you can lower your TCO for block storage by 30 to 50%. But more importantly, you can spend a lot less time planning at the volume level. And lastly, at hyperdisk, we have spent a lot of time thinking about how to reinvent enterprise level requirements for data protection and high availability at the block storage level. So we'll talk about what some of those things are. But we just had a session this morning downstairs in the expo hall on disaster recovery, data protection, backup, and DR in hyperdisk. That will be available on replay. I definitely encourage all of you to go find it. We have, we first announced hyperdisk about two years ago, and we have poured pretty much all of our block storage innovation into hyperdisk from that moment. And you can see that in what we have sort of relentlessly delivered on all three axes for hyperdisk over the last year. So on the workload optimized side, we have made hyperdisk broadly available. Hyperdisk should be available in every region and zone that you want to run it in, wherever your compute is. If you find a region or zone where it's not available, come find me, we should go fix that. We also have a new product called hyperdisk ML, which is purpose built for data loading and data acceleration. We've made that available on TPUs. That was a big effort on our side. In the middle, in terms of efficiency, we started pooling capacity. So we offered advanced capacity storage pools. Then last year, we announced advanced performance storage pools so that you can thin provision IOPS and throughput just like you can capacity. And then we made them bigger. So customers wanted to run storage pools a lot bigger. When we first launched storage pools, the max size was a petabyte and now it's five petabytes. On the enterprise ready side, my friend David, who's in the back there, and his team have been relentless about delivering new data protection features for hyperdisk. Instant snapshots, replication, multi-writer mode, high availability mode for hyperdisk balanced. So really a pretty broad suite of data protection primitives are now available for hyperdisk. Okay. So this slide is going to take a little bit longer because for those of you who have heard of hyperdisk but haven't tried it, this is my chance to make sure everybody has the right understanding of which volume types we offer and how to choose the right volume type for your workload. So first of all, hyperdisk comes in four flavors, balanced, throughput, extreme, and ML. And then along the bottom, you can see we offer pooling for each type of hyperdisk balanced volume or hyperdisk volume. Excuse me. What I'm going to do now is sort of walk through each one and explain is that help you figure out is that the right one for your workload. Starting on the left with hyperdisk balanced. First of all, before you do anything else, as you're thinking about which hyperdisk volume you might use for your workload, always start with hyperdisk balanced and prove to yourself that the workload doesn't fit in hyperdisk balanced before you move to any of the others. It's intentionally built to be very broad, very flexible, and very generous. And all of the others are really purpose built for very specific workloads. Ninety-ish percent of your workloads should fit in hyperdisk balanced. Hyperdisk balanced has the most flexibility. You set the capacity, the throughput, and the IOPS at the volume level. So you can do the most configuration of hyperdisk balanced. Hyperdisk balanced is our boot disk product. So if you are building a C4 node, you are going to use hyperdisk balanced as its boot disk. There are no other boot disk products in hyperdisk. Hyperdisk balanced is purpose built to be a great boot disk product. You get 3,000 free IOPS and 140 megabytes per second for free with each hyperdisk balanced volume. It's available in multi-writer mode. It's available in HA mode. It's got a broad set of uses. It also goes all the way up to 160,000 IOPS per volume and 2.4 gigabytes per second per volume. So you can do quite a lot with one hyperdisk balanced volume. Hyperdisk throughput is very unique. It is purpose built for cold data workloads and streaming workloads. So it costs about half a penny per gigabyte stored, which is pretty much the price of object. But it offers you the semantics of disk. So if you were building an HDFS solution and you wanted really low price, big data storage for all of the data disks for your HDFS implementation, but you want to use disk semantics, you don't want to rewrite how HDFS works for object. Hyperdisk throughput is the perfect product for you. Hyperdisk extreme is for your most demanding workloads, usually your really big databases that need one disk that is very, very fast. So the way to think about hyperdisk extreme is if you have a workload where you need more than 160,000 IOPS from one volume, then you should be using hyperdisk extreme. If it's less than that, you should be using hyperdisk balance. And lastly, hyperdisk ML. So HTML, hyperdisk ML is actually a very, very specific use case product. It is for loading stable data into lots of compute nodes. So if you have a set of model weights that are done and ready to be loaded into a whole bunch of GPUs, HTML is the product for you. What it does is it pools performance across all of those node connections. So let's say you have up to 2,500 nodes that one HTML volume can connect to. You can share up to 1.2 terabytes per second of aggregate throughput across all of those nodes. Now let's talk about pooling. So pooling is a fundamental concept in hyperdisk. If you are exploring how to think about using hyperdisk in your environment, you should definitely start by understanding the volumes and then understand how you can use pools to your advantage. So with pools, basically what you do is you configure a pool for the aggregate of the workload. So you go look at all of the SQL servers that you're going to be running in a zone and you say, okay, in aggregate, these need about 300 terabytes of written capacity. And on average, they're using about a gigabyte a second and 100,000 IOPS. Provision the pool for that amount and then go provision your disks as normal inside of the pool. It's super easy. You'll see it in the UI. You just say select. When you create the disk, you associate it with a pool. You can also do that in our command line. The trick is that creating that disk inside of a pool does not spend any money. Only reading and writing data from that disk will consume resources from the pool. Until you do that, the act of creating the disk didn't spend any money. That's thin provisioning. And you can provision more aggregate capacity and throughput in the pool than you have purchased in the pool because it's a thin provisioned product. We offer a pool for each disk type. So there are balanced pools, throughput pools, extreme pools. HTML is basically a pooled product on its own. So we don't offer HTML pools because it's pooled as is. Okay. So that's an overview of the SKUs that we offer and how to use them. HTML is the newest member of our family here in Hyperdisk. And this is just a short testimonial from a customer we're really excited to share with you. Resemble AI. So they do speech to text and speech to speech conversion. They build their own proprietary models to do that. And they've been using HTML to speed up training. As anybody who's thought much about training will tell you, Training is all about time to your next epoch. And because they can load data significantly faster with HTML, they can get to their next epoch faster at Resemble AI, which basically essentially they've doubled their cycle time, which is a super powerful example of how you can use HTML to go faster. Okay. That's the basics of what Hyperdisk is, how you can use it. I shared a lot of info there, as I mentioned afterwards. If you want to come grab me afterwards about your specific workloads or your environments and how you can use it, happy to dig in. Now I want to take a little bit of a turn and talk about what we are seeing in customers and how workloads are changing. And that will, I think, set up how Pierre and Ragnar, those stories they're going to tell us. Traditionally, block storage has worked in kind of a Lego brick model. So what happens is you plan your database workload and you buy the compute you need and the storage you need. And then you plan your next workload. And then you plan your next workload. And relatively quickly, you have a big aggregate amount of storage and compute. Hyperdisk works great in this environment, in this approach. The particularly ability to tune performance and capacity independently. When you're doing that planning, it lets you say, okay, this SQL Server workload or this VDI workload needs this much performance and this much capacity. I don't have to buy excess storage capacity to get it. I can tune it. That all works great. But we're seeing sort of a new approach develop. And that model is very different. In this model, customers are no longer, they sort of don't have the luxury to plan in that way. Instead, what's happening is customers like Pierre, for example, are saying, well, actually, I need all of this data to be in one unified layer. So that all of my applications can talk to the same data set. Or, as you'll see Ragnar talk about, the planning is not, by workload, is not really a luxury I have anymore. Because I have researchers and they come in and they create a workload and then they shut the workload down. And they don't really call me ahead of time. So, in that world, they need storage and infrastructure that's much more dynamic. Can change much more on the fly. And then across all of these, you've got a scale challenge. Where you have workloads now that are starting in the gigabyte size and ending in the terabytes, petabytes, and even exabyte size. And, excuse me, in that environment, the old approach doesn't really work as well. Which leads us to what we've been calling the cloud in a cloud approach. That's loud. I'll wake you up. So, think of the cloud in a cloud approach as more like a dynamic set of shared resources that can grow on demand as the workloads grow. So, instead of planning each workload one Lego brick at a time, what customers are now doing is saying, I'd really like to build one set of infrastructure with orchestration on top of it. And then let workloads come and go as they please and have the infrastructure adapt to the needs of the workloads. As opposed to me pre-planning the infrastructure for the workloads. That approach requires dynamic scaling that can adjust as the workloads come and go. It requires management by automation and by orchestration frameworks and infrastructure as code. And the thing that seems to be making this all possible is pooling. Because with pooling, I no longer have to worry so much about all of the pre-planning of those efforts. So, with that, I thought enough of me. I will bring up Pierre to show you what it's like to put that into action and talk about what they do at Century. Thank you, Ben. So, hi. I'm Pierre. I'm an engineer at Century. And I'm going to talk to you about Hyperdisk and how the project we used Hyperdisk on. So, at Century, we aim to build a platform to help you debug your application. And so, with that, we capture data, errors, tracing information, session replays, for example. And we're trying to show you all the relevant information that when we detect that a problem arises in your application. And so, for that, we need to store a lot of data. So, our key priorities and challenges would be around, like, we need our customer to answer the question as fast as possible so that they can focus on their app and not firefighting. And that creates a challenge for us to how do we store all the data, all that unified data in one place. We also, you know, want to serve customers globally and with fast response time. And so, we need to have a robust infrastructure that is scalable and reliable. And, of course, there's always a cost concern. We want to be able to do that as cheap as possible. And so, that's why we kind of, like, turned towards the Hyperdisk. We thought of the Hyperdisk of something offering, like, a cost-effective and flexible block storage. We thought we could ingest all of our data into that block storage and run our workloads on that. Also, as Ben mentioned before, it offers a way to pull the disk so that we can simplify how we manage and maintain all of our servers and block storage. And what was really, really interesting to us is we can tailor the performance of the disk to our workload. At Sentry, we tend to write a lot more data than we read. And so, it was very interesting to us to separate the capacity and the performance that we needed from the disk. And so, that's why we created what we call internally the events analytic platform. So, we use Clickhouse as a database. We have many different shards. Each shard is composed of, like, one replica. And each replica, sorry, three replicas. And each replica has a Hyperdisk balance and a Hyperdisk throughput attached. And so, all those disks come from a pool. And we use advanced capacity and advanced performance for those pools. So, the reason we started using advanced capacity, because we basically didn't plan for any of the future capacity we needed. What we wanted is to start a cluster and kind of, like, start using, start exploring our generic schema and how we could store all this, all the data that we needed. So, as Ben mentioned, again, Sydney provisioned disk. So, we basically created a pool. We created a replica that was, like, a certain size of disk. And we didn't have to pay for that capacity yet. We could slowly ramp up and keep our costs under control during development. Same for advanced capacity. We didn't need to have all the performance from those disks for the kind of query we were running. And so, that allowed us to slowly ramp up the cost as we were going. And, again, since we write way more data than we read, it turned out that our baseline performance needs are pretty small. But every once in a while, one of our customers will make a big query, for example, like 30 days of data. They will need to read and they will need way more throughput and IOPS in order to have a duration that is acceptable. And so, having this Sydney provisioning performance allows us to have a baseline and a peak that is way different. But we don't necessarily have to plan for the peak and provision for the peak. We can provision for the baseline and extend as we need it. And so, the different type of hyperdisk help us kind of manage cost also because we put the more recent data on some local SSDs which are expensive. We use hyperdisk bounds for something that will be around like 30 days of data and that needs performance. And we put on HD throughput like data that is less queried that we want to store just in case we need it. But we don't necessarily want to query it very often. So, that's how we use hyperdisk at Sentry in our analytics platform. And so, three outcomes out of this project. First of all, it allowed us to create this new platform, high scale, and we can store everything and still have performant queries. We did manage to reduce the cost by 37%. And so, when I say reduce the cost, it's compared to the way we used to provision things at Sentry by using persistent disk, by planning a little more for peak performance and peak capacity that we needed. And we didn't have to do that. And it was cheaper in the end. And, again, planning took... Well, we didn't do any planning. We really started and kind of went, you know, went as we go. We added new node, database node on the go that we didn't plan for. And based on, like, the kind of query performance we were seeing along the way as we were onboarding customer on that platform, we were able to kind of, like, tune and tweak the performance needed by those disks. So, overall, it was a good experience for us to use those hyper disks. Maybe three things that we can remember from that project is creating a generic schema for a unified data platform is still pretty hard. It sounds simple, and hyper disks definitely helped us store way more data than we need and kind of, like, test a few things. But it's still pretty complex, and you still need to pay attention to how your schema is designed so that you can have proper performance. The second thing is because we didn't plan very well... Well, because we didn't plan at the beginning and use storage pool to kind of, like, grow as we needed, it also means we started to use some, you know, some quota, some GCP quota kind of fast. And we constantly had to be reactive and ask our GCP rep for more quotas. I would suggest try to plan a little more than we did so that you don't have to constantly harass your Google rep for more quotas. And with us storing a lot more data, and we started doing, like, a bit of sampling, and with sampling comes its own challenges. When you do aggregation and you need to take into account, like, a sample rate in order to do that, to have a proper extrapolation of your aggregation, it tends to be a little hairy, and that was definitely, like, a switch at Sentry where we were not doing any aggregation but sampling heavily, and that was the problem. And with that, we allow ourselves to sample, do some tail sampling, and we store weights, and it means we have to calculate, extrapolate our aggregations. And it's not as easy as it sounds, and there's a lot of mass involved, and so if I had to do it all over again, maybe I would pay a little more attention to that part. But, yeah, hyperdisk were really useful, and so I'm going to give it back to Ben. Great. Thank you. Excellent. Yeah, thank you, Pierre. By the way, Sentry has a booth downstairs. I definitely recommend you go stop by. Now let's have Ragnar come on up, and Ragnar's going to talk to us about what they do at HRT and how they've built a dynamically scaling infrastructure. Thank you. See? So, yeah, like, I'm Ragnar here for Hudson River Trading. So thanks for giving me a chance to talk a little bit about how we scale our AI workloads with GCP and Hyperdesk. So before we dive into the technical part, let me just talk a little bit about Hudson Trading and what we do. So we are a quant trading company, so we buy and sell assets, try to buy low and sell high. So that sounds very simple, but finding the low and the high, that's the difficult part, so that's where the machine learning comes in. So we ingest huge amounts of data, so that's historical market prices and all kinds of indicators, and then we use this to build statistical models and machine learning models to tell us when it's profitable to buy and sell, and then we make trading decisions based on these models. And then, of course, things change, so there's new data, new data to ingest, new models to build, and the circle continues. So we have all these state-of-the-art machine learning models for our financial data, and for us, better quality models directly results in more profits. So that's very important that we're able to build these models, build them quickly, and also be able to do experimentation and try new things, see what works, and iterate quickly. So in order to do that, it's important that we can run model trainings at scale, and also have storage that keeps up with that. And our needs, as Ben mentioned earlier on, are very dynamic. So market conditions keep changing, and then we have researchers that come up with a good idea, and then they want to quickly be able to iterate, try things out, run a bunch of model bills, run simulations, see if that works in practice. So it's important for us to be able to do that quickly, scale up to reach that demand, rather than having our internal users have their jobs sit in a queue before it can run. And our storage needs are pretty dynamic as well. So some of our jobs need a lot of local storage to run, but the amount of storage that they need, that depends on the particular model, differs from job to job and from day to day, and we don't always know when we start a job how much storage it's going to need. So being able to accommodate that helps us a lot. So talking about how we can meet these requirements using Google Cloud. So to start with, GCP has a lot of GPUs and AI chips, so that's a good start. And then for the flexible part, then we make use of Spot VMs, of DWS, of on-demand, to be able to build extra capacity when we need it, and then when we have less jobs to do, then we can scale it down and reduce our cost. We particularly like DWS because for DWS, then we can allocate the machines when we need them, and then we can reserve them for a week, and we make sure that there's no preemptions, no disruption. So especially for large multi-node jobs, then that improves our efficiency a lot. And for storage, well, there's no use of having all of these GPUs unless we can load the data fast enough. And we like to use HyperDisk for that because the block storage is very convenient. We have workloads and software stacks that we run both on-prem and in the cloud, so optimized for using a local file system. So with HyperDisk, then we can use our preferred local file system and use the same software stack that we can use on-prem. And when we started out, we used the regular HyperDisk, but like Ben talked about, this requires more planning for knowing what we need, and as our jobs are quite dynamic, we switched to HyperDisk storage pools, so we no longer need to think about how much capacity, how much IOPS, how much throughput do we need for a particular disk, but we can install a plan in aggregate. So putting this all together, we get sort of a complete high-level architecture for what we do for model training. So we have an internal job scheduler that we do to run our jobs, so it can schedule our jobs on-prem or on the cloud. And in the cloud, we have a baseline of capacity for sort of committed use that's always there, and then we have an autoscaler that can add and remove capacity through Spot or DWS nodes as needed. And then for storage, these nodes come with very fast SSDs, so that's a good start, but our storage needs often exceeds that. So we can supplement that with HyperDisk and put them in storage pools for easier management. And then we also use the cloud storage products for sort of the longer-term storage for data that's accessed less frequently. And then since we have this sort of combination of resources that are always on and that are more dynamic, then we need to do sort of a balance between the two. So in the first figure here, you see sort of a plot over time for usage. You see we always use some amount of cloud resources, but the actual amount, that varies quite a lot. So if you look at the histogram on the second one there, you see there's a very long tail where, you know, we need huge amounts of GPUs in order to run all of these jobs at once rather than having them sit in queue. And cost-wise for our committed use, then we have, you know, a good price for that, that's independently of whether we use the resources or not, whereas DWS we pay only for what we use. So we can do, model this out and find a sweet spot of like the combination of committed use and more dynamic use that lowers our total cost. So putting this together, outcomes. So being able to make use of the hyperdisk storage pools, then we could more than double our capacity for running our training jobs. And maybe most importantly for our internal end users, then this reduced our queuing time, so their jobs typically start in minutes rather than being queued for longer. That directly goes to our bottom line. And then also with the hyperdisk storage pools, this allows with the thin provisioning for capacity and for performance, we can run it more effectively. And for us, it would reduce our cost about 50% from doing the regular hyperdisk storage pools. Sorry, the regular hyperdisk storage products. So maybe some takeaways. So sort of this combination of fixed capacity that you can always depend on is always yours and it's cost effective when you use it a lot. And then using the more dynamic products like DWS for flexible peak capacity works well. And in particular, DWS is great with the lack of preemptions. And similarly for storage, the local SSDs that come on these machines is great, but you can supplement that with hyperdisk when you need more space. And maybe a little tip if you need the most performance, then sometimes you can combine multiple hyperdisk for machines to get the most. And lastly, if you can use the storage pools to combine this, it greatly simplifies the management of the storage resources. And in some cases, including us, it can also reduce the cost quite a lot. Thanks for that. And then back to Ben to talk about what comes next. Thank you, Ragnar. Thank you, Pierre. Great stories. So I'm going to bring this home here for a close and just talk about what's coming next. So when you think about the future of hyperdisk, everything we're doing really going forward is about scale. Big scale workloads, big scale infrastructure. So let's start with hyperdisk ML. Today, you can load your models three to six times faster using hyperdisk ML than any other Google storage product. So if you want to get your models into your compute nodes fast, HTML is very likely to be the fastest way to do it. In preview right now, you can is the ability to connect your HTML volumes to generation four compute. So this is your C4s, et cetera. If you'd like to try that preview, if you're interested in giving that a whirl, come find me afterwards. I'm happy to sign you up for that. And then lastly, we've been talking a lot about HTML as a model loader. Well, those models tend to live in Google Cloud storage in a bucket. And we wanted to make it really easy for you to get models from GCS into HTML. So we partnered with GKE to build what we call the volume populator. So it's a tool that makes it very easy to scalably move models from GCS into HTML so you can get your inference pod up and running faster. And now we're going to talk about exopools. So some of you may have seen this in the keynote yesterday. We took the covers off a project we've been working on that we call exopools. You can think of exopools as a variant of storage pools purpose built for the largest, most demanding AI and analytics workloads that you are likely to run. An exopool is multiple exabytes of capacity, multiple terabytes per second of throughput for either HDB or HDT, hybrid is balanced or hybrid is throughput, in a specific zone that is co-located with the compute you'll be using. So if you have a giant amount of capacity and performance fueling tens of thousands of GPUs or TPUs in a single location, you are very likely to be using exopools. Other than that, exopools behave and perform a lot like a storage pool because that's what they're built on. So you get the benefit of thin provisioning and the benefit of aggregate usage management. A disk inside of a hyperdisk exopool behaves just the same as a disk outside of an exopool or in a storage pool. It's just a hyperdisk volume. This is just a new management construct. There are some other interesting differences with exopools. For example, the read and write performance is a little bit differently calculated inside of an exopool. If you'd like to learn more about it, I'm happy to talk to you afterwards about what an exopool is and whether your workload might be right for it. I will say one thing about exopools. If you are interested in exopools, we're very likely already talking to you because you're probably planning a workload at a large enough capacity that we should be having a conversation about that anyways. Last thought is just how do all these pooling products fit together? So for all four of our volume types, of course, you can buy them as a standalone volume. Storage pools we offer for hyperdisk balanced, hyperdisk extreme, and hyperdisk throughput. Exopools are really only offered for balanced and throughput. And then the other way to think about them is in terms of scale. If you are going to be using up to five petabytes or a terabyte per second in a single location, storage pools are probably right for you. And remember, that's actual used performance capacity, not the amount you provisioned at the disk level. But if you're going to be going up to two and a half exabytes or five terabytes per second in a single location, then you probably are going to be in an exopool. So with that, I think we are done. So I just want to say a big thank you to all of you for coming and a big thank you to Pierre and Ragnar for all their time. Thank you.