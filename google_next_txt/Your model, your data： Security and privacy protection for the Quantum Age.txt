 Hello everyone and thank you so much for this session that we called Your Model, Your Data, and Security and Privacy in Quantum Age. I'm Nelly Porter. I'm Director of Product Management in Encryption and Confidential Computing for GCP Security. And we will go today and explore how security and privacy impacting our world, especially when it's coming to AI. Choosing the privacy is very complicated and different customers coming from different types of and segments have slightly different challenges that they need to address. So all of them coming from financial, healthcare, or government need data confidentiality, especially when they're using this data to train the model, to fine-tune the model, or to do inference on the model, running the agents. But every single industry has their specific capability. For example, for financial customers, they more worry about how to avoid cross-league and cross-leaking data between customers. But also they need ability and necessity to communicate together in secure environments. Healthcare, they're finding that unlinization of the data is problematic, especially as this data used to train or fine-tune models, because the quality, fidelity of the data is actually degrading the quality of the models. And for government and government, federal companies, runtime information is critical. And it means training or fine-tuning is not the solution for them. They're more focused on what we call RUG or in-context learning to ensure that you always have runtime information in hand. If you're talking about AI risks, all our customers have these problems or challenges that we're trying to help them with. For those customers that are creating models and using data to train those models, model and data privacy is becoming critical for them. Model is a lot of IP, and model soft is problematic. And plenty of regulations and export control requirements is also very difficult to meet. So what we do to help our customers? We have interesting tools, interesting capabilities that we want you to explore. And it's called confidential computing. And confidential computing unlocking a lot of these ML AI use cases that our customers are looking forward to. For example, they want to protect IP of the models, or they want to securely collaborate with the data that otherwise they never would be able to mix and match. Running this in confidential computing helps them to address those use cases. It's the first aspect of this technology. Confidential computing is also helping with some policies for customers when it's coming to data sharing, law enforcement access, cross-border data transfer. Those use cases becoming very important to so many customers that we're talking to. And confidential computing is absolute factor to help. And we have significant confidential cloud portfolio for all of our customers with our services, our capability with GPUs, our ability to provide confidential AI and confidential space. And we're also at next, announced confidential A3. It's our machine family running on Intel confidential computing environments together with natively confidential NVIDIA H100. But the C confidential system is not a specific part. It's all about compute, networking, storage, and accelerators. And something common about all of them, they all build on the basis of encryption. And really, very recently, Google announced and brought to the market to the customers what we called quantum system. quantum chip called Velo. Velo is incredibly powerful. And it can perform these tasks so much faster than any biggest computer that we have. They can perform in five minutes the tasks that normal computer will do in 10, in 25 years. And as a result of that, this power can be used for good or can be used for trouble. And one of the interesting challenges that we will have with quantum computing is the ability for us to sustain our cryptography, our privacy. Because this particular threat capable of performing the attack on asymmetric crypto, digital signature, and everything else. But those two is critically important. And today, during Next, we announced that Google is actually ready to run quantum safe confidentiality in transit. And we enable it by default in Chrome, and also in all public Google websites. And also for customers that need to validate the quantum safe digital signatures. We have preview in Cloud KMS that allow you to use all of those FIPS standardized algorithms. And as usual with us, we enable open source libraries like Tink and Boring SSL with all of this cryptography. So to summarize all of that, what we're telling you, our customers, for your use cases, especially if it's coming to AI, please use confidential computing. Because security and privacy is not negotiable in our future. All of this capability is built as joint industry efforts. We build it with Intel, AMD, NVIDIA, ARM, and we build this confidential computing consortium that all of us coming together to convince the world and move the world forward. And finally, post-quantum future is coming. Don't wait for that. And get ready. Use all tools and capability to validate that you would not be surprised when quantum computing will show its power. Thank you so much. Thank you for joining me today. I'm Nelly Porter, owner of Confidential Computing and Encryption in Google Cloud. And with me today, the person that doesn't need introduction, WindServe. And Wind is incredibly famous for doing something incredible. In 1970, you together with Robert Kahn were able to create this interconnect, the whole world, with Internet, something that we're benefiting from now on. You didn't stop. No, no. Why would you stop? And again, one additional interesting thing. In 2020, five years ago, you first announced the first ever product in Confidential Space, Confidential VMs. And five years later, we'll be happy to bring you in with this very important conversation. So thank you so much for joining us. Well, thank you for the opportunity, Nelly. Thank you for joining us. And no surprise, we would not be able to talk about anything but AI. And again, as we understand that AI presents enormous opportunity to help in so many different areas and expedite our research and take our tedious work away from us. But also AI and again, our customers that are using AI need so much more to think about privacy and protection of the data and model. So, I think that's a question for you. And something I want us to discuss. Any experience, any lessons that you have for bringing and making Internet so accessible to everybody across the globe. What we will learn from that and how we will take those lessons to our today's challenge to maintain privacy. Wow. This is like being asked to describe the universe in 25 words or less than three examples. And you can do it. But look, the Internet was born in openness. The idea was the protocols were openly available. The network was open. Everyone could interconnect to everyone else by design, by deliberate choice. And as you think about that philosophy and you think about how that helped to support the development of AI, which it did. Think about Fei-Fei Li at Stanford who did ImageNet. And she couldn't have assembled that training base without having an Internet that was open, whose content was accessible that she could assemble into a training base. So, this openness is still very important. We made our AI mechanisms openly available to people so that they can try their ideas out. And we want them to have the opportunity to do that. At the same time there, some of them are doing this as part of their business development. And I think they would like to be able to do that with the assurance that only they have access to the results of their artificial intelligence work, which is why the confidential computing idea was so important, because you can deliver to our GPUs, our TPUs, and our CPUs an encrypted workload that includes the software, maybe the AI model, plus the data, all protected so that we at Google never see it. It's just in the machine. And the processing gets done, and then it comes back, and it actually gets double encrypted, because we encrypt for transport in addition to encrypting using the variables that only the customer and the computer happen to know. So I think this is our way of saying to our users, feel free to use these technologies. Don't worry that your intellectual property is properly protected. You have control over that. So now, I think the next question is going to be, what's the range of applications that we are able to support? And I have confessed to you that in the early days, I was a little skeptical, because I've lived through three cycles of what's been called AI since the 1960s. But now that we've got these large language models, and we have the machine learning tools that do like alpha fold and protein folding, for example, which is amazing. I've tried out some of the large language model applications with our Gemini tools, the note taking is spectacularly good. And so when I have video conferences, I don't take notes anymore, because that's distracting. You know, I rely on the conversation, I rely on the conversation, or focus on the conversation. I'm beginning to see other opportunities for getting things written that I would take much longer to do. And I'm sure there are an immense number of business applications. So agentic AI is starting to be a big idea now where these agents are smart enough, if we can use that word, to take actions on our behalf. Now, I always worry, however, that in the past, we've had occasion when the AI models do the unexpected. Sometimes they call the output hallucinations. And so we clearly have to understand that deeply and understand how to control that so that we and our customers are not surprised by the behavior of these AI-based agents. And I'm confident that we are well on our way to achieving that objective. That's a very good point. And again, at Next, we announced today, is this interesting capability that you're applying to. It's Gemini Cloud Assist. And the idea here is very similar to trying to take incredible risk and, again, complexity from admins' day-to-day job. Because the problem that they will face, not all of them want to think about security or compliance or privacy. And the second problem, if they will pick the wrong turn, those decisions are most likely not reversible. So what we have is Gemini Cloud Assist tools. It's agents, exactly what you said. They're trying to help, based on the information that we have, admins to provision the workload in proper compliance regimes and meeting the proper, again, regulations. And also ensure that they're confidential when it's applicable and using the right tools, the right keys, etc. So this capability is also amazing because we cannot train Gemini. It has to be so in context and so real-time information. So the data and how we do it behind the door, we're actually helping to add additional context and information into the prompts, so-called dragging the data. And Gemini then comes up with the solution. But we didn't stop there, as usual. We want to ensure that all of those tasks can be automatable. Automatable, and talking about hallucination, how to ensure that you will not automate something that you would not like. So we spent enormous of time trying to, again, bring different variation of terraforms or, again, automation. So Gemini, when we will feed this data to them, to those models, would be able to generate very proper results and also evaluate those results against it. So you're absolutely right, how to ensure trust and privacy of the data models and ensure that it actually do what it's expected. So, you know, on the agent side, one of the important questions is whether agents that we make using our models and agents that other people make can interact in a reliable way with each other. The same argument can be made about interactions between programs running in one cloud and programs running in another cloud. Our customers often are saying, I want to be able to run programs in one cloud and then bring some things to Google for our special capabilities, like our TPUs, for example. So agents interacting with each other coming from different places, we may need to invent a standard vocabulary whose semantics is well understood and well defined so that the agents don't get confused. If they're using natural language, they may get just as confused as human beings do, which we don't want. Because agents can do things faster than we can often, in which case they could make mistakes faster than we can. So we don't want them to do that. So coming up with standards for interactions among the agents may turn out to be another important contribution that we can make in collaborating with others. You're absolutely right. You're absolutely right. And also looking from security, as usually I do, between agents, we need to ensure trust because they need to work together on specific tasks or research. So we always think about how we will authenticate, ensure the identity, proper identity of those agents, how we will authorize them to properly access the right data for right tasks. And also attest this more confidential computing capability to verify that they're actually running and providing capabilities that you expect them to run. And to ensure this interrupt story, it's why we have built this industry, our confidential computing consortium. It's right now, we started with three, four companies back then. And now we have 57 companies in Ticket. And exactly for the way to do standardization, to ensure that customers are running their agents and their models in one cloud would be able to interrupt with the same agents running elsewhere and being able to have the common language, as you said, and also security guarantees that you can trust another agent to do what needs to be done. So this raises a really interesting question. Humans want to make sure that they've got to the right website. We use digital signatures as a way of confirming that. Individuals would like to be strongly authenticated so that someone else can't pretend to be them and take actions on their behalf. It sounds to me like agents may need similar kinds of strong authentication. Am I talking to the right agent? Which agent am I talking to? How do I assure myself that this exchange is taking place in the way it was intended? So we may have to have agents with identifiers just like people. Absolutely. So this is going to get very interesting. And different. And we need to have ability to separate identity of the agent and identity of the human. I think that's important. Otherwise, it's easy to fall into the trap of thinking that an agent is a person because they're so glib. But they don't have the same empathy that humans have. You wouldn't want to make assumptions about that. Absolutely. So we have some work to do to make sure that there are guardrails for behavior and also that our products are strongly authenticated. Absolutely. And have the security guarantees at our customers. And you mentioned something that's also interesting that I want to turn a little bit. It's necessity for us to think about digital signatures. And digital signatures is the way how we do authenticity and validate authenticity of anything. But with today's cryptography and, again, quantum that's coming to us, asymmetrical encryption and digital signatures becoming one of the biggest and most important things that we want our customers to start looking at. And one thing, again, is that Google has done already by the way, we spend significant time and effort in post-quantum cryptography. And we enable every single Chrome client to talk. Again, key encapsulation mechanism to establish sessions, secure quantum-safe sessions to every single public site like Google.com and everything else. And this capability, and together with digital signatures, that we have open source libraries and also cloud KMS support in that. We want to ensure that privacy and authenticity, as you said, is continuing to be relevant for humans, for the workloads, and for agents as well. So we're going to run out of time in a minute here. But the important thing is that we've adopted and are using the NIST standards for post-quantum cryptography. And that's important for our customers to know that their information could be protected for the next 25 years, as opposed to waiting until the big quantum machine comes along and somebody has recorded your information and breaks it 25 years from now. We don't want that to happen. So we have, as usual, invested in implementing these things early. Unfortunately, we are pretty much out of time, so we'll have to shut down this time. But it's always a pleasure to talk to you now and to hear more about the success of your efforts. Thank you so much, Benit. And it's a huge pleasure to talk to you as well.