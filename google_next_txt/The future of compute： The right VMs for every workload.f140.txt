 . All right. Good morning, everyone, and welcome to Next 2025. It's great to be here with you in Las Vegas, and thanks for getting up early to join us today. My name is Alex Bestavros, and I'm the product lead for Google Cloud's compute portfolio. And today I'll be joined by Matti Toya, VP of infrastructure from Shopify, and Ken Exner, the chief product officer from Elastic. So they're going to join us later today to talk about how they've taken advantage of our Gen 4 compute platform to help and what benefits that they've been able to achieve with their businesses. It's been another exciting year for Google Cloud, and I'll be covering all of our new product announcements across everything in our expanding compute portfolio. We'll be looking at the underlying titanium infrastructure that Google Cloud infrastructure is built on, and then we will get into some Q&A at the end with all of us up here on stage. And with that, let's begin. So I want to start off with what we are hearing from our customers. Of course, I can't have a presentation at Google Next without mentioning AI, which is rapidly becoming a utility, right? And it's an essential part of how all of our businesses are going to continue to operate. AI is leading to new and immersive applications that require more powerful and specialized compute. Second, for customers who are further along in their modernization journey, they are continuing to focus on highly scalable, modern applications that take advantage of everything Google Cloud has to offer. And then third, traditional enterprise workloads and homegrown applications that for many of you are the bedrock of your businesses. These are critical applications that you simply can't ignore. Clicker issue. There we go. So our approach at Google Compute Engine is to look at each and every workload and then design and optimize at the system level. And that means across everything from compute, storage, networking, both on the hardware and software to meet the unique needs of each workload. And to do this, we focus on workload-optimized infrastructure, which is designed to meet the specific needs of each individual workload. We work closely with our customers, right? To understand their problems for each workload and then design and optimize at the system level. But first, let's start by looking at the underlying technology that Google Cloud is built on. So we're going to be able to run through the data center. And we're going to be able to run through the data center. Over the past few decades, Google has pioneered the data center by building new technology to run Google's popular planet-scale services. These are services you use every day, like Search, Maps, and Gmail. Through Google Compute Engine, which is built on Google's innovations, we bring all those benefits to customers such that you can run and scale your applications just like Google does. The key that underpins all this technology and innovation is called titanium. We've made meaningful advances across the compute, storage, and networking performance that address the common bottlenecks that we see in customer workloads. On the compute side, we have consistently delivered market-leading price performance by bringing the latest CPU generations from Intel, AMD, and Google's Axion. That's our home-built ARM-based processor. For storage, we have our next-generation block storage offering called HyperDisk, where customers can tune the performance they need on storage independently of the VM for up to 500k IOPS with HyperDisk Extreme Volumes. Google has also built its own local storage called titanium SSDs. They offer 35% lower latency for access to your data to help scale performance and databases on your Gen 4 instances even faster. And then for networking, we built our titanium ML adapter, which is based on NVIDIA's CX7. And then we've also offered with our new HPC offering Cloud RDMA for the first time, which offers super low latency, approximately three microsecond latency, one-way latency, to help scale your clusters when you're running large HPC workloads. So lots of advancements across storage, compute, and networking to help scale your workload performance. But titanium has also helped us enable new features for GCE. One of the most meaningful ones that we've launched in the past year is the bare metal instances. So bare metal instances provide direct access to the hardware. So unlocking the ability to migrate your traditional workloads that are running on-premise, or to run your own custom hypervisor or OSs on Google Cloud. What's really exciting about the metal instances is they run the same way a VM would, with the same APIs and the same services within Google Compute Engine. Today, I'm excited to talk about we're expanding our metal offerings with nine new configurations across our C4 family, which is our Intel sixth-generation Granite Rapids-based VM. We're going to offer metal there. We're also expanding metal to C4D, which is our new AMD-based Turin, fifth-generation Epic Turin processor, and then on Z3, which is our storage-optimized family, which will offer up to 72 terabytes of the titanium SSDs. So really bringing a large portfolio of configurations for you to run your workloads on metal. Metal has also enabled new, exciting Google Cloud offerings. Nutanix has had a long history with Google and was initially founded by two formal Googlers and somebody else from another cloud to bring cloud computing architectures to enterprise data centers. And we're excited to partner with Nutanix on their launch of Nutanix Cloud clusters, or NC2, on Google Cloud, which is based on the new Z3 metal instances that I just covered, so up to 72 terabytes of local SSD. So organizations can now take... Now can now run a true hybrid full infrastructure stack whenever they need with UniFi's operations and accelerate their workload performance. So our strategy at Google Compute Engine is to service all customer workloads. And to do that, we've rapidly expanded our compute portfolio to offer a full suite of instances to address emerging customer needs and evolving market demands, and this expansion continues. We offer a highly competitive general-purpose compute portfolio featuring leading price performance and the latest generation of Intel, AMD, and Axion, Google's custom ARM-based processors, and we're the only cloud who can make that claim. Our specialized VM portfolio has also seen significant growth with multiple new market-leading additions to scale and optimize key workloads, including high-performance computing, large-scale databases, and data analytics, just to name a few. So let's take a closer look at our general-purpose portfolio. So pre-2023, we offered a single N-series family to address all the performance, feature, and cost needs of our customers. We have since made an intentional shift to better meet customers' evolving needs. Today, our general-purpose portfolio offers two major families, the C and N series, each providing value depending on what you're trying to optimize for. Our C family is all about the highest performance and enterprise features. So think leadership-level compute, advanced maintenance controls, larger instance sizes, and premium storage and networking features. C instances are available across all the latest-generation compute offerings from Intel, AMD, and our own Axion. And this gives customers unparalleled performance and choice. And then we have our N series. N series are designed to be flexible and optimized for price performance and TCO. One of the unique features on our N series is our custom machine types. This allows you to specifically tune how much memory and compute that you need for your workload. In some cases, we see customers saving up to 18% on their cost by leveraging CMTs. And the best part about these C and N is they're designed to work together. Right? Bring your high-performance databases, your machine learning inference on C, and then you can use your N family for a lot of your general-purpose web application server type of workloads. And using them together is what gives you the full power of our general-purpose portfolio. So let's talk about some new offerings that we've announced within our general-purpose portfolio. The newest one is C4D. So C4D is based on AMD's fifth-generation EPYC processor. They're called Turin CPUs. They provide customers with the latest performance. And why is this important? This is up to 45% versus comparable cloud offerings from other clouds. C4D is enterprise-ready. It comes with advanced maintenance to avoid any unplanned disruptions for your infrastructure. It also comes with Titanium's new SSDs that offer 35% lower latency to improve your database workload performance. And as I mentioned earlier, this will be our first time offering AMD metal shapes. This will enable you to bring your own custom operating systems and hypervisors onto Google Cloud. And we're in preview today. So you can actually sign up. I put a QR code on the slide. C4D brings meaningful benefits. Significant, up to 80% more performance for running key workloads, including web servers, MySQL, and Redis databases. These all help you scale the growth for the growth in data that we're seeing in the AI era to help meet and help meet the demand of new immersive applications. These are significant generational improvements that we've been able to achieve with our full Gen4 platform, including the latest generation compute offering from AMD, but pairing that with our hyperdisk block storage offering, which is where you get the full workload performance of our Gen4. So here's what customers who got a chance to try out C4D had to say. So App Levin, who helps mobile developers market, monetize, and analyze their applications, was able to achieve 40% improvement on their workload and achieve lower latency. Silk, who accelerates cloud databases with their software-defined storage platform, was able to achieve 40% improvement in performance as well, but that's on top of the 35% they were able to achieve in the other generation. These types of improvements for workloads are really hard to ignore. Moving to Intel. So last year we launched C4. That's our latest Intel-based general purpose instance family. We've seen significant adoption from our customers for their performance-sensitive workloads, from gaming servers to databases to machine learning inference on CPUs. Today I'm excited to announce that we're actually expanding the C4 family given all of the popularity. We're going to add new capabilities that are going to be exclusively on Intel's sixth-generation Granite Rapid CPU. First, we're going to offer six new instance options to scale up your databases, gaming servers, and container platforms up to 288 vCPUs and 2.2 terabytes of memory. And then we'll offer new C4 options with titanium SSDs. This is to get high-performance and low-latency access to your data. And then within the C4 family, we're also going to have five new bare-metal offerings, including the first general-purpose bare-metal offering with local SSD that offer 35% better improvement over the previous generation. And we are the first cloud to offer Intel's latest sixth-gen Granite Rapids CPUs, and I'm excited to see what customers do with it. Let's look at a few customers that have been using C4. So NetE selected C4 to launch their new game called Fragpunk, and they've seen a 50% improvement with a smooth gaming experience for tens of thousands of simultaneous players. Hugging Face, a platform hosting open-source AI models, data sets and tools, they ran various text embedding and generation inference workloads and saw up to 19x more performance with C4, leveraging some of Intel's specific instructions in ANX. Really, really impressive gains for workloads generationally. And then to complement C4, we have our N4 instances. N4 is designed to provide the performance of majority of general-purpose workloads while also offering the flexibility to optimize costs, something top of mind for a lot of you here today. So N4 enables up to 18% better price performance than the previous generation, giving you meaningful benefits for your general-purpose workloads like web and application servers, business application and microservices. With industry-leading flexibility, customers can use N4 with standard shapes that we have predefined, or you can use the custom machine types that I mentioned earlier to tune the VM exactly to your workload needs. Now with that, I'm going to invite Maddy Toya up to the stage to talk about how Shopify was able to leverage N4 for their recent peak shopping event. Hello, everyone. Let's see. This working? There we go. So I'm Maddy Toya, and I'm the VP of Infrastructure at Shopify. For those of you who aren't familiar with Shopify's business, or only maybe know a bit about it superficially, we are a platform that powers commerce for millions of merchants around the world, from small entrepreneurs to large-scale retail brands. You might think of us as the brand behind the brand. We sit as a platform that merchants can use to connect with their buyers wherever they may be. We also have a pretty substantial developer platform that we have a large ecosystem of software developers working on to provide apps and features that complement the specific needs of these merchants. And as it turns out, in society, commerce is very universal, as is using the Internet. And so we've seen a ton of growth over the years in Shopify's business and in the success of the brands that have come to us. Just last year in 2024, we saw 875 million unique buyers on our platform making purchases across all the merchants that we host. And on Black Baradi alone, we saw our merchants make something like $11.5 billion in sales. This, of course, requires a lot of infrastructure, and that's where me and my team come in to help design and build the necessary infrastructure to actually provide the services to these merchants and their buyers. And we really couldn't do it without Google Cloud. It takes a lot of compute power to actually make this work. And partnering with Google means that we have access to some of the best platforms in order to do that, and we have access to tremendous capacity in order to reach the scale that we need for these peak events. You know, at our global peak, we were using something like 2.4 million different CPUs in the fleet. We are running hundreds of GKE clusters across the world in order to supply that capacity to our app servers. And we're serving petabytes of data over our edge in terms of images and rendered pages, checkouts, and what have you in order to provide the best retail experience for our merchants. And what's really great is that we can get all this capacity and complement it with the amazing Google services, Vertix AI, Kubernetes Engine, obviously, BigQuery, Cloud Run, and having these services all work together at really low latency has been a huge benefit to our business. But we're here to talk about VMs, and as you can see, VMs are really at the heart of what we do. Our app servers really power our business, and we run a lot of them, particularly at peak. And as we think about our VMs that we run in our production environment, we think a lot about how we want to evolve them over time. When it comes to commerce, conversion rate is really one of the key metrics that we watch. It's so important to a merchant's business how easily they can convert a buyer to a sale. But we also internally really care about our total cost of ownership and our price per performance for the platforms that we have and we run. And oftentimes, you can see that latency and saving money and managing your TCO can be at odds with each other. And really, that's where these new platforms and families come in to help us achieve the goals that we have with the applications that we design. Generally, we optimize for the best price performance. We want that low latency. And when we looked at N4s, we were super excited to see in our early testing that we were able to achieve better P90 latencies in our workloads while at the same time reducing the number of replicas that we were able to run in order to supply those workloads. This is like the dream, right? And as we run these workloads, you know, we see significant different patterns in the traffic that Shopify receives over time. We have daily patterns, weekly patterns, and seasonal ones that, you know, make us scale up or down. And as we watch these patterns, we really take a look at the kind of performance we're seeing in our workloads in order to further optimize how much capacity we run and where we run it in order to really eke out the best price performance that we can with these new machines. And so what are some best practices that we've learned over the years in terms of how we adopt and how we then deploy these new machine families? First of all, like, this is a gradual process. When you're talking about, you know, 2 million plus cores at a time, you're not flipping the switch and moving them all to a new platform all at once. We're really taking a look at where we're seeing growth that could potentially take advantage of these new platforms, where we have committed usage that is expiring and then it's time for us to consider what our options are for renewal. And also places where just the performance has gone to a point where it's just smart to make the move even with the existing capacity that we have. The second thing is what I always tell my team, and some of them are here today, is that we have to always be forecasting, A, B, C, forecasting. And we really look at this continuous model of what the traffic is coming into Shopify because it allows us to plan out into the future. When we have such a huge retail event once a year at Black Friday where we see this peak of peaks, where during the normal course of the day you might see one merchant having a really big sale, but the other platform is relatively average in terms of its traffic, Black Friday everyone's having sales. Everyone's doing a lot of business. And really understanding those rhythms allow us to make plans that we can then partner with Google to make sure that the capacity is available in the right locations. And also we can build into our own engineering plan as to how we want to make these transitions where we're moving workloads to potentially new families or new machine types over time. And then, of course, we monitor our workloads and we optimize. I mentioned committed usage discounts. This is a really important part of our mix of the way we actually take advantage of Google's cloud, which does require a little bit of planning. That forecasting comes into handy there because we don't want to overbuy cuds and then have some wasted, and if we underbuy, then we're having some missed opportunities of cost savings. And so really understanding this workload and, like, where we want to target is very important. And if this is interesting to you, there's another talk that's happening, like, right after this about this. So check it out. Let's see. And so what's next for us, right? I talked about N4s a bit, which we've adopted very heavily and seen such a huge benefit. And Alex was just up here and he revealed, like, all these great specialized workloads and platforms that are coming from Google Cloud now and are really looking forward to taking advantage of those as well and finding the right places for those to fit into our own workloads. C4 VMs plus Hyperdisk, like, we definitely have places in our platform, like I'm sure many of you do, where vertical scale still really matters and we're considering where is the right place to make that transition, again, with price performance in mind. There's always opportunities to improve our forecasting models. There's so much we can do with AI right now to help assist with that process and we continue to refine the methods that we use in order to deliver that price performance. And then one place that we're particularly excited about is the Axion chips. We're considering where we want to incorporate them into our workload and one of the things that's been really exciting to see is that we were able to run our web application servers very easily and effectively on the C4As and continue to run a regression suite against our entire software so that we can understand that we have that optionality to move to an ARM platform from an Intel 1 when the time is right. So always being prepared as well. And with that, I think I'm turning it back to Alex. Thank you. Great. Thank you so much, Matty. Really, really impressive scale that Shopify has been able to achieve and the business benefits with our new Gen 4 platform. So as you heard from Shopify, as well as many other Google Cloud customers, there's a lot of excitement around Axion. And for good reason. So last year at Next2024, we announced Google Axion. This is the world's highest, best price performance server processor with up to 10% better price performance than what you see on competitive clouds. And up to 65% better price performance than other current generation VMs. And on top of that, C4A is 60% more energy efficient. Another key aspect of Axion is that it's built on standard ARM compute core designs. This helps customers migrate faster and have confidence that the ARM software they're already running elsewhere will be compatible. We have some really exciting new launches with Axion that we announced at Cloud Next, including our Cloud SQL managed service and AlloyDB. We have a lot to talk about with Axion, but there's a talk just following this one at 10.30, where you can get a lot more information about Axion and some of the customer momentum and new product launches along with Axion. So please join that one if you're interested. I think it would be a really interesting talk. Now I want to talk a little bit about our specialized compute offerings. Our specialized compute offerings are ones that optimize for a very specific resource type. In this case, for HPC, we've continued to innovate both at infrastructure and platform level to offer solutions to run your HPC applications on Google Cloud your way. We offer support for all major ISVs and partners, including schedulers such as Slurm and application ISVs such as Ansys, Altair, Siemens, and Cadence, to name a few. We are also innovating with Google's cluster director to manage large, dense clusters with seamless maintenance and dynamic workload scheduler to offer high obtainability and flexible consumption when running your HPC workloads. And as we talked about in the general purpose section, right, Google Cloud offers the broadest range of offerings across the infrastructure stack, including CPUs with instances designed specifically for HPC use cases, and then our high-performance GPU offerings, as well as our proprietary TPUs. But I want to share something new that we've launched in the HPC family. So I'm excited to talk about H4D. This is the new HPC optimized family that we've built on AMD's fifth-generation turned CPU. The latest performance is critical for HPC to reduce and get the best TCO per job. And H4D is showing up to a 90% improvement for HPC workloads. So H4D offers the best price performance for HPC workloads in our compute portfolio. And to take advantage of H4D's high performance, we're introducing for the first time up to 200 gigabits per second of cloud RDMA enabled by Titanium to enable efficient scaling to over tens of thousands of cores and providing approximately three microseconds in one-way latency. But we know HPC is not all about just infrastructure. Google Cloud offers support for all major ISVs and partners so you can run your HPC environment. And with the proliferation of AI, we've seen customers need to store, manipulate, and move more data faster than ever. And last year, we launched our local SSD storage optimized Z3 family. This is the first instance to feature our Titanium SSDs and provide up to 36 gigabytes per second in throughput and 35% lower latency access to your data. We are expanding our Z3 family to include nine new VMs scaling from 8v CPUs to 44 CPUs. So giving you smaller shapes with 3 terabytes to 18 terabytes. So giving you a lot more flexibility to right size your databases and platforms running on our Z3 VM. And as I mentioned earlier, we also have our new metal offering within the Z3 family that will offer even more storage up to 72 terabytes. So plenty of offerings from 3 terabytes to 72 terabytes on our Z3 family. And we've also seen a similar trend with customers who have asked us for more memory to store their expanded data sets. And they want really rapid access to the data. Our newest M4 instances offer up to 65% better price performance and 2.25x more SAP application performance standard or SAPs compared to the previous generation for SAP workloads. What's great about M4 is it offers multiple sizes ranging from 372 gigabytes of memory all the way to 6 terabytes of memory. A common thing I've heard at next year this year has been, I need more memory, right? I need a lot more memory. So M4 is one of the options that we are providing to give you more memory to the compute ratio. And I'm excited to share that M4 is already certified for business critical in-memory SAP haunted workloads ranging from 744 gigabytes all the way to 3 terabytes. And also certified for SAP NetWeaver application server. Now I'm going to invite Ken Exner to come up to the stage. And he's going to talk about Elastic's AI search platform and the benefits that they have been able to achieve with C4 instances. Cool. Hey. Thank you, Alex. Hey, good morning. Good to be here. I'm Ken Exner from Elastic. And for those who don't know what Elastic is, we're the company behind Elastic Search. So you could say that like Google, we have deep roots in search. We all know Google from its ubiquitous web search, but have you ever thought about what powers all those other e-commerce and website experiences in terms of search? Search is everywhere. Search is everywhere. Like it's in Shopify, as we saw earlier with Maddie's presentation. It's in various e-commerce stores like Home Depot and Dick's Sporting Goods and Walmart. We're the company behind those search experiences. Elastic Search has been powering search experiences for more than a decade now. And as search has evolved from sort of text-based search into semantic search into vector search, and now powering generative AI applications through retrieval augmented generations, we've been evolving with these customers and leading the way in helping customers build search-powered applications. But people have also found other uses for search. People use search to search through logs and metrics data. And we've provided out-of-the-box experiences for observability and for security for customers that want to do threat hunting using the Elastic platform. So there's lots of different use cases for search. But customers also have a number of different options for how to deploy it. We are open source. So there's millions of downloads of Elastic Search. It's actually the most popular Java open source project of all time. And people have been using this to power search applications. You can also buy a premium license and get access to premium features and manage it yourself. We also, of course, we wouldn't be here at Google Cloud if we didn't have a cloud offering. So you can use Elastic with the Elastic Cloud service. And with the Elastic Cloud service, Elastic Cloud hosting, we will provision a VM. We will install the software. We will keep it updated. We'll keep it patched. But one of the things that we've been hearing from customers is that they would like even more. They would love it if we could provide a more fully managed experience. They would love it if they didn't have to think about the underlying resources at all. So with the current cloud offering, customers still had to think about things like cluster management, cluster health management. They had to think about things like auto scaling. What they really wanted was a more fully managed experience where it was more SaaS-like. So we introduced recently Elastic Cloud serverless, which is essentially a fully managed experience where customers are completely abstracted away from the underlying resources. Now, how did we get here? This was a bit of a challenge because Elasticsearch is a data store. It's a database. And like traditional databases, it's a stateful system. So you scale it by essentially adding additional nodes. You achieve durability by creating a replica. But it's a stateful system. Compute and storage is in one system. So the first thing we had to do is we had to separate compute and storage. We had to essentially make it a stateless system, which was a bit of a challenge. So we essentially wanted to take advantage of cloud storage. We wanted to be able to use object storage to achieve durability, to achieve the efficiency we wanted. So we separated compute and storage. The next thing we did is we said, there's all these other components of Elasticsearch, like a search tier, like an indexing tier. We wanted these to be separate systems that could scale independently. So we essentially did a bunch of surgery in Elasticsearch to create a modern cloud-based architecture, where each of these different components could scale independently and be fully automated. And Google was a huge partner in making this happen. We leveraged Google not only for expertise and how to do this, but also what services on Google to use. So if you look at our architecture today for our serverless offering, it's kind of a poster child for sort of a cloud-native experience. We, of course, used Google Cloud Storage for its object store. We used the Kubernetes engine to power most of our compute services. We used pretty much every networking service we could find at Google, from NAT and routing and load balancing, VPC and more. But the thing we were most interested in trying was the new C4A processor. With Elasticsearch, we support ARM-based architectures. We support people running ARM, but we wanted to try this ourselves. And what we found was that with the new C4A processors, we were actually able to get not only better search latency, but 40% better indexing throughput. 40% better indexing throughput. That's a big deal for us. That means that we have a lower cost structure that we can use to get better margins, but also pass a lot of those savings on to our customers. So I wanted to just finish with this. Alex went through sort of a dizzying array of different VM options from Google. I wanted to share how we choose VMs. And for us, this is essentially an optimization exercise. We're trying to find the right balance of price, performance and resources. And by resources, I mean what is the shape of the resources we need for any workload? Do we need something to be heavy on CPU or heavy on disk? Do we need more memory? Do we need more storage? And because we run so many different use cases on Elastic, we actually have a mix of these. So if you're running a logs workload, logs analytics, we tend to be more storage bound. We tend to be more memory bound. But one of the things that we found as we moved to this new serverless architecture is that most of the different services that we had, because we had separated out storage, we were mostly CPU bound. So we could run specific benchmarking tests to try to optimize for CPU for particular workloads. And we had benchmarking that tried to optimize for indexing throughput, that tried to optimize for search latency. So my advice to you is figure out the shape of the resources you need and then pick a key performance indicator that you're trying to optimize for. And then you can usually get the best price performance for that specific metric that you're trying to optimize for. And if you get stuck, talk to the account team. The Google account teams are amazing to help you get to the right answers quickly. So with that, I want to just finish with a plug for Elastic Cloud Serverless. It's available in preview today. We're going to be going GA later this month, so very, very shortly. It's a magical experience built on Google Cloud. Thank you. Great. Thank you, Ken. That is really, really impressive to see how C4A has actually led to real benefits, but also new products. Really, really interesting stuff. So just to recap, it is a dizzying array of options, but there's a lot of things. There's a lot of options, but there's a purpose why we're doing that. We've innovated and invested in both our general purpose and workload optimized compute portfolio for customers to get the benefit. I want you all to have the choice. So you can pick C4D with the latest gen AMD, C4 with the latest gen Intel and the first cloud to have that available, C4A with the leading performance on ARM, and our N4 family, which complements everything. It's a beautiful TCO offering. So we've also expanded our portfolio specialized, right? So these are where we optimize for specific resource type, like Ken was saying. So depending on what your KPI is, you can use products like our M4 memory optimized VM family, our H4D for HPC, or the Z3 offering I talked about, which has dense local SSD storage. And just to bring it all back together, right? Underpinning all this is titanium, and this is going to continue to bring new capabilities and innovations to customers, including CloudJRDMA, which we talked about today, and the titanium SSD. So look out for a lot more coming in the following years. And we're really, really excited to see what customers will build with these new compute offerings. So with that, I do want to talk about a few other sessions coming up today that I think will be really, really relevant to the group here. Following this, we have our spotlight session by Mark Lohmeyer, the vice president of Google Compute Engine and infrastructure. So he's going to give a talk. He's going to talk about a lot of new launches and innovations. We have our Axion session coming up at 10.30 today. And then tomorrow we have another session which is all about how to work on Google Cloud, some of the best practices around optimizing cost, workload management, and scalability. So with that, I did save some time today to do some Q&A. So I'm going to invite all my guest speakers up, as well as Salil Suri, Director of Product Management for Google Compute Engine, for any questions that the audience has for us today. So feel free to raise your hand. We have a few mic runners that can take your question. Hi. I've heard Maddie and Ken both mention your companies also use Google Kubernetes engines in addition to Compute Engine. So we're actually looking to migrate our existing compute running on GKE and Cloud Run onto Compute Engine. So my question is, do you have guidelines on what kind of workflows would be more appropriate on Compute Engine versus the other Google Cloud offerings? So we do use a mix of both in our fleet. What I'll say is that GKE's node management is kind of the secret sauce there, more so than I think even the Kubernetes layer that goes along with it. Like, if you have a large enough fleet, you'll figure out how to run your nodes one way or another. I mean it's kind of a big, big, big, big, big, big, big, big, big, big, big, big, big, big, big, big, big, big, big, big. It's really great. I think, you know, I'll give you a little bit of a lot of the code that you can use GKE to manage the resources for auto scaling in particular is what sort of drives our use cases there. Now we have also a lot of use cases that are much more static, particularly for our stateful systems. And those are places where sometimes just running them in VMs where we have a lot more consistency in the configuration of those VMs is actually better for us. But ultimately, we like having both options in our fleet. I'm not sure I have much to add other than if you are using Kubernetes, are you using Kubernetes for your workloads? Yeah, we're primarily a data engineering organization. I mean, that's the first choice. Are you going to be using Kubernetes? And if you're going to be using Kubernetes, you should be using a managed service that does it. And a lot of that's going to be based on how big is your fleet and how easy is it for you to just swap in and out different VMs. Like if you're completely stateless and you can essentially have a scalable fleet, you should just be using Kubernetes. It's the industry standard way to do this. I think that's your bigger question is, is it going to be managed by Kubernetes? If it's going to be managed by Kubernetes, use the Kubernetes service. OK, thank you. Do we have any other questions? No. OK. Well, I want to thank you all so much for joining us today. It was really, really great to talk to you all about some of our new innovations. Please, please enjoy the rest of your next and check out some of the other sessions.