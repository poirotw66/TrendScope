 . Let's cover what's new in day two ops on Google Cloud. We're going to talk briefly about our unified observability platform and then jump into app-centric management. When we look at how customers collect, route, and store data with us and then make that data useful, we've done a lot of work to improve that experience since we last met last year. That includes automatic observability out of the box for popular generative AI workloads, meaning things like NVIDIA Triton or Jetstream, automatic out of the box using Prometheus metrics on your Google Kubernetes engine workloads. That also means things like automatic tracing for LLM observability improvements, allowing you to understand what's going on inside, going on inside in the flow of your agents, and better understand how to troubleshoot issues there when things go awry. We've also introduced a new OpenTelemetry native router to allow people to send us OpenTelemetry native data so that they can ensure that whatever they use OpenTelemetry for, we can ingest and store it and make sure that it's useful for you. That also means if you choose to route it to us and use it internally to our tools or send it on to a third party, OpenTelemetry as a native implementation means there's no weird changes or behavioral things that might be different from the rest of the upstream projects. All of this is in pursuit of building an observability platform that allows you to have access to all types of data using metrics, trace, and logs in a single location. That ultimately allows for us to deliver API and AI assistance access. Things like event annotations, correlations from dashboards, and dashboard versioning become available so that you can automatically understand what's going on with very little configuration beyond what's automatically out of the box. That allows us to go from a metric to a trace to a log from a single location without you having to move across multiple tools or windows as you're trying to put out a fire inside your environment. We've also introduced a new cloud application management platform. Powered by our App Hub data model, that provides things like application monitoring. You get golden signals and real-time alerts without having to configure any additional telemetry inside your environment. Cost management to help you understand what the total scope of cost is for your individual workload. It also brings everything together in a new surface called Cloud Hub that shows you this and many other surfaces for how you think about applications when trying to understand how your things are performing, what they're costing you, and what you might need to do to get ahead of things. On top of that, we're introducing Gemini Cloud Assist. Gemini Cloud Assist provides new generative AI capabilities on top of our observability and app-centric data to deliver new experiences, like an application design center where you can have human interactions or almost human interactions to explore, change, and modify your architectures and then deploy to your environments. It also brings things like investigations where you can work with Gemini to provide error messages or symptoms that you're experiencing when having issues and try to troubleshoot those with Gemini. It can give you recommendations on ways to solve or resolve issues, and it can also help you pass this along to support if you need additional help. We're really looking forward to getting this in the hands of customers, and we look forward to your feedback. Thanks.