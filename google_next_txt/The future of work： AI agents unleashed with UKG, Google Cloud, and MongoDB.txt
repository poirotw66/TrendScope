 Welcome everybody. Thank you for taking the time out of your day to come hear us talk about AI agents. I've got a partner here with me to talk about their implementation from UKG. My name is Rick Houlihan. I'm the field CTO for Enterprise here at MongoDB. I have a long and storied history in NoSQL and I actually have a much longer intro but it usually it's going to play very well into the first chart here so we'll talk a little bit about that when we get into that but the agenda today is really we're going to talk a little bit about MongoDB, how we play in the AI space and I'm going to turn it over pretty quickly to Matthew to talk about what we're all really here to hear about which is what UKG has done with this wonderful technology. But one of the things that we always like to talk about to our customers is what we're faced today in the industry. We're faced with what we call a crucible moment, right? A crucible moment is an inflection point where a choice that we make today has an outsized impact or bearing on the trajectory, either your corporate trajectory or your personal trajectory for years or even decades. And in my personal life I faced a crucible moment about 10 years ago, a little over 10 years ago when I was asked by the then GM of DynamoDB, a man named Jim Scharf, who's currently our CTO here at MongoDB, to run the black belt team for NoSQL technology at AWS. And it was a very big, big project. You know, we were deprecating 3,000 Oracle server instances. We were moving 10,000 application services and we were moving all of those workloads, about half of those workloads anyways, moved to NoSQL. And my team built the center of excellence, you know, for that process. It was a tremendous challenge and an enormous risk to my career to take because I didn't really feel that I was qualified, you know, to do that work. But again, these are the choices you make that end up having huge impact on your career. And I can say that 10 years later, probably the best choice I ever made was taking on that enormous project and seeing it through to its successful conclusion. It really had an amazing impact on my career. And today, we are all faced with another crucible moment, right? For all of us today, it's about AI. And AI promises to reimagine everything about our business, right? About how we run the business, about how we interact with our customers, how we build our products. And because of that promise is so compelling, we're waking up every day, we're seeing more and more headlines about AI and what it means to the business and what it means to you. But really, what it comes down to when we look at, you know, AI or any new technology, it's about how does tech adoption actually happen, right? And we're seeing this, you know, with AI today when we look at major platform shifts, you know, internet, cloud, mobile, we see a very consistent pattern. We're seeing that reproduce itself with AI today. And that is a tech adoption. It happens in S-curves, right? Generally speaking, we see that the impact increases and slows down over time. But over time, we see that impact increase and go up and to the right. But if we look at things at the wrong time, what we see is hype, right? And then again, if we look at things at another time, we might consider that, you know, maybe this was a myth or this was a fad. And probably the best example of this was in 2000, right, when the internet bubble burst and we saw that the headlines in the newspapers everywhere telling us how the internet may be just a passing fad, you know, everyone's giving up on it. But the reality was we were around the cusp of a new era, right? A time when the internet was about to change everything about how we do our business, about how we interact with our friends and family around the world, and fundamentally change our lives and change how we do business. And I think today, obviously, we're faced with that same thing that, you know, that we saw then. We're faced with that same situation, you know, with AI. And if you look at what we faced over the time, as we've evolved from legacy applications into the cloud, and we've seen this sprawl of cloud services kind of evolve around the core database, right, to add layers of functionality like full text search, vector search, we'll talk about in a second, you know, online archive, and all of these various areas of functionality that are kind of becoming table stakes now for modern workloads. What we're seeing with AI is it's reigniting this sprawl, right? It's another service that we need to bolt on top of our operational data stores. And it's increasing the complexity of how we work with our data. And to understand why this is, let's talk a little bit about vectors. I mean, people are generally starting to become more familiar with this technology. But vectors, essentially, they allow us to correlate data in ways that we were never able to do, right? Find similarities in unstructured data, between data of different types of, you know, media, documents, audio files, video files, and essentially pull these things together to do really cool things, like, you know, semantic search, or retrieval augmented generation to feed our AI workloads, our LLMs, to provide more accurate responses, right? And the reality is that as we've looked at this new technology, again, it's been that kind of bolt-on functionality that we need for AI workloads, but it doesn't exist in the current operational data stores. So we have a couple of choices, right, when we go do this. The first choice is to use a standalone vector database. This would be something like a pine cone or whatnot. We could put that out there on top of our operational data store. But you, as the developer, then, are responsible for connecting those two pieces, right? For making sure that the vector index is synchronized to your operational data and that those embeddings are actually pointing to the correct records, maintaining that ETL pipeline and that synchronization. That becomes overhead for the developer. And as a result, we see the other option that people turn to is the integrated, you know, vector solution. Maybe a PG vector on top of Postgres or something like this. And one of the problems that you're going to run into very quickly with this and we see customers run into is the fact that it's competing. The vector index and the AI workload starts to compete with your operational data for the same resources, right? It doesn't scale independently. We learned this lesson with analytics nodes in MongoDB Atlas years ago. As we introduced new functionality, customers would tell us, well, that's great, but I need dedicated nodes that can scale, right, with that for that functionality. So inside of the MongoDB replica sets, you can actually configure different characteristics or different hardware characteristics for different nodes in the cluster that support different use cases. In this case, now, we have Atlas Vector Search that rides on top of the MongoDB Atlas platform and it's integrated with your operational data. You don't need to worry about the ETL or the pipeline. You don't need to query multiple systems to be able to get pointers back to your operational data. You query one API, one system, and you get the results you need. And not only that, but we can scale those vector search nodes independently of the other nodes in your cluster as we can with every function that we put into the Atlas platform, right? So with MongoDB Atlas, it's not just a database. It's a collection of functionality that developers need to build modern applications. And again, deliver it in a package that allows you to scale those workloads independently of each other while maintaining the replication of your operational data across all of those silos of functionality. So it's an enormous amount of work that you don't have to do as a developer. And it is what we believe is and we know is the world's in the first developer data platform. It's more than a database that provides all of these areas of functionality. And what I'm going to do is now turn it over to my friend and partner here from UKG, Matthew Marchand. He's going to talk a bit about what UKG did with this platform to build an excellent Asian architecture for their workloads. Thanks, Rick. All right. Hi, everyone. My name is Matthew. I'm a senior engineer at UKG in the AI team. And today we'll be looking into who is UKG. Our purpose is people and we are an HR, payroll and workforce management solutions company. So let's look at technology in terms of how it can be used for people throughout their career, right? Here I have an example of Yusuf who is looking for a new job, right? So he'll go through many steps for that. The first thing would be applying to a new job. Hopefully he gets it. You know, congrats. He will go through the onboarding process. You know, this means choosing benefits, choosing maybe some other stuff. And then he might want to manage his schedule, right? So change his schedule, apply for some overtime, for example. And then he might want to upscale and get a promotion. So he will contact his manager, set some goals with him, review those goals, make sure that that process goes smoothly. And then finally he will get a promotion, right? So congrats to him. And then, but he needs new skills now. He needs to upscale. He needs to learn new stuff, learn to manage people and learn how those people are behaving in his team, for example. So technology should be there at every moment of that process to help people create better outcomes. That's where UKG comes in with our unique data set because we are a system with a suite of software and we have three categories of data. You can see we have people data, so performance, skills, and pay data, for example. We have work data, so time, scheduling, like I said previously, leave or accruals, for example. And we have culture data, so sentiment analysis. We have survey, messaging, ERGs, right? So this data set enables us to use AI, for example, to create, classify, summarize, and search insights, for example. And we'll look into that with our newest product called Bright. So what is Bright AI? Bright AI is a trusted advisor helping your people unlock better outcomes. And in this presentation, we'll look into how we build Bright. But first, let's look into use cases and the value added by Bright. So the first thing is Bright starts with a simple text box. And you can ask questions or actions or any insights you'd like to. We'll look into three use cases that can be resolved with Bright. The first one is frontline assist. Here we have someone asking Bright, make me available on Thursdays from 10 a.m. to 6 p.m. And Bright answers, we're done. You're now available for those hours. So Bright can understand the action and perform the action. That's the frontline assist use case. The second use case is employee assist. Here we have two examples that are very different. The first one is someone asking, what are my benefits? You can see Bright answering about a bullet point list of benefits. But then another very important point is that Bright is citing its sources. You can see at the bottom, there's a U.S. employee handbook link. There's a view the complete list link. So Bright enables the users to go further down their questions, but serves as an entry point for your question. We have also another question about how much do I make each week. Then Bright got this action, got the data, summarized it, analyzed it, and returned with the answer. Again, Bright proposes links to enhance your information, go to the specific page, maybe some other products we have to enhance your pay or understand your pay a bit better. The next use case is a great place to work assist use case. You know, we have someone here asking, how can we make our company's promotion process more fair? Very tough question to answer. But how can we trust an AI to answer this tough question? Well, we have the great place to work content that we have access to, and we can look at that content and then provide sources. So here we have the answer, which also includes links towards sources for a trusted answer from a created data source. It's a great fit for people manager, business leaders in general, and HR managers. So we saw a couple of use cases across the board. We saw scheduling. We saw pay. We saw people use cases. So how does that work? Well, Bright is powered by a set of Flex AI agents. So I'll come back to what Flex AI is, but the main point is we have a Flex AI agent orchestrator and many sub-agents, which then have also sub-agents. So, for example, we have the pay agents, which has sub-agents. All right? So coming back to our use case about asking for pay details or pay explanations, we have the pay agent, which has more details agents. And we can build those agents with all the products we have. So in this case, we'll look into how this graph or this agentic system was built. So how is UKG doing AI? We'll look into two components that are very important for us. The first one is the agent framework, which powers the Bright agent experience, and the retrieval augmented generation, so the RAC platform, as Rick was mentioning. But first, let's ask ourselves, why are we building an AI platform? What's the purpose that an AI platform solves? Well, three main purposes for us. The first one is centralizing knowledge to drive innovation. The goal is to avoid all teams at UKG, so scheduling, pay, timekeeping, to create their own AI with their own tools in their own ways, right? We want to drive innovation by having a centralized team, which can learn together and drive innovation. Second point is a very important one, ensuring secure and responsible AI usage. By having an AI platform, we can ensure that we have guardrails in place, telemetry, metrics, and checks throughout the company to ensure that everyone at UKG uses AI with the most standards, the most uphold standards. And the third one, which is maybe my main focus, is enabling domain team to provide value. So in the AI team, we're not experts on pay. You know, we have experts in pay. We've been building pay systems for the last 20 years. We have a lot of expertise in that. So us, the AI team, we're building common components, boilerplate components, observability evaluation, and we're letting domain teams actually build use cases, build a value, so that we can have great products. Just a small slide that I'm sure everyone knows about. What is an LLM? Just to make sure we're all aligned. A large language model is a statistical language model that was trained on static data to generate text. Of course, you can generate other models, as we've seen, but the point here is about text. And how are LLM used in Bright? Two main purposes. First one is generating an answer. So when you see a result coming back from Bright, it was denuded by an LLM. The second thing is determine action. So when you see the flow in the graph of the agents, the LLM drives the action. That's a very important point that creates an agent. All right, now the fun part begins. It's about the agent framework. We'll look into how do KG build a platform to drive AI innovation at scale. But first, let's ask ourselves, from a product perspective or a technical perspective, why do we need an agent framework? Well, a couple of problems. First thing is rigid workflows. Previous systems had rigid workflows. This means that they were programmed or set up by someone manually. The second thing is usually rule-based chatbots or previous chatbots, they had limited understanding of what they were doing. So there's a need for adaptability, meaning faster and faster automation, faster and faster value to the customer. And there's a need for scalability, so to reduce dependency on manual workflows. So as a solution, we'll have orchestration, which can execute tasks and integrate seamlessly with enterprise systems, which is our APIs. And we'll have enhanced interaction. So we're using an LLM to provide enhanced interaction with context and more detailed answers. But for that, how does it work? We talked about agent throughout Google Next, but no one talked about function calling. That's the power of an agent. So function calling is about application that sends a list of available actions and a prompt to the LLM. So, for example, when I ask what is my pay, I'm going to send that query to the LLM along with a list of possible actions. Those actions would be tools or functions. The LLM will then identify the action or, very important point, ask your follow-up questions. That's the power of the LLM. They can ask for follow-up questions, meaning that if the action needs some fields or the LLM is unsure of what action it should trigger, they can ask follow-up questions to precise and have very accurate results. Then when the LLM identifies the action, it will return back to the application, so in our case, the agent framework, saying, hey, please invoke this action or this tool or this function. And tools functions, they can be APIs, data sources. As we've seen throughout the conference, it can be anything. The application then performs the action and sends back the result to the model. Now, two things can happen. The model can take the data and generate an answer, saying, hey, your pay is this amount and you're done, or it can ask or create another action. That's where the power of LLMs comes in by being able to create workflows because you could fetch some data. Then the LLM says, you know what? I've analyzed this data. I need more. Now I need this, right? So now they could have a workflow built with this or a step-by-step workflow depending on data that's being returned. That's the power of function calling. So if we summarize quickly what is an agent from a technical perspective, we can describe an agent as a system that leverages function calling to drive goals and execute actions. So it ultimately decides which function invoke based on context and user input. And this enables us to create dynamic workflows. So now, again, another important slide about what our framework does, a couple of components that are key points. First thing is no-code agent creation. If you want to scale our agentic experience, we need a way to create agents very quickly. This means no-code agent creation. So for us, an agent is simply a configuration. So we're talking about configuring the prompt, configuring the instructions, for example, or configuring tools. We provide seamless eval and testing. This means we have services in place to evaluate your agent with a golden set, for example, to give you back results, saying, hey, you created an agent, but it's not answering the questions that you would like, right? Maybe some of the questions you're providing are not getting answered. So we can provide metrics on that to make sure that when you deploy your agent, it's actually been deployed and offers value. Then, of course, we have monitoring and observability, meaning that when we deploy your agent, we can have metrics on hallucinations, you know, completed tasks, and how it's been performing. So we can have feedback to those domain teams to inform them how their agent has been performing. The main step that we've seen throughout is multi-agent orchestration. This means we're able to connect agents together, right? Bright is a big agent with many sub-agents, so we can connect those agents together. What this means, basically, is that when an agent knows that it can't trigger an action or answer the question, it will redirect to another agent. So agents work together to help the user. We have the last two points are kind of together. So tool and API integration. At 2KG, we've been building APIs for decades. So we have many data sources and many APIs. The challenge is how do we connect those APIs? So we have tools in place to help you connect your APIs to an agent. And the next point is about usability. So once you've connected all those APIs for your agent, you can register that API, and we can reuse that API throughout the agents. So if the benefits team wants to create agents, they can register their tool and then create agents faster after that. So now, how does it work? I'll go through the flow, and I'll come back to the key components after. The first thing is when someone asks a question, it goes through the agent runner. The agent runner will fetch the configuration because, like I said, every agent is simply a YAML or a JSON configuration. Once it has the configuration, it will create a land graph graph. Land graph is a popular Python graph library to create dynamic workflows. And then the land graph will call either vertex AI LLMs for function calling, for example, and then tools. And tools could be any UKG internal APIs that was configured. And then we have the state store with Mongo as well, which enables us to store the state of the graph because there can be interrupt. There can be errors happening during the graph. So we need a way to store its state to resume in case if something happens. So we have a state store and a config store with Mongo. So if you summarize how our agent runtime works, we have the land graph orchestrator, we have the Gemini LLM for reasoning, and we have MongoDB configuration and state store. So a bit about land graph, that's what we use to build our framework. It's built on top of land chain, which I'm sure most of you know. Land chain is a popular Python library for LLMs. Land graph has graph-based execution, so we can create workflows that are either deterministic or non-deterministic. Land graph offers state management and memory handling. And on the right, we have an image of a small graph I've done, which is the land graph. But of course, it's a graph library, so you can imagine the graph being way more complicated. So if we resume what I've said so far, you know, for us at UKG, running an agent simply means running its configuration. So when you send a message to an agent, you're simply running a configuration, calling APIs, storing your state, right? There's no pending resources. There's nothing running in the background. It simply means running a configuration, running a graph, storing its state. All right, and now we'll talk about the RAG platform. So the RAG platform is a very important tool for the agent framework because the RAG platform enables use cases for search and act use cases. So we can search with data and then change or update data, right? So, and the search part comes with RAG. So it's about how to build effective search on proprietary documents. As an HR company, we have many documents throughout our suite, so we can think about HR or benefits documents. We can think about policies, right, like travel policies, remote policies. You know, we have thousands of policies at our companies, so we need a way to search through them effectively. We also have use cases with knowledge-based access, so this means that we can use our RAG system to power, for example, customer support use cases. We can power, you know, if you need help about your company and how does it work, we can reduce time for IT support or HR support by having those documents into a RAG system. So same thing as with the agents. Why do we need a RAG platform? Well, problem is LLMs are trained on static data set. If you ask your LLM about your policy at your company, it won't know the answer. It was trained on a static data set that wasn't updated with your data. Poor retrieval. The previous way of searching through a policy was to use control F, for example, with keyword search or some sort of text search. That's not efficient in our current world of natural language. We need a more powerful way to search through documents than just keywords or text. Knowledge silos. Enterprises are often storing their policies or data across many data sources. This means that you might have a policy about remote work on some website, but then the benefits documents are someplace else. So there's no unified data source. So there's a need for three things. First is semantic understanding. Like I said and what Rick talked about, there's a need for a more precise and complex search algorithm. There's a need for enterprise adaptability. So they need to be able to leverage their own documents. There's a need for context-aware responses. So if the context of the employee matters in the answer, they need to use that when looking through documents because when you look through documents and you find something, who knows if it applies to you or not, right? You don't know what applies to you, what doesn't apply to you. So there's a need for context around those documents and context about you and that information. So a solution, intelligent search, semantic search, more powerful way of searching through text, and augmentation. So augmenting the LLM with more details, context, and information that's proprietary. So if we summarize what is RAG, the first thing is retrieve relevant documents. This means we're going to convert the user's query into a vector, and then we're going to compare this vector to the vectors in our database. That's called using an embedding model. Once we do that, we'll get, let's say, the top 10 chunks in our database. A chunk is a part of a document. We can take a chunk, like a paragraph or a few sentences, for example. So we'll retrieve the top 10 chunks in that vector store using semantic search or vector search, and we'll then enhance the prompt with that context, meaning that the prompt will read something like, here's the question. Here are some chunks. Here are some context about those chunks. Please answer the question. And then we'll send that prompt to DLM for a generation that will be way more thorough and complex than just a regular question. So I talked about the vector store, vector search, so there's a need for ingesting documents. That doesn't happen on its own. So it's a four-step process. First thing is text extraction. This means, you know, if you have a PDF, a Word document, a web page, you need to extract the actual text. Text splitting, this means chunking or splitting the text into smaller chunks. So if you have a 40-page document, a 100-page document, you can put that in. It won't work well if you just do that in a vector store. You need to chunk it to smaller pieces. There's embedding creation. This means calling the embedding model from Vertex.ai to generate those vectors that we were talking about, and then you index those vectors into a vector store. So, for example, when a new document comes in, and that can be from a UKG-specific document, a tenant document, so if a customer has a new document, for example, or public document that might be relevant to our use case, when it's available, triggers the process. The first step is loading, like I said, extracting with Vertex.ai, chunking, splitting it to chunks, embedding, which is calling the embedding model to generate a vector, and finally inserting it to the database. This is all powered by Langchain, which is, again, a Python library for a length of interaction, and we use Vertex.ai for embedding model, and MongoDB as a vector store. So in the vector store, you'll have chunks embedding, chunks metadata, file metadata, and the collection ID. Retrieval process. So now we've loaded our vector store. That's great, but now we need to ask questions. So the first step is, similar to the agent framework, we'll fetch its configuration. Why? Well, because every use case is different. You might need a more detailed answer than some use cases, and a more precise one, or a shorter one for some other use cases. So we have configurations to enable one platform for many use cases. So you can configure things like the prompt or some retrieval settings you might need to change or switch, depending on your use case. So once the rag runner has fetched a configuration, it will create a Langchain chain. If we remember with the agent framework, we're creating the Langgraph graph, now we're creating the Langchain chain. Same concepts, just different libraries. And the chain will be embedding the query. This means converting the query into a vector. It will be retrieving documents, chunks from the vector store. This means doing a semantic search and comparing vectors. And then we'll be generating an answer with the enhanced prompt. MongoDB Atlas Vector Search. I talked about a vector store, and Rick talked about it, so I'll just summarize why we need one. So a vector store is a database optimized for storing and retrieving high-dimensional embeddings. It's commonly used in AI-powered symmetry search. And why Mongo Atlas Vector Store? Well, as Rick mentioned, cloud-native, fully managed. We want to scale our solutions as much as we can. It's something that scales for our needs. Integration with MongoDB's document model. So we can store embeddings and vector search as well as regular data in the same database. This enables us to have one database for two use cases. And we have dashboards and analytics to perform optimization. When doing a RAG system, the retrieval is very important. So MongoDB provides us with the insights, metrics on how our index and our search indexes are performing for better retrieval. All right? That's it for me today. Thank you all. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.