 Rakesh Dhrupar My name is Rakesh Dhrupar. I'm part of the product management team at Google. Joining me are one of my colleagues from Google, Alok Srivastava, and a couple of distinguished colleagues from TCS. Deepthi, who is the head of the SDLC offering at TCS, and Laks, who is the global head of cross-industry offerings. So let me just set the stage a little bit, like what are we going to cover today? So we're going to divide up our overall session into two parts. First, Alok and I will primarily focus on the NIDE experience for coding assistance. We'll not talk a lot about the features. I think there have been a lot of sessions talking about the features of coding assistance. But what we will talk about is how do you adopt Gemini Code Assist in your enterprise? A lot of folks think that, oh, it's just another tool. You install it, give it to the developers, and off they go. But it's really about managing change in the enterprise. So we will talk about what that journey looks like and how should enterprises adopt coding assistance tools in general, but Gemini Code Assist in specific. In addition, we all know that developers spend a certain amount of time in the IDE, but they do a lot of other tasks outside the IDE, right? The entire SDLC. And so Alok and I, like I said, we will cover the IDE experience, and then Deepthi and Laks will cover the entire SDLC journey. So let's get started. All right, you've probably seen a lot of slides, a lot of numbers around how AI adoption in general, not just around coding, is rapidly picking up. One of the things that I find really interesting about this technology is that there is a push for adoption from both sides, from top as well as bottom, right? So executives, C-level suites, they're all working towards how do we adopt AI, not just for coding, but in general, in the enterprise, to derive all the benefits. At the same time, there's a lot of push from the IT and the development organizations as well to rapidly adopt this technology and make their businesses more competitive. Now, as this adoption journey has been going for the past couple of years, we have started looking at what is the impact this is creating. So before I jump into the next slide, I wanted to do a quick show of hands. How many of you in the audience here have heard about the Dora Report? Just a show of hands. Okay, about 20%. That's good. Let me just give you a little bit of a background on what the Dora Report is. So the Dora Report is basically research coming from Google, and we've been doing this for the past dozen years or so, where we talk to a community of about 40,000 developers, and we run through surveys, and we try and find out what are they doing in their engineering practices. So for the past 10 or 11 years, we've been summarizing this along different metrics for measuring development throughput and development velocity and development stability. This year, we actually enhanced the research to ask these developers questions about AI adoption. And some of the results we actually found were pretty interesting and pretty amazing. So I will share those with you as we get into this. So first of all, here's a quick summary of the benefits that development teams are actually finding based on this research. So if you look at documentation quality, that seems to have improved quite a bit. Code quality is moving in the right direction. That has moved in a positive way. Code review speed is also moving in the right direction. But the things towards the bottom, you know, they're slightly off the main axis. You know, some are positive, some are negative. Okay, so that's interesting. But the thing that was the most surprising from this research, that despite the fact that development productivity is improving, code quality is moving, but two main metrics that the DORA report actually measures are moving in the opposite direction. So what we found was delivery throughput and delivery stability have actually moved in the negative direction. Delivery throughput a little bit, but delivery stability a lot. Now that was a very interesting finding for us. To some extent, we're still puzzled as to why that happened. However, the team and all the data scientists that are involved in this research have put forward a hypothesis. We do not have data to validate the hypothesis, but the hypothesis is the following. That over the past 10 years or so, as the DORA research has been collecting this data, what we have consistently seen that throughput and stability have increased because the pull request size has constantly decreased. So development teams and software organizations are able to make these pull requests smaller and push them out faster. However, our hypothesis is that because of the use of GenAI tools, these pull request size are probably going up. Developers are able to write more code, and as they're able to write more code, the pull request gets bigger. So they're certainly being more productive, but as the pull requests are getting bigger, it's unfortunately negatively impacting the throughput and stability. Let me be clear. This is again, hypothesis. We have not yet validated that, but as the research evolves, we will certainly be looking at whether this hypothesis holds or not. I also want to clarify one more thing so that people don't get confused. The DORA research is actually based not on Google developers. It's for all the enterprise and digital natives and all the developers outside of Google. So if you're finding some of this information conflicting because over the past four days you've heard, you know, how Google has benefited from this personally, don't be conflicted. This research is not based on Google developers. This is based on developers outside of Google. Now, another interesting fact is that a lot of developers still don't trust the code that is generated by generative AI tools. Now, again, analyzing some of the reasons why this may be happening, it's probably because in a lot of teams, generative AI coding assistance has not been rolled out in a structured way, right? Its adoption is probably very ad hoc. Expectations are sky high. And when the expectations are not met, you know, of course developers do not land up trusting or are not enamored by the technology. And last but not least, many organizations are still struggling with how do we actually measure the impact of all of this stuff, right? Should we measure productivity? Should we measure, you know, code acceptance rate? There's a lot of different metrics. A lot of different companies are putting out there. And there seems to be a lot of confusion about what metrics should be measured to actually see what is the impact of all of this. So I'll just make a statement here. And I would actually invite my colleague to actually walk us through more details. We are recommending that adopting generative AI for coding assistance is actually significant change management in the development workflow and the software engineering processes. And they should not be done in isolation, not without incorporating, you know, what your overall workflow looks like, what your policies look like. And that have to be taken into consideration. So that overall, you're not just improving one point in the SDLC or one little portion of the SDLC, but actually making an impact across the entire SDLC. So our suggestion is, which we will drill through, drill into more detail, follow a very structured adoption approach to ensure that Gemini Code Assist, as you deploy that in your environment, it helps build developer trust. And above all, actually delivers the expected value that is expected from your business out of the software and IT organizations. So with that, I would like to invite Alok up here and walk us through what that structured approach could actually look like. Thank you, Rakesh. All right. Thank you, everyone, for showing up. It's Friday morning after a concert at Vegas. So it's a pretty good turnout. So Rakesh talked about three critical success factors for adoption of Gemini Code Assist. They are structured adoption, building trust with developers, and the third one is about delivering the expected value. So how do we realize these success factors? In order to realize these success factors, we have to get on a journey. A journey of Gemini Code Assist adoption that comprises of three parts. Day zero, day one, and day two. And there are three personas tied to these three day zero, day one, and day two operations. For day zero, your primary persona is cloud administrator. So the cloud administrator will work with the relevant engineering departments to define the use cases for which you will leverage Gemini Code Assist. And then the cloud administrator will secure the alignment between various stakeholders in the organization. Right? And eventually, the cloud administrator persona will roll out Gemini Code Assist for developers. And that's where you have the day one operations that begin with developer persona. So developer persona is responsible for enabling and empowering the developers by offering trainings like prompt engineering, Gemini Code Assist, and hackathons. And then comes your day two persona, which is where you want to measure the impact. And your primary persona here is the engineering leadership. Right? So you measure the impact in terms of productivity and then the engineering persona will take a call to expand the number of teams that have. and the number of users in their environment, in their department, who will be using Gemini Code Assist. And that will eventually lead to enablement of new use cases. So let's consider a scenario where you are a cloud administrator and you have to enable tens or hundreds or even thousands of developers. So how will you go about it? First and foremost, you will do an alignment with key stakeholders like engineering, security, and compliance. Now, when it comes to engineering, every department may have different engineering workflows. They may be using IntelliJ. They may be using Java or Python or some other supported programming languages. So you need to integrate Gemini Code Assist into the existing development workflows. You will also be responsible to enable the day two persona to configure every setting required for day two persona for them to be able to monitor the productivity metrics. And last but not the least, while working with engineering, the developer administrator, sorry, the cloud administrator, will be working to build the relevant training and documentation content. When we talk about security and compliance, so let me take an example of a very large customer. We work with them. And their security and compliance team sent us several questionnaires from the responsibility, responsible AI perspective, from data privacy, from data governance to data residency perspective. And it took us several weeks to work with this customer to fill in those details and get all the approvals in place. So it's very critical to have the alignment with the security and compliance teams, not just with the internal standards, but for the external compliance standards as well. So once you've got the alignment done, you need to select the right code assist edition or the right code assist version. Right? So we have two versions, standard and enterprise. In case you want to have a deeper integration with GCP services like APG or BigQuery, then you go with the enterprise edition. Once the license decision is taken, you need to assign these licenses to the developers. And how do you go about it? Fairly straightforward. You can go to the license management portal and Google Cloud's license management portal. And from there, you will be able to assign the licenses to the individual email IDs of your developers. And all the prompts related logs will be stored or will remain within your GCP tenant. Right? So let's say you are a large organization. This is, again, coming from a real customer scenario. So this customer has thousands of developers. And they have their own self-service portal. Right? So they don't want to go to our license management portal. But they need APIs. Right? So we have the license management APIs. You can leverage them and run your own self-service portal for license management. So once license management is done, you need to configure GCP projects or map GCP projects to your Gemini code assist. Right? And this is a must have. And once you have assigned the GCP projects, so in an organization, not everybody needs to have the same privileges. Right? So developers will probably work with the least privileged concept. And the cloud administrator will have the highest level of privilege in terms of getting access to the billing accounts, for example. Right? And some of you might have heard about or might have even done the golden image copies for operating systems or applications. So similar concept applies here. We have seen in some large organizations that the IT teams, they vet or test several versions of GC, and then they make it available to the developers on their internal portal. So when we talk about any code assist, assisting tools or code generation tools based on AI, then context is the king. And this is where Gemini code assist excels. We provide you a bunch of options to have the relevant context. You can get local context by going to the ID and you can say at this file name and then you can provide your prompts or you can provide the inputs, which will result in the code related to the local context. You can do code customization. So this is about getting the enterprise code awareness where you can have your repos, your private repos, mapped to Gemini code assist. It involves several configuration steps. And that will result in the tailored code suggestions for you. Right? So it's not that you will enter the prompt and it will provide you generic code and then you have to do a bunch of editing. Right? So whatever best practices, schemas, naming conventions you follow, it will provide the output in that way. So you can do a lot of things in that form. Third one is the engineering context. So the developers do not want to leave their IDs. Right? So you want to remain within IDs. So let's say I have to read a PRD. Now I don't want to go to Google Docs outside of my ID and then read through 17 pages of PRD. Right? So we provide tools. You can do add Google Docs, say summarize this PRD, PRD name, and it will give you a summary. Right? So you can build upon it. And likewise, we have other tools like GitHub integration, GitLab integration. So you will be able to list the PRs and act upon them, all within the ID. Yep? And for enabling your day two persona of engineering leadership, there are certain things you should do to enable the logging for observability and for measuring the impact. Right? So you need to enable the code assist logs in logging. You can even configure cloud logging to store logs in BigQuery for running some advanced analysis. We'll talk a bit more about what kind of metrics it results when we get to the day two persona. So you have got GCA up and running. Now you want to get your developers to generate code of highest quality. Right? So how do you go about it? So Rakesh was talking about the low trust and he was right low trust in the AI generated code. So how do you ensure that the trust factor goes high? Right? So I was doing this workshop at one of the large financial services company and the developers were new to AI based coding assistants. And a lot of them were using the tools like you would use Google search. Right? And that's not correct. So there are certain prompt engineering practices. There are frameworks like CoStar. There are frameworks like there that is based on iterative prompts, context based prompts. Right? So you have to follow those practices. Then you have to enable your developers with trainings like Gemini code assist basic training so that they are using the supported ID, supported languages. Right? Along with the right prompts. And it's very important to empower the developers once you have enabled them. What we mean here is that you should be running events like hackathons, workshops. Right? To generate more excitement among the developers where they see the actual value. Right? Before they go back to their desk and get going with their daily tasks. Right? So, and it's also very important when you are organizing these kind of events, it's a must have, I would say, to have Gemini code assist champions within your organization. So these champions need to come from the developer community. They need to talk or they will be talking the same language which the developers understand. And that's how you will build trust. And then, let's say you are a developer, you want to try out the latest features. It's all very straightforward. You go to our release page, you see, week over week or in some cases, as you can see in this screenshot, we have developed releases like within days. Right? So, you can then go to your ID and select a specific release which has those new capabilities. With a single click, you will be done. Then comes data operations where engineering leadership is the primary persona. So, here you want to measure the adoption metrics and you want to measure the trust metrics. So, when we talk about adoption metrics, this is about how many users are actively using the product. Right? And how many chat responses are being generated. But these are just numbers. Right? So, the numbers can shoot up and you will see that adoption is very high. But, again, going back to one of the points that Rakesh highlighted that was about low trust in the AI generated code. So, how do you measure trust once adoption is there? So, for measuring trust, you have metrics like acceptance rates. Right? Which is the percentage of total number of accepted suggestions divided by the number of suggestions which have been generated. Right? And then the number of lines of code which are accepted. There are certain bespoke or custom things that you should be doing in addition to measuring the metrics that I just talked about. So, what you see here, these are the four, I would say, sweet spots or key use cases of Gemini Code Assist, unit test, documentation, code generation, and code refactoring. So, you can run surveys with your developer team. They can rate saying that, okay, how much satisfied they are with respect to these key use cases when they are using Gemini Code Assist. They can do a difference in timing, let's say, with and without using Gemini Code Assist, and that's how you measure that. Right? And when it comes to code coverage, then how much, by how much percentage code coverage has increased, that is obviously applicable only to the unit test in the sheet that you are seeing here. Right? So, by the way, so all of the day zero, day one, day two operations that I talked about can be performed all by yourself. We have online resources available, training videos, etc. In case you need assistance from Google, we have got some onboarding options. We have got some out-of-the-box options training modules that we can provide you. You can work with your account team, and we'll be happy to work with you. Yep. So, we talked about three personas in day zero, day one, day two. Right? So, for day one, your developer persona works mainly in the IDE, but when you talk about entire software development lifecycle, then there are several other key personas. Right? So, I would bring LACS from TCS onto the stage to talk about how they enable other personas in software development lifecycle and how they have done integration with Gemini Code Assist. Thank you. Thank you, Alok. Thank you, Alok, for setting the stage. Good morning, everyone. We saw a lot about developer productivity. We saw how Gemini Code Assist can kind of lift the value for the developer community within an enterprise. But what is also important that the customers realize that as we kind of look at the developers today, they probably spend, and this has also come out from the Dora metrics, they probably spend about 25%, 30% of their time doing coding and unit testing. The rest of it is all about trying to understand what those requirements that are flowing in are, getting to the richness of that requirement, granularity of it, understanding it better, so that the developer can actually flow the impact into whatever they are coding or unit testing and so on. And not only that, it is not only with relation to the product owner or with, I mean, with regard to requirements, but also with regard to design and architecture and so on. And this is not only true for a developer persona who is dependent on a product owner or an architect to get those details, but also downstream, if you look at the entire application software engineering cycle, if you will, from product owner right through to your support engineers, there is information silos today. How do you flow this information across these personas is an important element, is an important question that most customers ask. And you have an opportunity with Gen AI to actually elevate the benefits, not just for one community, but for entire team that is working on an application. Basically, the name of the game is how do you lift the knowledge parity, bring about the knowledge parity across all of these personas, so that when you look at this as a team for an application, you're able to elevate benefits. Apart from that, there were other challenges that the customers speak about, including the grounding of your coding or any artifacts that you are producing using Gen AI to their current standards, to their principles, and look at how do you address vulnerabilities right at the beginning through the development cycle and so on. So what we have as an offering, we launched this with Google way back in August of last year with Gemini as well as GCA and importantly built it on Vertex AI. A lot of what me and Deetia are going to talk about is, will kind of relate to the agent space that you might have heard a lot about in this event, but we started this a little while back with Vertex AI and agent builder. So three core characteristics that I'll call out. One is, we have built this as an agentic, using an agentic architecture, mostly right now keeping human in the control and establishing a multi-agent architecture, wherein every persona has an assistant, a Gen AI assistant with which they can do their day-to-day tasks. That kind of ensures that as a product owner, I'm able to get to richness of the user stories, which then moves downstream to the developer and they get better at doing their part of the work. The second is, an important element in an application world is that its knowledge is spread across multiple knowledge stores. You have user stories in Jira, you have, you know, perhaps architecture documents in SharePoint, your code itself is a piece of knowledge. Then you go downstream for test cases for your service now or any incident management tool that has your incident tickets, knowledge articles, and so on. So the whole knowledge of an application is spread across these knowledge stores. And especially for Gen AI to be effective, it is important to get the context comprehensively. What that means is, I have to dip into all of these knowledge stores to pull the right context for any prompt. To that end, what the second core characteristic that we have is the knowledge fabric, which primarily catalogs and indexes all of these knowledge sources, and also builds a graph in terms of the relationship between these knowledge sources, knowledge nuggets, if you will. For example, user stories to test cases, user stories to code, and so on, which then ensures that I'm able to get the right context across to my prompt when I ask for any, I mean, if I have to have a user story change and then flow that impact down to test cases, it is not only the user story, but also the test cases that has to be part of the context so that Gen AI is able to come back or the agent is able to come back to us on what exactly is the change impact on those test cases. The last but not the least is, you know, we don't want to bring in one more platform into an enterprise where there's already enough investment in tools. So the way we have kind of structured and designed this is to integrate those agents to your existing tool chains in an enterprise so that the adoption is that much simpler. And in doing all of this, if I ask a product owner, I want to generate decomposed user stories, I can actually generate it, review it, accept it, and push it back into the specific tool, be it Rally or Jira or whatever. So all of what I articulated, this is a snapshot of what that is. So there will be an agent for every persona across the software engineering lifecycle. And we have built this on Gemini, on Vertex AI for the agent building part. And in fact, we are on the process of, in the journey of moving this to agent space right now. But the important construct is the knowledge fabric, which kind of pulls in all of this data or knowledge across multiple siloed tool sets, be it Jira or SharePoint or whatever, and pulls it all together. So when I have an intent and I ask for a content generation or whatever, based on the entities in the intent, it is able to, through the graph, pull up all of the relevant context and then come back with the right content. So I'll now pass this on to Deepti, who will kind of get into the details of the architecture and also take us through a demo. Thank you. Thanks, Lex. All right. So you have been hearing about developer community. Rakeh started off. Alok took us through all of that. You heard lags about all the other communities that we can influence by directly targeting the entire software engineering value chain, right? We spoke about how we can do that by creating agents for each of the key personas. We heard about how the agents can augment these personas by helping them with their tasks. So now let's get into the details in terms of how exactly are we doing this. So if you look here, as Lax was alluding a few seconds back, the core of our solution is the enterprise knowledge fabric, a fabric which really brings all the information together. Because today, no matter what your tool is, wherever you are using it, as a product owner, if you're using a Jira or a Rally, you have a lot of information stored in those tools. Now, if you are a developer, you have stuff in GitHub. If you are a support engineer, you have service now. But none of these sources are connected together. They are all information silos. If I were to go and ask a question that how was my incident resolved, would I be able to derive an answer which relates it back to one of the user stories or code that was changed? No. And that's where our knowledge fabric comes into play. What we are doing here, we are actually connecting all these sources together and integrating it with the agents that we are creating. So a product owner agent or a support engineer agent, if a support engineer comes and asks through the agent, that can you provide me examples of how this incident was resolved and what issue was created and what code was changed, the agent is going to dip into the knowledge fabric and it is going to answer that because behind the scenes, we are creating an advanced RAG mechanism which starts by connecting to all your sources. Now, when it comes to the sources, be it VJira or GitHub, we realize that moving the data is a heavy liftment, so we do not do that consciously. We only dip into the metadata piece of the information. We do not get the actual data. We pull the metadata. We create the catalog. So indexing and cataloging the metadata piece of that and then what we do is we take it a step further. By understanding your intent, we create the nodes and relationships, thereby creating a knowledge graph and integrating that with your traditional, you can say the search that you have with RAG, where you have a vector search. We integrate along with a vector search. So an advanced RAG mechanism which has the nuggets of a graph database, a vector database, and a metadata catalog. And as you can see, all of these are integrated with your existing tool chain and it's integrated with the agents. So how does it really work out? Now, if you take a scenario, now let's forget about Gen AI for a minute or AI for that matter. Any project that you pick up, any project archetype, you will have three critical KPIs always in the mix. You are going to worry about your quality, of course, and then you have velocity and productivity. Now, if I were a product owner, or for that matter, any persona, the user stories that are written by the product owners today, for several reasons, the user stories are not elaborate enough. It is probably a couple of lines or maybe three lines here. Now, if you bring Gen AI into the mix now, by using these agents, a product owner agent as an example, you can create user stories which are elaborate enough and which clearly articulates the requirements that are given by the business analysts. Right? Now, if the user story is elaborate, I can then take it a step further and I can create my test cases and the code based on the user story. And all this is going to be done through the multi-agent framework that we have created, where an agent is passing on to the next agent and that is only done after careful review and approval by the persona involved. A human in the loop, we have brought that by design because we understand that today we very well need to monitor the output of the AI agents. We need to ensure that it is up to the accuracy level that we want and only then we should approve it. So once a user story is approved, it goes to a developer and a tester agent and they create the subsequent artifacts. Now, if you think about the impact this brings, if your user story is good, the subsequent test cases, they are comprehensive enough, the code that gets tested, it's going to have better quality than you had before. If the code quality is good, obviously it is going to move from phases to phases multiple times faster than what it was able to do before. So your velocity, productivity, and quality, all of these three metrics are intertwined and they influence each other, right? Now, imagine the amount of productivity improvement that we can derive when we target this entire value chain. We are not only impacting a developer community, but we are also impacting a product owner community, a testing community, probably a CI, CD engineer. So that's the important aspect of looking at it from an end-to-end perspective. From a business outcomes, if you now correlate it to what really you achieve from a business outcome, obviously when your code quality gets better, as I said, your speed to market improves, right? When your speed to market improves, every persona that's involved in the software engineering value chain, their productivity goes up. Now, since the quality has improved, I am directly impacting the support or the maintenance side of things as well, where the number of incidents that will happen now, it's going to reduce because my quality of code is better. Now, if my number of incidents reduces, my cost of maintaining that application also goes down. So as you can see, this is the whole nine yards of the software engineering value chain, which we are able to impact through the solution that we have created, the TCS SDLC assistant that Lacks eluded. And based on implementations that we have done at several customers, we have seen anywhere between 20% to 30% improvement in the productivity across the value chain. So what I'm going to do next, I'll probably spend a few minutes in showing you a demo of this solution that we have. And what I'm going to do, I'll pick up a couple of agents, a product owner agent, a developer agent, and a test manager agent. You will see how the agent interacts, how does it create the user stories, the review mechanism, and the transfer of the output from one agent to another through this multi-agent framework. That's going to be the crux of the video that you see here. So what you see here is the UI of the solution that we created. As you can see, we have multiple agents that can be created. This is just an art of the possible, but I'm showing you a product owner agent. So as a product owner, let's say if you have an insurance-specific use case and you want to create a user story for first notice of loss, you have your requirements. You give it to the agent. The agent, based on its training and how it was built, it creates a user story within a matter of few seconds or a minute, which gets generated for your review. As a product owner, what you see here is a pretty comprehensive user story that can be reviewed for its completeness, for its accuracy, and for the contextualization as well. So the product owner is going to review it for all the steps. It is going to check whether it is making sense or not. And as you can see, there is a clear reference to what template was used. So there is an attribution. There is all the steps followed. Only when the product owner is happy, he or C is going to hit an accept. Now, the moment there is an acceptance, the transfer of the output is given to the other agents, the developer agent, and the test manager agent. And it is also integrated to your tool, Jira, as you can see. So real-time integration with your tools, thereby enabling seamless adoption for all these elements that we are bringing, right? Anyone who is using Jira, they would be able to see the user story that was created. Now, imagine when a developer comes in and they enable a developer agent. So the developer agent is already going to have a code that would have been created based on this user story. He or she needs to review that. They need to see, okay, what was the code developed and which user story it was developed for. So there is clearly a linkage there. They can click on that. And not only that, they will go through this entire code piece. Is the code 100% accurate? Of course not. But it's a significant head start for the developers to go and take a look, review, and focus on the complex pieces. Now they can have their brains applied to the complex functions and thereby do the modification and accept it, thereby giving that significant productivity benefit. So the same workflow goes for the test manager as well. When you look at a test manager, he or she is going to look at the test manager agent, invoke it, and they are going to, well, I'll probably pause for a second because that's what you see when it's accepted. But for the test manager, you'll pretty much see the same workflow. When a test manager agent comes up, there is going to be a very similar workflow where you have your test cases generated on the user story that was reviewed and approved by the product owner. So what really happens here, as Lax was alluding and our partners Rakesh and Alok also alluded, we are not only focusing on a single community, we are expanding the horizon, thereby deriving the benefit from each of these communities or the personas, thereby allowing each of these personas to have their maximum productivity in complex pieces. Now, the test manager part that I was going, if I go back to that, imagine a scenario where a test manager is creating the test cases, but they probably wrote only six or eight test cases. They forgot about the edge cases. All of that is now taken care because you already have this created. All you need to do is review, understand, and only then approve it. Right? So that's the overarching flow. As you can imagine, this can be extended to any other persona. This can be done for a CICD engineer. This can be done for a support engineer, as I was referencing earlier. Or it could be done for an architect as well, to begin with. So if we move forward here. Okay. So in a nutshell, to wrap things up, to summarize, the power of GenEI is truly brought forward by integrating all the personas together, bringing the value across the value chain of software engineering, integrating it with a knowledge fabric which brings the contextualization, and finally, integrating with your tool chains so that you don't have to imagine another platform that has to be brought into the mix. So we would greatly appreciate your feedback. Please take a look at this. You have your mobile apps. You have the QR code. And for any questions, we'll be happy to take the questions outside. So please feel free. If you have any questions, bring it up. We'll be happy to answer it. Thank you all.