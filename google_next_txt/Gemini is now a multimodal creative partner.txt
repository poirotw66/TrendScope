 All right, hi. Welcome to this session. I'm Mustafa, and this is Kaushik, my colleagues. We're both researchers at Google DeepMind and working on Gemini, and today we are going to talk about Gemini multimodal image generation. So basically, Gemini has been trained multimodally from the beginning, from the pre-training all the way to the post-training, RL, and all the stages that we did for delivering a model and deploying a product. And image generation has been a part of that from the beginning of the Gemini. And what we've done is basically for this version, for Gemini 3, we took the model and we deployed it in production. So it's now available at AI.dev, so we can actually try it on and basically even use it as an API. All right. All right. What is the goal here? So the simple goal, if I want to just simplify it and mention it in simple words, is to have a language model that can respond in not only text but also images. And by responding in text and relevant and useful images, we mean it has an understanding of the word from both language point of view and visual point of view. So if I want to take us back a few steps and mention why this is important, like think about the word. Word is not about just text. It's like a multimodal word, right? And if you want to have a model that has perfect understanding of the word, we need to have like multimodality, being part of that like from the beginning of the training. And let me just give you an example. So like why this is relevant and why it is important to have a model that can generate both text and images. Imagine if you want to, like for example, install a washing machine in your place, right? And what if you like want to know how to install it, you have no knowledge, and you just ask a language model about how to install this washing machine with this brand, this model. And the model is going to explain the stuff and it's going to be like, you know, you can even ask for a detailed explanation. It's going to be a long blob of text. But what if you can upload an image of a place or a spot that you want to install your machine and your washing machine. And then the language model is going to basically generate like step by step instruction with the text and the images that are exactly images of the place that you want to install the machine. And it becomes like way easier in terms of how to install the machine, right? So it's going to be much better response for users. So this is just an example, but it gives you a bit of idea that how we talk about like, you know, image and text generation being like an integrated part of a language model when users are interacting with this model. All right. Okay. But we are emphasizing on native multimodality here. Why do we emphasize on native multimodality? Like why native? So there are a ton of standalone image generation models. And like you've heard about like many of them like Midjourney, Dolly, Imagine. And these models are extremely strong in terms of, you know, generating an image that is like really beautiful, like great in aesthetics. And they target for like making the image perfect and like basically be the exact things that you want. But most of the time it is limited to the case of like, you know, having a text that translates to an image. Or some of these models also offer image editing. So you kind of like pass an image, ask for some edits, and then it gives you the target image. But what about native multimodality? What if you have a language model that has image generation as a native part of that? In that case, you can think about a situation where the language model can like naturally go back and forth between like text and image. So it generates an interleave, like text, image, text, image, text, image, as part of a single response. This is like one of the things that you can actually get from native multimodality, which is extremely useful and also improves the user experience by a lot. And the other thing is that you're going to get like this interaction between modalities when you train your model natively with all these modalities from the beginning. And like there's a potential for positive transfer. So your language model knows how to basically speak with visual assets or visual outputs. And your image model knows about the knowledge of the word. So like all the good stuff that comes with these language models, like, you know, reasoning, math, like knowledge of the word, everything is basically something that your image generation model also has access to that part of the capabilities that come with language models. This is number one. And the other thing that is basically, as I said, when you have language model going back and forth between these modalities, it can also go back and forth across these understanding capabilities and generation. So you can get your model to reason about its output in a loop, right? So you can have some sort of like self-conversation with the model generating some text and the image and it sees its image and go back and forth. Let me just show you, like in a few slides I'm going to show you how does that work. But just one quick preview is that Flash, Gemini 2 Flash is the first model that we released as of like a Gemini model of handily that has a multimodal in multimodal out to the outside world. As I mentioned, we always had multimodality as part of the training of Gemini, but this time we released the model and everyone can access those models. And as I said, it's like basically a native part of that. So everything is trained like multimodal in and multimodal out from the beginning to the end of the training. All right. So this is the example that I was basically referring to. So imagine your model generate a text, a blob of text, right? So you ask a question, you ask about how to install your washing machine, and then the language model start generating some text. And then based on your prompt and the text that it was generated by the model itself, it just starts generating an image. So this is image generation condition on your prompt and the text that the language model already generated. And then in the second term, it's going to continue generating another piece of text, which is again condition on the image that it was generated, that was generated by the model itself and the text like before that and your prompt, right? So, and then it continues, you know, like the second image and so on and so forth. So you can see that there's a basically like, you know, a seamless switch between these modalities and the model has the ability to see its output and continue until like the condition for giving a proper answer to the user is satisfied. So this is basically the magic of native multi-modality. You get the model to generate output in any modalities and like, you know, related to that, you can actually get like multi-modal test time compute. So the model goes on and on until, and this like test time compute is basically provided to the model through both text generation, like many tokens of text, and also many images in between. So this is the benefit of multi-modality, native multi-modality. From the next slide, we're going to cover some capabilities and use cases. And I'm going to switch, give the slide to my colleague. Yeah, so one of the first capabilities to talk about when it comes to this model is just pure image generation, right? So it can still do what a lot of these standalone models are able to do, which is if you give it a text prompt, it will generate an image. Here are some example images that it generated. Actually, some of these images, I just asked the model to generate, to think of and generate 10 images, and I just picked a few images that I liked from that. So just in one output, it gave me a bunch of images that I could use for this slide. You can also see that the model is pretty good at text rendering, like it can render this birthday card from a single prompt. So that's also pretty cool. I think it opens up quite some use cases. And one other interesting thing we noticed when we were playing with our models was that if you just ask it to generate an image, say, of a car, it'll give you an image of a car. And it's not really the type of output you would expect from most of these standalone image models. Like if you went to Imagine 3 and you said generate an image of a car, it's going to give you a way nicer image than what this model gave you if you just asked for an image of a car, right? So if you just use this model this way, you'd be like, why would I use this for text image, right? So a lot of these other standalone models, what they actually do under the hood is they expand your prompts. So instead of what the image generation model actually sees is not generate an image of a car, but it's something that the language model would have expanded on, given a lot more details, and then sent to the image generation model. So that's what we can actually simulate with native multimodality as well. On the right-hand side, you can see what happens if the prompt instead says generate an image of a car, but first describe it in detail, then generate the image. So just in one interleaved output, then the model decides to describe the car, imagine a sleek, et cetera, et cetera, et cetera, and then generate the image. And this is basically chain of thought for native multimodality, and you can see that the end image is a lot more aesthetically pleasing. So the model that we've released is definitely kind of a raw model in the sense that it will kind of reflect the distribution of the training data in the sense that these short prompts may not always end up with the most aesthetic images, but these longer prompts, which traditionally get actually used for these standalone image generators, will result in much nicer images. So this is something that if you're a developer who's using the model, you can either use the native kind of expansion capability of the model, or you could even kind of expand prompts separately and then send them to the model, and you'll get better images. So yeah, this is just something interesting we noticed while we were using the model. So another big capability of this model is editing. And I think this is one thing that really captures people's imagination. So one example from Twitter once we released this model was taking like an ancient painting, saying can you make them all hold ice cream cones and look happier? And the model does it, right? So it preserves the other elements of the image quite well, while just changing their faces, their expressions, and giving them ice cream cones. So this is one example of editing, and we'll show you some more. One of the things that we really focused on for Gemini was pixel perfect editing. And I wouldn't say that we're 100% there, in the sense that there are some very, very low level details that if you really pay attention to, they can change slightly. But compared to most other solutions out there, this model does a really good job of preserving the granular details of an image while changing just the parts that you ask for. So for example, if you say make the train red, it will just figure out which parts of the train it needs to make red and change it. And we can actually kind of animate this, right? So you can see it kind of preserve everything about the surroundings. If you really, really pay close attention, it will change like very, very fine details. But for the most part, it's the same train. The windows are all in the same places. The tracks are all the same. And just the color has changed. So this is something we've cared about from the beginning. And I think another thing that this enables is preservation of human faces, for example, where human eyes are very, very sensitive to changes in faces, right? So even if the train changed a bit, maybe you wouldn't notice. But if there's a human face in the image and you know that person, and then you go through and edit, like you say add a hat to them and their face changes in very subtle ways, you'll notice. But with this model, it actually does a good enough job at preserving these low-level details to maintain most human faces pretty well. And another capability of this model that kind of arises from native multimodality is conversational multi-turn editing. So you can ask the model to add some flowers to the table, and it'll do it. And then you can follow up. You can say, you know, I prefer this type of flower instead. And it will understand that you mean that you want to replace the flowers that it added in the previous turn, and it will do that. So I think this also unlocks a huge variety of use cases. And this will only continue to get better over time. I think one thing that we want to improve is like the ability to refer to older images in the conversation. And there's so many more potential applications of this as this gets better and better. But it's already pretty, it's pretty addictive actually to play with just taking an image. Or you can start the conversation by generating an image and then continuing to refine it. And I think it's pretty fun to play with. And it's available to the public. So anybody can actually play with this on AI Studio. This is one more example where you can ask for very targeted changes, like in this case replacing this person in the scene. And it actually maintains the style of the image with the new person, right? While making them lean back and sip a cup of coffee while retaining the other aspects of the scene. And we'll later talk about how people have actually put this editing capability into some really cool apps, which I think Gemini Flash is basically the only model out there that enables these applications. Because of number one, low latency. And number two, this granular preservation of detail. And overall, the instruction following abilities. All right, okay. So another capability that we notice is basically that we've been basically spending time and investing data and energy to enable is just through contextualization. Which is like really great capability, especially if you are, for example, building an ad, right? So if you have an object, you want to take a photo and we contextualize it in a new scene, right? So in this case, for example, here is a lamp, standing lamp, like on a, sorry, a lamp. And we want to basically place this lamp next to a window that is basically facing to like a busy street. I cropped this out of like a multi-turn conversation. But you can see that, you know, it preserves exactly that lamp. And you can play with the lights, add text to it. So you can basically recontextualize the object that you want and then make a completely new scene. And we have like, for example, another example. So if you have an image of a chair, this is actually like a photo from the default media that like AI Studio is offering. And I just ask simply that, can you place this chair next to a table and with a rug in the room? And you can see it's basically pretty much the same chair. There's no difference between these two. It has a bit of like, you know, it changed the angle. And it placed it in a scene that you can also continue playing with it. And you can ask, again, like similar to the previous example, you can ask for a vase on top of the table. Or changing the color of the rug. And this is actually a really powerful feature. Again, like compared to pixel perfect editing. So like when Kaushik talked about like pixel perfect editing. That's already super, super hard. Because you need to like target only the part of image that user asks for a change. In this case, it's even harder because you want to preserve an object. But you want to have some changes like in terms of angle, placement in a new scene. But still you want to preserve the object. Like the identity of the object. So you don't want the chair to be changed. And this recontextualization is like basically there are a ton of applications that you can build on top of that. And like Kaushik again mentioned, we've been amazed how external users like made a ton of cool examples and use cases out of this. Like we've seen like many people actually playing with the model on Twitter. We have a bunch of like those tweets in our presentation. But it's like really interesting to see how people work with this model and build new stuff that are like we hadn't imagined those capabilities. Right? All right. The next one is multi-image input. Again, back to native multimodality. So if you have a language model that has an understanding of like any context, you know, which is basically text, video, images, audio. So it does like encoding or does basically like on the understanding side it can digest any type of modalities. If you have image generation as part of that native model, you can feed the model with any input, type of input and ask for like a new image. In this case, we have multiple images as the input. And you can see how the model takes this image of the cat, takes the logo of Google Cloud, and then it makes a 3D wooden toy and put it in the same scene. Basically the same cat, you can see, you know, some of the details are preserved. It adds a bit of like, you know, aesthetic to the image and it's created a new image that is exactly a combination of those two input image and following the prompt that you have. So this multi-image conditioning is also a great capability that like Flash, Gemini 2 Flash is offering to you. So, like related to this, we are enabling also other like modalities. Like, you know, right now, like we have Gemini to digest audio, video and everything. And we've been spending most of our time on, you know, like text and combination of like, you know, many images as the input. But hopefully in the new versions, you can see that, you know, you can do like directly audio to image or, you know, have video to image. Imagine, like you, like, send a song to like, like no lyrics to Gemini and ask for a cover album, you know, which is basically like directly from audio to the image. So the model has to understand the concept of this music and like, you know, come up with a visual representation of this concept, which is super cool. Because it's like super creative and very, like, it's like generalizing across these modalities, which is a great target for us. Another thing that we mentioned is basically this interleave. So you've seen a bunch of examples for multi-turn conversation. So the users, like they ask a question. There's an answer by the model. And then user follows up with a new prompt or a new question or asking for a new change. And then the model responds and so on and so forth. But this one is basically when the model goes back and forth between these modalities in one single response. So there's no interaction with the users across these, like, exchange of modalities within this response. Which is, again, like a new capability that's been unlocked by Gemini. And when, if like, back to the title of the talk, if you think about it, it is actually a really, really great, like, brainstorming multimodal partner. So when you want to brainstorm about something, you can ask Gemini about, oh, can you maybe come up with three different ideas for generating a logo for, like, my business or for my, you know, like book club or whatever. And then what Gemini does is basically comes up with ideas, generates a blob of text about that idea, follow up with an image that matches the text that it describes, the second idea, and so on and so forth. So this is basically something that you can, you can see how you can, like, help, get help from Gemini to, like, come up with, like, creative ideas. And, like, it does a great job in terms of diversifying these ideas and coming up with things that are useful and relevant. And you can choose and then follow up with the next turns with the one that you've chosen from the generated ideas. Another one, which is, like, not brainstorming, but something that you can ask Gemini to do is, for example, oh, can you show me step by step how to draw a dog step by step? And then it goes again in one response, generates text for each step. First do this, like a circle, and then generate an image that matches that description of that step and goes on and on. And we have actually plenty of examples like that. So, like, if you go to, again, like, ai.dev, like, ai.studio, there are three predefined examples. One of them is, like, which is one of my favorite examples is, like, the children's storybook. So you can ask the model to generate a storybook for children about a goat in a farm, and it just goes, like, between these turns. So it generates a text, a follow up with an image, which is, like, really nice. Like, the second episode of that story with an image, and you can see the consistency across this. You know, it's basically the same goat going through an adventure in a farm. So definitely give this a try. As Kaashik said, this is, like, super addictive. It's just, like, really nice to play with the model. And I've been seeing my colleagues, for example, generating, like, I don't know, how to explain neural network using cats. And people are basically being super creative for creating contents for their children to basically teach them about, you know, very complex concepts with, like, super simple visual elements as part of the same story. So, all right. Another thing that we've been actually looking into is this real-time collaboration. This is exactly what I said. Like, this is one of the use cases that we've never even thought of when we were developing this model. And then people got, like, really creative. So what they do, they make a drawing by hand, like, roughly what they want to do. And then they choose a style. And then they ask Gemini to basically condition on this, like, hand-drawn, like, raw image, plus the style or the theme of the final image. And then Gemini goes and generates, like, a polished image based on these two. And this is, like, really cool simply because it has all this, like, it preserves exactly what you draw. And it has this, like, multi-image, multi-asset conditioning. And one of the things that actually kind of enables this is, like, this, like, real-time and, like, super low latency of the model. So you get to play with this, like, super fast. You can actually go through the turns. And, again, like, in terms of latency, Gemini is the fastest model that you can actually build something like that on top of that, like, out there. We've been talking about this, like, brainstorming and creative partner. One of the things that people actually did a ton of time, and this is, like, one of the features that, like, everyone wanted to have is, okay, you know, what if I send an empty, like, an image of an empty room and ask for, like, for Gemini to come up with an interior design for this room and decorate this room with, like, furniture and stuff like that. And then, again, it requires pixel-perfect editing or basically, like, preserving exactly the room that you uploaded. So if it changes the room, like, you know, with different window, everything, just, like, it's not going to be useful anymore, right? So this is, like, one of the use cases that you really need, like, the capability that Gemini offers already. And you can see that, you know, it comes up with a... I'm going to try to go back and forth. And you can see that, you know, it comes up with a set of furniture, how to decorate this room. And I think, like, imagine this being part of a language model that you can come up with. Like, you can ask about, you know, different ideas for decorating your room. And the language model says, oh, you know, like, this is a design that I can suggest to you. You can ask for, oh, I don't like this, you know, like, black chair on the right side. Can you remove that and replace it with this one? And then it does for you. And then being part of Gemini, it connects you to the rest of the world. So maybe you can ask for, oh, where can I buy this bed, right? So this is, like, basically when you have an integrated multimodal model, you get the benefit of all the features that this language model has. You know, like access to web, like deep research, everything basically comes together in one place. And you can get the benefit of this native multimodality. All right. Next. Yeah, so continuing with some use cases that people have found and I think that would be useful to a lot of people. Another one is virtual try-on, right? So if you have some item of clothing and you also upload a picture of yourself and you ask, how would this piece of clothing look on me, the model can generate an image of what it would look like. It might not be 100% perfect, but in this case you can actually see it's pretty good. Maybe the exact width of the stripes on the shirt are a little bit different if I had to nitpick. But if you look at the jean jacket, like the rips on that jacket are in the exact same place. So it actually does a pretty good job being faithful to both of the input images in a very close to pixel perfect way. And I think this unlocks a whole wide range of possibilities for applications that people may want to build. I think as we move further and further into this world of native multimodality, we will see that complex pipelines that people build for these sorts of solutions, hopefully they get even easier as it just becomes a matter of finding a good prompt and then automating these sorts of workflows. One more thing from Twitter which I thought was pretty cool was someone created an image of a character. And they actually kind of took this character into a virtual world. And through prompting, they kind of just created a simulation where they could make this character move around in certain ways, do various things. And Gemini retains the consistency across these different scenes in a pretty good way. I think, like for example, after generating the first image, it kind of flips the character around, shows the back view, right? And then integrates it into a game. And then from that point onwards, the game remains pretty consistent, both in terms of style and all the icons on the side. So this is pretty cool. I actually don't really know what the exact use case from this would be, but I'm sure someone here would be able to find a pretty cool application of this. One thing that is really cool is, and this is actually something that we did not at all plan for when we kind of trained this model. This is something that, after releasing it, like the community found, which is that through interleaved generation, you can actually get the model to produce GIFs. So all you have to do at the end is take each of these images that the model generates and then stitch them together into a GIF. No additional work needed. And the fact that Gemini outputs these as separate images makes it so that you don't have to, like, you may be able to ask a standalone model, like, imagine3, generate within a single image a bunch of small frames of, you know, a flower growing as pixel art. And it might do that, but then you would have to kind of go manually and divide up this one image into smaller frames that then you stitch together, and also you'd have to align them. Here, with Gemini, it actually maintains the alignment across frames, and all you have to do is just stitch the frames together. So with a simple prompt, it just does an interleaved generation of this flower growing. And you can see the output is pretty cool. And actually, internally, a lot of people have been playing with this ability of the model, and we thought it would be cool to share a few GIFs that the model made. And you can see that this is all done in an automated way, right? It's, you go from a prompt to a GIF. And you can see that across frames, the model itself maintains consistency and alignment. So if you have a model that's not quite good enough at maintaining alignment, you'll see a lot of jerking back and forth across frames. But with Gemini, it does a really good job, because of this pixel perfect capability that I mentioned earlier, it does a really good job at preserving things across frames and making pretty smooth GIFs with some interesting movement as well. Another thing from the community that we found was someone compared what it would take to use a tool like Photoshop to adjust, you know, this woman's posture, whereas you could also just prompt Gemini. And within a few seconds, you just get back an image with corrected posture. Now, this is not to say that there's no, there's going to be a future where there's no need for tools where you have like very, very precise control, right? But our vision for this is that as we make these models better and better, we should be able to build applications that allow you to do very, very fine-grained control of images, but perhaps through natural language or even user interfaces, but maybe going through Gemini as a layer for all of these so that you could get all the benefits of a better world understanding, maybe better automation, but I think we're probably moving towards a world where for a lot of quick and dirty use cases, you don't need to be a Photoshop expert, for example, in order to quickly fix up your images. You can just ask Gemini. And if you are like a real professional and you want things done a very particular way, you may still need these other tools while models like Gemini get better and better, and hopefully they start to be able to do more of these precise edits as well. Yeah, and another thing that we've seen is people trying to integrate this into a product. So one thing is this model, Gemini Flash, may not always give you what you're looking for on the first try, right? So what developers are doing based on this model is sometimes given a prompt, just running multiple API calls at the same time and returning back all of those results and letting the user choose how to continue with this. And I think this is super powerful because you basically are doing test time compute with like a human verification, right? So you can do best of four and then let the human verify, pick the best of the four outputs, and then continue with it. And we've noticed that if you do kind of best of N on top of the outputs of the model, it actually significantly improves the overall aesthetics, prompt following, etc. So that's one thing, just like a tip for any developers trying to build on top of this model. And I think just building also graphical user interfaces on top of this model make it quite satisfying to use and really showcases the power of the model. For example, in this use case, they've built the model integration into an application which also has image to video features. So you can actually go from a single product photo and then translate it into someone holding that product and then translate it into a video of that person moving while holding that product. So through these couple of model calls, you can get something that's pretty magical. Yeah, and a brief note about the API is that the model is actually available today. Like you can use it right now. Right now it's in an experimental preview, so there are some rate limits associated with that, but it is free to use. And a lot of the examples that we've shown are from people using the API as it is, which is currently free, as I mentioned. And there are higher rate limits and billing that are also coming soon. So definitely stay tuned for that. In terms of how the API itself looks, it's just a Gemini API. But now you specify in one of your parameters that you want not only text back, but you also want images to be returned. So it's just a matter of toggling the response modalities. And this is kind of our vision for how things should kind of progress, right? We ideally just want one model that you can call that automatically knows when it would be helpful to return an image. Right now you do manually specify image just so we make sure that if you're really just going for text only outputs, we don't start returning images, which would probably ruin a lot of people's days. But yeah, for now you can specify image and you'll get images back if you ask for them. And so our vision for the future of Gemini native multimodality is that Gemini is going to enable collaboration as a multimodal creative partner, right? So just the same way that people vibe code with language models these days to create applications, people will kind of vibe create all sorts of things, whether it be like an invitation to a wedding or whether it's an asset for a game or so many other possibilities, right? And native multimodality enables this because it allows the image generation process to be aware of any arbitrary context. So we want anybody to be able to pass in an arbitrary interleaved input of text, images, audio, all modalities, as well as video, and get any modalities back. That's the goal for Gemini, and we're working towards that. And yeah, that's the end goal, to have Gemini be able to be truly multimodal. We've only talked about images today, but all modalities in the future should just work together and work very seamlessly. Another pretty interesting area that I think is still underexplored with image generation is how these models can be used for education. I don't think we have any models in the world today which are good enough for this because for education you require a really, really high degree of factuality. Even a small rate of hallucinations or imperfections in the image can totally ruin someone's understanding of a concept, right? So I think we have to get to a point where these models can be trustworthy enough to be used as an education partner. But I think we can probably start to experiment with these ideas in a limited manner in domains where this model is good and kind of slowly expand the scope of that as the models get better. And the nice thing here is you can really personalize education to any student, right? Maybe a student needs a concept explained in a very particular way and maybe they could ask Gemini and it would return an interleaved response which kind of describes how that concept should be explained and also returns images in that response. So that's another area that we're super excited about and we'll be pushing in the future. And another kind of underrated area about area of focus for image generation in general and native multimodality is that Gemini should really become smarter because it's able to model the world through images, right? For example, like if Gemini is able to look at a long video and really like learn to generate videos like this, it also is probably going to get smarter overall. Like that model will be now able to understand more concepts and it's basically able to learn from multimodal data just like humans would be able to. So these are all areas that we're very excited about and I think these represent the future of multimodality in the scope of these large language models. So far, the biggest use cases for these vision language models has been multimodal and that's understandably so. Like I think Gemini is the best multimodal model for vision understanding and our goal is to make Gemini the best model for multimodal generation and have visual understanding and visual generation work together so that Gemini overall becomes the strongest multimodal model, strongest world model and enables a ton of new use cases going forward. And just like we mentioned, you can try Gemini native image generation at AI studio or AI.dev. It's available for everybody to try. You just have to select Gemini with native Gemini 2.0 flash with image generation and you just prompt away. So thanks everyone. I think we ended like maybe five minutes early, but. We're also happy to chat with any of you. Right afterwards. Yeah. . . . . . . . . . . . . . . . . . .