 I wanted to start with a very quick overview of Gemini 2.5 Pro. It recently went into preview. It's been a model that we've been very excited to bring to all of you, to the whole world. One of the big breakthroughs for the 2.5 Pro model is that this model has controlled reasoning, dynamic thinking. What you're seeing is the model at inference time can introspect and create a much more nuanced response than the models that came before. We're seeing this manifest in a bunch of different ways. First of all, it manifests nicely on leaderboards. Some of the different leaderboards are always out of date the second you start here. If there's anything that's not 100% up to date from this morning, I take full credit for the slides having been prepared ahead of time. Some of the places where we're seeing some really interesting results are on things like LiveBench for coding. We've seen some interesting and positive results on other places like the chatbot arena, formerly known as LMSIS. These are different types of benchmarks. We get asked quite often, what do they mean and why are they important? There's probably three different kinds of benchmarks. One is the side-by-side analysis like the chatbot arena where real humans look at the two outputs from different models and they sort of blind side-by-side and they pick the one they think is better. So that's that bottom middle picture here. So we're very excited that our model across all of these different categories is doing well on that. It means that when you put the model out into the real world, it's not just doing well on academic benchmarks. But it's also doing well for real humans in the wild. So that's really important because if we have a model that does really well in the lab but doesn't do well out for all of you, then it's not really worth us being as excited about it as we are when it's just doing well in academic benchmarks. And then we, of course, have so many different internal benchmarks that we look at. And one of the things that's really important is that when we have cloud customers that come to us and describe their problems to us, if you're working on something and you say, I can't get this model to work for this exact prompt. I'm trying to do something. That we're inspired by that and we create evals and then we create tuning sets and we work with our colleagues at DeepMind to make that capability that maybe wasn't quite performant enough for you that actually manifests in the next version of the model. So across all the different types of benchmarks, we're really excited so that we're seeing this model do well. And to dive in a little bit more to what these capabilities are, start here with this state of the art performance in lots of areas. These are all the different benchmarks. We think we gapped up quite a bit between the 2.0 and 2.5 models and we can get into sort of why that is in a little bit as we bring out Tulsi to have a little bit more of a discussion there. One of the key areas where we're seeing resonate in the market is coding improvements. So we've heard from the world's developers that our newest model does much better at their coding tasks. And it's really great to see the feedback cycle from developers come through because it's a really quick feedback cycle. A lot of the IDEs that implement our models are just putting models in a dropdown. So developers can choose the model that is doing best for them. It's a meritocracy. It boils down very cleanly. And so for us to hear from our customers who run these products that the new Gemini models are highly in demand from developers is a very clear signal to us that what was working well on an academic benchmark is also working well in practice. We continue to, in a very usable way, deploy our longer context models. So the Gemini Flash models have 1 million token context window. The Gemini Pro models will have up to 2 million tokens. We have some other interesting developments coming around context as we'll have over the year. Right now that model, the Gemini Pro models at 1 million tokens, we'll have it up to the 2 when we get it into... It's in preview now when we bring it to be generally available. It'll be 2 million tokens. What we're also seeing, though, is that the accuracy across the entire context window, something we worked on quite a bit. The initial variants of these models from last year did really well on sort of finding a single needle in a haystack. Here what we're really trying to do is have the model find all the needles in the haystack. So it's really good for things like legal research or complicated tasks. If you're going to do time stamping on video and you need to find every instance where something appears in the video, you really want the model to perform well across the entire context length. The other areas that we're seeing, I mentioned before, reasoning. Again, these models can now introspect. I think it's an industry trend that you're seeing us and all of our competitors have different variants of reasoning models. One of the things that we're doing that's along one of the industry trend lines but it's not the norm yet is that we have our models are hybrid models. So that same pro model or the same flash model will be both a reasoning and a non-reasoning model. And you'll be able to toggle that and you'll be able to set up dynamic thinking. And we'll talk a little bit more about that in terms of flash later. The multimodal capabilities of our models are incredibly fun to play with. I'll just say from a consumer standpoint, from using our models, I'm continuing to be really impressed with our models' ability to understand all of the multimodal inputs. I'll say it was about a year ago when we interleaved audio and video into the models as inputs before we launched with silent video and then we added interleaved audio and video. And that was an aha moment for me. And we've just continued to progress on that. The whole world has been asking us to take any input to these models and then be able to reason over it. So it's not just about generating. We have some other, some of our other models are doing things like generating multimodal outputs. So we have the VO model, the Imagine model, and then we have a multimodal generation model and experimental. So it's a flash model that can take an image as an input and generate an image as an output. And you can do all sorts of really awesome image editing things with those models. But the ability for the flagship model to reason over like long form video is only getting better. And it's been really, really fun to watch as our customers have continued to iterate on that. This chart over here describes the throughput on the models. It's nice to see the green bars all the way to the side of the chart on the left. Those are, that's flash and pro. Our models are faster than most of the competition. There are some models out there that reason at a different level, you know, for the bang for your buck in terms of quality and speed. We're really excited that Google has been able to engineer some of these throughput optimized models. And really that manifests in all of you being able to have a really snappy experience and provide a snappy experience to your users. It makes the best model. So the 2.5 pro model. It makes that a really usable model for your end users in all of your applications. So it doesn't matter if a model is like amazing at quality. If it's too slow to be usable, that's not that helpful. And so we've been really attuned to making, to hitting the sweet spot. So we have the pro and the flash models. They're at slightly different places as you can see. But they're both anchored to be models that really have low latency. So both time to first token back, but then also really high throughput so that you get the whole answer back quickly. And then the last one on this slide is tool use. So having the models understand how to use tools is a very important step in the way to enabling agents. So as we talk about agents more broadly around the conference this week, you'll hear all of the capabilities that we have under the hood of things like agent development kit and agent engine. You hear how we have integrations with things like MCP. So that's model control protocol. And then our agent to agent protocol that we just launched. All of those things are fantastic. But if the model can't reason well enough to make a plan, so that reasoning, and then can't go and learn when to use tools, then all of that model capability is kind of latent. And you won't be able to get the full use of the model if you don't have the ability to combine reasoning and tool use. So we think those are the two key enabling technologies for agents. To quickly touch on a model that we announced this morning is coming soon, Gemini 2.5 Flash. We're excited to make some progress at the mid-end of our model family. Just to give you a quick overview, that model will be another hybrid thinking model. So you'll be able to turn thinking on and off. You'll have dynamic thinking as well. And so again, you'll be able to set a thinking budget. So I want to use up to X number of tokens to think. Or you can let the model decide how many tokens it needs to think. We think this is going to be really practical as you have your different use cases. So there's a couple different types of use cases. One is if you have a highly predictable use case where you want to do the same thing over and over again, you probably will know the setting you want. But then if you have more of a universal chatbot use case where you're getting any kind of question that comes in you want to be able to answer, some of this more dynamic thinking is going to be really important. So we're excited to launch 2.5 Flash soon. So keep an eye out for that. And with that, I'd like to pass it over to David, who is the VP of product at Shopify, for a demo of another really exciting capability, which is our live API. DAVID BUSHERMANN Thank you very much. Hello, everyone. Thank you, Jason. It is an honor to be here. I'm really excited to showcase how Shopify has been building on this amazing technology of Gemini, specifically the live capabilities. So I know there is a lot of hype around AI launches. I hope that what I show you today doesn't actually feel like hype. The idea is I want you to feel it as real. It's a reality I think we're quickly stepping into. It's a reality I'm personally really excited about. And I've been building for 20 years in tech. And honestly, I can't remember a time I've been more excited to be building. And I think hopefully by the end of this demo, you'll see why I'm so excited. For those of you unfamiliar, Shopify is an e-commerce platform. We power entrepreneurship. And it's entrepreneurs of all shapes and sizes. We've got entrepreneurs that are just starting on their journey. And they've got a big vision. They're working out of their garage. And they're working out of their garage all the way to the biggest and fastest growing brands in the world. Entrepreneurship is exhilarating. It's liberating. But it's also challenging and confusing. Our merchants need to succeed across hundreds of tasks. They don't need to succeed as an entrepreneur. And as they scale, they have to hit these various sort of success metrics. At setup, they have to create a store. They have to figure out what they're going to call their store. They have to create a logo, a brand, figure out what they're going to sell, identify suppliers. At launch, they need to understand the basic concepts like how do we advertise? How do we engage in social media? How are we going to fulfill? How are we going to deal with order returns? As they optimize and scale, they have to do these types of things all over again for every market and every geography that they enter. So it is a lot. And if you think about it, a lot of the folks that are on the Shopify platform aren't necessarily experts in all of these areas. So the journey for entrepreneurship is definitely exhilarating. But it's also intimidating and confusing. The very premise behind Shopify as a company is that we try to lower the barrier to entry for entrepreneurship. We want more people to be able to choose it as their career. And to help solve all of those challenges that they'll encounter as they scale up. And one of the biggest ways that we do this today is with our human support advisors. These are amazing individuals that support our merchants in their journey, unsticking them when they get stuck, offering advice, encouragement as they go along. As much as we'd love to, we can't be there at every single moment, every time a merchant has a question or is stuck. It just doesn't scale, considering the scale of Shopify, millions of merchants around the world. Until now, perhaps for the very first time, we see a path to offer unlimited support. And this support is augmented and powered by AI. And thanks to the incredible technology and people behind Gemini. This support is endlessly patient. You can talk to it for hours about your business strategy, your ideas. It never tires. It's always on. It's ready to take the call when you want to call. Whenever inspiration strikes. When you're walking the dog. When you're packing boxes in a warehouse. On a weekend. It has perfect memory. It gets to know you. It remembers everything you've ever expressed to it. Your fears, your hopes, your desires, your goals, your current set of challenges. And finally, it has unlimited knowledge. It knows everything about you, your preferences. But it knows everything about your Shopify store. What apps you've used. What you've clicked on recently inside the Shopify product. It knows everything about Shopify and the products and services that we offer. And of course, it knows everything about e-commerce broadly. It has all of that knowledge in its purview. And it can connect the dots. But I don't want to just talk about this new paradigm in customer support. I want to show you. Because we've built it. At least the very earliest versions of it. And today, it's going live to a small group of Shopify merchants through Shopify Sidekick. So I'm going to roll the demo here. I'm on my home screen to Shopify Admin. And you see the Start Screen Share button. Hi, David. I'm your Shopify assistant. I'm ready to help you with your Shopify store. I see your Shopify admin dashboard. How can I help you today? Hi. So I have a domain name I've registered at Cloudflare. And I want to use it for my Shopify store. How can I do that? To do this, you'll need to add your domain to Shopify. And then update your settings in Cloudflare to point to Shopify. Go to Settings. Then click on Domains. From there, click on Connect Existing Domain. Then follow the prompts to add your Cloudflare domain. Let me know if you need help with the next step. Okay. It looks like Shopify is verifying the connection. This process can take a few minutes. Once the connection is verified, you'll need to update your DNS records in Cloudflare. How do I do that? You'll see a list of DNS records that Shopify needs you to add. Copy each record and paste it into your Cloudflare DNS settings. Okay. So in Cloudflare, where do I go on this screen? Okay. In Cloudflare, you'll want to select your domain. Do you see where to do that? Where's DNS on this screen? On the left side of the screen, you should see a menu. Click on the option that says DNS. Okay. Now why do I click here? Now click on Add Record. You'll need to add the records from Shopify one at a time. Start with the A record and copy the values from your Shopify domain settings. What do I put in the name field? For the A record, put At in the name field. What do I put on proxy status? For the proxy status, make sure it says DNS only. Okay. And then press Save? Yes. Press Save and then add the CNAME record. Okay. Adding the CNAME record now. Okay. For the CNAME record, in the name field, enter www. And then for the target, enter... Does that all look good? Yes. That looks right. Now go back to your Shopify admin and click Verify. Thanks for your help. You're welcome, David. Is there anything else I can help you with today? That's it. Have a good one. Thank you. Thank you. I don't know if you guys have the same feeling that I did when I first encountered that technology, but something just hit completely different for me. It felt like we flew past the uncanny valley. We now have an AI that keeps up with us. It can converse with us in real time. You can interrupt it. You can solution with it. You can debug with it. And it feels very different. In early tests, we've seen Shopify merchants use this technology. And one of the most interesting observations is that it's almost as if the users are forgetting they're talking to an AI, which feels historically significant. Also, in early tests, we're seeing merchants express themselves to the AI in ways they would never have expressed into a chatbot. They're opening up to the AI, and there's something uniquely engaging about the real-time voice modality that I think encourages this rich, nuanced conversation. We're seeing our users talk a lot longer, half an hour or more with this AI, learning all sorts of things about our platform. It's incredibly exciting. As you saw, for me, in the demo I just shared, it actually took me off of a Shopify surface and brought me into Cloudflare. And that's also incredible. Think about it. It's giving me complete, holistic support. It knows what I need to accomplish, configuring a DNS record in Cloudflare, and it brings me into the Cloudflare interface. By the way, all of that was done with zero prompting about DNS, domain names, Cloudflare, and even how Shopify works. It's the foundational model, effectively out of the box. We're rather excited about this. I know what some of you may be thinking, which is, where do the humans fit into this really exciting AI future? We believe humans fit beautifully in. And actually, the best support is a combination of humans with AI. Humans are there as a safety net, especially at the start when this technology is very new. It encourages people to engage because they know a human is behind the scenes, ready to take over and assist if the conversation goes off the rails. Humans are there to catch edge cases. There's always going to be the gnarly issues that the AI is confused about and the human can come into the rescue. But maybe most importantly, humans are there for genuine human connection and to offer empathy. And as we talked about, the entrepreneurial journey is an exhilarating but very difficult one. And so sometimes that connection from a human is necessary. And that can be uniquely delivered by human advisors. It is true AI will change the role of human support advisors. But in fact, it's going to change the role of every role in every industry. It's not unique to support advisors. At Shopify, we have really thought hard about how it changes our world and our job and how we show up to work. There was actually a memo by our CEO, Toby, about this just a couple days ago, posted on X. And it talks about how we have a responsibility, a mandate almost, to reinvent ourselves, to move faster in this new world. And no doubt, our support advisors at Shopify are going to be helping us pave some of that path. Thank you. Grab a seat. Thank you, David. So we wanted to iterate on the format of how we normally do these things. We have the three of us up here. I wanted to ask in more of a fireside chat format, ask a few questions and then leave some time at the end for all of you to ask some questions as well. So we've got Tulsi up on stage now. And David, we are sitting roughly in opposite order of our pictures. So I don't think you would have gotten us confused anyway. But let's get into some of the fun behind the scenes stuff. We wanted to peel back the curtain a little bit for all of you. So we'll start, Tulsi. We moved quickly from the Gemini 2.0 models to the 2.5 models. Can you explain what happened and why the 2.5 models are better? Yeah. So first of all, hi, everyone. Thank you all for joining us. As Jason introduced at the beginning, I'm Tulsi. I lead the product team for our Gemini model efforts. And I think what you're seeing with Gemini 2.0 to 2.5 is we've built a lot of foundations for how we develop these models across pre-training and the starting of our model training process all the way to the end. And that is now delivering in terms of the model's capabilities. So when we developed the roadmap for the models, we knew 2.0 was already going to be a step change when we released it in December. And 2.5 indicates two other big differences. One is this is us introducing kind of thinking as being a native capability of the model. What do I mean by that? Well, when we introduced the 2.0 models, they were extremely good at reasoning, but they don't actually expose or output thought tokens as they're doing that process. With the 2.5 models, this is actually baked into how we're building the models. So instead of there being a separate reasoning model and a separate flash model or a separate reasoning model and a separate pro model, the pro and the flash models are thinking models. So that's one big step change between 2.0 and 2.5. The other big one is just like the raw capabilities of the model. So we knew that as we were starting a new kind of end-to-end training run from pre-training to post-training, that if we saw these capability shifts, we want to make sure that that's something that we also signal to all of you in how these models are different from one another. And so that's kind of the difference between 2.0 to 2.5. And don't get me wrong, the 2.0 flash model that maybe hopefully many of you are using is also an amazing model. And with 2.5, we're taking all of the pieces that we learned from that model and continuing to step it up. The iterative cycles have gotten faster and faster as I think we've learned more about the training process and what good looks like. And so I've been very pleasantly surprised at how quickly we moved from a model where we thought we could do better to another model where we've got it all out there right now. And the research teams will continue to innovate. But being able to move in a matter of a few months from one iteration to the next I think shows Google's commitment to staying at the forefront and pushing the state-of-the-art. Yeah, and I think one thing that's interesting with all of this is, you know, I spoke earlier about foundations and you're talking about kind of the iterative cycle. It's also about how we bring in feedback, how we actually hill climb these models, right? So one thing we talk a lot about is hill climbing, right? So we want to basically start with where the model is today and then keep iteratively improving it. And, you know, a big thing to do that is you need to have the right metrics to improve. You need to have the right pieces of feedback to bring back from customers or from, you know, users of various Google products. And so now I think that we've built a lot of those foundations. We've built eval sets that we trust that we can continue to improve. And we're going to continue to improve on those as well. But it allows us, I think, to speed up our overall iteration process. And I think you're seeing that with 2.0 to 2.5 too. One of the things that I've learned over the last couple of years is the model development process, when you do it right, it's this eval-based process. And so what does good look like? And then how do we figure out how to get to what good looks like? And what I'd actually encourage all of you to do as well in your development process, often the first thing to do is set up your own eval set. Figure out what does it look like when I think I've succeeded. And then you can run that against all the models in the world. The Vertex has not just the Gemini models. We offer third-party models and open-source models as well. So if you're using Google's infrastructure, you have many, many options. But you'll understand what it looks like. And the model development cycle is so quick in this industry right now that you're constantly looking at, okay, I made a bunch of trade-offs to get to production in the previous cycle. Three months later, do I have to make those? Or can I get rid of that whole infrastructure that I created around the model? Because now the model just does it. And we're seeing this over and over again. And it's super exciting. But it can feel overwhelming sometimes to try to keep up with it. So if you start from a good eval, you can quickly understand, okay, can I confidently upgrade to the next model? Can I move to the state-of-the-art thing right now? Otherwise, you end up in this cycle where you're kind of stuck on a pinned model version for a long time. And then you're actually seeing many, many of the capabilities of the models pass you by. So if you can have this eval-driven process, it'll make you more nimble. And it'll keep you at the forefront. Speaking of which, David, you've been one of our earliest partners on the Live API. Can you describe what partnering with Google on an experimental product feels like? The good, the bad, the ugly, all of it, please. The amazing. Yeah. I mean, first of all, it's just so awesome to see how fast Google's moving in this space. And I think in general, I feel very fortunate personally because Shopify has its risk tolerance turned up to 11, which allows us to use the state-of-the-art models right off the shelf, so to speak. That comes with a cost. We basically have to pay to be early, right? The models that exist now have all sorts of quirks. We know many of them. And we've stumbled upon them. What's encouraging, though, is that even though there's these sort of, like, ridiculous things that happen every once in a while, you know, I showed you a nice, clean clip of the DNS. They're not all that clean, right? Users are forgiving the machine. It's a weird thing that's happening. But it feels like there's a human connection being developed. And I think it's actually allowing us to go to market with something that's a bit more bleeding edge. It doesn't have to be perfect because the parts that it does, it does super well. Google's been an awesome partner, like, giving us access to the APIs before they've gone GA and allowing us to experiment. I mean, we saw the first glimpse of it in AI Studio, which I'm sure most of the folks in this room have seen. But incredible because as soon as things are out and in the wild, you can try it for free in AI Studio. And I feel like every weekend there's something else to try in that product. So keep up the good work. We're liking what you're putting out. Thank you for your patience with our teams along the way as well. As you said, there's some quirks. There's quirks in the models and also in software engineering. And it is the live API is one of these areas where you're bringing bleeding edge capabilities of models where you're introducing new latency requirements, new interactivity, sort of these workflows that are not well-defined, and having to match that all with a production environment, understanding your customer. You were mentioning the importance of eval sets, right? And we totally agree. We have a golden data set that we're constantly curating. And like you said, it allows us to know whether the latest model is actually better than the old. But what's interesting about these multimodal models is that the golden data set is sort of a different animal entirely. And we're so early in the multimodal space and the screen share and real-time voice that we're still trying to understand what the best golden data set looks like for those models. If you have any tips for me. I was going to ask you, can you expand on maybe one or two things that you look for when building that data set? Like is it a resolution to an issue or what are the... Sure. I mean, yeah, we have... So actually, it's an interesting thing that happened. We had sort of a single-dimensional golden data set initially. And I think that's how most golden data sets are. It's like here's the utterance from the user, and here's what like a really great outcome looks like. We actually have moved to a multidimensional model where we sort of rack up points if it does certain things. Because we realize like what is right about a fairly complete answer, right? Like maybe like the actual right answer to like a deep research style query. It's like, you know, a page, right? It could be two pages. And so what we realize is that it has to be more nuanced. So we basically have a set of things we're looking for in the response, and we're grading this. We're scoring the entire thing. So it's sort of like breaking it down into its components. We've seen a bunch of emerging capabilities come out of the models. And the multimodal streaming is certainly one where we're combining models, understanding, and streaming, right? This interactivity. One of the other capabilities that we've seen excitement around is around code. Tulsi, can you help us understand where these models are doing better on code than our previous generation and what you're most excited about around code? Yeah. And so if you haven't tried 2.5 Pro yet and you are a developer or you just want to vibe code, please do try it. I think you can try it in the Gemini app using Canvas. You can also try it in AI Studio or if you're a Vertex customer in Vertex. I think with 2.5 Pro, one of the things that's really exciting is its coding capabilities. I think we've really focused on trying to make it good at a few key areas, right? So one of the key things we pushed on is web app development. So we really wanted the model to be able to, with even just a single prompt, be able to generate richer experiences. And you can see that playing out in Gemini Canvas if you're trying that today. You can also try coding intersected with the multimodal live API eventually, and that also lets you play with these kinds of combinations of things. I think in terms of what we're really excited about 2.5 Pro. So I said web apps. The other one, of course, if you use IDEs like Windsurf or Cursor, is you can use 2.5 Pro directly in these IDEs. And you can pick it in the dropdown and use it. And what we're seeing is we really tried to emphasize use cases that we think are common in IDEs, whether that's code editing or code transformation. That was another big push. And then another big push is these agentic capabilities. How can we actually enable sort of end-to-end workflows and software engineering workflows, and how do we build that out, right? So these are the areas that we really were excited to push on. These are the areas we're starting to see people give us feedback on. And now as more and more users are trying 2.5 Pro, we're getting feedback on what use cases are exciting and motivating, what use cases maybe need to continue to improve, and we're continuing to bring that into the model to make it an even stronger coding model. And it's hoping that this is something that can actually accelerate workflows, that it's something that opens up more doors to be able to prototype quickly, to build and to experiment. I think that's really what we want 2.5 Pro and Gemini models going forward to be useful for. We had touched earlier and introduced the concept of thinking budgets. Can you explain what that is and how it works? Yeah. So right now when you use 2.5 Pro, it's an always-on thinking model. So that means no matter what you ask the model, it'll first output this set of tokens, it'll think about how it's going to respond, and then it'll actually produce the response. And that's great because in many, many cases, that improves the quality significantly. But we also know that for a lot of folks, latency matters, cost matters, right? Depending on your use case, depending on your workflow, we want to be intentional about all of those elements. And so what thinking budgets allows is for you as a user of the model to actually be more granular in saying, hey, here are my limitations, right? So for example, you could say, hey, I only want the model to think up till 8,000 tokens. That actually limits how much the model will actually think. That means it will limit the cost of the model. It'll also be clear about what the latency of the response will likely be. And that becomes then a ceiling for how far the model will go. It might think less if it doesn't need to think as much, but it won't max out beyond that 8,000 tokens. And so when we ship Flash, which is coming soon, the 2.5 Flash model, we're excited to bring budgets to Flash. And that's really because Flash is our workhorse model. It's the model we really expect kind of across the board to be great for broad-scale use cases. And with Pro, we continue to want to push its quality and make sure that it is really the best model for any complex task you might want to do. What are some of the other differences? As DeepMind is thinking about the different model sizes, what are the other considerations between a model like Pro, a model like Flash, and then a model like Flash Lite? Yeah, it's a good question. So currently, we basically have these three big model sizes, Pro, Flash, Flash Lite. Starting from the kind of lowest end of that, Flash Lite is really meant to be the cheap, fast model. So I would think about it as if you really have simple tasks that you want to complete, whether that is like translation or summarization or really large-scale workload where cost is really important, that's a great offering because it will kind of allow you to get the basics done. Flash is what we've been calling our workhorse model. And what we really mean by that is like Flash is the model that's behind the multimodal live API. It's a model that is fast enough to support real-time streaming, but it also has pretty strong quality capabilities. It's great with multimodal understanding and reasoning. It's great with code. It's a model that will, you know, complete a lot of the kind of core tasks you need to do well. And then you have Pro. And Pro is really meant to be the best model you can use for any sort of complex task. So large-scale reasoning, imagine with long contexts, which I'll talk about in just a second, being able to actually put in a large-scale repository and actually understand and analyze it, being able to do kind of complex financial understanding and analysis. that Pro really will meet kind of that high-quality bar that you might be looking for in a model's performance. What's also interesting about Pro and Flash is we've introduced in both of these, or all three of these models, actually, this idea of long context. What that means is you can put in immense amount of content into the request that you're giving the model in the first place. So Flash and Flashlight have a 1 million token window. 2.5 Pro also has a 1 million token window, and we're bringing 2 million to that soon. And so for all of these models, what you can actually do is put in an immense amount of long documents, video content, and actually analyze that content. And 2.5 Pro, as we scale it up to that 2 million context window, is going to be able to do that even more. It's all so exciting as we've seen this progress sort of month over month. It's not even quarter over quarter. Week over week. Week over week. And we look at benchmarks as they're coming in, and we get excited about all of this. David, I'll throw this over to you. So you've told us in our prep sessions how some of the demos that you've created and your team has created have been inspiring for the company. What are you most excited about, either in terms of the technology or the outcomes for this coming year? I just feel like we're in such a significant moment in history. I know it sounds cliched and a bit over the top. I realize how it sounds. But, you know, I think if just a couple years ago I could speak to myself from just a couple years ago, and I said you'd be building a robot that you'd actually love to talk with, I wouldn't believe you. And the fact, just if you extrapolate out on how great they're getting and all these amazing capabilities you all are building, and the fact that those are eventually going to be available for customers like Shopify, and the fact that we get to use them and continually upgrade these things, it's already incredible now and has leapt past the uncanny valley. What does this look like in two years from now? The answer to that question lingers in my mind often, and it's very exciting. It's also very intimidating to think about. Given how your risk profile, I'm sure you'll be there with us first. Yes. Which is amazing. From the Google perspective, we're very thankful for partners like you, where the risk tolerance matches the reward. Yeah. And it's really fulfilling to hear that your customers also match that. And so it isn't just a team pushing technology out, but it's really a team that's looking to make meaningful impact for millions of small business owners. That's, you know, as we talk through this, I find that to be very inspiring. So there's like a slightly zoomed out picture, which is that this technology is making us better humans. Right? So like in Shopify's case, our users are merchants, and it is sort of an endearing use case. The mom and pop that's working for themselves and sending their kids off to college on that income. It's amazing, right? But if you think about what we're doing at Shopify, we're taking these tools and powering that use case. That's making those humans more able to tap into their full potential and realize what their dreams are. But that's like just one of many examples of how this technology is changing people's lives. Right? People are using it at work for all sorts of reasons. People are using it to teach their kids things. They're learning things faster than ever before. Everyone's a data scientist now. Everyone's a UI designer now. Everyone's, you know, an engineer. You know, it's sort of like allowing us to reset our own expectations about what we're able to achieve ourselves. And I think that's just so, that's such a phenomenal time to be alive. Yeah, one of the things I keep talking to my team about is how can we use Gemini ourselves in our jobs, right? So not only how do we build Gemini for, you know, the Shopify's of the world, but how do we actually start using Gemini for our own productivity? And that's also been really fun to kind of see and get to actually understand how the models can accelerate your own work every day. I found both at work and in my personal life, I'm relying more and more on Gemini. It's not just models overall. I actually think I'm very excited to see where all the Google products across all the surfaces are really coming together. It's been really the last six to nine months have gotten super fun. So I think we're at time. I wanted to thank David and Tulsi for being up here and thank all of you for being enthusiastic Gemini users. We love your feedback. We love your feedback. We love your feedback. We love you. And you did. We love your feedback. We love you. You could love you. And you did. You could love you. And you did. And you could love you. You could love you. And you could give generale rعonol11. You could love me. You could love you. And you could love you. And you could love you. And you could love you. And you could love you. And you could love you. And you could. And you could love you. You could love you.