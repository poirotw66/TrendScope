 . Hey folks, and welcome. As I'm sure all of you have learned over the last couple of days, everyone is either using AI already or in the process of pivoting to AI. But how do you differentiate your business if everyone is kind of moving along in the same direction? One way is leveraging AI at the right time. My name is Nick Orlov and I'm a BigQuery product manager focusing on many of the real-time capabilities of BigQuery. I'll also be joined by Pratik Dublay, who's a product manager on PubSub and supports the real-time messaging capabilities of PubSub, as well as Sandhya Kampoor, the SVP and head of the central platform organization at Flipkart. During my conversations over the last six plus months with customers, a few big trends keep emerging. The first is that there's a strong push for unification and simplification of data systems in order to eliminate the challenge of incorporating disparate systems or systems that are piecemeal in nature. This is really opening the door for IT leaders to consider and adopt new and open design patterns. The second is with such a huge portion of generated data being unstructured, customers need a platform that can handle diverse types of data, things like multimodal data, in order to incorporate that into your data analytics and AI pipelines. And the third is AI is really fundamentally changing how businesses think about data. The challenge is, however, that they need to use existing tools with AI in order to reduce the time it takes to go from data to insights. And lastly, businesses see an incredible need to upskill their employees in order to boost the productivity from gains like AI, but without the requirement to retool, retrain their team and their staff. So what do these trends have in common? AI, AI, AI. AI everywhere. It's surprising to hear that at our conference, right? Well, maybe not so much. But it's true. AI is permeating every single industry, every single market, and really every interaction that we are having with our customers, other companies, or each other. Whether it's using AI to do healthcare patient monitoring, thwarting cybercrime, or just making sure that you and your friend are in the same photograph together, AI is unlocking new kinds of possibilities in order to improve efficiency, personalization, as well as identify untapped new opportunities. Now look across the slide. Besides AI, what big theme do you see? Real-time. Real-time capabilities are really paving the way and becoming a huge driving force behind AI advancements. To talk more about that and more about the industry trends, I'm going to welcome Pratik on stage. Pratik Pratik Pratik Pratik Pratik Pratik Pratik Pratik Pratik Pratik Pratik Pratik Pratik Pratik Thanks, Nick. exciting use cases. But the challenge for businesses to stay competitive is that they increasingly need to have real-time intelligence capabilities. We are living in a world where innovation is happening faster than ever. Business and business units, they cannot wait for days or weeks for the KPIs to be computed. They want the results within seconds, even subseconds. Customers and users, they expect personalized recommendations and experiences. So IT and business leaders have realized that they need to invest in real-time intelligence capabilities to deliver on these set of use cases and capture the value. But we do understand streaming is really hard, and we are on a mission to simplify streaming. Before we look at simplified streaming architecture, let's take a quick look at three key streaming patterns that we see with our customers. These patterns are ingestion, analytics, and event-driven processing. Ingestion is simple. You're taking your raw processing data, raw data, streaming directly into your data warehouse, which is BigQuery or Data Lake, cloud storage, and you are processing the data once the data has landed into these destinations. This is your classic extract, load, and transform pattern, and you can use processing engines like BigQuery using your standard SQL, or you can use Apache Spark with Dataproc or Apache Flink to process the data that's sitting in cloud storage. The second pattern is streaming analytics, where you are streaming raw data in. You are processing the data in real-time, in-flight, performing operations like join, aggregation, enrichment, and then taking the process data to these destinations like BigQuery and cloud storage. This is your classic extract, load, and transform pattern, and processing engines here you can use are Cloud Dataflow or Apache Flink. The third pattern, and the one that we are seeing more often, is event-driven processing. In this scenario, the events are coming in. You are processing those events, generating insights, and taking those insights back out to drive downstream actions. Examples of downstream actions could be mobile notifications, data synchronization with third-party systems, operational automation. One thing you would notice here is BigQuery is at the front and center across these three patterns. Let it be ingestion, analytics, or event-driven processing. And as I was saying earlier, streaming is hard, and there is one pattern that you can implement to simplify your event-driven AI ML workloads. And it could be as simple as this. It looks really simple. It's basically you have Cloud PubSub, followed by BigQuery Cloud PubSub. How does it work? You start ingesting your raw streaming data into Cloud PubSub. For PubSub, you create a BigQuery subscription that gets the data from Cloud PubSub into BigQuery with one click of a button in a more native and seamless fashion. Once the data starts landing into BigQuery within subseconds, you can use that data to perform your analytics using standard SQL, or you can also use BigQuery continuous queries to process the data as the events are coming in, generate the insights, and take that data out to another PubSub topic for activation or data synchronization type of use cases. Cloud PubSub is also one of the supported destinations for BigQuery continuous queries, so it makes it really, really easy to implement this streaming architecture, something that you can actually implement within minutes. So now let's take a quick refresher on Cloud PubSub. I'm assuming many of you know Cloud PubSub, which is our fully managed serverless messaging service for event streaming and ingestion. It's really simple to start with, extremely easy to use, highly reliable, durable. We store the messages across multiple zones within a region. There is no infrastructure for you to manage, maintain, or scale. It auto scales up and down from few kilobytes per second to gigabytes per second, depending upon the data volume that's coming in. And there is literally no operational overhead whatsoever. All you have to do is create your streaming topics, create your subscriptions, and the data will start flowing in as soon as you start publishing the data. The service also comes with rich set of event streaming features. So we have global routing. You can publish the data from anywhere in the world. You can subscribe to that data from anywhere in the world. There are four delivery types, so push and pull always existed. In the last couple of years, we have launched BigQuery and Cloud Storage subscriptions that take the data from Cloud PubSub to BigQuery and Cloud Storage with one click of a button. The service also comes with standard messaging features, such as message ordering, exactly once delivery, schema, schema evolution, and so on. Cloud PubSub is also one of the most integrated services within Google Cloud ecosystem. So we talked about BigQuery and Cloud Storage subscriptions. One thing I would note here is we recently previewed BigQuery tables for Apache Iceberg. So if you are trying to get data from PubSub into BigQuery tables for Apache Iceberg, all you have to do is create a BigQuery subscription, point that BigQuery subscription to BigQuery tables for Apache Iceberg, and the PubSub data will start landing into Iceberg tables within minutes. I'm also excited to share that stream sharing in BigQuery via Analytics Hub is going GA next month. So we previewed this feature six months back that enabled customers to share their high-value streaming topics or PubSub topics through Analytics Hub internally with their customers or externally with their customers if they are in the data monetization business. So many customers across multiple verticals are using this capability to share their streaming data across the board. Apart from that, we also have connectors for Apache Flink, Apache Kafka, and then there are 70-plus third-party integrations built by third parties to help you get data in and out of Cloud PubSub. Let's take a quick look at a customer example and how they are leveraging Cloud PubSub. So Spot AI is a startup that transforms security cameras into video AI agents that work alongside with their customers, safety, operations, and security teams. And they were looking for a streaming service with three key needs. First is scalability. The service should scale up as their data volume was exploding. They were also looking for a service that has native integrations within Google Cloud ecosystem so that they can take the data within different parts of Google Cloud. And the third key need was low latency. So they wanted to get the data from Cloud PubSub, perform certain actions on top of that, and drive automated actions in physical workspaces using that data. So Spot AI chose Cloud PubSub, and now Cloud PubSub handles 18 billion messages of AI metadata daily for Spot AI connecting 75,000 plus cameras. They are also using BigQuery subscription, so all of that AI metadata from PubSub lands into BigQuery so that they can perform analytics on top of that data and drive actions based on the insights. So the key benefit that Spot AI was able to achieve was they don't have to worry about the infrastructure anymore, and that enabled them to focus on high-value, high-ROI kind of activities downstream, which is analytics in BigQuery and driving actions or automated actions in physical workspaces. Now, shifting gears a bit, I would like to now share a couple of features that we have been working on for the last one year or so. So the first one is import topics for streaming ingestion. It's going to be generally available. So multi-cloud workloads has becoming a reality for many organizations. These organizations want to run certain type of workloads, let's say operational on one public cloud, and they want to run other type of workloads, let's say analytical on GCP using BigQuery. And streaming data consolidation has been a challenge for these customers. They have to run, manage, and scale custom connectors to get data from these cross-cloud sources into Cloud PubSub. So we embarked on this journey last year where we introduced import topics that enable customers to get data from Kinesis data streams into Cloud PubSub with one click. And now we are expanding that capability where we are supporting three cross-cloud Kafka sources, specifically AWS MSK, Azure Event Hubs, and Confluent Cloud to help you get data from these cross-cloud Kafka sources into PubSub for streaming data consolidation type of use cases. It's really easy. How does it work? Basically, all you have to do is create a topic, enable ingestion, specify one of these cross-cloud Kafka sources and credentials, and we will manage the ingestion from these sources into PubSub in a fully managed way for you. The next capability that I'm really excited about is PubSub single message transforms in JavaScript UDFs. It's coming very soon. And an overarching goal for Cloud PubSub has been to simplify streaming architectures. So a great example is BigQuery and cloud storage subscriptions where we enable customers to get data from Cloud PubSub, into these destinations with one click. We also introduced import topics where you can get cross-cloud sources, data, streaming data, into PubSub, again, with one click. So the next big frontier for PubSub was to simplify transforms where customers have the need to transform one single message to validate, filter, or alter the data as it's passing through Cloud PubSub. So we are planning to launch PubSub's JavaScript user-defined function soon, and these are simple native single message transforms within Cloud PubSub without the need of having to use additional products to transform single message. They are also easy to use. You can write it in all-familiar JavaScript to build a diverse set of transforms. How does it work? So you can apply up to five JavaScript UDFs on the topic side and or on the subscription side. If you have specified a JavaScript UDF on the topic side, we will apply the transform and persist the transform message. If you have specified it on the subscription side, we will apply the transform before sending the transform message out to the subscriber. And this is just another example of how PubSub is simplifying streaming architectures for customers. And with that, it is my pleasure to invite over Sandhya Kapoor, SVP of Platforms at Flipkart, to share how Flipkart built a massively scalable streaming data platform using Google Cloud technologies. Thank you, Prateek. Hello, everyone. So I work at Flipkart, and Flipkart is one of the largest e-commerce company in India. And to know a bit about Flipkart's business, the scale that we are operating at, we have any day more than 500 plus million customers. We have more than a million sellers who onboard onto Flipkart's platform to sell their products. And one of the highlights that Flipkart has every year is called Big Billion Day. And that runs for a period of time. The unique, I would say, feature about this festive event is that it scales up with a lightning speed. And from a business-as-usual scenario, we scale up to 12x instantaneously on the very first day, the very preview that happens. So we have the point here that is that as far as infrastructure is concerned, we have to be ready for it. So with that said, let's dive a bit into Flipkart's data platform. Every second, we are ingesting about 3.6 million messages. And our ingestion rate is about 10 gigabytes per second. Also, in real time, we are processing about 3.7 petabytes of data per day. And this is fueled through 1,000 plus real-time data pipelines. Also, batch processing about 20 petabytes every day. And we have 10,000 plus batch processing pipelines. Hot and cold, hot data storage about 30 petabytes. Cold data, 60 petabytes. The unique thing is that we are creating 70,000 plus reports from BigQuery. We are heavily invested in BigQuery usage and PubSub, as you have heard from Prateek and Nick. So to get an idea about how the scale-up is during any festive event, during BBD, as I mentioned, the big billion day, it's for over a period of 10 days, we are issuing 5.7 million queries to BigQuery. So it's just the scale-up and the provisioning we do in GCP on the BigQuery instances that we are using. Now, what is our architecture? Just a quick dive into it. At a high level, we are sending clickstream data to PubSub. And this one, along with the server events, such as events of serving a product page or serving the search items to the user, all of these server events, they are going over to PubSub. We do have an open-source Kafka instance on-prem, which is taking these events to PubSub. But then it's all getting merged into the PubSub instance on GCP. The reason we have the Kafka instance still fueling on-prem is just to avoid any network issues that can cause a data loss, and hence we will lose what the customer is trying to do on our platform. So once the data is in PubSub, we are running streaming jobs, joins, aggregations, and then sending the data to BigQuery for real-time analytics. Also, from PubSub, we are sending to our Lakehouse, that is the Flipkart data platform, for any kind of model, tuning, inferencing, and running agentic AI workflows. So this, in a nutshell, is our architecture at a high level. Let me now get more into our PubSub usage and the pros and cons that we have considered over a period of time and what our deployments are today on PubSub. So as I was sharing earlier, we are ingesting per second 3.6 million messages, and our ingestion rate is around 10 gigabytes. We definitely try to optimize the ingestion rate so that by sampling, just so that we can have a more optimized input flow. Now, before we started using PubSub, our architecture looked in this manner. All the server events log, as well as the clickstream data, was coming to the open source version of Kafka that we were running, and then we were running stream processing jobs in real time. Now, once we moved to PubSub, as I had shown earlier, our clickstream data from the users is going directly into PubSub, and then the server event log is coming through Kafka just to avoid any losses in data because of any potential network issues. So this is very helpful to us because, as you can see, after moving to PubSub, we can get immediate scale for big billion days. Also, all the data is being pushed to PubSub and collects there and streaming jobs run on it. We pay as we go for PubSub. It is fully managed, and we don't have the ops overhead, as is evident. Also, the producer and consumer, as also is very clear, are fully isolated. And if we wanted to, we can scale the consumers to 100x and have a faster lag catch-up. And I'll elaborate more on how the lag catch-up is very critical for us, especially in very high-scale events. And needless to say, the global consumption is given to us by using PubSub on Google Cloud. Today, we are spread across two regions in India with the GCP instances running there. So our journey started with this evaluation that Kafka was not giving us the auto-scaling that we needed. We had to do the manual management of cluster scaling ourselves. And also keep the hardware around for the peak scale, which, as I was sharing, can be 10x to 12x in terms of the festive events. Also, our scaling unit was a partition, and that is a very difficult unit to keep on gauging how we have to scale up. The throughput of a lagging consumer, we were not able to increase immediately. To be able to do that, we had to keep a lot of hardware for the entire year just for a few festive events. And even if we had to fuel in new data for these consumers, that would only be the portion of new data, not any data that had been lagged behind. So these were the reasons we had to, we decided for ourselves to move from Kafka. We did not have multi-region support as well. So as I have mentioned before, PubSub is providing a serverless experience out of the box. We are scaling to 100x as needed for producers and consumers and scaling them independently. And we don't have to manage ourselves. So how do we, what were some of the reasons we had to do a few manipulations in terms of being able to make PubSub work for Flipkart scale? And that is just the compression and decompression of data. Because of the volume of messages, this ability to compress and send it to PubSub and then decompress at the consumer level was important. Now, because PubSub is global, we can consume from any region. And as mentioned, we are using two regions in India. And we have the flexibility that fail back is very easy for us. Now, I will definitely share that to have auto failover without losing any data loss, we did add a resilient publisher on-prem so that it can failover to the instance of PubSub we have in a different region, that is the second region for us, and seamlessly feed to the consumer that has the global endpoint. So it was a small adjustment that we had to make by adding a resiliency layer. Let me now move on to our BigQuery usage at Flipkart. We have about 450,000 queries being processed every day via BigQuery. More than 10 petabytes processed every day. And our end-to-end response time at an average is less than 10 seconds. So it's really important for us to keep up to those SLAs. What is our architecture for real-time analytics? Earlier, as the data was flowing in through Kafka to our streaming jobs, it was then pushed to Elasticsearch. But Elasticsearch definitely did not serve our purpose very well. So we have substituted Elasticsearch with BigQuery. And the journey from Elasticsearch to BigQuery has been very, I would say, fruitful for us. We did this migration in 2021 to 2022 timeframe. Now, in Elasticsearch, we were not getting the auto-scaling. The fundamental purpose of Elasticsearch is search. So the aggregations and joins were not native to Elasticsearch. Elasticsearch. And so moving to BigQuery, which provides us complex joins, allows us interactive analytic queries to be issued, and streaming inserts has been absolutely beneficial and game-changing. Now, one change that we had to do, at least in our workflow, was to ensure that our latest record per entity is available as we query for it and use it in our different business features. BQ supports appends. So now we run jobs to write to BigQuery in a streaming fashion, which then appends the rows to an existing BigQuery table. And after that, we are using the Windows function to create a BigQuery view that will provide the latest record per entity ID for us. And this is important for our various business features, such as search and recommendations and doing the order path for our customers. So with that said, I will go on to how real-time AI ML workloads are being served with these two services being used by Flipkart. Some of the very prominent AI ML use cases at Flipkart are definitely hyper-personalization. We are focused on giving a very personalized homepage and search results for our users along with ranking the ads that we should present to them based on their preferences that are available to us in terms of user attributes. Under product recommendations, we are using AI to detect various opportunities in real-time to cross-sell and up-sell and also providing certain product alternatives, suggesting product alternatives to our users, again, based on their attributes. In the area of trust and safety, we are definitely using Gen AI for fraud detection in payments, in product reviews that come through to Flipkart's website and mobile app. Also, we have a very huge catalog. I mean, if I have to phrase it in terms of the number of items in the catalog, definitely the usable items at any day is of the order of 200 million. So, checking the quality of the images and videos in our catalog along with the guardrails for safety in presenting these items is where we are using Gen AI models. We are planning to use agentic AI workflows, have already started on that journey, for real-time business interventions such as dynamic pricing, fulfillment, loyalty programs, optimizing the logistic routes that our last-mile delivery people take, enhancing the catalog through automated quality control, and on and on. So, with that, I'd just like to take a small example of AutoSuggest, which is personalized to the user in real time. So, we use attributes such as recent activity bias and other attributes such as gender. So, if I were to type in L, the list on the top of the list would probably be lipstick, whereas if someone else were to type in, like my colleagues, some of them here, it might be like something which is more so suited for their preferences. So, that is just the AutoSuggest feature. Another attribute that this feature uses is affluence and geography. So, for example, if we are searching for shoes, the affluence geography may suggest a different ranking based on the cost and the availability in the region. So, these attributes are very commonly used. The architecture for AutoSuggest at a very high level is that the click stream data is coming from the user to PubSub. We are applying in real time streaming joins and aggregations to generate the real time features. Out of the batch processing, which happened from PubSub to Lakehouse, we update the historical features and both the real time and historical features are then sent to the AutoSuggest ranking model and this model will give the personalized suggestion to the user who will then give feedback through additional click stream data and we will factor that in in a loop. So, that's how our AutoSuggest is working today. And some of the areas that we are looking forward to are definitely in PubSub, the cross-region replication, the single message transforms using JavaScript UDFs, as you just heard about, in BigQuery cross-region replication, CDC tables, combining our BigQuery tables with Apache Iceberg. We have already started on that journey, making it more native to our BigQuery usage and the BigQuery metadata store. So, with that, thank you for listening and let me invite Nick here to tell us more customer stories. Thanks, Andhya. So, I've been supporting BigQuery for over eight years now and I just absolutely love hearing customer success stories and hearing things like running 450,000 queries every single day against BigQuery is just really inspiring. So, hopefully, as evidenced by Flipkart's talk and hearing about the 10 gigs per second ingested per second, as well as any blogs or previous next sessions that you've seen from me in the past, it shouldn't really be a surprise that BigQuery handles streaming and ingest extremely well. We make streaming into BigQuery incredibly easy and scalable. In fact, BigQuery has hundreds of petabytes of streaming and ingest every single month. We also make BigQuery streaming accessible and integrated with things like PubSub, Flink, Kafka, Dataflow, Dataproc, et cetera. But let me pose this question to you. Is streaming only data ingestion? No. Streaming data ingestion is an important part of real-time data processing, but the key word there is processing. Getting real-time data into your platform is one thing, but processing that data in a real-time manner can be tricky. So tricky, in fact, that stream processing is often equated with very expensive and complex to use and manage tooling. Enter BigQuery continuous queries, which I'm super excited to announce is going to be GA at the end of this month. BigQuery continuous queries is a SQL-based, fully serverless, and fully managed approach to running streaming pipelines. It's the ability to write a long-running SQL job that ingests, processes, transforms, and analyzes your data in real-time the moment that data arrives within your table. This is huge for three reasons. The first is that continuously processing real-time data is no longer complex and expensive. It is now easy. The second is your continuous query can include calls to Vertex AI models like Gemini in order to layer in generative AI for real-time application development. So things like real-time sentiment analysis, real-time customer personalization, or real-time anomaly detection are not only feasible, but easy. And third, continuous queries allows you to stream the export or the results of your query into a destination. So things like PubSub, BigTable, Spanner, as well as another BigQuery table can be used as the exit point of your continuous query. And this makes things like real-time reverse ETL very easy and possible. And looping back to Pratik and Sandia's messaging about PubSub and the value points, supporting PubSub is where things get really interesting as a destination point. And that's because you can easily build event-based data pipelines using the continuous query to feed data into PubSub. And this is great for building custom applications yourself, but also exciting because you get to leverage PubSub's great partner network. In fact, we already have 13 fully verified partners which can adjust and process the results of a continuous query. And we'll demo one of these service now in just a couple minutes. So we discussed what continuous queries is. Now let's talk about how you actually use it. And it's really as easy as writing a SQL statement. Much the same way that you'd write a SQL statement to do batch analytics today. You first write out your SQL query, enter the continuous query mode, and hit run. This starts a long-lived job that will continuously process new data as it arrives immediately within your table, and then does a computation defined by your SQL statement. And as you can see from this animation on the screen, my continuous query is using the Gemini 2.0 Flash model to create a personalized email based on the message that was received into BigQuery, all in less than 60 seconds, and all orchestrated by a SQL query. And as you can tell, I'm very excited about this feature, but so are some of our customers. Customers like DMM, a Japanese on-demand video streaming service, they're using continuous queries and Vertex AI to simplify their real-time processing to deliver personalized video recommendations to their users. And when I spoke to them about continuous queries and what they valued from it, the big thing that I learned was what continuous queries unlocked. Yes, it allows them to provide a better application to their users, but the other benefit that they highlighted which was interesting is that they were able to use their existing data, existing assets, existing architecture, and they were not having to write any new complex code, tooling, or custom development. This really unlocks substantial new value with their existing capabilities. In their words, BigQuery Continuous Queries is a great service that allows us to launch products quickly and easily. So what are we announcing today with continuous queries? We're announcing the general availability of stateless data processing, support for BigQuery slot autoscaling, Spanner as a destination, support for PubSub message attributes, as well as a variety of features that make using and managing continuous queries easier, and some new regions. And believe me, there's a lot more exciting things that are planned and that are coming, just not things that I can announce just yet. And zooming out a bit, Continuous Queries is just a single component of BigQuery. And thanks to BigQuery's unique architecture and incredible capabilities, it offers customers a platform with AI directly built into it, capable of processing all of your data workloads, whether it's structured data, semi-structured, or unstructured, whether your processing is through SQL, Python, Spark, or other technologies. BigQuery is a truly serverless product without any compute clusters or infrastructure to manage and provides an incredible 54% lower total cost of ownership compared to some alternative offerings. And looking back at the three streaming patterns that Pradeek identified earlier, streaming ingestion, streaming analytics, and event-driven processing, BigQuery can handle virtually any real-time use case that you throw at it. So let's dig into a demo which actually demonstrates this and combines those three streaming patterns together. This demo is going to be for a fictional company called Symbol Pets. All right. If we can... Oh, there we go. So this is for a fictional pet company, Symbol Pets, that has IoT sensors that are located on each fish tank. And those fish tanks are reporting data about the water conditions to BigQuery, streaming that data into BigQuery. And then a continuous query is picking that up, determining if any water conditions are unhealthy, and then if so, making a call out to Gemini to request some guidance on how to fix the problem, and then streaming that into ServiceNow, their ServiceNow workflow data fabric that allows them to basically page one of the Pet Superstore, a Symbol Pet Store employees, in order to rectify the problem. So to demonstrate the real-time streaming capabilities and the ingestion and processing, I have a Python script which is running in a notebook within BigQuery. And what this Python script is doing is it's just writing these messages or the IoT data from the fish tanks to the PubSub topic around 1,000 messages per second. And these 1,000 messages are supposed to represent a variety of fish tanks across all of our stores within the United States. And this is only streaming the healthy ranges of the water conditions. So things like the pH level, ammonia, nitrate, nitrate, the temperature, salinity, things like that. Now one thing that I wanted to highlight and go back to was Pratik messaging about or talking about PubSub single message transforms. Now the value that I'm incorporating that into this demo is with IoT, it's generally very difficult that once you deploy the device out into the field, how do you go and make changes to the device once it's already out there? Maybe you don't have access, maybe getting push notifications or the ability to push updates to that device is tricky, etc. So what I'm doing within my Python script is I'm actually generating two timestamps, a correctly formatted timestamp and an incorrectly formatted timestamp. And I'm sending about 50% of my data through the correct timestamp versus through the incorrect timestamp. And rather than that incorrect timestamp being dropped, what I have is a PubSub single message transform that's just in JavaScript as a UDF, which is basically checking to see each message that's coming in. Is it correctly formatted? If not, then it will make the changes necessary and then it will continue processing that message in order to write it to BigQuery. In this way, nothing is being dropped whatsoever. And if I go ahead and refresh my PubSub dashboard, you can indeed see that I am ingesting about 1,000 messages per second, all of them successful thanks to that single-message transform. And one fun fact, going back to my Python script, about 90% of it was generated through the Gemini 2.5 model, so it's certainly saved me a lot of time and effort in terms of my productivity. But let's look at what this data is actually writing and get to the continuous query. So all this data is landing in our Fishtank IoT data ingest table, which has hundreds of millions of rows. And as you can see from the data that's being ingested, it's really just machine-generated data, nothing super interesting or fancy, just some information about the temperature, et cetera. And here's my continuous query. So I'll scroll down to the bottom. The continuous query is reading from that Fishtank IoT data ingest table, and it's checking to see if there's any ranges that are outside the realm of normal. And if there are, it generates a prompt that basically says, hey, Gemini, based on these values, what are the most likely cause for this water tank issue and what kind of steps would I need to remediate that? It takes all of that and as I mentioned, packages it up and sends that prompt to the Gemini 2.0 flash model and then writes this to the ServiceNow writer that's through another PubSub topic. And as you can see from MySQL, it's pretty basic. There's nothing super fancy, nothing very complex. The most complex piece is really just writing that prompt, which is just purely natural language. Now let's go ahead and insert an incorrect event or an unhealthy event for the Fishtank data. So what I did was I just inserted one single row of the same kind of values that are unhealthy water conditions and I wrote it to my Fishtank IoT data ingest table. Now I'll go over to my ServiceNow, Service Operations workspace, and ServiceNow is really good at building and automated workflow managements. And that DML insert that I inserted just a moment ago, what it's doing is it's writing to that same BigQuery table that's being ingested by thousands of other topics or thousands of other messages from that PubSub topic. It's being analyzed by that continuous query. Once it gets detected that there is a fault, it's generating the prompt, sending it out to Gemini, getting the response, and then sending it to a PubSub topic, which is going to be read from ServiceNow. Now me, as a ServiceNow employee and as a technician, I'm just waiting for this prompt to arrive, and there we go. So this prompt has arrived in less than 30 seconds or so. A good chunk of that time is waiting for Gemini, the 2.0 flash model. So I'll go ahead and click Accept, and let's see what Gemini generated for us. We'll just give it a moment. Okay. So the first thing that we notice is that it actually converted a lot of that machine-generated values into legible and useful content. So it gave us a description of what the issue is, and so it's messaging that here are these water conditions. It's providing us some likely causes. So the low temperature is a major concern, potentially indicating a heater malfunction. So that sounds like a serious issue. And it's providing us some recommended actions. So immediately check the heater, ensure that it's functioning correctly, and set the appropriate temperature to between 78 and 82 degrees Fahrenheit, as well as some other steps. And so me, as the service technician, I can say, I'm on it, post my comments, and then go ahead and save those fish. And now, if we can go back to the slides. There we go. Awesome. We have about two-ish minutes left, which isn't a ton of time for the Q&A, so maybe what we can do is take the Q&A off to the side. But definitely leave us some feedback. We'll be happy to chat more offline, and really appreciate everyone attending the talk, as well as Next in general. Thank you. Thank you.