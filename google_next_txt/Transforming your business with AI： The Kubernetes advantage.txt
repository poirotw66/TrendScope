 I'm Gabe Munroy. I'm the Vice President of Cloud Runtimes at Google Cloud. It's a privilege to be here. Really excited to share how your existing expertise in Kubernetes is your most valuable asset in the age of AI. Now, as many of you know, starting back in 2014 with the birth of Kubernetes, the evolution of GKE has been nothing short of extraordinary. It's changed how we build and deploy applications. In fact, this year marks the 10th anniversary of GKE. How awesome is that? 10th anniversary. So from stateless applications to stateful workloads, you know, hybrid cloud, edge computing, and now with AI, GKE's evolution is a testament to the brilliant community behind it, to the contributors and the builders who make Kubernetes happen. So today, we're going to be announcing some exciting updates to GKE that are going to save you time and resources so you can focus on innovation. We also have some guests joining us on stage, and we're going to do some fun demos and show you some of the new capabilities that we're unveiling today. First, we're going to walk you through how to optimize your existing workloads to save compute resources with new GKE autopilot launches. Then, you're going to hear about new innovations in Gemini Cloud Assist that will help you reclaim development time. After, we're going to cover how you build your own AI platform. We're going to dive into new inference capabilities on GKE that will make your application smarter. We'll also share some exciting news about the Ray project from any scale before introducing new supercomputing capabilities to scale AI for those of you operating at the bleeding edge. Okay, let's get into it. So if you're running web servers, API servers, queue processors, CICD agents, or other common workloads, you, all of you, are most likely over-provisioning some capacity to ensure that your apps are responsive. GKE customers often leave, like, nodes running all the time, and that leads to under-utilization, unnecessary costs. So that's part of the reason why in 2021, we launched GKE autopilot, the first service of its kind to combat over-provisioning. Autopilot dramatically simplifies Kubernetes cluster operations and enhances resource efficiency with a fully managed control plane, fully automated management of nodes, and automatic right-sizing of cluster resources. In fact, 30% of GKE clusters that were created in 2024, well, they're using autopilot mode. Customers like Toyota and contextual AI, they're all using GKE autopilot to reduce their operational burden. Now, today, we are announcing that autopilot supports faster pod scheduling, scaling reaction time, and capacity right-sizing, all made possible by unique hardware capabilities that are only available on Google Cloud. We are thrilled to announce that starting in Q3, autopilot's container-optimized compute platform will also be available to standard GKE clusters without having to use that special cluster configuration. So now, all GKE customers, all of you, can save on compute resources to create more space for innovation. And we're pleased to announce the private preview of Gemini Cloud Assist investigations. Investigations help you understand the root cause so you can resolve issues faster. It provides structured AI-powered workflows that begin with a user-provided symptom, analyzes that against a set of comprehensive data, logs, config, things like that, metrics. And the best part, all of this is available to you inside the GKE console. So you can spend less time troubleshooting and more time innovating. If you sign up for the private preview, you're going to be able to do some special things like diagnose pod and cluster issues from within the GKE console, even across other Google Cloud services. So you have VMs, IM, load balancers. You can also derive insights from logs and errors across different GKE services. So controllers, pods, underlying nodes. And because Cloud Assist investigations are done in the context of the GKE UI, you don't need a separate chatbot or a separate observability tool. It's all available right in the console. Companies who are operating at the cutting edge of Kubernetes in AI, like LiveX AI and Moloko, they all run inference on GKE. And in discussions with these customers, and many like them, we found that platform teams that are deploying inference on Kubernetes face two key challenges. The first one is striking the right balance between performance and cost. Tuning accelerators to meet performance targets without over-provisioning them, it requires extensive knowledge of Kubernetes, AI models, GPUs, TPUs, like specific metrics like time to first token. It's really difficult, right? And as a platform team, you have to navigate that balance. The second challenge is the way that load balancing works in LLM-based applications. So with AI models, the response length is often highly variable from one request to the other. So the latency also can vary really widely. So this means that traditional load balancing techniques that we've used for years, like round robin, they can break down and cause high latency, uneven resource use. So to address these challenges, we're excited to announce two new GKE inference capabilities. First, we are announcing a new GKE inference quick start in public preview. So you can now pick an AI model with your desired performance characteristics, and GKE will configure the right infrastructure, the right accelerators, and the right Kubernetes resources to match. Second, we're launching the GKE inference gateway in public preview, which reduces serving costs by over 30%, reduces tail latency by 60%, and increases throughput by up to 40%. So you'll get a model-aware gateway, it's optimized for intelligent routing and load balancing, and it has advanced features for things like routing to different model versions. We can learn from the folks who are already doing this at scale on GKE. For that, we have just the person. Let me introduce Christian Lindwall, director of engineering at Spotify. Christian? So Spotify and Google Cloud have a long history together, dates back to 2016, when Spotify migrated from on-premises to Google. Since then, we've come a long way. Maybe you could tell us about first just some of the features that folks who might be using Spotify are familiar with that are actually powered by GKE. Absolutely. So one recent AI-powered innovation, and maybe my favorite feature at the moment, is the AI DJ. So this is something that we have conceptually thought about for quite some time, actually, but it was made possible more recently through the power of generative AI and this new paradigm that we're in. And this was, in deploying this to market, taking this to market, GKE played a crucial role. So some of the models here, so what it is, is a highly personalized DJ right in your pocket, automated voice talking to you about your music. And some of the models powering this, such as text to voice or figuring out what to say next, our models also are on GKE. Love it. And I actually have been using the AI DJ feature. I personally really enjoy it. You know, before we jump into the tech, though, I think the people side of this is also pretty interesting. Maybe you could tell us about how the platform engineering teams at Spotify are like structured to make the most out of GKE and Kubernetes. Yeah. So we have the core platform team, core infrastructure team, which looks after things like compute, network, networking storage, these things. And sets up GKE clusters for us. Then we have my team, which is the AI platform team that builds and deploys the ML platform, AI platform on top of what they provide for us. And then we have the personalization team, for instance, who do a lot of the research and heavy lifting when it comes to modeling and figuring out the strategy for our AI efforts, building their things using our platform. Yeah. And then they in turn have the experience team who builds actual interfaces using that. We are committed to making Kubernetes the best platform for using Ray. And we've been working closely with the AnyScale team to do that, the creators of Ray, and making sure Ray's optimized for Kubernetes, particularly the open source version. And we've been consistently impressed with the team at AnyScale. And that's why today I'm pleased to announce a new partnership between Google Cloud and AnyScale. Please welcome to the stage Robert Nishihara, founder of AnyScale and the creator of Ray. Robert. Super, super excited to announce AnyScale's partnership with Google Cloud and the GKE team. So we're working together to bring the absolute best Ray experience to every AI developer. So as part of the partnership, GKE users are getting access to AnyScale's Ray Turbo. This is AnyScale's optimized Ray runtime, purpose-built for AI at scale. This is delivering superior performance for workloads like training, like inference, like data pipelines. This is done through smarter scheduling, through faster task execution, through better resource management. Right? Our users are seeing things like four and a half times higher throughput for multimodal data processing, or serving large models on half the number of nodes. So this is really trying to enable AI teams to develop in Python, run across any compute, CPUs, GPUs, TPUs, with Ray and Kubernetes scaling everything dynamically under the hood. Love it. And now through this partnership, we're committed to enhancing Ray Turbo to make it even better on GKE, bringing together some of the features of our performance and scalability on the autopilot side, for example, with GKE, and bringing that together with Ray Turbo. GCP will be the best cloud for AI ML engineers to build, run, and scale Ray workloads. Yeah. Thank you so much, Robert. Really appreciate it. I'm thrilled to have you here and thrilled about the partnership. And it's thrilled to see what everyone here in this audience can build with any scale of Google Cloud. When workloads gain traction, scaling demands can be really intense, right? Things that we take for granted at smaller scale, they can start to break down. And it's the same with AI. As models continue to grow in size and demand more machines for compute processing, platform teams have to deliver architectures that are going to spread models across multiple hosts and operate massive clusters of GPUs and TPUs, but as a single unit. Because without these capabilities, customers are often going to struggle to complete large training jobs or deliver the inter-machine performance that they need for AI. So to handle these scaling challenges, we are excited to announce that our super computing services platform, Cluster Director for GKE, is now generally available. It delivers exceptionally high performance and resilience for large-scale distributed workloads by automatically repairing faulty clusters based on their bill of health. And one of the best things about Cluster Director for GKE is that you can orchestrate all of this just through standard Kubernetes APIs. So that means no new platforms, just new capabilities on the platform that you already know and love. So for example, you can use GKE node labels to do things like schedule pods based on network topology to maximize efficiency and minimize network cops. You can report and replace faulty nodes by gracefully evicting workloads from the node and then automatically replacing them with spare capacity. But not just any spare capacity, spare capacity within your co-located zone. You can manage maintenance windows so that you can manually start host maintenance from within GKE or use maintenance information while scheduling your workloads. Now, to be clear, not every customer is going to operate at the scale where you're going to need these kind of capabilities. But it's important to know that Google Cloud has your back when you do need them. And you can get started with Cluster Director for GKE using configurable blueprints or we have this hands-off approach with something called Accelerated Processing Kit. And that requires no previous Kubernetes experience. So, in summary, AI introduces new challenges for platform teams. But we're confident that the technology and products that you're already using, Kubernetes and GKE, with those you can tackle them.