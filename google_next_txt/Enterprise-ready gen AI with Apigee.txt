 Hi, my name is Gar Shursa and I'm an outbound product manager for Apigee. And in case you missed our session on Gen AI, I thought I would give you some of the highlights. So you may be wondering, are API management practices, are these things really relevant to Gen AI use cases? And we think they are, right? So Gen AI serving challenges really are the same types and the same family of serving problems that you have with API management. So there's going to be questions around distribution. How do I make these available? And kind of controlling the limits and access of that distribution. We want to make sure it's safe and secure, you know, so that we're not allowing our large language model services that ultimately are APIs to be abused and that we can control that access. We want to control the latency, provide things quickly. We want to control the cost. It needs to be performant and it needs to be highly available. And with that availability comes also observability. So we want to make sure that we can keep track of the type of prompts, the types of responses, the types of usage overall that are coming through that platform. And ultimately these are precisely the same types of concerns that we have with API management. Now with Apigee as an API management platform, we provide a bunch of value added services. API hub that's going to help us catalog your APIs and that's including those large language models that ultimately are APIs themselves. Providing flexible run time so you can run these and serve these use cases in different areas. developer portals that ultimately allow you to take the large language models APIs that you have and provide them in an interface so that users can find them, use them, and get their own credentials to access them. Analytics and anomaly detection so that we can keep track of that usage over time. Advanced security, right? So because ultimately these APIs or these AIs really are just APIs in their own right. And so we want to make sure that we can provide security interfaces for that that covers not only the API use cases but the actual AI specific pieces so that we can make sure we're guaranteeing that type of security as well and abuse. Productization and monetization in case you want to decide to take these large language models and in their own right turn them into products and ultimately monetize them. So find a way to get money back for those use cases. And things like app integration. So you may want to combine the API use cases you're doing with some more iPaaS style integration to back end services to build agents, for example. At the bottom here we'll see that we've got third party cloud services we can pull in. We can run our run times on-prem and of course we're a first class citizen as part of Google Cloud. So imagine that we've got someone like Emma. Emma has a series of large language model consumption challenges. She's going to be hosting or serving this new AI model and she needs to make sure that she controls for token limiting. So when I receive a prompt I need to understand the consequences of that prompt when it hits my large language model and control that access. I need to be able to make sure I can route to the appropriate model because sometimes different models are going to serve different types of use cases better. I might want to invoke semantic cache so I can take a back end response and serve it directly without forcing an additional call back to that back end service which ultimately can be very costly. I'm going to want advanced analytics so I can keep track of the observability information I have. And finally I might want a model failover. So if in case one of my back end services goes down I want to be able to cleanly reroute that service to something else. And finally I want to make it easy to constantly update and update new models and put them into production. And I'll finish by just talking about one of the new features that we're really happy about and that's the server sent events support in Apigee. So with these large language models they've begun to use these streaming effects that kind of provide that human-like interaction experience. So when I ask a prompt tell me a story about why the sky is blue a large language model is going to send back a series of events as if a human interaction was actually talking to me in that way. And what we might want to do in Apigee is be able to on a per event basis execute policies, provide enforcement, check for quotas, perhaps call something like Model Armor and make sure that we don't have any abusive content as part of that response. So with service center event supports now we can have something that we call an event flow inside of Apigee and therein we can put per policy pieces that will go for every single event that comes in independently. That's it for me. I hope this was helpful for you and thanks. Thank you.