 Good morning, everyone, and welcome to today's session on MLOps. I hope everybody got their coffee this morning after the killers last night. I know it's bright and early, so thank you for joining us here. My name's Chase Lyle. I'm product manager for Vertex AI Prediction in our MLOps portfolio, and we're super excited to talk to you about MLOps today. I have a wonderful solution specialist, Elia Sechi, and Damian Ramuno Johnson, a customer of ours at Block, who's going to talk about how Vertex AI training has accelerated their MLOps workloads. I'm super excited this year because last year we stayed mostly in theory. We talked about the theory of mixing predictive and generative MLOps, and this year we're going to focus mostly on a code-based demo, which is super exciting. Elia has one fantastic demo for you where he will be evaluating Gemini and Gemma 3. He will be using Gemma 3 with tool use to build a retail agent. He will then evaluate that retail agent, so super excited to get into that. And then Damian's going to talk about Vertex training and accelerating their MLOps, and then we'll have about five to ten minutes for Q&A, so get your questions ready. Jumping into MLOps. In all my conversations about MLOps, I love to start by grounding us on what I mean when I say MLOps. MLOps can be quite a large umbrella. It can mean many different things. When I talk about MLOps, what I mean is the people, the processes, and the tools to deliver ML applications to production rapidly and reliably. And what this generally looks like is your continuous integration and your continuous deployment of your data pipelines, your ML engineering pipelines, and your application engineering pipelines. And the latest, greatest applications everybody wants to talk about, of course, is agents. And arguably this started back here at Google in 2015 with this research paper. And if you think about MLOps, it's really similar, if you think about it, to the public education system, right? I have a couple neural nets I'm training at home, my one-year-old and my four-year-old, and they're going to go to school one day. And at school, they're going to get an education, they're going to get an evaluation, and then they're going to eventually get a graduation, right? And that's what you need to do with your models. You need to build and educate them, evaluate them holistically, and then graduate them into production. So let's go through the MLOps lifecycle. It all starts with discovery. Discovery is where you find out what other people are doing and try to incorporate it into your practice. In predictive AI, we used to talk about discovering ML frameworks, discovering TensorFlow, PyTorch, Scikit-Learn, XGBoost, and using that to train on your own data. Nowadays, we talk about foundation models. And there was just a Stanford paper earlier this week that showed that there were 58 relevant foundation models launched in 2024. So the pace of change is amazing, and that's why we provide you the Vertex AI model garden that has up-to-date models that are relevant to you. We, of course, sim shipped Llama 4 in the hour that Meta launched it. We were a launch partner with them. And we also launched DeepSeq v3 March 24th within 20 hours of the DeepSeq launch. So we're always trying to give you access to the latest, greatest models. And what we launched here at Next this year is also Agent Garden. With Agent Garden, you have access to the latest, greatest agents from Google and our partners that you can incorporate into your practice. The next step is, of course, curating your data. Even if you're grabbing a model off the shelf, it's a very capable, high-quality, very safe model. You still have to evaluate it against your business jargon, your tasks, and your topics to make sure that it works for you. And what we found with our customers is that data curation is by far the most common bottleneck. If you think about predictive AI, think about what it took to label, like, a classification data set. Well, I think I could have been a pretty good classifier of hot dog versus not hot dog, right? But in generative AI, art was my worst subject in school. English was my second worst subject in school. You do not want me necessarily building the reference generation you need for your chatbot. That needs to be an expert in your business, in English, in all the languages you do business in. So labeling and curating your prompt generation pairs for your exemplar corpus that you're going to use to evaluate models, to fine-tune models, is by far the biggest bottleneck. And the exciting launch we had at Nexus this year that's going to help you is BigQuery multimodal tables. BigQuery's had object tables for a long time. With object tables, you could reference single objects in GCS, like an image or a video. With multimodal tables, you can now store those references in line with your data. So that if you have a large context of text, or if you have images or videos, you can reference them in line, and therefore be able to analyze them holistically, and then bring them back into your data curation loop. The next step, of course, is the development and experiment loop. This is the inner loop by which your ML application is changing quickly, but the data's staying fairly stable. It's all about generating batch predictions from your data, evaluating those holistically, comparing them to the baseline to make sure that whatever change you've implemented is advancing your system, and then deciding, is that good enough to go to production with, or do I need to augment the model? The most common form of improvement to these systems is RAG. We have to give the models access to up-to-date information from your business corpus. Or customize the model. Perhaps you need to fine-tune the model further based on your nouns, verbs, topics, and tasks, and you want to give the model your personality. And what we've found from dealing with many customers is that too limited of an evaluation metrics is the most common mistake businesses make, right? They might think through evaluating coherence, fluency, groundedness, but you may forget to evaluate on presentation, evaluate on deflection, how the model responds to negative cases that it can't actually answer, which has gone a long way to improving how the models deal with hallucinations. So it can be a very complex process to think through every dimension that you have a concern about that you want to evaluate in your inner loop and then in your deployment loop. The next step is, of course, releasing and deploying to production. This is where you're going to do your infrastructure validation, your security privacy reviews, your adversarial and integration testing, and then your shadow and canary deployment to make sure that you're not regressing on things like latency, on things like cost, and still improving quality and safety. And what we found with our customers is that because these are such hyperdimensional inputs and outputs, there are dimensions of concern that you may not know that you need to measure and evaluate. You might find that your evaluation loop actually overfits to your prior models. And so, therefore, we recommend that if you have an external use case, you definitely run an A-B test experiment that you can validate your next, you know, your challenger model versus your champion. Or if you have an internal use case, we actually recommend you do a side-by-side deployment where your internal business users can look at both models side-by-side and give you the rich feedback you need to say, yes, this is ready for production, or no, you guys need to go work on this a bit more. Then you'll be in production, and now you'll be serving at scale. And this is where you'll get into serving infrastructure, logs, metrics, and traces. And the number one thing that enterprises need to do in production is actually implement that telemetry that you need to understand if you're regressing along any of your dimensions of concern, your business performance, your model performance, your application performance, the data is consistent, and then your infrastructure is performing well. And therefore, I definitely recommend that you check out OpenTelemetry's semantic conventions for Gen.AI. We'll see that in our demo a bit later. But the open source community is really standardizing the way that we log Gen.AI events and traces across multi-turn chat and RAG loops. Each of these are fairly non-deterministic loops due to the temperature of the system. And they can have five to ten models in a RAG loop, for example, the most common way that you'll improve these models. And so you'll need robust telemetry by which you can understand if you're improving or regressing every time you evolve the model or evolve the system. And that can look a bit overwhelming. Thankfully, we at Google have the tooling to help make it easier for you. This is an architecture diagram from Dr. Sokratis Kartakis that I'd love to walk us through. He breaks it up into five parts, right? You start with the landing zone. This is your CI-CD tooling and your infrastructure is code that every time you have a new use case or a new team, you can deploy them the projects they need and the tools they need to get started and run reliably. Then you have your data engineering pipelines. This is your data lake houses, right, that are constantly improving your data, refreshing your RAG corpus, refreshing your feature stores with up-to-date information. Then you have your ML engineering pipelines so that you can continuously retrain, retune, based on the latest, greatest up-to-date information. And then you have your application integration. And this is where all the latest excitement is, obviously, in agents and how you're building out your agents, how you're structuring your orchestration, reasoning loops, reflection loops, getting tool use, for example, in your application. And then the last layer I'll talk about is the governance layer. This is where you're capturing artifacts that go into deploying those systems, but then that those systems also produce out back to you. And then, so looking a bit deeper, these are all the tools that go into this architecture. What you'll see in the ML engineering loop is the batch evaluation, the batch prediction and evaluation of Vertex is the foundation of that loop, right? As you're introducing change to your system, you're adding new data to your exemplar corpus, you're customizing the model weights themselves, or you're adjusting your agents. You need to be able to run batch predictions around the evaluations that you care about. Then once you have a robust model, you'll be deploying it into production, and that's when you'll be building out your agent further. And Vertex has a Vertex agent engine to make deploying an agent easy with a bunch of cool features like tool use and context and memory management. This can, again, look a bit daunting. I understand that. MLOps is quite a broad umbrella along many different domains. And so that's where, if you want to get started quickly, you can build upon Google's MLOps system. Google puts tons of time and money and effort, thousands of super labelers, experts in their field to build Gemini. And so if you want to get started quickly, we definitely recommend building upon the highest quality, most safe, low latency, low cost models that we have available. We make many available to you in the model garden, but Gemini is definitely one that I recommend trying. And with that, that's enough from me again on theory. I'm going to hand over back to Elias Echi so that he can walk us through a demo. Thank you. We are now going to do a demo that is going to show you what the whole journey that Chase explained earlier using some of the tools in Vertex AI and Google Cloud. Yeah. So we're going to start with the discovery phase where we are going to use Model Garden, for example. And in Model Garden, we're going to use Gemini. We are going to, of course, Gemini doesn't need to be deployed, but we're going to use it directly into a collab to build an agent. We are also going to deploy Gemini from Model Garden. You will see how fast it is to deploy an open source model in Model Garden. We're then going to move to the curation phase. In this case, we are talking about a dummy use case. So really, like, we are not going to make a big effort in creating the data, but we are going to use Collab Enterprise to build a dummy evaluation data set, which then is going to be used to then evaluate a model and then evaluate an agent we are going to build using Gemini and then using Gemma. Yeah. Yeah. So this is going, we're doing this to then build confidence so that after this phase, we're going to start the release and deployment process. So as part of this process, we're going to use Vertex AI agent engine, which Chase mentioned earlier, which is a great tool if you want to deploy in a matter of a couple of seconds an agent in Google Cloud. And we're going to use, I'm going to show you Cloud Build, which is a managed CICD engine, which will allow you to build automated testing pipelines and automated deployment process for your agent and your data pipelines. Finally, we're going to move to the prediction and monitoring phase, which is kind of like the production part, where ideally in a real scenario, a real user will use your agent, will use your application. And you will use all the cloud observability tools like cloud login, cloud monitoring, cloud tracing to monitor how your users are using your application and build insights so that you can go back to the discovery, curation and development phase and, you know, iterate on the process. So if we can now move to my screen, thank you. You can see that we are starting our journey from Model Garden. So I'm sure many of you are familiar with Model Garden. It's this beautiful repository of models that we make available for you so that you can use it in a matter of a couple of seconds. And you can see Gemini there, but you can also see that if I now search for Gemma 3, we released Gemma 3, I think, three weeks ago, and it's fully available in Model Garden. You can use it in many ways. So, for example, you can open it in a collab notebook. You can fine-tune with it. But there is a new feature we just released, which is super interesting, which is called fast deployment. So this feature here allows you to test the Gemma 3 model in a couple of, I think, a minute or something like this, which is extremely useful because many times you just want to have a vibe of the model, understand how the model performs for your use case, before you actually commit to make a fully blown deployment using Vertex AI prediction. So let's try to deploy this model in Vertex AI. So this is opening up a tab on the right side, which is going to check for my quotas. So if you give it a couple of seconds now, I will be able to essentially deploy it into the US West region using a foreign video H100 GPU. So let's click on deploy here, and you can see that there is a notification bar that will show us when the model is deployed. But now let's move to collab. In the meantime that this model is deploying, let's move to collab. And in collab, we are going to build our agent, evaluate it, both using Gemini and then using Gemma. So as part of this journey, one of the products we are going to use to do the evaluation is Vertex AI evaluation, which is an open source SDK that will allow you to evaluate your agent, your model, in a couple of seconds. Let's define all the different environment variables, and then all the libraries we are going to use, and some helper functions that will allow us to essentially display some of the outputs we are getting with the model. We are going to use LanGraph to build the agent here. LanGraph is an open source framework that allows you to build agents. Now, we are using LanGraph here, but you might actually have seen that we released a new open source framework from Google called Agent Development Kit. You can use also that. So the philosophy here is really in Vertex AI, you can use any tool, both coming from Google but also open source tools. And you will find actually that the journey to build your agent is pretty similar no matter which tool you use. So as part of the agent we are going to define, of course this is a dummy use case, we are going to define an agent that is available, we make available to the agent two tools. So essentially one tool is the capability for the agent to get product details. And so let me zoom in a little bit, but you can see that these tools are pretty dummy. So like we are giving five different products to the agent, and for each product detail we give like the description of the product. And the second one is the product price. So for each product we assign a price. So you can imagine in a real use case, this tool call will actually connect to your data. So your DB, it might be a NoSQL DB, it might be a vector store, but the idea is that the agent will be able to pull information from an external source. So let's define this. And then we are going to build the LanGraph agent. So the LanGraph agent is pretty simple agent, but the only thing that I will highlight is that based on the parameters we are going to use, it's going to either use Gemini or to use Gemma to perform, to power the agent and power the LLM loop. So let's define it. And I can actually show you a pretty example of how the agent will work. So you can see here the agent itself, powered by the LLM, which can be Gemini or Gemma, will have some tool call available. And if the model decides that it needs to perform a tool call, it will then return the tool call back to the agent, which will then return a response to the end user. You can actually start testing the agent and see how he performs. So in this case, you can see we are using Gemini 2.0 Flash, yeah? So you can see the agent, if we ask for the question, get product details for the Wave Phone, it will actually make a nice response using the tool call we defined earlier, get product details for product name Wave Phone. And the response looks good. So let's try another call where we are going to ask for the product price of the OnBuddy Mini. So you can see also here the response looks good. The product price was 80. Now, we prototype our agent. It's time to do evaluation. And as I mentioned, we are going to use Vertex AI evaluation to do this kind of exercise. So Vertex AI evaluation is a product that we launched, I think, for more than a year. But the really cool thing we announced a couple of months ago is the support for agent evaluation, yeah? So not only will you be able to evaluate the responses of your model and the agent, but you will be also able to evaluate how the agent is achieving that given response, yeah? Because as part of the response, maybe the agent is making a tool call, right? Or maybe he's making 10 tool calls, right? We want to understand how the agent is making those tool calls and evaluating, for example, the trajectory or the chain of thoughts of the agent, yeah? So this is all things that you can do right now with Vertex AI evaluation. Plus, you will be able to understand and monitor other metrics such as latency or failure rate as part of the evaluation process. So using the evaluation service is pretty simple. You'll need just to build an evaluation data set. And, of course, this links back to the part that Chase mentioned on data curation. This part, you will need to essentially work with business experts to actually create an evaluation data set for your agent. So in this case, here is five really simple prompts. But for each prompt, we are actually giving a reference trajectory. So we are essentially trying to define how that agent will achieve a certain response and what the tool call is going to make. So I can actually show you in a better visualization how the data set is being created. For example, for the prompt get price for WavePhone, we are expecting our agent to make a tool call on get product price. And the tool input should be WavePhone. If we actually call something else, we want to monitor it because it may be a mistake. It's now time to define our metrics. So the definition of the metrics is super important because it's essentially setting up what's the target. How can we evaluate our agent? And so, like, the best thing about Vertex AI evaluation is that for many scenarios, you don't actually need to build a metrics. We provide you a set of pre-built metrics you can use to evaluate your agent. Yeah? And so, like, things like evaluating the single tool usage or, like, evaluating the trajectory of a given tool call, you don't need to build it yourself. You know, you don't need to rebuild the definition of what's precision or what's recall. We can give you a pre-built metrics for it. And so, like, we are actually defining all these pre-built metrics you see on the screen here, as well as we are going to use some metrics to evaluate the final response of the model. In this case, we are evaluating for safety and coherence. Now, in reality, many times you will find that you will need to do an hybrid approach. Some metrics will be pre-built. Some metrics will be customly created by you guys. But the beautiful thing about Vertex AI evaluation is that you can mix and match both. So, we define all the metrics. It's essentially a list. And then we are ready to perform evaluation. So, to perform evaluation, we literally need to run a method, in this case here. And you will find that in a couple of seconds, we are actually performing batch scoring and batch evaluation. And you can see the results on screen here. Yeah? So, now, how can we understand these results? So, first of all, the Vertex AI evaluation service will provide you a summary of all the metrics you have across all your data sets. Because you might want to understand, as a high level, what's the mean of a given metric? What's the standard deviation of a given metric? You know, but then you will find that when you are evaluating your agent or your model, you want to actually zoom in into the specific evaluation to understand why that metric was achieved or how that metric was achieved. And so, this is where the evaluation service also gives you row-wise metrics. Yeah? So, essentially, for each given prompt, we can have a look at what was the response, what was the ground truth, and what were all the different metrics we produce. Yeah? Of course, then, these metrics will be aggregated, and these metrics will essentially produce the summary metrics I showed you earlier. Now, we now evaluated Gemini using the agent we just built. Now, it's time to compare Gemini, compare the agent we built with Gemma 3. And, you know, like, it will be a matter of using the model we deployed earlier from Model Garden. So, let's have a look at Model Garden, and let's have a look at Vertex AI predictions, which is essentially an amazing product which will allow you to deploy and use models from any source, no matter if it's Model Garden, but also, like, if you have your own container, you can use prediction to serve models as scale. In this case here, you can see we just deployed the Gemma 3 model. We will need to copy the ID. And let's bring it to the colab. So, we are inserting the ID here, and then we define it. Okay. Let's make some authorization calls, and let's define the class that we are going to use. Let's try a simple query just to see if the model is capable of responding correctly. And, yeah, you can see that the Gemma 3 now, in a matter of minutes, is capable of responding to our query. So, the deployment went fully successful. Now, let's integrate the LLM we are using here into the agent we defined earlier. So, what I'm going to do is simply passing the endpoint ID to the class, and because we already had the code earlier, this is going to now use Gemma 3. So, let's try to ask a question to the agent now being powered by Gemma 3 and see if the agent will respond correctly. So, you can see here, the response is pretty similar to what we had earlier. Actually, maybe it's the same. So, for the product price of onbody mini, we get 80. So, you can see now the agent works correctly. Now, it's time to run the evaluation with Gemma 3 and the agent. And you can see that the code is literally the same. We are just changing the name of the experiment we are using. In this case here, we are calling it Evaluation Gemma 3 Flash. So, let's run this code. And again, in a couple of seconds, you will see all the metrics being generated for Gemma 3. And yes, these are the metrics. But, you know, now, how can we understand which model or which agent are better? Well, this is the part where Vertex AI Experiments helps you. Essentially, Vertex AI Evaluation, every time you run an evaluation, will send data automatically to Vertex AI Experiments. So, Vertex AI Experiments is another great product in Google Cloud, which will allow you to log every single experiment, every single test you're doing with your models or your agents. And so, we can click on View Experiment here, and Colab will show us a preview of, like, our experiment. We can actually open it up as a different tab. And you can see we have the two experiments, and for each experiment, the metrics we just produced. We can even, you know, click on them and then compare and run a side-by-side comparison. So, to understand which model is better in which metric. You know, and I find this super useful. For many, many use cases, I use this to actually produce a report of, like, all my experiments and how they went. Let's go back to the Colab. So, you might now have a situation where, after many iterations, you find out a good setup that works for you, and you're ready to build it to production. So, this is the part where, as I mentioned earlier, we are going to use agent engine to deploy our agent to production. And, you know, defining the agent and deploying it is super simple. Literally, you can just use the class we defined earlier and then pass it to a method called agent engine dot create. And in a couple of minutes, you will have your agent deploy to production, and your users will be able to use it. Yeah? So, you will just need to define the requirements and the class you're going to use, and boom, the result is done. Now, of course, I'm skipping a lot of parts here, because in the middle of the process, you will agree with me, you will need to have maybe a Git-based approach where you make a commit, and then there is a CICD process that is deploying the agent. And so, for actually enabling developers to do this kind of approach, we also released a solution called Agent Starter Pack, which is a public solution every one of you can use, which will allow you to essentially bootstrap the whole process. You can see it as a collection of templates, which will allow you to essentially deploy your agent to Google Cloud, an agent engine, really quickly, following a set of best practices. And because it's a template, you can always change it yourself. So, what I've done yesterday was to actually take the agent we defined here and using the starter pack to deploy it to our production environment. And I can actually show you the journey and how it looks like. So, if you actually go to Cloud Build, you can see we have, we created here a repository for our agent, yeah? And so, because it's a template, I actually changed the template editing and inserting the code that I showed you earlier in Colab. And you can see that this is like a normal repository, which will contain a Python code of our agent and, you know, like the different requirements and the different code and so on. But you can see here there is a tab, which is actually showing us that this agent was actually deployed to using Cloud Build to different environments. And we can actually see that we were able to trigger, by doing this action, we were able to trigger Cloud Build pipelines. And we can actually zoom in in some of them to show you what's happening, you know. So, this is actually enabling a continuous delivery process, which is taking our agent code, deploying it to a staging environment using Vertex AI agent engine, and then performing load tests in integration. So, what we are doing is we deploy the agent and then we start sending a lot of calls to the agent, yeah? And the agent needs to respond correctly. So, you can imagine when you run this in a real-use scenario, you want to make sure you're able to verify if the agent is capable of performing under a given load, you know. So, this is all about building confidence. So, we use the Vertex AI evaluation service to build confidence into the quality of our agent. And then we use load test Cloud Build to build confidence into the operational part of the agent. And so, like, latency, throughput, and things like this. And so, like, what we will obtain is a report that looks like this, where we will be able to understand what's the, you know, median, average, latency of our agent, how the agent is performing when we start increasing the number of users, and things like this. And the beautiful part is that, as a user, you will tend to be able to approve or not the deployment to production if you are happy with the results. So, in my case, I already approved the deployment. And so, you can see we triggered another CICD pipeline, which in this case is deploying the agent using engine-taging to a production environment. So, ideally, this is the part where real users will use the service, you know. And so, if I go back to the call-up, I started to actually, you know, sending requests to the agent and see if it responds correctly. And you can see that here we are actually triggering the production environment. Yeah, so we are not doing anything locally here. We are actually triggering a production endpoint. And you can see the agent responding well. So, you may think that we are, this is the end of the journey, right? I would say this is the beginning of a new journey. Because when the agent is interacting with users in production, we are able to collect data. We are able to collect insights into the agent, right? And so, this is the part where the observability and all the tools in cloud observability become super important. So, one of the tools is cloud tracing, which will allow you to understand how the users are using the services and what kind of requests are being sent to the agent. So, for each given request, and here we have a list of different requests, and this is going to be all automated, by the way. Every time the user is sending some messages, we are going to forward the log of the request and the response to CloudTrace. So, you will be able to see a step-by-step execution of the chain of thoughts of the agent. So, like, you can see that in this case here, the agent made a first tool call, sorry, a first call to Vertex AI, Gemini, then he performed a tool call, and then he finally made a response to the user. For each different call, we can zoom in and see what was the input and the output of the request. And, you know, like, now CloudTrace offers a really nice GNI-specific dashboard or, like, some helper utility displaying that will allow you to understand how, in a nice format, what was, for example, the system instruction, what was the input being sent to the model, and what was the output, and maybe even which model you use. But then, you know, like, CloudTrace is great for having an overview of, like, the current status, but you want to also make sure you have a long-term offline storage of this data. And so, this is where, by using some syncing kind of rules, we are able to forward this data to BigQuery. You know, and so, like, what we are achieving is our table, which will hold all the data, all the telemetry data that we are producing with the production agent. And, you know, like, this includes, like, hold the input, hold the output of every single call we are using with the agent. And so, this is the part that becomes super interesting, because from this data here, you can build dashboards to build insights on your data, and maybe going back to the data curation phase, development phase, to iterate on it. But you can also extract this data and create tuning data set, for example. So, you might say, hey, I'm not really happy with the performances of Gemini Flash. I want to build a tune model, and I want to essentially build a tuning data set with this data here. So, to improve the performances. And so, like, this is where Vertex AI offers a tuning product, Vertex AI Tuning, which will allow you, in a couple of clicks, to tune an open source model or a Gemini model in a couple of seconds. So, you'll only need to pass a JSON file, which will contain your tuning data set, and then click on Start Tuning. In a couple of minutes, you have a tuned model with your tuned data. And so, now I'm handing over to Damien, which is going to essentially show you how they're using Vertex AI training and the Vertex AI suite to accelerate their MLOps journey. Thank you. Thank you. Yeah. Yeah. So, hi. My name is Damien. I'm a machine learning engineer at Block, and I'm going to be talking about how we use Vertex AI for training with scale. And for everyone who doesn't quite remember Block, it used to be Square. We own brands such as Cash App and Tidal, and I've been there since 2016. And why does it matter that I've been there since 2016? It's because we started using Google Cloud in 2017. So, I've been involved with our process all the way from the beginning. And we can sort of give you, like, highlights of how we're doing today. Here's, like, a high-level overview of what we do use at Google Cloud. We're not going to go over everything. You'll see things like Vertex training, Vertex workbench, Cloud Run, container registry, all of that. And then you'll see some stuff that's outside of Google, like Snowflake, Prefect, Terraform, and BuildKite. But for today, I'm going to be focusing on AI workbench and AI training. So, our modelers have always enjoyed using notebooks to explore data and interact. And historically, we built an in-house solution. But as everyone knows, like, if you build it in-house, there's a lot of, like, maintenance. Things break. Users find new ways, like, make new challenges. So, like, we've then managed to move over to AIP via Google. And that made things a lot easier for us. We then moved to Vertex notebooks, which then became Workbench today. And it's really been a great fit for us, because even though we came from an area where we, like, fully owned hardware and managing our notebooks, we were able to still have full customization of our environment to make sure our modelers were still able to have, like, a really quick response and customization. And because, like, we have modelers who want, like, expensive GPUs or a lot of memory or the small ones for different visualizations, we have persistent storage via Google File Share and GCS Fuse. So, like, they could have multiple notebooks but have the same data and notebooks available across them. But over time, like, notebooks are great, but they're not great for things that are reproducible. So, we actually grew it into a full remote dev environment, because it is so customizable. So, we can host things like VS Code, Copilot, or other IDEs from our notebook platform. And this is great, because since we run in our company, Shared VPC Network, we're able to access the real data, because, as we know, modelers like to enjoy all the data you allow them to have. And they can't do that locally on their laptop, because that's not very secure. But we need an environment where they're able to interact with their data without a lot of friction. So, this was able to make that possible. Some challenge, early challenge was really that UI is great for Vertex Workbench, but there's only a certain amount of customization that you do directly via the UI. But they have a great SDK, Google, that allows us to fully customize it and make a good experience. Next, you'll find, you've done some data exploration, but you won't actually, like, have a model ship to production. We use Vertex Training. For that, we only use custom jobs, partly because we've always, modelers and our teams, have had a very, like, different way of, like, doing their modeling in terms of not wanting to migrate to Kubeflow or, like, other pipeline structures over time. And custom jobs does allow us to have a fully controlled environment. And we've had it over time. We've started small, but now we have over 100,000 jobs per month that we're executing with Vertex Training, with the modelers having pretty much full control of their compute. And I did say we don't use Kubeflow, but we do use Prefect, hosted via Cloud Run to actually handle triggering all of our Vertex Training jobs. And GPUs, you know, they're still not readily available unless you want to pay for reservation. So we've been using Dynamic Workload Scheduler via Google to ensure, like, our jobs that we schedule with Vertex Training can actually grab a GPU in time and actually not fail. But one challenge for Vertex Training is the API for modelers can be a little more difficult. And it can really slow them down. So we open source a library we call Cascade. And really this makes it easy for submitting a managed job to Vertex Training without having to really worry about the raw API for Vertex. So, for example, this is a Python library. And you import the remote, the environment config, machine config, and the resource. And you do things like I want a small machine or a big machine, how many GPUs you want, what environment you have, service account network. The usual things that you normally do is part of any Vertex call. You can define it here. And you feed it into the resource, but then you have this remote decorator where you can then specify in your Python script which resource you're using. So in this case, I made this complicated addition function. But you say, I want to use this GCP resource. And the remote decorator means in the Python script when it actually runs the addition command, it'll package up that code, ship it to Vertex Training, execute it, and then return the output back to the script. So this is a toy example, sure. But you can imagine scenarios where you don't want ‑‑ you could have data processing that does not need expensive GPUs. That could happen on machines that are a lot cheaper. So your script could call out remotely to do that. And then when you're actually ready and have your data processed, then you, like, trigger your H100 or whatever GPU you actually need for your use case. It's really worked out well for us. And this is how we're actually doing our 100,000 jobs per month. I will admit we haven't had too many external customers yet after we've open sourced it. So if you're interested, come talk to me. Because there might be surprises externally. But what's next? Like I said, we've been using Google Cloud for a very long time. And it's really been great for us in terms of the degree that we've been able to scale where we've had rapid increase with the users. Literally a 10x in, like, a couple of years, a few years ago. And we're able to really scale with, like, very little issues. The current focus right now is really improving our development velocity. Because, like, we found, like, in these 100,000 jobs that we've submitted, like, it's been very stable. Like, the uptime has been great. And, like, once you actually have a working job, pretty much perfect. But there's sort of, like, an iteration loop where, you know, it takes a couple minutes, like, to deploy. So, like, I've made a syntax error. I have to deploy it again. And if you're not... If you keep running the errors, like, sort of builds up and causes frustration. So we've been working with Google to both, like, have a better, like, interactive debugging environment. So we can sort of, like, fail fast and then fix it without having to wait for deployments. And this is more of a challenge where... With the GPU, like, even though dynamic workload schedule has been great, you're still not likely to get the GPU in, like, the next five minutes. So you don't want to have, like, a bug and then, like, wait around for a little bit, get a GPU, and then try again. But Google's been great. And we've been working with them for this and trying to figure out how we can address this in the future. But from here, I'm going to pass it back to Jason Helio. Thank you. Thank you. Thank you. Stay up for the Q&A? Awesome. Well, thank you so much. We're going to have just four minutes for Q&A, so a couple questions. Before we get into the Q&A, I wanted to highlight the Kaggle five-day Gen AI intensive. Last year's talk actually informed a white paper operationalizing generative AI on Vertex AI. Elliot was a co-author of that paper. I was just a contributor. So you can ask us all questions on it. I recommend it. There's five other papers in that five-day Gen AI intensive that I definitely recommend going and reading. You can read them directly, or you could pass them into Notebook LM. And there's two other papers that are not in that intensive recently announced and launched, which is evaluating large models from the Vertex AI evaluation team, and then also the agent's companion, just building upon all the lessons learned from agents. And with that, let's jump into Q&A. Do we have a question? Thank you. That's right. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.