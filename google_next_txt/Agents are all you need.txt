 Thank you. So I'm actually impressed by the turnout. I thought it's lunchtime, the day after a concert. And, you know, it's funny. A lot of this talk is going to be about planning and reasoning. And I feel like I could have planned this talk better. Because firstly, unlike a lot of other talks here, it's going to be close to 45 minutes of me talking at you, just one speaker. So we'll try to keep it as fun as possible. But I was thinking that, you know, maybe in the near future, one thing that I could have done is maybe if I hadn't lined up human co-presenters, I could have had AI co-presenters. Maybe it would have made me a little less nervous. Maybe you all could have been AI digital proxies of yourself. But we don't have any of that right now. But I did try to use AI through the course of this talk. So we'll see it show up in a few places. But first, some introduction. So I'm Shrestha. I'm a group product manager in Google DeepMind. And what I specifically work on is I'm the product lead for the Gemini developer APIs. So Google has two sets of APIs. The Gemini developer APIs. And our front end is called Google AI Studio. Maybe you've heard of Google AI Studio. And then you also have the Vertex APIs, which are more, which also, you know, the two sets of APIs pretty much have the same functionality. And, you know, depending on your use case, you could decide which set of APIs to use. So I'm the product lead for the Gemini developer APIs. And as I said, as I was prepping this talk, I thought, can I use AI to help me come up with the flow of this talk? And so what I did was I used... Let's see if I can get this to play. So Gemini 2.5 Pro is our best model. It is a thinking model in that, as you can see, the model comes up with a plan and a lot of thoughts before it gives you the answer. And so I asked Gemini 2.5 Pro, hey, I have this talk that I need to give. It's called Agents Are All You Need. And can you help me write the flow of this talk? And 2.5 Pro, by the way, has been going really, really well, has been really, really well received, so much so that initially we released it as an experimental model through Google AI Studio. But we got a lot of requests from developers to be able to scale with the model already, even before we could get the model to GA. So what we did is we've increased... About a week ago, we've increased the rate limits that you have access to through 2.5 Pro. And, you know, we've basically allowed you to scale your usage on that. There is a payment involved, but we've been getting a lot... Like, our TPUs are literally burning up with wanting access to this model. So let's see. So I showed you the whole response that 2.5 Pro gave me. Here are a few things that stood out to me that I wasn't expecting. Firstly, you know, my prompt was a pretty basic prompt. It said, here's the name of my talk. Give me the flow for this. It tried and identified who the audience of the talk might be in its planning. It then... I did not tell it that the talk was 30 to 45 minutes. I was actually surprised that it made a guess. Now, this guess could have gone wrong, but I suppose it sort of indexed on the median of talks. The flow was overall pretty reasonable. Like, I had... Before I asked Gemini to come up with a flow for me, I'd kind of written down my own flow, which is the flow we'll be following for the talk. And it mostly lined up. It came up with a catchy end phrase, which I wasn't expecting it to... But it was like, are agents really all you need? Maybe the way to end the talk is to say agents orchestrate all you need. And finally, as you can see on the slide here, it even gave me some suggestions for what visuals I might have with the talk. And, you know, this was sort of how it ended its whole response. So, just as I said, in terms of flow, like, here was the flow that Gemini recommended for me. It said, you know, start with an introduction, try to hook the audience, provide some background context, which we'll not be doing today just because I feel like you all probably have heard a lot of talks on this. Explain what agents are, show some examples, which we'll do, discuss where this is going, talk about challenges, of course. We're just getting started with agents. And then conclude. It's pretty similar to the flow I had written parallelly for my talk. So, you know, to me, that was a sign that this model is able to kind of reason through the arc of what it takes to tell a story when you're trying to, you know, talk about a topic with the audience. All right. So, let's start with the customary. What is an agent? I bet this is a slide you've seen at pretty much every talk in this conference, but it's almost a requirement. And I'll tell you why. But let's start with what do we say is an agent. Now, an agent, this is my definition, is an entity that can be asked to accomplish a goal, and it goes towards accomplishing that goal with some notion of autonomy. And in a future slide, we'll be talking about the different levels of autonomy that people have been, you know, defining for agents. But there has to be some notion of autonomy. Now, the agent is set in an environment, and in order to achieve the goal, the agent needs to be able to perceive that environment through its senses. It needs to be able to come up with a plan, and as it executes that plan step by step, it needs to be able to adapt that plan. And then finally, you know, LLMs by themselves can only output words or images. In order to be able to take actions, the actions, LLMs and agents need access to tools. So an agent is typically getting access to a set of tools. There's a couple of interesting points to be made here between the relationship between the environment of the agent and the tools that are available to the agent. So the environment that an agent can operate on is typically defined by what's the use case for the agent. Like, we're still a bit far away from general-purpose agents, but let's say if an agent is developed to play a game like Minecraft, its environment is that game. On the other hand, if the goal of an agent is to take control of your browser and do stuff for you, then the environment of that agent is the Internet. And depending on what is the environment, that will decide the set of tools that the agent has or should have access to. So a gaming agent may have access to, you know, actions that the agent can take inside of the game, whereas an Internet agent has access to things like clicking, dragging, typing. Those are the set of actions that a browser control agent has access. And it can sometimes go the other way. So if an agent, let's say it's a robot, if a robot can only swim, it will be restricted to marine use cases. So there's this interesting dependency on what is the set of tools that you can give an agent access to and what is the environment it can operate in. Or the other way around, what is the environment you want the agent to work on and what are the set of tools. Now, why is there so much debate, though, in terms of what is an AI agent? I think fundamentally, it's a question of degree. So we started with, this is a schematic from Crunchbase, we started with chatbots that could do, you know, through chain of thought, some basic reasoning. We then moved to copilots and assistants, which could reason, and then they had access to some kind of memory. Then, now, where we are starting to get to is agents with guardrails. So these agents have access to tool use, these agents have access to memory, these agents can reason through some reasonably complex plans. And then where we want to get to is ideally fully autonomous agents for specific use cases. Here is Gemini's version. When I asked Gemini, why is there so much debate about what an AI agent is? And, you know, Gemini kind of made this nice table for me. I tweaked the table just a little bit, and it said, well, the primary role, the nature of the task, the output of what the agent is required to do, the scope, the level of autonomy, of course, and the components. This is a chart from Gartner, which kind of, again, it was surprising, it kind of got to the same points. What we have today from LLMs today is kind of, they're kind of low agency. They can typically work on static tasks. They are typically being reactive. The user has to prompt them a lot, think a lot about prompt engineering. They can do simple tasks, working in simple environments, like, you know, maybe a single type of environment, and they have to be supervised. And high agency agents are going to be exactly the opposite of this. So, just to wrap this section up, though, and, you know, this is a question that I haven't fully answered for myself yet, but to me it seems that a lot of debate online about what's an agent and what is not an agent hinges on the level of complexity. So, if you have an entity that schedules meetings for you, is that an agent? Some people say yes, some people say no. Whereas if you have an entity that controls the entire operating system for you, most people will agree that that's an agent. So, I think it's the complexity of the task and the level of autonomy. Like, everybody agrees those are the dimensions, but where people draw the line kind of leads to a lot of the online debate about what is an agent and what is not. And, obviously, if something is an agent, there are other questions that come up. Is, does this entity have any notion of understanding? Who has accountability for the actions that the agents take? You know, and there's a whole discussion to be had around ethics, bias, liability, regulation. But these are the conversations that kind of lead to the questions around what is an agent and what is not. Okay. So, now we're going to get into what are the components that make up an agent? Okay. So, typically, when you design an AI agent, it has a few parts. There's the model. And as we've been talking about, and we'll talk about this a little bit later also, the 2.5 series that Gemini is coming out with, like, we've come out with Gemini 2.5 Pro, will be releasing Gemini 2.5 Flash in a matter of weeks, those are thinking models. These models explicitly think, and you will be able to see some version of those thoughts before they decide what response to give you or what plans to come up with you. And this planning by the model is a critical part of every agent's brain. Now, there's a few challenges that we still see when we are testing these models for planning. So, let's say I give the LLM a question like, find all the companies that have raised at least $1 billion but don't have revenue. So, you give this question to the model and the model can approach it in one of two ways, right? It can say, find all the companies that don't have revenue and then filter by companies that have raised $1 billion. Or, it could do it the other way around. It could say, find all the companies that have raised $1 billion and then figure out which ones don't have revenue. And the second way is the more efficient way. However, when we test any model on the market, sometimes it gets it right, sometimes it doesn't get it right. And this is a dimension along which all of the frontier labs are pushing. Not only to make multi-step plans but make them such that they are efficient, effective, you know, and cost effective. And one of the things that, you know, is part of design principles right now and you'll see in a lot of agent applications is, and you see it in our Google's deep research also, you first see the plan and the user explicitly has to approve the plan or edit the plan before it goes into execution. And that's just a good design principle for where we are with these LLMs, especially for more complex tasks which take off the order of minutes or eventually hours. you can also have, you know, and now we're getting into a little bit of implementation, instead of having the human verify the plan, you can even imagine designing agent systems where an LLM verifies the plan. You could go even more fancy and have the agent generate multiple plans and have some kind of LLM discriminator rate those plans. So there's multiple ways that people are sort of tackling this planning and reasoning step, and it's all very fascinating. And, you know, this is a dimension along which the models will keep getting better and better. The next category is tools. So we've talked about tools a little bit. What I do want to point out is typically if you look at all the tools in the market and, you know, very much in the news because of MCP servers, they can be broken down into three categories. The first category is tools that augment your knowledge or the LLM's knowledge, I'm sorry. So these would be tools like, say, web search tools. Google has one. You know, the other frontier labs also do. The second one could be augmenting the knowledge based on your proprietary company data. Right? So all of these tools, though, will fall in the category of things that increase the LLM's knowledge. The second category of tools are tools that increase the LLM's capabilities. So let's say you give the LLM access to a computer, a calculator tool, or you give access to a tool that can run code, like a code execution tool. Those are tools that basically extend the LLM's capabilities. So that's the second category. And then finally, the third category of tools are tools that actually enable LLMs to take actions. These are the most impactful. These are also the highest risk. And so these would be tools like the computer use tools that are very much in the news. And I realize I should have put that up on the slide, but I'll just summarize three kinds of tools. And pretty much any tool you see out there, you'll see that it falls into one of these three categories. Knowledge augmentation, capability augmentation, taking actions. Memory. So memory is basically what enables the LLM to remember stuff. And there's two versions of this. Short-term memory, which people often implement using context windows, especially plug for Gemini. Gemini models have the largest context windows out there. Two million on the pros. We're bringing that to 2.5 pros soon. And one million on flash and flashlight. And so a lot of your session memory can be stored in the context. You can also use RAG. And then there's the notion of long-term memory. So when you think about agents which are like productivity agents or personal assistants, they will have to remember stuff about you across sessions, potentially into some notion of perpetuity. And, you know, there's multiple, I mean, we can go into there's multiple implementations for long-term memory. But that's very much an area of active research right now. And then finally, perception. So perception is, as I said, the sensors that give the agent awareness of the environment. And the two things that are not on the slide is you'll typically have this whole thing wrapped in an orchestration layer, which will also be the place where you can specify what the goal of the agent is, what the personality of the agent is. So some agent instructions. And then there'll usually be a runtime in an agent implementation where you can basically execute all of this and the agent goes through the agent loop. Let's see, was there one more point? No, I think I made the points I wanted to. So we, I wanted to throw this up. Chip Huyn is doing some of the best writing around AI agents, so I encourage you to read her work or listen to her talks. And here's a slide she has on how to make your AI agents a better planner. Prompting is still very, very important. The interesting thing is you can now use some of these thinking and reasoning models to come up with better prompts. So you can give it a basic prompt and the model itself can build that out into like a, you know, a more fleshed out prompt for you. So that's one way in which I've been playing a lot with these. Giving good examples of the tools, defining the tools to make them simpler and cleaner and more composable, use a better model, that one is obvious, and then fine tuning. So if you feel that your prompt engineering has taken you after is up to a certain point, typically when people use fine tuning is, let's say you've pushed prompt engineering as far as you can, but you still have to meet some sweet spot of quality, latency, and cost. You know, and for all your applications, you'll have those thresholds, quality, latency, and cost. And so for those, typically you then need to go to fine tuning. And you may sometimes find that with a smaller model, but with good data, you may be able to get to those, you know, that frontier of quality, latency, and cost that you need. Let's see. Okay. So a little bit on the, what does the agent ecosystem look like today? Where are companies building? So the first set of companies that you see are the AI agent development platforms. I mean, these are just platforms that let people build all kinds of agents, and it's interesting, they all start from a specific place. So Adept, for example, started with computer use, computer control, but they have broadened. Of course, there's been a few business decisions that Adept has made, whereas E2B, started as a code execution tool, and then they broadened their platform from there. The second, the interesting companies in this space, which are basically the agent tooling space, are the companies that are thinking through how do you authenticate between different agents? How do agents make payments? Will they be using credit cards that humans use? Will there be specific, virtual cards for agents? Will there be crypto maybe, which is obviously, as always, a contested topic? I talked a little bit about tools. Again, you can see web search is one of the tools that really make agents very powerful because knowledge cutoff is no longer a limitation. There are data curation tools. memory is an area of a lot of exploration these days. It's really the next frontier in terms of solving for memory, and you have frameworks that are dedicated to memory like letter. If you check out that framework, they have a set of very specific tools in terms of how to make ads or edits or deletes from the memory. These are memory-specific tools. you have evaluation and observability. I think that should be obvious to folks here. It's the wild, wild west right now in terms of agents evaluating, figuring out what's happening in an agent's trace, evaluating agents, monitoring agents, people are just getting started. Then there's a specific category in the market that's blowing up, and that's voice AI agents. If you look at investments across any of the VCs, like voices amongst the top categories, the reason for that is there's just a lot of near-time monetizable use cases such as customer support with the voice AI paradigm. Finally, multi-agent and orchestration. There are a lot of crew is the most famous example of that, but that's the next frontier, agents that work with each other. And then in terms of the specific agents that are being built, if you look at horizontals, the most killer use case, and this is a slide from SWIX, are coding agents, but this is also replicated in when I go and talk to my customers who are trying to build agents using the Gemini APIs. Coding agents are perhaps the most common type of agents. Research agents like Google's deep research also are a very, very hot category, and as I said, customer support agents. Some of the up-and-coming categories are screen sharing, so this is browser control and operating system control, sales agents, recruiting agents, education, finance, and then personal productivity. I would love an agent that schedules my meetings and fills out my forms and orders my groceries. And SWIX especially has this request out to the world that there are certain types of agents which are just over exploited at this point, and travel booking agent is one of them. One thing I want to highlight, though, about where the market is, is the categories of agents that you're seeing being built more often are the categories which are the workflows are well-defined, and the environments are easy to test, relatively low cost of failure. So while you do want your research agent to be amazing, the risk there is different compared to the risk of a browser control agent going wrong, and easily verifiable which coding agents are. Okay, so now I'm going to stop talking for a bit and show you some examples that we have across Google. example. So let's start with a vibe coding example. So this is Gemini app. This is our coding agent, and what I used it for is to build this game that is basically, you know, capturing or befriending aliens game, and what you can do is you can switch between planets. So there's Mars, there's Jupiter, and then there's Saturn. The interesting thing here, let me see if I can forward this a bit. So what I did with this is I kept telling it, make my background better, make my background more scientifically accurate. And this is a bit sped up, but what you can see is how it thinks through, how to make, like, I give it a simple prompt, make my background better. It draws on its world knowledge to say I need to make Saturn's rings more prominent. I need to make, you know, give more of a gassy texture to Jupiter. So it was very fascinating as I kept building this agent, and just to see how it was thinking through, you know, just this iterative prompting in terms of I will add a volcano, on Mars. So then let's go to the next one. So this is deep research. We just made Gemini 2.5 Pro available on deep research, and as you can see, I give it a very complex prompt in terms of what are the competitive dynamics between the autonomous vehicles company and the traditional car manufacturers. And it just did a fantastic job of not only, you know, doing an in-depth analysis across close to 100 sources, it even created this Google Doc for me, and it, let's see if we have it here, it even generated these very helpful tables for me. things. So, and I want to call out a couple of challenges with building out deep research agents like this. I did say these are relatively low stakes, however, a few things that the team had to solve. The first thing that the team had to solve is this agent thinks for a really, really long time. So, you have to have some ability to maintain state in case there is a session disruption. Secondly, context management is a problem, because as you can see, in every step, the LLM generates a lot of thoughts and a lot of output, which it then uses to iteratively get to the next step. So, context management, they had to think through it creatively and say, you know, maybe we store some recency in the context, like maybe we give more weight to the recent steps that the LLM has gone through. The other challenge they faced was sometimes you don't have the same information available in one website. So, say, you ask, one example they gave is, what are some, a list of roller coasters that are suitable for 10-year-olds? And you might have one website that has a list of roller coasters, and you might have another website that has a list of 10-year-old suitable roller coasters. And so, the agent had to cross-reference the best roller coasters in the U.S. across and then with a 10-year-old. So, that's deep research. Let's see. I am going to... Here's an example of Google's browser control agent. I'm not going to play this whole video, but the prompt given to it was, here's a list of companies. Find their websites and find a contact. So, this goes back to using something like this as a sales agent. agent. And here, if you... This video is on YouTube. If you go through this, you'll see that it goes to every company. It scrolls through, goes to the team web page, tries to find a contact. So, that's an example of Mariner, which should be... You know, we should be making some announcements about that soon. And then, this is our data science agent. I'm just going to skip this one. Okay. This one's a fun one. This uses the... the Gemini real-time API. So, if... Sounds good. Let me know when you're ready to start the attack. Yeah, that's all trained. So, let's go on and set. Awesome. Let's get the attack started. I'm ready to analyze the base and provide a strategy. Which base are you targeting? Well, I quite like the look of this one, the first one we found. So, where do you recommend I attack from on this base? Given the base layout, I recommend attacking from the bottom or south side. This direction allows you to target the town hall directly with your giants, while the wizards can handle the surrounding defenses. All right. And what is the... All right. So, this is an example of a gaming agent. It's one of the top categories of agents that developers are building with our real-time API. And what's cool here is you can see that the agent is proactively engaging with the user to give the user advice. All right. Let's see if I can move forward. So, very quickly, what are the primitives that are available from Google that you can use to build your own agents? So, the first one is models. Like I said, 2.5 pro is a thinking model. It's out. We have high rate limits. 2.5 flash is coming out soon. And one key thing that you'll be able to do with 2.5 flash is you'll be able to set thinking budgets. You'll be able to tell the agent whether it should think for 1,000 tokens or whether it should think for 30,000 tokens. So, that's a level of control that we'll be giving you with 2.5 flash. In terms of tools, we have two first party tools. One is web search. Second one is code execution. On Vertex, you also have the ability to ground on your own data. That's a capability we want to bring to the Gemini developer APIs fairly soon. And then one of the most important things for agents is function calling. And we do support function calling and are always looking for feedback on how to make it better. Not only do we support calling single functions, we support calling parallel functions and compositions of functions. Basically, some kind of if then. If A happens, call function one. If B happens, call function two. Those are capabilities you have access to through the Google Gemini developer APIs as well as through Vertex. And then in terms of APIs, we have two kinds of APIs. We have the regular chat API, which is request response. And then we have the real-time API, which was used to power the gaming agent that you just saw. And the benefit there is low-latency multimodal interactions. Here's the Vertex agent development kit that was released a couple of days ago. And this basically helps you. This is a framework. It helps you set up multi-agent orchestrations. And, you know, everything that you need to do to basically design an agent with a human in the loop. Demis and Sundar, during Cloud Next, announced support for MCP, given that a lot of our developers have been asking us if Google is going to support MCP. And yes, we will. We'll start by supporting it in the SDK, the generative AI SDK. and MCP is good for, you know, enables developers to build this bidirectional connection between data sources and their agents. And as a complement to that, Google also released the agent-to-agent protocol. So this lets agents which are built with the same protocol, but not necessarily by the same company, interact with one another. And this agent-to-agent protocols like this are going to be very, very important for a multi-agent future. So to quickly wrap up, are agents really all you need? Well, sometimes, depending on your use case. If it's a use case that needs a lot of autonomy, complex problem solving, those are the use cases where you might want to consider building agents, but one benefit of the agent paradigm, as opposed to just using an LLM out of the box, is it kind of weaves together all of the components that we talked about, the planning models, the tools, the sensors, the memory. And so it kind of lets you leverage, the agent brain lets you leverage a rich set of primitives to get to this complex goal. goal. And this is borne out in the data. So you can see, this is data from ARC and Gartner. Google trends for agents are really spiking. Enterprise companies have started mentioning agents more and more in their calls. And here are some Gartner predictions. 15% of work decisions will be made in four years. I left out, but they said in four years from now. And by 2028, they think 33% of enterprise agent applications will include, enterprise AI applications will include agents. Let's quickly talk through some limitations, though. There are a lot of limitations that agents have. By the way, that image on the side is generated using Google Imagine 3. But right now, agents can get trapped in these self-reinforcing logic loops where it kind of starts to go circularly. It happened a little bit in the game I tried to code. Agents have limited long-term memory. This is why memory is a hot area of implementation and research right now. Because one part of the problem is giving agent access to the memory. The other part of the problem is the agent's ability to recall from the right parts of that memory. Planning abilities, we talked a lot about that. Still lots of improvements that can happen there. Getting costs and latency, this is where fine-tuning becomes important. But hallucinations, costs and latency, still problems. There is no protocol. All the companies are trying to standardize. So getting one agent to talk to another agent or getting an agent to talk to a tool, still problems that are being solved. And of course, lots of security concerns. People worry about data manipulation. They worry about an autonomous agent being able to manipulate the stock market, steal copyrights, violate privacy. privacy. So these are all problems that any company that's taking agents into products will need to solve. And then, of course, there is ethics, right? Bias, misalignment. That's a big one. The more autonomous these agents become, you want them still to be aligned to goals that are ultimately beneficial to humanity. and who has liability if an agent's actions go wrong. So finally, a couple of slides on the future. So this is a chart that Bessemer came out with yesterday. This is kind of a riff on the five levels of self-driving car. They've put seven levels of agents in there, ultimately ending with agents like some kind of guardian agent managing teams of agents. And here's the final slide, some of my predictions. Most of these are fairly obvious. Agents will get more sophisticated over the course of the next year. One thing that gets talked about a lot, a couple of labs have put out AI co-scientists. And one of the things people think about a lot is, are these scientists and engineers going to accelerate the advances in scientific fields and especially in AI. Multi-agent systems, robotics is a big one that we finally have a chance to get right. It's interesting to think about when a world where agents interact with agents, will the interfaces be the same? It's something I think a lot about in the context of browser control agents. Right now, we still think about them in a way that a human uses the web. Will that paradigm evolve when it's an agent using the web? And maybe, long term, you might have high EQE agents so that they could truly become your friends, companions, tutors, coaches. And that's it. That's my talk. Thank you for being here. Make sure you work for the event for the years. Thank you again, absolutely. Thank you. You really have three questions. Thank you.