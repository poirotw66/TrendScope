 SIMON BENNETT, My name is Simon Bennett. I'm a product manager on Google Cloud Compute Engine. I'm joined by my colleague, Elo. ELO BENNETT, Hey, everyone. I'm Elo. I am from the Cloud Security team here at Google. SIMON BENNETT, We're really, really grateful that you were able to join us today. I hope you've had a great show. I certainly enjoyed the event last night and learned a lot from my colleagues, from many customers and partners I've met this week. This session, we're going to focus on a few things. We're going to be focusing on Google Cloud's security building blocks. We're going to be showing you some of the innovation we've been doing to secure your workloads on our cloud. We're going to show you how to meet baseline security requirements. We're going to be using a real application to do that. We're going to be showing you how to protect PII and other sensitive data in your environment. And finally, we're going to be covering how to create guardrails, either to meet your security, your cost, or your best practices. Since you are here after a doubtless very exhausting week, we're going to do things a little bit differently. You've probably seen enough slides this week, right? Anyone want more slides? Yeah, didn't think so. So we're going to give you a little peek behind the curtain. What we actually wanted to do was take all of you, all of our friends, into a Google data center. Turns out there are some reasons why we couldn't do that. What we were able to do is bring parts of the data center to you. So we're going to be showing some of the fundamental hardware and building blocks that Google has been investing in over many years. And then we're going to do some demos. Demos are way more fun than slides. You've probably seen some slides and some videos this week. We're going to do things live. This comes with certain risks. One of them is conference Wi-Fi. One of them is sometimes things take a little bit of time to take effect. It doesn't matter much when you're sitting at your desk in your office and there's nobody looking over your shoulder. When we've got the video camera rolling, the lights are on, and there's a 40-foot screen there, unscheduled learning may happen. We ask for your patience. A round of applause would be appreciated. And if you're so inclined and offering to the demo gods would be appreciated. All right, quick walkthrough of what we're going to do. I'm going to start by introducing some of our baseline infrastructure security primitives. Many of these are founded in hardware innovation, which is what we brought today. These are primarily things that Google is responsible for. We are representing the work of hundreds of Googlers and partners who've invested over many years to build these security primitives. The other areas we're going to cover are more of a shared responsibility, where Google is providing capabilities on our cloud platform. And we work with you to help you satisfy your compliance objectives, your security objectives, your cost management objectives. We're hoping to put this all together. We're going to do it live using a real app. So here we go. Maybe we could start with the hardware. All right. Well, we're going to talk about some of the, thank you, Simon, some of the infrastructure security and talk about Titanium. For those of you who haven't heard about Titanium, it's our trusted hardware security infrastructure that has been purpose-built to be able to handle our tier offloading. With that, often we don't get a chance to be able to talk about how we think about that hardware. When we think about that hardware, you might call them design principles, but we do a lot of security innovation at the silicon level. And this goes beyond just any sort of single card or device, but rather we want to make sure that, like that last point there, move things in the perimeter, move that perimeter to the silicon package itself. By moving things into the silicon package, it allows us to be able to embed that technology and innovation for security pervasively through our data centers. So speaking about data centers, who's ready to see some hardware? Anyone? I don't know. Maybe we should try that again. Who's ready to see some hardware? Yeah? All righty. So let's just get started with some of these things here. So we'll talk a little bit about two devices that have the Titanium offload processor. The Titanium offload processor is responsible for the cryptographic operations to ensure the confidentiality and integrity of your data. This first device that we have here is the Titanium offload processor for networking. It allows us to be able to ensure that during the offload process, your data is encrypted in transit by default. You will see this is what it looks like in the wild, if you will. But in the image behind me over there, we decapped it, and underneath the thermal paste, you can see what that chip looks like. We wanted you to be able to see the chip because of all the pinouts. The data and the signals that leave those pinouts is encrypted. This is a Titanium SSD. It looks like a regular drive caddy. This is the drives. It's actually a completely self-contained independent system. Like the Titanium networking card you saw earlier, there's a Titanium storage card in here, too. This has a really amazing property. Just like any data that's passing through that Titanium networking card, any data which is stored on these SSDs will be encrypted by keys that are protected by hardware on that main board. Hang on, Simon. Did you just pocket that hard drive? What if it has customer data on it? Maybe. So even if I get through all of Google's many layers of security and somehow end up in physical possession of this drive, it doesn't help me. Fully encrypted at all times. We care a lot about customer data. I also care about getting this back to the lab in one piece. So I'll put that back there. Well, thank you for doing that, Simon. Now, some of you might be familiar with this piece of silicon here that I have. We introduced this back in 2017. This is called our Titan chip. It is responsible for the cryptographic root of trust and identity for our systems, the underlying hosts that run your infrastructure. Or that, sorry, that run your workloads. You can't see it here, but if you look at one of our newest hand models, I don't know if that was an inspiring job for you, Simon. But if you look there, you can see that it is physically tamper-proof here. This little piece of silicon is important for us because it allows us to be able to find and revoke any tampered machines, cards, or peripherals by keeping them from booting. We focused on individual components, but I think it's worth highlighting some of the other ways that this is manifesting. This is my colleague Mo. Mo is the product manager for the newest Axion line. These are available in GA as the C4 instance type. This is the Axion machine. Hopefully many of you got to see this on the show floor earlier in the week. We were going to bring one up here, but those things are real heavy. The key thing about the Axion platform here is a lot of these security technologies are being baked directly into the SOC, which is Google designed. And there are some fundamental investments like the Calyptra open standard for securing these systems, which we are co-developing with industry partners, including Microsoft, Nvidia, and many others. So we're hoping that by investing in these areas, we can all raise the bar of all cloud infrastructure. So we focused a lot. I should have mentioned, if you're interested in sort of the standards work there, there is a deep dive white paper there. Just scan the QR code. Well, we're excited and hopefully enjoyed seeing a little bit about behind the scenes, what some of that infrastructure looks like, or some of that hardware looks like in our infrastructure. But as you can see here, the intent of these components are to work together. So you have an interleaving layer of protection to protect your data against a wide range of threats. You might be asking, Elo, so what are some of those threats? Well, thank you for asking. We actually have a list here of a few of those threats that are important for us and how Titanium helps with those things. There's customers that have asked me about, you know, what happens if there's a vulnerability, some sort of escape that might allow lateral movement attacks. So how can Titanium help with that? You can see here about in the middle of the slides that we have three hardware components that we just looked at that's able to help mitigate some of these things. The Titan and the Calyptra are able to stop persistent access and threats and help us detect any sort of configurations or mismatches. And then we have confidential computing, where we partner with a lot of our, within the CPU, with some of our partners, the CPU and the GPUs, that build that technology into the hardware itself. So that way we can protect your application's memory and also eliminate the need to trust in the hypervisor itself. Now, we've gone through so many different things and briefly touched on them. And so we're excited to share with you today that our Titanium hardware security architecture paper is published. And so if you are interested in reading more and learning about that, feel free to use that QR code and download your own digital copy, where you can go more in depth about the different hardware that we showed as well as some of the hardware we weren't able to show as well. This will give you some insights, too, in terms of our threats. Now, we have a lot of different things to demo, but I don't know about you all. I don't typically deploy things into Google Cloud wearing a jacket. I don't think Simon does either. So we're going to get a little bit comfortable for you all. There's a lot of live demos that we're going to show today that will highlight the different security compliance controls that are baked and built into our platform for you. And so with that, I think we will move into some of our live demos. Oh. I think click on. Jump to the demo. Oh. We have one more. Go back to the slides really quick just to give you a little bit of overview really quick before the live demo. Yeah. Go to this one. There you go. So for our demo application, we're going to take a live transaction ledger app. We're using a number of different GCP services. There's GKE in here, some Dataproc. It's not a particularly complex app, but it covers a number of different services, and it's doing both data storage and transformation. What we're going to do through a series of steps is put that hardware that you just saw to work in protecting this application and this data. We're going to do that by starting with some baseline controls, adding in some confidential protections for the sensitive data that's handled by the app, and finally add those guardrails in. Maybe, Elo, we could briefly run through the architecture. Now, in this demo, what we're going to do is follow that data. You probably have, like Simon was saying, very sensitive information there that you want to protect. So with that, your data is probably going to be coming in from somewhere. Maybe it's needing to be cleansed or processed, and maybe you're using something like Dataflow. The data lands, it's up to you, but in this particular case, we're going to have the data land on Cloud SQL, and sometimes that can be used. Oh, sorry. Can we swap to the demo? There we go. This is Dataflow, and then right here we have Cloud SQL like we were just talking about. In addition to that, you're probably needing to deploy that application that's going to be using some of that highly sensitive data and information, yet it's probably your most business-critical application, so you want it to be highly scalable and highly available. And with that, you probably deployed it on something like GKE or Kubernetes, and you can see here that we've deployed this in a set of microservices that are running on top of Kubernetes, and this is a sense of what that application looks like, and of course we've been depositing some funds into that at the moment. So we'll go back to the slides quickly. And then this will summarize, essentially, just what we walked through by following the data. This is just a visual representation of what some of those things look like. And what we'll do is we'll continue this by adding three additional layers of controls in the demos that we have here, so that way you can kind of see what we're doing with that. So with the time left, we will continue adding new layers. Thank you. First layer we're going to start with is baseline controls. Baseline controls can be really important for making sure that your data is in the right places and you're conforming with common industry best practices. The tool that we're going to use to do this is Assured Workloads. Assured Workloads is a set of logical controls that you can configure within your projects and resources to reach a range of compliance outcomes. This is really a set of pre-configured best practices that can help you meet different compliance levels. At the most base level, a simple compliance goal might be make sure that my data only belongs in, is only stored or processed in the countries that I want it to be. Sometimes you want a global cloud, but sometimes you just want your data kept in the EU. Sometimes you might be serving a customer who's in a vertical industry that is more regulated. They may have questions and concerns about how are you going to conform to HIPAA best practices from healthcare or FedRAMP moderate, FedRAMP high in the public sector. Assured Workloads gives you a template that you can apply to your applications and to your data. And also provides the monitoring capabilities so that you can ensure you stay in compliance over time. We all know that applications change. You add new features. You make improvements. You do new integrations. By adopting Assured Workloads for your workloads on Google Cloud, we give you built-in compliance monitoring. We will highlight for you services that are enabled. You can go check, make sure that there's no sensitive data being passed to those other services. If you need to deviate from the standard policy, that happens sometimes. There's a way that you can make an exception for those. And it's based on the actual state of your workloads. So this will work with all of your existing tooling. If you're working with a system integrator, open source software, this is all based on the actual configuration of your cloud. One of the other things that's very common is moving an existing workload into a compliance regime. We don't always have the luxury of starting with a greenfield application and a new development. To ease this, we have the Assured Workloads Migration Assessment API. This is great if you have an existing application and existing data and you want to dry run moving that into your compliant area. We're going to do a quick live demo to show you how this works in practice. The Assured Workloads Migration Assessment Tool is provided as a bare API. In that API, I will give it a source project and a destination project. And it's going to run an assessment that's going to tell me three different things. It will tell me resources in that project that are maybe in the wrong place. It might tell me services that my workload is currently dependent on that I might need to tweak or change. And it will tell me about any policy incompatibilities. The best time to find all of those things and work through them is before you've started migrating that application. This is an opportunity to try run, see how it's going to go, so that you're not already in downtime. You don't have an aha moment at the weekend when everybody's out. The actual API provides a very rich JSON interface. The power of Gemini, which is apparently much better at jQuery than I am. I've given you a quick summary and you can see the sorts of detail and output that you would get from an assessment. Since this is an API, you can create a migration factory, build it into some of your other tooling. It's a great way to make sure that you are both moving quickly, but also with caution, care and adapting your workloads as you go. We can jump back to the slides, please. So with that, we've applied our baseline control. In this case, we're going to make sure that our data is stored in the EU, a greater EU region. In this case, we chose the EU. We could pick an individual country. But now we're going to apply some additional controls focused on data protection. Thanks, Simon. So now that we have some of those baseline controls in place, we wanted to, for the data protection that he was just talking about, find ways to protect that, especially if you might have additional data privacy requirements. What does that look like and how can we help you through some of those things? Now, you may have a need to have a harbor-trusted execution environment or maybe an enclave, per se. So that way your data is protected while it's being in use. Or maybe you want to further remove the cloud provider away from where your workloads are running. Or you might have internal stakeholders or external ones that are interested in understanding how and prove with evidence that how you're protecting that data. In whatever case, we do believe that confidential computing is able to help provide that isolated compute environment to be able to process and handle some of that sensitive data for any of those workloads, including the AI ones that you've been hearing about so much. So before we do the demo, I'm just giving you a little heads up about what we're going to do. We understand that you probably have that sensitive data that exists somewhere else in your environment. Maybe it's on disk as one particular use case, and we'll look at that. And another one might be where you need to cleanse and transform that data. For us to be able to do that, we want to make sure that you don't have to spend any development effort to change those pipelines or the application code. And just be able to ensure that additional layer of security. So we're going to move over into the demo. And we're going to start with the disk use case. So when you're creating a disk and want to enable confidential computing, because you may already have confident or highly sensitive data that you've already taken a snapshot on, we're going to walk through creating a disk live here. You also can see that beyond using the UI, you might want to use Terraform or G Cloud. I have the G Cloud one that's the command line that's running as well. And you can see with the shared workloads that's enabled that the correctional disk is going to be able to use the disk. Direct regions are only allowed for us, for myself as a developer, to be able to create the disk. We're going to pick the region. And like we said, that this is probably going to have sensitive data from a snapshot that we've already created. Now, if you remember that hard drive that Simon was able to sort of attempt to pocket over there, we want to make sure that we're using some of that latest titanium offloading processor capability, which is allowed with our hyper disk balanced offering. So once we select that, we can kind of go through and enable confidential computing to be able to not only protect your data at rest, but also in use. And with that, we want to make sure that we select a key that is backed by a Cloud HSM hardware. Now, we've gone through and we'll let that create. And while it's creating, you can see that the command line has already finished down there. There are different ways where you can just validate that confidential computing is enabled for this particular disk. You can run a G Cloud command to look at some of the attributes as well. Or this disk is not finished creating, so we'll go to the backup one. You can see also visually if you wanted to inspect that disk to see that confidential computing is in fact enabled. So that gives you a sense from the first demo of how to enable confidential computing for your block storage. Now, what if you need to, for that second use case, process some of that data and enable confidential computing for that? So let me move to that demo. Oh. Reconnect. Oh, that's interesting. All righty. So like I said, you might already have an existing data flow pipeline based off of a flex template. As you can see here, what we've done without reconfiguring anything, you don't have to recompile it, change anything there. You simply can add a single switch to enable confidential computing. So then that way, any of that data, while it's being processed in use, it will be protected by the infrastructure that hardware, some of the titanium hardware that we shared with you, will be protecting your data as being processed and transformed through those things. Now, there should be, yep, it's building. We're not going to have you sit and wait seven to eight minutes for this to build. So instead we have one that's already pre-built. And we'll just take a look at this as well. You'll know that confidential computing is enabled to help protect your data while it's being transformed in use by seeing that the confidential computing is enabled. In addition, not only is the launcher VM that's created going to have confidential computing, but also any of the worker nodes that are doing the ETL, in this case we did about 10,000 records, will also be protected by confidential computing. All right. So we'll switch back to the slides. Thanks. Now, in the interest of time, we're not going to show you how to take that workload or you saw that it was a Kubernetes environment. And you don't need to, similar with that, you don't need to rebuild anything. We won't have time to show you how to do this, but you can also take your existing way that you deploy your cluster and add two switches there, one to enable confidential GKE nodes and another to enable confidential hyperdisk. Now, for those of you that also need to have that particular use case where you want to prove to your internal stakeholders or auditors that your data is being protected by confidential computing, we've provided you a link here to one of the labs. So then that way we spared you from staring at, you know, binary bits from an attestation report. Instead, you can kind of go there, learn more about some of the attestation reports. There's a CoLab at the very bottom. And when you're running that CoLab, you can convert it into, you know, ASCII or APSIDIC or whatever your favorite, like, mainframe language is from that perspective. But at least you'll have that there for attestation purposes. And with that, we've quickly enabled a second layer of protection using confidential computing to add that data privacy for you. Thanks, Elo. The last layer we're going to be focusing on now is building guardrails. There are a couple of reasons why you might want to build guardrails in your environment. One of them is to enforce security standards. We've gone from the hardware to the logical controls that are built on that hardware. But controls are only as good as whether they're used or not. So enforcing the use of things like confidential compute for your most sensitive data, requiring the latest generation of hardware where some of these innovations are landing, is one of the concepts you can express as guardrails. Another common use case we see is cost management. Developers are the lifeblood of many modern companies. And they want to work as fast as possible. One of the things they're really bad at is picking the right size VMs. We've all seen that one. By building guardrails, you can create sandbox environments for your developers that give them flexibility. They can still self-serve. They can get the resources that they need. But they're not going to accidentally spin up a 4-terabyte machine and leave at 5 p.m. on a Friday. The other area where guardrails can be really important is making sure you're meeting your reliability goals. Some applications you want to be very tightly packed together in a single region or a single zone. In other cases, you want them spread out for some geographical redundancy. These are some of the concepts you can express in guardrails. Going a little bit deeper, network, security and configuration. Another really, really popular area. Open ports on the Internet. Huge problem for many applications. So, expressing which ports are allowed or not. All the way from dev test all the way through to production. Another very, very common use case. We focused a lot on block storage as a compute product manager. I think a lot about block storage. But especially for modern applications, object storage is where a lot of those things are living. So, enforcing things like object versioning on those buckets. Let's make sure we're keeping all versions. Or requiring a specific storage class. You know, to hit your performance or cost goals. The way that you write a custom org policy is there's a pretty simple four step workflow. First step. Identify the org policy that you want to implement. There's a simple expression language. We'll show you an example of that in a minute. That expresses the basic intent. This is how I want to manage cost. This is how I want to add to my security baseline. And the first thing you will do is simulate that policy. So, in a sandbox environment, try it out. Enforce that policy. You're probably going to tweak it a little bit. Even as somebody who spends a lot of time thinking about infrastructure. Getting that policy right first time usually does not happen. A little bit of tweaking. A little bit of refinement. The third step that folks do, and this is a new capability we introduced since next last year, is the ability to dry run these policies in your production environment. What that means is the policy is enabled, but it is not enforcing. So, anything that happens in your environment that would contravene this policy will give you an audit log entry. This is amazing for finding those stragglers, those corner cases you didn't think of, those things that happen maybe once a month or infrequently. Running your draft rules for 30 days in an audit policy makes sure that this is not going to accidentally disrupt your current users, your current business, and the services that you depend on. And the final step is to enable enforcement. Here's a really simple example of a custom org policy. This is a cost control example. I've decided that my developers should probably go for some of our medium tier VM instances. I want them to have the power to chew through the data that they need to process. I want them to be able to develop. But I want to put some controls around which instances are available to them. The key thing I want to point out here, and you'll see this in action in a minute, we provide in that description field a place where you can elaborate more on why this rule exists. Everybody hates an error message, right? Computer says no, does not help me do my job. But you can provide context to your users through that description why a rule is in place and what they can do. So that's really, really important, a very powerful capability of the platform because you can guide them towards better choices. If your first choice was not available, we recommend this other option. Let's jump to the demo and I'll show you what this looks like in practice. All right. I'm going to run through three of the four steps that I showed earlier. We're not going to dry run. We're in a test environment. So we don't need to dry run. The first thing I'm going to do is define my custom constraint. I do that using this familiar YAML format. So that's a simple example. And then I create the custom constraint. At that point, the constraint exists in a catalog of constraints specific for my environment. But it's not doing anything. It's just an entry in the catalog at that point. The next thing I'm going to do is provide a rule for when that policy should be enforced. In this example, we're simply going to say enforce is true. This is probably the simplest enforcement rule that you can think of. It simply blanket applies that rule to all of the projects in the organization. You can get fancy. You can say things like, if I have any instances that are tagged as containing PII, require confidential compute. You can say things like, allow my users to create disks, but don't let them create enormous ones, which I'll be paying for the storage for. And this is just a really simple example. What I'm going to do is apply that rule. And then we're going to see if it works. First thing I'm going to do is create an M3 Ultram M32. I don't remember how much RAM those things have. I think my colleague over there can tell us. It's a lot. And as you can see, I immediately got an error message. It both stopped me from creating an inappropriate instance, but most importantly, I know what to do about it. I can go create a large N2 or an E2, or I can drop an email to Susan and she can help me get an exception if I really need that for my job. Just to show that we didn't break your existing workflow, let's create an instance that is allowed via the policy. So this is a N2 standard 2. And we picked that because there should be some. And that is now running. Let me just hit refresh. Okay. This VM is allowed. Whew. Thank you. I really appreciate that. We showed you G Cloud. We showed you the web console. These rules and these policies that you define will be consistently enforced regardless of which tools you use, which interfaces you use. If your team loves Terraform, they still apply. If they love the cloud SDK, they still apply. If they love to script, if they love to click through the web console, this is a way that we can make Google Cloud programmable to meet your security practices, your best practices, and a very powerful concept that applies across many services in Google Cloud Platform. You can jump back to the slides, please. So we started with hardware. We showed you some foundational investments. We built on those investments to establish data residency for your data, both at rest and in use. We added on hardware protections for your most sensitive data using confidential compute. And then we showed you how to build on those with your own opinions, guardrails, and best practices specific to your organization. We think that Google Cloud is the best place to run your workloads, to store your data. We are obviously very biased. We hope that we've given you enough to take some of these concepts. And our ask is, please go and try them in your environment. The QR code at the bottom there includes a link to all of our demos, all of the command lines, all of the setups that we used. We would love for you to go try this. We don't have to start with a whole environment. One project. One set of data. We would love to hear how that goes. Elo and I are very, very keen to learn from folks. We would love to chat with you afterwards about your experiences using Google Cloud. It's a pleasure to come talk with you today, show you some of the things that the team has been building. Two quick things. I know it's been a busy week and you are probably all sessioned out. These are the sessions that really caught our eye. Some of them go deeper into topics that we touched on. Infrastructure is very close to my heart and there's a lot of great innovations on the cost side that we announced this week. One final ask. You probably, sorry, somebody wants to go back. All right. All these pictures. I know some. All right. One final ask. While you have your phone out taking pictures, please fill in the session feedback. I know it's a pain. It is really important. We only get to come here and show you this cool stuff because of the session feedback. Elo and I will read everything that you write in there. The scores really do matter. So, if you want to see sessions like this or you have ideas for other topics we could do in the future, we would love to hear from you. With that, thank you so much. I hope you had a wonderful Google Cloud Next. Thank you, Al. Thank you. Thank you.