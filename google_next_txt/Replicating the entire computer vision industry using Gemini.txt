 . Cool. Yeah. Thanks for coming. So this talk is going to be about Gemini's multimodal capabilities. Yes, I'm a product manager at GDM. And, yeah, this presentation is on behalf of the entire Gemini multimodal team. Cool. So, like, a quick, yeah, kind of overview of the talk. So first I'm going to go through, like, a brief background of Gemini, then walk through some of the multimodal use cases, and then kind of talk through getting started with Gemini 2.5. Cool. So, like, a quick overview. So, Gemini, so pre-Gemini, large language models were primarily text in, text out. So this is models like Palm, GPT-2, GPT-3, and so on. Gemini from day one, so from Gemini 1.0, was built to be natively multimodal. So Gemini can take in text, images, video, audio, and so on. So I can also generate images natively. So why do vision capabilities matter for Gemini? So the way that I like to think of this is giving large language models vision capabilities, let them see the world like humans do. And that's pretty powerful. Like, now these models can take in research papers, spreadsheets, charts, diagrams, natural images, pictures of your homework, YouTube videos, and so on, and then turn these into, like, any other data formats, like code, detection and bounding boxes, segmentation, and so on. So the kind of, like, the upside of these models is that they let, or, like, the upside of vision is that they let these models see the world like we do. So a quick background on Gemini 2.5. So we launched this model a couple of weeks ago now, and it's the best model in the world. And it's even better on vision and, like, multimodal capabilities. So this is, like, the public, yeah, like, user preference, yeah, language model arena. Cool. So this next section, I'm going to kind of go through the various things that Gemini can do with vision capabilities. So there are kind of three big components to this. The first is image understanding. The second is document processing. And the third is that these models can actually take in video now. So they can take in and process videos that are up to, like, one hour long with, yeah, two million token context window. So first, a natural image understanding. So these models can take in images, process them, and, like, let you, yeah, like, ask questions about the videos. These are kind of the capabilities that these models have with, yeah, vision and, like, image understanding. So, yeah, like, this next section, I'm just going to go through, like, a bunch of examples of the things that Gemini can do with image understanding. So the first is captioning. So here's an image of Vegas right there. And the question that asked the model is, like, here's an image of Vegas. Can you tell me about each of the monuments and, like, the history of, yeah, like, how these were built? And Gemini is able to both, like, understand the image but then also use its world knowledge to provide a detailed description of Vegas from this image. So it actually, like, goes over, like, the various entities in this image and, like, provides a pretty nice detailed description. So this example I'm, like, especially excited about. And this is, like, Gemini 2.5 was the first model that was able to do this. So a big topic currently with large language models is reasoning. And this example is, like, pretty great because it shows reasoning over pixels. So the model doesn't just reason over, like, text space. It actually needs to reason over pixels in the image. Oops. So if we kind of look at this here, the model provides, like, a prediction of each step of the trajectory of the ball and which bucket it will fall into. And Gemini is the only model that can do this, yeah, like, in the industry today. And this leads to a bunch of cool use cases for, like, education, robotics, physics, and so on. Spatial understanding. So Gemini is also the only model that can, like, accurately detect objects in an image with fine granularity. This is very powerful for use case and applications such as robotics. And the cool thing here, like, the upshot is previously spatial understanding models, you would have to specify the object in the image that the model would generate a bounding box around. But this is pretty cool because we can, like, ask the model to, like, reason over the image or the document and then generate a bounding box and, yeah, like, provide that detection. So this example here, we tried this out after Demis and John won the Nobel Prize. And the question we asked the model is, like, in this document, who are the winners of the prize? And this is pretty great. Like, the model generates bounding boxes around David Baker, Demis, and John. And this is pretty cool because it, like, unlocks a bunch of use cases for, like, large document processing and so on. So with Gemini 2.0 and 2.5, we took this capability one step further. So now Gemini doesn't just generate 2D bounding boxes. It also generates segmentation masks and 3D bounding boxes. This is pretty cool because it means that, for example, such as the eggs on the far left, previously we have a bunch of overlap between these bounding boxes. This is not quite nice because it can generate segmentation masks and provide you that, like, fine level of detail. Same thing with the baseball example in the middle and just an example here of 3D bounding boxes. Cool. So now these capabilities are getting more complex and cooler. This we tried a few weeks ago, and it's pretty incredible. I took a sketch of, like, the Google AI Studio UI and asked the model to write code to create this web page. And in one prompt, the model provided me with, like, a very accurate UI image of this sketch. If we go back, something that I thought was especially cool was with these boxes, I, like, made an error. So with my, like, very dirty handwriting, scratched this stuff out, added arrows in, and I was like, actually have this line here turn images to code inside the bounding box. Same thing with the second, same thing with the third. And the model actually reasons over the image and adds the text exactly where I wanted it. So the cool thing here is with, like, model scale, with models like Gemini 2.5, like, we can, like, these models are now super strong at, like, having a good understanding of user intent and then, like, answering these types of complex queries. Cool. So the second big use case is document processing. So this is especially useful and, like, important for our cloud customers, and we see a lot of demand for this. And there are a bunch of, yeah, like, use cases and capabilities here. So the first is, like, document question answering, transcription, OCR, financial analysis, document grounding, long context, diagrams, tables, and charts, and image to code, which I showed an example of as well. So let's go through some examples of these. So the first is financial analysis. So Gemini 2.5 is the only model that can take in up to 2 million token context. This is pretty powerful because it lets us feed in, like, a ton of information to the model. So the example here, what I did was I fed in, like, the last 15 Google earnings, like, 10Ks, and with, like, one prompt asked the model to generate a table which, like, extracted a bunch of, like, revenue numbers from the earnings reports, then used that table to write code to plot, like, quarter-on-quarter earnings growth and so on. And the model does, like, a perfect job of this. This is pretty cool because, like, with, like, a million token context window, you can feed in, like, over 3,000 pages of PDFs and, like, get Gemini to process this and, like, reason over that information and then turn that into a format such as code, tables, text, and so on. Transcription and OCR. So this is another, yeah, like, big use case for Gemini. And the cool thing here is previous OCR models were just able to transcribe text in an image. But the delta that we get with large language models and their reasoning capabilities, such as Gemini, is we can now transcribe these documents in a layout-preserving manner. So in this example here, the model, like, is, like, not just able to transcribe the handwriting with, like, 100% accuracy, but it's also able to look at the diagram and transcribe that into Markdown. This is pretty cool because it, like, enables us to transcribe complex research papers, financial reports, like, medical records, like, legal case work, and so on. And it's also useful, actually, if you want to transcribe documents to then use for, like, RAC workflows and things like that. Here's another example of this. So this is real-world transcription is, like, what we call it. And this is kind of taking OCR to the next level. In the example here, we fed in an image of three nutrition labels and asked Gemini to transcribe these, preserving not just, like, accuracy of the text, but also layout, font style, table structure, and so on. And the model does a perfect job of this. So, like, each of these nutrition facts and calories and, like, these types of things are in bold. The, like, transcription OCR is, like, perfectly accurate and so on. So the upshot of this is, like, any digital data form we can now transform into text, Markdown, and these types of data formats, and then ingest these into, like, large language models. And that, yeah, that, like, unlocks a bunch of use cases and is very powerful. Cool. So the third use case is video understanding. So this is kind of what I wanted to spend most of the time on because this is, like, the big delta that we got with Gemini 2.5. And there are a bunch of use cases that this model now unlocks. So the first is video question answering. So with Gemini 2.5, you can feed in, like, MP4 videos and, like, YouTube links and ask questions about the video and get, like, get answers and, like, get information from that. The second is, like, generating timestamps. So this is actually something that we, like, see a lot of demand for where folks have very long videos of things like full-length sports games. And you want to be able to extract highlights of when touchdowns happened, when wickets were taken in cricket, these types of things. And these models are now, like, very accurate at doing this. The third is AV dialogue. So to me, this is, like, really, so, like, the deep mind vision is, like, building AGI. That's what keeps us up at night. That's what, like, gets us really excited. And AV dialogue, to me, is, like, one step in, like, getting to that vision. And with this, you can have an actual conversation with Gemini. So you can talk to it. Gemini can see what you can see. It can see you. And then, like, answer your questions. And that's pretty cool because for the first time, you can have a conversation with a computer, with a machine, just like you would with a human. Screen understanding is another big use case. And this, I think, is, like, especially powerful because humans spend a lot of time on screens. Like, a lot of GDP value tasks are done on screens. So if we could hook a model up to your screen and, like, get it to help you with, like, day-to-day tasks, that's very useful and powerful. Video captioning, like, high motion recognition. Temporal reasoning is taking some of, like, the reasoning stuff that I showed earlier to the next level. So this is, like, giving the model the, like, ability to understand physics across time horizons and so on. Video to lecture notes and these types of formats is, like, also pretty cool. The big use case of this is, like, education. So, yeah, I mean, in, like, college, you have a bunch of lectures. Like, maybe you don't go to your lecture. You want to be able to study afterwards. You can turn that into lecture notes and then ask questions about it. And, yeah, like, it makes learning more efficient. And then video chaptering is the last one. So I'm going to go through some examples of these now. Cool. Video timestamps. So, yeah, like, I'm not sure if you guys saw this last year, but this turned into, like, a big internet meme where at Google I.O. last year, Sundar said AI a lot in the opening keynote. And this is something we've been trying with Gemini for a long time. So we fed the opening keynote from 2023 into Gemini. And we asked Gemini to find every instance where Sundar says the word AI and provide timestamps and broader context for that instance. And Gemini did a perfect job. Like, it found all 27 instances of when Sundar said AI. I'm not sure, yeah, like, I'm not sure how to play the video. Okay. AI. AI. AI. Generative AI. Generative AI. Generative AI. Anyway, yeah. So that's kind of the, yeah, so, like, yeah, so this is pretty cool. Gemini was able to do that. And, yeah, like, this is, this is, like, a fun example, but you can imagine, like, the various, yeah, use cases where being able to, like, extract accurate timestamps is helpful. Cool. So, at DeepMind, we're, like, very passionate about, like, education. And this is also part of Google's motto of, like, making the world's information universally accessible and useful. And we think video is a big part of this. So, in this example, we fed in a three blue, one brown video from a little while ago. So let me play that. In 2019, I put out a video about how two colliding blocks can compute pi. It's very fun, very surprising. And if you include the adaptations into shorts that I've posted since then, it is the most popular thing that I've put on the internet. And... Oops. Anyway, yeah. So, in this video, the person shows a simulation of these two blocks colliding. And it's, like, a pretty cool video if you guys haven't seen it. But what we got the model to do was, in one prompt, generate a web app, which actually runs a simulation of what happens in this video. And provides, like, an explanation that's, like, interactive for someone to be able to understand this content. This is pretty cool because now, like, instead of needing to watch, like, a 20-minute, 30-minute video, Gemini and, like, Gemini 2.5 and, like, similar models can look at the video, like, understand the information in the video, and then generate web apps, generate interactive lecture notes for people to learn the content. So, in this example on the right, in the simulation, we see that, yeah, Gemini is able to understand what happens in the video, reason over that, generate the same simulation, and then provide this reasoning. Video to code. So, this is, like, a similar example. So, what, so, like, this capability of Gemini is, like, the, I guess, like, the high-level picture is, like, given any data format, so image, video, and so on, we could turn that into code and use that as a medium to process information. So, in this example, we fed in a video of Gordon Ramsay, like, cooking a burger, and we got Gemini to generate a web page which has, like, an ingredient list of, like, the various ingredients that he uses in the video and, like, a step-by-step recipe. So, this is pretty cool, like, similar to the previous example, instead of needing to watch, like, a 10-minute video, Gemini is able to understand this, like, understand, like, when in the video, like, ingredients went in to the, like, recipe, generate a step-by-step guide based on that. Right, right, welcome home. Cool. So, another capability of Gemini is the ability to ask questions about videos. So, sports is a use case that we see a ton of and, like, a ton of, like, interest for. And the reason for that is, like, a lot of sports is, like, stored in, like, video format. So, you have full-length sports games, sports highlights, sports, like, media, and so on. So, in this example here, what we did was we fed in a cricket game. And asked Gemini to provide us with time stamps for when wickets were taken in this video. And Gemini is able to do that. So, you can imagine a world where you feed in a full-length sports game and then generate highlights clips and highlights videos from that. And then we ask a follow-up question which tests the model's ability to, like, understand what happens in the video, but then also connect this to broader world knowledge. So, in this case, the interesting thing about this is the answer to this question actually isn't contained in the video. So, it tests for Gemini's ability to factually ground its answers in its context. But the cool thing about this example is that the model kind of says that, hey, while this, like, stumping, which is like a cricket action, didn't happen in this video, this was like a very fast action, and actually, like, the answer to your question happened somewhere else. So, it has a broader world understanding, connects that to the context from the video, and then answers the question. So, real-time dialogue. So, like I mentioned earlier, this, to me, is like the next big frontier with these AI models. It's the ability for us to be able to talk to Gemini and talk to these large language models like we do to humans. So, this is a little video that our team put together, which I think showcases this capability. Move them to the top. Move them to the bottom. Move them back in the middle. Put them back in the middle, but swap red and blue. Move them to the bottom. Move them to the bottom, by the top, for something deep. Move them to the top. Move them to the bottom, my face, dingen to the bottom, by the top, Ditowma stan. Move them to the front, my face, части split. Move them back in the middle, but swap red and blue. Can you make it is an example machine Einsatz therapy? Can you form a snowman? Can you scooch the snowman parts like really close together so they're just barely touching? Can you make the red big and the green small? Can you actually make the red humongous and the green like really, really, really tiny? Cool. So basically what's happening here, let me pause this or go back. Cool. So what's happening here is the model takes in a video stream of the screen and then given each frame, it writes code to then move the objects in the screen based on the user instruction. So I think there were like various capabilities here that are pretty interesting and applicable. The first is it has like a very fine-grained knowledge of vision on the screen so it's able to understand like relative positioning and so on. The second big one is that it's able to reason over the information on the screen. So it's able to reason that, hey, when I combine like a circle of one color and another color, like this is the new color that I get and so on. It's able to reason over shapes and like layout and then like write code to then rearrange the objects in the screen to like provide that visual. Awesome. So I'm going to go through kind of steps to get started using Gemini 2.5. So AI studio.google.com is like our surface to try out the latest Gemini models. So yeah, this 2.5 model was released in preview like a couple of weeks ago and it's like super exciting, super strong at these, yeah, vision capabilities. One capability that we're really excited about or feature in AI studio is YouTube support. So for the first time, you can feed in links to YouTube videos and get Gemini to analyze, yeah, like any video on YouTube and provide you with answers. So this is down here. So yeah, if you hit run and then go to YouTube video, you can then paste in any, yeah, YouTube URL and get Gemini to answer. Yeah, questions about YouTube videos. And yeah, this is an example of YouTube support in AI studio. So yeah, this is like a fun example where I fed in the first video that was ever like uploaded to YouTube. So this is, yeah, like me at the zoo. What's cool about this is that like Gemini not just provided a description of the video, but it also realized that, hey, this is like the first video that was uploaded to YouTube and had that broader context. So yeah, this is pretty exciting because it now like unlocks most of, like most videos are on YouTube. And for the first time, you can use large language models and their reasoning capabilities, like understand content in videos and yeah, answer questions about them. The reason why it's so important to That's great.