 . Hi, everyone. Good afternoon. Thank you so much for being with me today, especially because I'm competing with the developer's keynote. So thanks a lot for being here. I'm Luciano Martins. I work for Google DeepMind. I'm based in Sao Paulo, Brazil. And I lead the DeepMind DevRel, especially for the Gemini API and all the developer services we create to let you folks to start using the models we research. So we are here today especially to talk about how we can experiment and start developing solutions using the Gemini models, as also Imagine, Veo, and all its siblings. But we have this amount of research and products influenced in the past years from both what we call now Google DeepMind as also from specific researches from DeepMind including AlphaFold, AlphaGo, quantum computing, etc. is also to the world we previously called as Google Brain. So has anyone heard about TensorFlow before? Right. And how many of you use Android phones? And for those using Android phones, how many of you use the Gboard, the Google keyboard? So, yeah, if you are using Gboard, for example, you have very specialized in small language models to help you typing better or fixing typos or recommending words. So the idea is how we can use our research to make the world better in any way from small interactions like using Google Translate, using Gmail, using your Android phones to big things like proteins folding, AI on quantum computers, and predicting weather, and et cetera. But we are here in particular to talk about Gemini. And how many of you have heard about Gemini? Yay! So, Gemini is our flagship LLM model. It's our most advanced model available to all developers in the world. And one of the key features of Gemini is what we call to be natively multi-modal. What does that mean? It means that before Gemini and before the advancements we did with the LLM research, when you wanted to have one LLM to be able to ingest and process other modalities or other data formats like videos, images, binary files like PDFs, spreadsheets, et cetera, you needed to first train one common language model dealing with text only, and then later extend this model capabilities to handle with more information. What we did with Gemini is since the day one of its development, we are always playing with different modalities of information. Why it is important? It is important because during the Gemini model training and while it's learning the information it's been exposed to, it is also able to understand how the data is represented on different formats and it is part of its learning process. So it helps with the modelability to be better on understanding semantic information from videos or for images or correlating information from one PDF file and one spreadsheet and giving information better for you. So basically when we say that the Gemini is natively multimodal, it means that virtually speaking you can use any format of data just throw at the Gemini API and the model and we on the back end and the model will take care of how it is handled to be used by model. And by that I mean you don't need to be concerned about, for example, who has heard about OCR? Nice. So basically OCR is the machine learning process or normally based on machine learning mechanisms where we get the documents and we try to extract all the text or all the characters from these documents. And with Gemini, if you have like a finance report or any kind of information inside the PDF file, you just throw the doc to the API and the model will digest that and give the information back to you. So at the end of the day, for you using the API, the key advantage of this multimodality is you have less friction or less extra efforts to bring your data on any format it is to be used within the model. How Gemini is currently available for you? Right now we have different model variants with different sizes for different goals. The largest and most capable Gemini variant available now for you is what we call the 2.5 Pro that is currently available in preview and it is the largest model with more capability for complex tasks and in particular it brings something that was a huge feedback from developers around the world which is a good reasoning for Gemini. How many of you know the concept of models reasoning? Right. So like as one explanation in a tweet, reasoning for the LL models is the ability of the model keep thinking or keep evaluating its answers and its own information before forging one answer for you. We're going to see in a bit how it works like in a live experiment but it helps to have better and more accurate answers because the model will try to find its own flaws and using any data it has as reference like external APIs, a PDF file or even a text context as part of the prompt, it will keep enhancing the answer or the direction the explanation is going until it gets to a point you have a better answer than the first one it should have shared with you. Then you go to the Flash family and the Flash family is less complex or smaller than the Pro but it is like the best for the most of cases with lower latency. So you have a better pricing, you have lower latency to answers so for the majority of cases you can start with Flash or Flashlight and you may have wonderful results. And last but not least, we also have what we call the Gemini Nano which was developed to focus on mobile development so it is available on what we call AI Core which is part of the Android Studio or when you are developing applications for Android and it works for a set of phone models like Google Pixels and Samsung Galaxy families and you have the same Gemini quality on a model that can be run locally on the phones. So it is like the best of both worlds. You have the same technology and the same research process we had for the main Gemini variants but for a model that can run on the phone. And what do we look for when we keep evolving the Gemini family in general? Basically we want to try giving you the ability to do tricky things that helps for many different scenarios. First, having a model that is able to take actions and those actions may be connecting to a service or running a code for you. Give the ability to use tools and by tools on the LLM word basically we are talking about things like how many of you know Google search? Okay, so one of the tools Gemini have and we're going to see in a bit is it can use a Google search result as context to give you an answer. So that's the kind of easiness we are looking for to give you to make the experience better. And also the real-time streaming. We launched yesterday as preview something that we call live API where you may have live interactions or live conversations with the model using voice or using your voice and camera or sharing your screen and then you can have like a live conversation with the model while you're asking the model to take actions for you. Like for example, if you have, if you are talking on your phone using the camera you can point to a city sky view and ask which city it is and what's the best restaurant close to you given the place you are. That kind of simpler interaction. And also we work with this idea of keep sharing with you what we call long context. And in short, the model context is basically the amount of information that the model is able to handle on a single request. Or in other words, if you talk for example about the Gemini 2.5 Pro it has one million tokens as context. And if we convert that to any data, data metrics it means that on a single request you can send one video that is one hour long or you can send 11 hours of audio captures like from a call center or something like that. You can have up to 700,000 words. So let's say on a single prompt you can send a whole book series like Harry Potter or Calculus or any topic you want. Send all PDFs and the model will be able to analyze the information on that data correlates the information and give you something back. So the key idea of the long context is again, looking for a better experience for you folks is reducing the efforts to, for example, if I have a too big PDF and my model does not have a huge context capacity it is on me as a developer to keep, for example, splitting the PDF file in multiple pages or multiple group of pages and it is on me as well, on my codes to keep controlling how I can process the small bits of that and get the results, append the results to a single context so that's not trivial. So with longer context, we can overcome this situation and have simpler experiences. Other example that I use to get to like workshops and to interactions with developers and customers and it is something closer to the developers world is with longer context, we can start wondering about combining full code bases with, for example, documentations. So let's say we have one internal code base, it is not public, it is not available on places like GitHub for everyone or something like that. I can still use this full code base, send as a prompt, then send my documentation about security practices or best practices for links in code or something like that and then I can have my own assistance to help fixing bugs. So, bless you. So, it is something really practical, really easy to be built and that every places I go when I deliver this kind of experience it is something like very nice to experiment because you can get like, how many of you like to keep volunteering on open source projects on GitHub? Right, so on those open exercises, normally we pick one project let's say Keras, TensorFlow, something like that. We download the code base, we get some open issues and we try to fix that together. So, that's how practical it is. But okay, if you go, if we go to the Gemini API land, basically what we have or what we propose giving you is be part of this ecosystem or this group of services where you can interact with Gemini. And right now we can interact with Gemini through different surfaces like if you are a Google Workspace user, you have Gemini there like for Gmail, for Slides, for Docs, etc. If you are a Google Cloud user, you have some console assistance, you have some coding assistance, and you have Gemini on the database services on BigQuery, on Vertex AI, etc. The focus here, and that's the place where my team is responsible to keep covering, is what we call the Gemini models but not Gemini only, including Imagine and View and etc. for developers, which is pretty much the combination of the Gemini API, how we can directly interact with the models, and the AI Studio, the tool or the UI, we build to make your experimentations faster. Why you could start using a Google AI Studio? How many of you have heard of, have experimented with Google AI Studio before? Nice. So we're going to see in a bit how that works. But basically, it is one chat-based experience, no coding at all, where you can experiment all the model functionalities, all the model features, like, for example, code execution, grounding with Google search, all of them, and, without writing a single line of code, and totally for free. You have a very generous free tier, where you can experiment with all the models, have the best of them, get some simple coding to move the experiments from the AI Studio to your own code, and it makes the experience really fast. That's the interface of the AI Studio, but we're going to do that live in a moment. And the next step is, once you are okay, you are comfortable, and you like the results you had on the AI Studio, you can do the next step as a developer and go to the Gemini API itself. It means that you're going to use your preferred language. How many of you are developers here? It can be even with JavaScript. No problem at all. Okay. Nice. So you have a lot of SDKs ready for you to use with the Gemini API, like, for example, for Python, for JavaScript, for Go, and also you can interact via REST calls. So, virtually speaking, any programming language, even without a dedicated SDK, if the language supports REST calls, you can be using the API. And I will share the docs with you later, but basically, you can find the docs at ai.dev slash docs, and there you have a rate limits page and one pricing page. And the key value of this is basically to show you what are your rate limits when you are using the free tier. That's the best way to experiment. You don't need to care about credit cards, billing accounts, anything like that, and you can go straight to experimenting. But besides Gemini, we have this effort inside DeepMind that we call the Gem Media Team, or the Media Generation Models. And as part of that, have you heard about Imagine? So, Imagine is our highest quality text to image model. So, basically, you can send a prompt describing one image, describing one situation, and Imagine will generate this image following your instructions and trying to bind in the best to the key elements or to the key descriptions you have on your prompts. And as you can see, you can experiment later if that's using the Gemini API, but the quality of the images are impressive. And just giving, like, a spoiler, that's also, that's not the only way you can generate images. You have the Imagine with the highest quality possible as a model provided by us. But also, now, Gemini has some experimental capabilities to generate images to you, and we'll get there. Also, you have Vio. How many of you have heard about Vio before? Nice. We launched Vio as publicly available yesterday. Basically, Vio has the same or a similar idea of media generation as we have for Imagine. But the key difference is it goes from text to video or a sequence of frames or a sequence of images. But also, you can do two more interactions. So, it has text to video. You can do also image to video. Or, in other words, you can send a reference image, and the model will try to understand the context of that image and create some animation out of that. And also, you can do video to video. So, you may have another video, maybe even generated by Vio itself. And you can send it as reference, and you have some continuation for this image or for this scene. Okay. So, having said all of that, let's now take a look for some experimentations. And could you please bring the screen to the demo laptop one? Thank you. So, here we have the AI Studio. And as I told you, the key experience here is to be as simple and as fast as possible. So, you can use your own personal Gmail accounts, go to AI studio.google.com, and simply start experimenting. It is as simple as that. And it can start with simple interactions like text interactions, asking simple questions. For example, what is artificial intelligence? So, starting really, really simple and really naive, and then you have the model interacting with you. It supports many, many different languages. So, for example, if you prefer to talk or to have conversations in Portuguese, in Spanish, in French, in Chinese, you can simply start talking on your own language, and that happens. But that's not even the best thing, or it is not close to the best thing for developers. The first thing I'd like to show is how many of you work with some sort of scenario where you have an ecosystem of APIs. You have services in different APIs, and things connect to each other, send payloads to one another. And a big pain for those environments is when you bring one LLM as part of it, you need to make sure you are parsing the LLM outputs in a way that the communication won't break. So, what we do with the structure outputs is using... How many of you have heard about the OpenAPI schema? It is a common schema to have you declaring API functionalities, and also all the fields and all the features it works with. So, what we do is you can create your own OpenAPI schema to describe how the model answer would be. And the answers you have, like, for example, generate for me one chocolate cookie recipe, including its ingredients and directions. So, what we have here is the model will follow the schema I requested. I passed a very simple one, but following the OpenAPI schema, you can have more field details, you can mark which fields are required, so you can push the model to answer exactly as you need. So, in a case you have a pipeline, or you have more steps to be taken after the model answer, it simplifies how you can connect or keep connecting to other APIs across the path. Another thing very useful, too, is how many of you have heard about the struggles that many LLMs have to count letters in a word? Like, I guess the most common question people do to LLMs is how many letters are in the word strawberry? So, what we can do? We have a feature called code execution. How does that work? The API and the model identifies if the request you are sending is better solved using a program or a code. So, without your manual intervention, the API will generate a Python code that solves that problem. Then it will run on a container you have on the back end, and also you don't need to care or to pay for that. So, the code is run, and the result from the code is used as context to the model answer your question. So, for example, if I ask the model to calculate for me how many letters are in the word... Oops, in the word... Oops. ...strawberry. ...strawberry. Okay, it didn't run as a code. Let me more explicit. Using a Python code, calculate for me this. And there you go. And basically what you have is you can see the code it used. You can see the code execution result. And then it uses this result to compose the answer. It's kind of neat because you can protect yourself from what we call hallucinations or from wrong answers because generative models are not that good at calculating or doing programmatic stuff. They are good at generating stuff. So, here, using that, you can even use... Like, for example, how many of you are familiar with Python? Okay, so in Python, we have this pretty popular library that we call matchplotlib. So, matchplotlib is the easiest way you have to generate charts using Python code. So, you have... You have... You have... ...metplotlib support for code execution as well. So, for example, if now I ask the model... ...generate for me... Oops. ...generate for me a metplotlib chart... ...including the top 10 U.S. states by population. I can get this code execution. And basically, the model will do something similar. It will get the data. Then, it will work with pandas to manipulate the data and matplotlib to generate the charts itself. And then, I have my chart generated. It is... It makes the interaction a little bit funnier because you can also keep incrementing this interaction. So, for example, if I tell the model, now make each bar in a different color. Like doing any kind of change to my chart. What we'll do is... ...it will modify the codes using the same data and have the modifications that are requested. It is like a very logic and a very simple example. But you can expand that to, virtually speaking, any kind of data manipulation scenario you have. And you have very fast results like that. And the last one I want to show you here on the AI studio is the Google ground search grounding. So, how many of you have heard about reg? Nice. So, basically, with reg, what we do is we develop some structures outside of the model, like a vectors database or any source of information. And for any question our users ask, we're going to grab some context information on this external source. And you're going to use this data as context for the model. It works. It helps us to have more accurate information. But it is a lot of work for developers. So, what we do with the Google search grounding is it is kind of similar to reg environments. But instead of checking external databases or external manually defined sources, basically what the API does is a Google search. And uses the Google search results as reference to answer your question. So, let me show you one practical example before using the Google search. So, I'm from Brazil. I love what we call football. We'll focus here in the U.S. called soccer. And a practical example is if I ask the model, what was the score of the latest Brazil versus Argentina soccer match? What we have here? We have this concept with LLMs called knowledge or information curve of date. That means given the period of time when the model was trained, it only has knowledge about that date or that month. So, specifically for this Gemini version, we are talking about a curve of date of January of this year. And the last match was last month. So, basically the model is not aware of this information. It is out of the knowledge of this model. If I ask the same question, enabling the grounding with Google search, what the model will do for me is checking on Google search first. And just to have some reference for me, it also gives me a link for the search. Just to help me, like, validating the source of this information, et cetera. And then, using this search as reference, it can tell me that the last match happened at last month. And it was a very sad game. Argentina defeated Brazil by 4-1. So, it helps because it's like a shortcut to have more accurate information with some references to validate the information accuracy, but in a very simple way. Okay, I'm talking a lot about simplicity and how we can experiment things. So, let me show you now, using the SDK, how really simple those things are. So, I will start showing, and by the end of this talk, I will share with you the GitHub repo with all those examples. The first one is using the view via the API. And the fourth I have, and it will be kind of similar to all of the other examples. I need to first install the Gemini API SDK. And here I'm using Python, so I'm doing a pip install. Then I need to have declared somewhere the Gemini API key that I'm going to use. And it is pretty simple to be created. If you go to the Google AI Studio, you have this dashboard link up here. And then you have the button create API key. That's as simple as that. Then you save this API key. You declare a variable with that. And then, to, for example, generate a video using a view, I just need to have my prompt. And the prompt here is a neon hologram of a cat driving at top speed. That's my prompt. And the result I get from the model is the video with my request. Let me reduce this one here. It's a video like this. So, you can see that, programmatically speaking, I needed to install the SDK, declare my API key, and then send my request. That's the programmatic or the developer force to have a video generated. On this example, on this notebook, I'm sorry, I included some examples using other methods. So, for example, I sent this image as reference. And I asked a Gemini to animate it or to generate a video out of that. And basically, what it did, and that's very important to see how the model tries to interpret the image. So, it understood we have fireworks on the sky. And also, it understood that you have a lake or a river on the bottom side of the image. So, it could see as the parts able to be animated being the fireworks, but also that animation must be reflected on the lake. So, that's pretty much the kind of animation we had. We have the fireworks happening here and their reflections on the lake. Right? Another perspective, or going a little bit further with the possibilities, even going through some of the experiments we did on the AI Studio itself, all the interactions we will have with the Gemini API will follow the same logic. So, you first need to make sure you have the SDK installed. Then, you declare your API key. And then, we start having your interactions. So, for example, the first and most basic one I have on this notebook is what's the largest planet in our solar system. So, a very trivial text input and text output. And you go through, and it is something very important, especially when we are thinking about creating products with this API, which is when you talk about calculating pricing for using LLMs, that's normally a struggle because it is hard to imagine how your prompts, especially if it involves images or docs or extra information, how they can be converted to, like, tokens or words, doesn't matter which metric the API you are using have. What we did with the Gemini API is we created one method that is free of charge called count tokens. So, you can send your prompts. It may include images, attachments, videos, etc. And the API will return to you the total tokens of this interaction. So, basically, you can work on estimates or cost estimates for, virtually speaking, any kind of interaction you have before spending the real money about that. So, just as a small recap, you have the free tier that's very generous to start experimenting. But once you want to do the next step and have it in production or inside your product, you have mechanisms to calculate your costs. And what about the multimodal prompts? What's going to change is, on your request, you need to save, in some form, your image or your data in base64 format. Then you embed on your request and send together a text or a reference of what you want to be done considering this image. So, here, we have a manual draw of a jetpack backpack with some written references and things like that. And I ask the model to write a blog post about that. So, what the model does is, it is not simply looking to the image, but also it is understanding all the text references or all the text on the image. And it will use all the information from those parts to generate my results. So, it is pretty much correlating the visual and the image information with the text directions I give to get to a final result. What else is nice to use our remaining time here? I will go a little bit to the end and, again, I will share with you folks those notebooks. You can run that by yourself. I show you another example of using the structured outputs to let you see how we can use that. So, here, I'm declaring one class. It is not just one open API schema. You can declare, in this case, a Python class. And use the response mime type as JSON. And point the schema you want to use, like this class. And my result, the same way we have seen on AI Studio, we will obey this output direction and will be formatted in this structure. Then, we have some examples of how to create images. And, again, our idea is to make it as simple as possible. So, we follow the same drill. You point which model we want to use. You send, as your contents, the directions you want for this image, like how the image must look like, characteristics, et cetera. And then, the model will give you back the generate image. And what's the key difference here? Here, in specific, we are not using Imagine. We are using Gemini itself to generate images. What's the key difference? When we use Imagine, you're going to have higher quality images, but you have single interactions. What does that mean? You cannot keep chatting with the model and asking for changes. Like, it generated this image of a pig with wings and a hat, and you can say, now paint in blue the wings, or make the image on the night, not on the day, that kind of semantic changes. But with the Gemini image out mechanism, you can keep interacting with the model, or you can even ask for directions with images. So, just to show you a nice example, let's go quickly back to the AI studio. And if you pick the model able to do that, that is the 2.0 flash with image generation, you can have interactions like generate for me, I'm sorry, I'm hungry, I'm only using food examples, chocolate, cake, recipe, including details for each step. What's the difference using the Gemini here? Instead of having just a single image as a result, let's see if it will include the images. Okay, let me ask the images in the directions. Including images and details for each step. Okay, so what we have here? We have the ability to, in real time, create answers like, for example, on a chat bot or an agent or something like that, without having a folder or something with pre-baked images, you can have the model improvising in the answer, but giving nice instructions with images to your users. It is a very logical example of, like, baking a cake, but it can be something more related to do a maintenance on equipment or to changing something on a car, something like that. So that's the key difference between using Imagine and Gemini image out. With Imagine, you're going to have only the image with the highest possible quality, and with Gemini, you may have interleaved text together with the images. Okay? So let me see here. The last thing I want to show you is how many of you have heard about Gradio? And how many of you have heard about Hugging Face? Nice. So basically, Gradio is one Python library that makes it easier to quickly prototype chat bots. Instead of having you having all the efforts to generate UI details, interactions, frame interactions, and that kind of thing, you can focus on your business logic, on your code logic, and the Gradio API, you will take care of all the UI for the chat bot. So this last example is more thinking beyond how to push things in production, because right now we are only talking about prototyping. On this example, what you're going to do is, using Gradio, you will declare all the information or all the directions for the Gemini, as we keep doing up to now. We use system instructions, defining, since we are planning to deploy a chat bot, how this chat bot could interact with your users or with your audience. And then, using the Gradio interaction, we make rates the application using the Gradio standards, and then we can run it as I already have it running, and pretty much I have my chat bot ready to be used. So it's really fast to have something running like this, so I can start interacting with the model, like, hi, how are you doing today? So I can have simple text interactions like this, and I can also interact with the model, like sending an image, and asking the model to describe some details about this image. So, I was playing in Italian with that before, that's why it is in Italian, because it kept my last session. But basically, the same interactions we were doing before, using the AI Studio as example, or using the Python codes, now you have the ability to have your own branded interaction formats on your own internal website or your public website, and very quickly bring those AI capabilities to your products. Okay? So, please, folks, could you please go back to the slides? Well, that's it I had to share with you. With these QR codes, you may access the GitHub repo with all those codes I showed you all today. I hope you enjoyed that. I don't know if you had a chance to go through, but there is one deep mind for developers, both on the expo area. Our team is there, like, the full day, really open to talk to you both, share ideas, share... hear feedbacks. We love your feedbacks. We love your feedbacks. And that's it. Thank you so much, folks, for being here with me today. That's it. Thank you so much. Thank you. Thank you. Thank you. Thank you.