 Hello, everyone. Thanks for joining us on an early Friday morning after a party. It means you are really interested in moving your SQL server into Postgres. So it's great to have you. We're going to talk about how you move your SQL server or other legacy databases into GCP and specifically into Postgres. I'm Erez. I'm leading the product suite for database migration and replication. I'll be joined later by Kerry from our BlackBell team and Shawshank from Wafer for a bit of a real-life view and customer view on exactly these topics. And we'll start. You've been hearing, you know, the last couple of days, hopefully, all about our modern data cloud, including our database stack, our analytics stack, and all the innovation we're making with AI. So from the ground up, you know, Google has GPUs and CPUs supporting, you know, accelerated by AI, moving on to storage layer, networking, and, of course, our data platform, all infused with AI, all integrated to facilitate, you know, AI-driven applications. And if you drill down into our database stack, which I assume most of you are more interested in, we have a very wide range of databases ranging from third-party engines like, you know, MySQL, Postgres, SQL Server, all the way to our own proprietary databases and spanning through in-memory relational databases, which we have a big, you know, wide offering, key value, document databases, and, of course, our BigQuery analytics stack. All of those are integrated with Vertex AI for real-time inference from your own, like, SQL interface and SQL queries. Some of them, like AlloyDB, are infused with AI indexes and AI-facilitated natural language queries. I'm sure you've seen all of that in the last couple of days in all the presentations and announcements. And I think what you should be thinking, what I would have been thinking, is how do I get there if I'm now looking at this, right? So you have – I'm sure you're all using still, you know, some of your on-prem legacy databases and some of, you know, cloud-managed databases. And, of course, we have all of these, you know, the cloud-managed database have, like, infinite, you know, advantages, right? It's fully managed, both from a perspective of backup, high availability, disaster recovery, performance, and all the AI enhancements. But still, you know, we have a lot of these relational databases, which are just there, working for, you know, 20 years, right? And sometimes it's challenging, you know, to migrate and modernize those. But I assume you are facing the same challenges that most of our customers are facing. And those are, you know, things like, I would say, non-simple contracts and financial models, vendor lock-in, the lack of, you know, cloud connectivity or AI readiness. All of these – I mean, legacy databases have been around for, like, you know, 20, 30 years, even more. So naturally, they're not, you know, all cloud-native or AI-native. And I'm sure you're thinking, you know, why wouldn't we modernize all of those in 2025? And the reality is it's challenging, right? You cannot just – you know, you have sometimes dozens, hundreds, or thousands of applications running on, you know, the same number of databases, and it's challenging to move all of those. And the challenges span – it's not just technology, right? You have, like, personnel trained on different technologies. Your organization is built in a certain way for, you know, a long time. And all of this may change a bit, right, when moving to the cloud. Jobs change a bit. Expertise change, right? Sometimes the structure of the teams change. We used to have, like, you know, very clear, like, storage, database, you know, development. Now we have things like cloud engineers, and we have, you know, cloud security teams. We have different types of teams in our organization. So it starts from the challenges with the organization's knowledge, and, of course, spans to the technical challenges. So migration projects can be very long, can be very, you know, tedious. A lot of them involve manual work. So it's challenging. We acknowledge that. And the good news about all of that is that it's becoming easier, right? So technology doesn't solve everything, but hopefully by the end of the session, you'll see how technology can expedite and accelerate your journey for database modernization. And with that, to kind of give you a bit of, again, more real-world experience and feedback, I want to welcome Kerry on stage. Kerry is, I would say, our blackest belt on our Black Belt team, and he has seen everything around migrations. So... Yep. Hi, everybody. Yeah, long, long... Don't worry about that. Hey, we're a small group, too, by the way, so all the back row Baptists can move up a little bit if they want, and feel free to ask questions in the middle of it that actually, you know, is such a small group. That'll be fine. It won't be a problem. But, yeah, I'm a long-time database person, very long time, my whole career in that space, and have done lots of performance work, but also lots of migration kind of work. So I'm probably a pretty good guy to talk about how these projects go and how it's really changed over the last few years. One of the things, you know, when we were doing projects, let's say we had a big medical company that was migrating a, you know, a database that's like 100 terabytes or something like that 10 years ago with a few million lines of code to be migrated. It's always the code that is the long pole and the tent, right? It's the data, regardless if you're going from one platform to another, that can be handled fairly straightforward, right? We map the data types. We move the data. We validate that the data is, you know, correct. But the code is the hard part. So, you know, those kinds of projects 10 years ago would literally take a few years to get done, right? We'd spend two years, three years going through the process. It was all manual. People were, you know, coders were sitting looking at the code, rewriting it into the new dialect. And despite the idea that we have ANSI standard SQL, you know, every vendor extended that, particularly adding procedural capabilities like Oracle has PL SQL and T SQL and SQL Server, right? So those things are distinct. They do similar kinds of things, but the syntax is distinct. So fast forward to where we're at now, and we have this new 30-year-old AI thing, you know, that really is starting to make those kind of projects much, much faster. And so we also have deterministic ways of doing that. So we have a way of doing an AST-based, you know, a syntax tree to represent a language and be able to convert it. And so we do an awful lot of that. But then on top of it, we have the Gemini or AI, you know, based translation that helps us make it through that last mile. And I'll add one other piece to that, which is a big part of what holds us up on these migration projects is not just the translation of the code, but also the people, right? So the people, you know, I've been doing Oracle for 30 years, right? I'm really good at Oracle. I've only been doing Postgres for, you know, less than 10 years. I'm not nearly as good at Postgres as I am at Oracle. And when I first started on Postgres, I knew nothing, right? Like I'm basically going, how do I even do a select statement? I don't even know how to log on to Postgres, much less, you know, convert complex kind of code. So one of the great things about AI engines is their language models. So they're really good at looking at something and explaining it to you. So in my journey, I have found that it is an accelerator. I'm going to guess it's something like 10x because when I say I've got this thing that I did in another platform and I want to move it to Postgres, the AI engine will do something. A lot of times it won't work. There'll be some, you know, something that doesn't work correctly. But what I will get out of it is I'll get a complete description of this is what I did. You know, this $x view that you use doesn't exist. But there is an extension in Postgres that has similar data. And this is the column that matches to that column. So I'm going to try to rewrite it with that. And you'll have this whole verbose thing. So, you know, you can move forward way, way faster. So that is a big piece of what makes these projects go a lot faster now than they used to do. I'll mention two other things real quick and then I'll get errors back up. But we do have a comprehensive set of tools at Google. DMS is our database migration service. And it's got a lot of it in. It's got the code conversion, both the deterministic base with the AI sitting on top of it. That's built into that DMS tool. So the data movement with CDC for near zero downtime is, you know, also part of that tool set. We have some pieces around it that aren't completely integrated with DMS yet, including testing and performance tuning phase at the end and assessment at the front, right, which is also really, really important because we want to know when we get into one of these projects, we don't want to just go, well, I want to convert my database, but I have no idea how long it's going to take and I'm just going to swallow hard and, you know, start working on it. So with a very robust assessment, we actually know about the tooling that we've got. And so we can predict not only how complex it is, but also how long in man hours we think it's going to take to do that. So that gives us, you know, not only a way to see what we're going to get into, but also to sort of stack rank. You know, we often have hundreds or thousands of databases in our fleet that, you know, we want to go through this process with. So it's good to be able to say, well, I don't want to pick the hardest one, you know, to start with. And I also don't want to pick the easiest one. I want to pick one that's got sufficient complexity that I'm going to learn enough to apply to the later ones. So, you know, it gives you ability to stack rank that and do that. And then the last thing I'll mention is Google also has a number of incentive programs. So our team, this Black Belt team, which, by the way, is all a bunch of longtime database folks, manages some of these funding programs. So as you're, you know, thinking about how can I get from one platform to another, if you have questions about that, you know, you can definitely reach out to us and we can walk you through what plans are available and how much funding could be available. But Google definitely wants to participate with you in that journey and are willing to invest in that, right? Obviously, it's good for us if you guys do decide you want to, you know, move to the cloud. That's a good thing for us. So we're willing to invest in that. So with that, I'll pull errors back up, I think. I don't know what's on the next slide. But, oh, I'm supposed to talk on the next one, too, a little bit more. Yeah. So here's the tooling or the phases we would normally do in a project. And I kind of talked about it already, but we start with an assessment. This planning phase is where the Black Belt team would often be heavily involved. That's where the expertise, you know, comes in. We can help not just establish which ones we're going to go after first, but also, you know, do wave planning. So if we had 1,000 databases, we may decide we're going to do 100 at a time in phases, right, and be able to stack those up. We also get involved in tuning. And if there are, you know, really difficult things in the code base that need to be addressed, we work on that as well. And we do work directly with the customers. So we're, you know, half and half. We do a lot of work on the engineering side, but we also work directly with you guys. Then this is where DMS comes into play, right? So DMS does the schema conversion for us. DMS does the, gives you the ability to map the data type. So if there are things that you have that are unique that you want to do, you don't have to go with the defaults. You can map it the way you want. Then the code refactoring. Again, we'll do that deterministic, you know, first pass, and then we'll apply the AI stuff on the top of it and the explainability. And then the data migration itself. So we have, you know, the initial load process and then the CDC process so that we can have a very low downtime cut over. And then the next step, we often, in the app tier itself, we also have the ability to automate the translation of the application tier. So if we've got code embedded in Java or .NET or whatever, we can scan that code. We can access the same translation, SQL dialect translation coding that we've got in DMS. And then we can take that SQL and put it back into the original language that it came out of. With Jim and I, we can even go a step further. And we've done some projects where we've taken T-SQL, converted it to Java, embedded Postgres SQL dialect in it, and turned that around. So we've got now a job application with Postgres SQL in it when we started with a SQL server application with T-SQL as the driver for it. So fairly complex kind of a project. And we wouldn't expect, by the way, ever to get 100% automated conversion. We would always expect that there'll be, you know, a handful of things that fall out. And that's where the expertise, you know, the Black Belt team and others, partners, comes in, right? Any questions so far? I tend to have a low, monotonous voice, so people tend to fall asleep. So don't do that, please. What's the last thing? Last thing is test and optimization. Most of the companies we work with, testing is a difficult thing to do, right? We have a lot of big, complex applications, and having a really robust test suite is very difficult to do. So a lot of times we have teams of testers that, you know, know the basic patterns to go through, you know, the code. But, you know, as human beings, we tend to get sort of focused on a certain path, and there are lots of things that don't really get, you know, tested adequately. So one of the things that we're building right now is the ability to automate that testing. So as we're doing the AI-based last mile and the code conversion, we also want to, you know, have a set of tests that comes out of that automatically. And the reason, I think, should be obvious, you know, for converting a few million lines of code, it's very difficult to be able to test that methodically. And so we want that to be built automatically, and we have some ability to do that already. But, you know, eventually this will all be built into DMS, and we'll have a completely automated way of doing that. So that's the last piece of it. There's always an optimization phase as well, as you can imagine, when you're moving SQL dialects. There's always some things that, you know, behave a little bit differently in different platforms that you have to take a look at and make sure the performance is where you want it to be as well. And that's also part of the testing and validation phase, right? We want to make sure that, you know, the code executes. It executes in a reasonable amount of time. The results are correct, right? All three of those things have to happen. Okay. That's it for that. And with that, we'll give it back to Ares. I have a question. Yeah, yeah. Go ahead. So what's the typical timeline bias, and what would be the typical timeline? Hey, I'm an old rock and roll guy, so you'll have to use the mic. All right, mic now. So if we start the process for a mid-level application that has about teradata, one terabit of data, what would be the start to finish timeline would look like? Man, you know, the fact that it's a terabyte doesn't really define how long it's going to take, right? The data movement is a relatively easy thing to do compared to the code conversion. It's really more important what the code looks like. So if I have, you know, a couple of procedures and functions and, you know, there's a thousand lines of code, that doesn't take much time at all. If there's, you know, several million lines of code, that's where most of the time is spent. So it really is, and that's the reason for that assessment, right, and the reason for that complexity analysis that we do in the assessment. It's mainly looking at the code. Does that make sense? So, but to give you some ideas, we did a project for a big satellite company. There was a few million lines of code, two or three million lines of code. The whole project took a year, but really it took about three months. There was about seven months of messing around and not really getting a good start and backing up and starting over. In three months, that code base actually got converted and they went live and have now done a second one because it becomes a repeatable process. The second one was weeks. Do you have time for another question? I think we're going to do questions at the end. Okay. Did I ask one more question? We'll have a lot more time for questions, so let's first take a look at the process and how DMS can help, and then we can open up for questions for all three of us at the end. Just to answer the question that was already asked, for the speed of data movement, you can expect a few terabytes a day of data movement for the bulk loaded database, so roughly a few hours for a one terabyte database. But we'll get to all of that in a second. So now that we understand the full challenges of database modernization, which are there, we acknowledge that. Now let's start to see how technology can help you accelerate that process. And again, as Kerry said, the key word is acceleration, right? It's not a double-click, like a one-click thing. It's not like a couple of days project typically, but technology can significantly accelerate this journey that years ago was a multi-year journey. So what is DMS? DMS is our one-stop shop for database migrations into GCP, so it supports a lot of different migration paths into Cloud SQL and AlloyDB. The first type of migrations, which we'll focus less on today but are still very relevant, are what we call homogenous or lift-and-shift migrations. If you have your own SQL server, MySQL or Cloud SQL or Postgres database running either on-prem or other cloud environments, you can much, in a very easy fashion, much easier than the process we just described, you can migrate them into GCP. That's kind of an apples-to-apples migration. Typically, it's like, I like to call it green apples to red apples, but it's still apples, and it's very easy to move them across. And, you know, that's typically a very self-service process, which involves mostly data movement, right? You move the data. There's the same container on the other side, more or less, and it's a very swift process. DMS gets those done very quickly. You can start, you know, doing that within typically minutes, just setting up connectivity and, you know, a bunch of questions, you're good to go. DMS is always serverless, secured, scalable, fully managed, nothing to install, nothing to – you need to configure your source and destination database and achieve connectivity, and that's pretty much it. So you don't need to, you know, worry about installing servers and high availability and all that. It's all taken care of. It's secure with Google security standards and very easy to use. That's a common experience, hopefully, once you use DMS. The main thing that, you know, stands out is simplicity. Even though it's a very complex process, DMS is a one-stop, you know, tool to do to achieve the entire process. DMS is also always low downtime, meaning we always leverage native technologies for either database CDC or other change-dite capture or other means to achieve low downtime and high throughput migrations. So you can expect your applications to stay up and running until the very last moment of cutover, and that applies across the lift and shift migrations and the ones we're going to focus more today, which are the database modernization journeys or lift and modernize, as we call them. So for those, the process is a bit more involved, right, because it's not just moving data. Now it's removing apples to carrots, you know, chairs and mics, because it's a very different, like, SQL Server and Postgres are very different, right? So it's not just about moving data. Even when moving data, you need to translate that data, translate it from SQL Server data types and standards to Postgres ones. So that's one challenge, but that's, you know, an easier challenge relatively and definitely doesn't involve a lot of effort from your end. That's on us. We're going to do the initial load of the data, whatever tables you choose. We'll convert the data types correctly, followed by changing a capture. All of that is taken care of. We'll talk a bit more about it. But then there's a challenge that Kerry mentioned, which is typically the longest part of the project. That's the schema and code conversion. And within that, schema conversion, again, a challenge because the schema is very different. Data types, different types of indexes, you know, index views in SQL Server versus a materialized view in Postgres and so on. But that's the easier part because it's, you know, at the end of the day, you have these many data types and features. The code part is super complex, right? Because you have many code patterns, many code features in T-SQL that you need to convert to Postgres. You can do it manually, right? But you need a lot of expertise, a lot of, you know, knowledge and a lot of time to do it manually. Luckily, DMS automates the entire thing. So we do the schema conversion, the code conversion, and the data movement only in one process, same console, different parts of the process on the same system. And we'll see a demo and everything in a second. We'll get much more into it. The newest announcement, which was just announced this week at Next, is that we've added support for SQL Server to our lift and shift, sorry, lift and modernize migration offerings. And you can now use everything we just mentioned, schema conversion, code conversion, and data movement facilitated by CDC and low data migrations for SQL Server, whether it's on-prem or on any other cloud vendor. We support pretty much every deployment type into Cloud SQL and AlloyDB. So that was just announced. And let's kind of double-click and see that in action. So the first challenge, as we mentioned, databases can take a long time. And during that time, we want to not disrupt our production systems, right? We have a production system, mission critical. We want to migrate it, but it's still our production system, right? We want it to keep running until the very, very last point where we're ready to migrate. For that, we facilitate two different frameworks. One of them loads your data, your current, you know, historical data in high throughput in a paralyzed way. You can control the level of parallelism. But at the end of the day, the purpose is to get all of the data as quickly as possible to the other side. So while converting the data types, doing all the magic underneath, but getting all the initial load of the data through. At the same time, we start collecting changes, right? Because the second we start moving the data, the data changes on your primary production application. So we keep collecting changes through change.capture, which is based on SQL Server's native CDC framework. So it's very easy to set up, starts collecting all the changes that occur on all of the tables and stream them to our infrastructure. Once we finish the initial load, we start applying those changes. And once we're up to date, basically, you're good to switch to the production system. So that's for the data migration. For the other challenge, which is the biggest challenge, the schema and concord version, that's where we really have a very innovative and unique solution. So as Kerry mentioned, we have a first engine, which is a deterministic or algorithmic engine that scans through all of your schema and code and says, okay, I understand what a pivot query is or whatever integer data type is, and I know exactly how to convert that to Postgres, right? So whatever data engine can convert, that is super accurate, super fast, also customizable. Because you can tell, as Kerry mentioned, you can say, I prefer Postgres var cars to Postgres text columns, right? So you can tell us, I prefer that generally, or I prefer that for this table or that column. And you can plug in your insights or preferences into the tool. And all of that is done as a first pass. Whatever the algorithmic engine knows how to convert, it will convert usually perfectly. It doesn't mean you don't need to test it. You need to test everything, but you don't need to verify it. It's functionally equivalent, and that's the general assumption. So that's the first pass. These kind of tools, like DMS, have been there for a few years, accelerating the process of migrating code, but still leaving a lot of manual work, right? Because these algorithmic engines cannot cover everything. There's always patterns that you developed in your company. You may use them sparingly, or, you know, like all of your stored procedures may have a specific dialect that these tools do not support. For these situations, it's exactly where we leverage Gemini. So Gemini is kind of the opposite, right? It's not limited, right? It's super kind of innovative. It's boundless, right? It will give you suggestions for any question you ask. So you need to ask the question very, very specifically, very clearly. So what we do is we don't just tell Gemini, convert the code. We tell Gemini, okay, there was a deterministic engine that already tried converting the code. It succeeded converting 95% of the code in this stored procedure. These are the problems that that engine encountered, right? In these specific areas. Please fix those for us. So you don't need to, while it's Gemini, you do not need to interact with Gemini in terms of prompt engineering. We do all that. We do very, very sophisticated prompt engineering. Feed Gemini with all the data I just mentioned and tell Gemini, just complete the conversion, right? So it's already high quality for the parts that we already did. Please finalize the conversion. And Gemini will most likely, in most cases, will finalize the conversion and you end up with 100% converted code. So now instead of getting some converted code and what we call conversion issues, you now get fully converted code. The problem with Gemini or any LLM is as good as it is, you cannot 100% rely on the outcome. You need to verify it, right? Because LLMs can hallucinate. LLMs can get things almost right. It's still a fact. Maybe it will change a few years. Maybe they'll say humans get things wrong. But at this point, LLMs may get things almost right. So you better verify that. As you'll see in a second, acknowledging that, we not only asked Gemini to verify, but we also asked Gemini to give you a very full and detailed log of what it changed, where it changed, and why it did so. So it will give you all the information to verify the output of the conversion. It looks something like this. We'll see it a bit more in detail in a demo. As you can see, this is not just the converted code, but a full report of what Gemini did, what issues it fixed, why it did that, right? And a code side-by-side comparison so you can actually verify the code. And then you can either keep the Gemini code or revert back to the algorithmic or deterministic route if you think that's a more safe option to start with and manually fix it. So these two approaches together, what we've found is significantly accelerate your journey. Think about the starting point. Like years ago, you need to convert everything manually. Then with algorithmic tools, you got like 80%, 90% done. It's still a lot of manual work. And now you get everything converted. You just need to verify the part that Gemini augmented. And as you'll see as you start testing this, this is amazing. This really accelerates the journey in a way that, you know, five years ago we couldn't even imagine. And then another question is, okay, so, yeah, so I see the converted code. I have like a 500-row procedure, and we converted to Postgres. But even given the explanation of how it was converted, right, I am a SQL Server DBA. I'm looking at this. I don't know what is like this Postgres feature or that Postgres, you know, structure, right? So I am on my journey, as Kerry said, right, he's like a, I don't know, 30-year DBA. He knows everything about, you know, the traditional databases. But Postgres is new, right? There's a lot of different features, different structures, extensions. And how do I learn that? So, of course, you can go to our course. You can Google online. We want to make it easier for you. So what we do is we leverage Gemini for a side, you know, task. The code is already converted. We want to help DBAs learn Postgres and learn how to migrate and then maintain the Postgres code. So we have a Gemini assist, a conversion assist, that even though the code is already converted, you can ask Gemini, please explain why this was converted this way. And that doesn't apply only for Gemini augmented code, even for the deterministically converted code. And even if you, even like one of your team converted something manually, you can still ask Gemini to explain why they did that, right? So Gemini will go through the source, go through the output, the destination kind of dialect, and will say, okay, this is what's going on. This is how you write a procedure in Postgres, in a SQL Server, like create procedure. This is how you pass parameters, right? Like this is how you do it in Postgres. This is how you define variables in SQL Server. I know you all know that, but do you know exactly how to do it in Postgres? That's how you do it in Postgres. This is how you do daytime, you know, functions. This is how you do string functions. This is how you do pivot queries. Whatever is in your source procedure, Gemini will explain why, what is the equivalent in Postgres, why it's the most suitable one, and how to use it. Which is amazing because it's, I like to call it crash course on a specific function or procedure. Right? So you don't need to Google every single bit. How do I do this? How do I do that? You just get everything in a report and you'll see it live in a second. So that's a very important part of the process. Not just converting the code. Getting your developers and DBA trained on Postgres and on converting code and verifying the code. The other aspect is tuning of your code. So you can ask Gemini to tune your code. Right? So it will go through the code, remove unnecessary where clauses, will combine different if clauses, you know, that are not necessary. Will remove unnecessary code. It will do a lot of cleanup to the code so it's both more readable and more fast to run. And again, we'll explain exactly what it did and why it did so you can review and either accept or reject the recommendation. And the last thing is, like, if you have very super hardworking developers, maybe all of your code is super, like, fully commented and, you know, everyone can read it in a second, figure out what it's doing. Real life says that, you know, not always 100% of the code is commented. So you can ask Gemini to take the hat of a developer commenting a procedure or a function or a trigger or a view and explaining what it does exactly. And as you'll see again in the demo, it's not just a technical kind of readout of what the procedure does. Gemini understands what the procedure is meant to do on a business level or a functional level and comments the code like a person would do. Not like checking if A is greater than 5, but more like, is the amount of the order large enough to do this or that? So you'll see that in a second. Super helpful. And, again, you can accept it as it is, tweak it yourself, but the key word is, again, acceleration, right? Don't 100% rely on Gemini. Let it do the heavy lifting for you. Verify instead of develop or, you know, comment or tune. All right. With that, let's see all of that in action, and then we'll hear a bit from a customer angle and go to Q&A. So this is how it looks like. I'm starting. This is the DMS console. I'm starting after achieving connectivity and pulling in your database schema and converting it. So this is still the deterministic or algorithmic approach. We haven't enabled Gemini yet. So you can see we have different conversion issues. Some objects are converted. Those are the ones in green, right? They're converted fully, 100%, no need to do anything. But some of them do have some issues. So things like tables, you can see we are converting data types to the most appropriate ones in Postgres. Things like identity columns that have different syntaxes and options in Postgres. All of that is fully converted for you. Same goes for indexes, constraints, user-defined types. You know, all of those objects are typically 100% converted, and we know how to convert the different options. But in some cases, in the more complex objects like procedures, there are sometimes conversion issues. So if you take a look at one of the procedures here, you see some of them were converted successfully and some were not. So the one called process orders has a bunch of different conversion issues, which our algorithmic engine does not support yet. And if you go through the code, you'll see exactly where in the code, right, things have issues, what issues, and you'll also see something about that in the destination code. That's our deterministic approach. Most of it is converted. Whenever it's not, we'll tell you exactly what the problem is. The new part is the Gemini part. So we have the Gemini auto-conversion, which I'm now enabling here. And then we'll ask DMS to run the conversion again, but this time with Gemini on top of the deterministic model. Now, you can do that in the first place. I just wanted to show the difference between the two. So this time you see no errors at all. We don't have errors. The only thing we do have is a review-recommended event saying, use Gemini augmented code with care, and you better review the output. But Gemini already fixed all of the issues that we've seen before. So again, no action-required events, only review-recommended. So if you go into the exact same procedure, this time you'll see there's no errors. The full thing is converted, right? The code should be functionally equivalent. And if we look at the Gemini auto-conversion details, everything I talked about. So Gemini tells you I've converted the code. I found a bunch of issues in the deterministic route. So raise error is not supported, so I replaced it with this. Cursor and fetch replacement, you know, I changed that to implicit cursors. Output parameters, you know, whatever I changed. So you can read through everything here. Like, you know, each specific issue, there's a full explanation of what was changed. And if you switch to the code comparison up here, you'll see exactly what Gemini changed and where. So you get a full understanding. Yeah, give Gemini a clap. Gemini loves that. There's also a way to, like, formally give it a clap right here. That gives us a feedback whether you found this useful or not. It doesn't, by the way, send any of your code to Google. It just marks that you found this useful, like, in general. So that's the first part. The other procedure... Yeah. Sorry. Yeah, I skipped that one. Let's stop for a second. So this is the Gemini assistant. So let's say you completed all of your conversions or Gemini did it for you. But now, again, you want to understand a bit deeper. So you got what Gemini changed compared to the deterministic route. So if you're experienced Postgres DBA or have done multiple migrations, then you can maybe assess it right away. But if you want to learn a bit more, that's where we have the Gemini assistant coming in here. And as you can see, we have four options. Right? The most, I think, interesting one is the one I talked about in length. Explain the conversion logic. So, again, put aside whether Gemini augmented this. You did it manually. Whoever did this... Sorry. Whoever did this, Gemini can explain what was the logic behind the conversion. This is the explain logic. As you can see, as I mentioned, everything is covered. I'm doing this again. This is the code optimization one. So you can see Gemini go through the original code, explain what that does. In this case, it's an update statement that is done in a non-efficient way. The optimized code replaces the correlated join. And it explains exactly what the benefits are and why functionality is preserved. Right? So this is exactly what you'd ask your DBA to do. Right? Why are you doing this? How are you doing this? And what's the benefit and how do you make sure the code is functionally equivalent? So now, in this case, we analyzed the code. We optimized the code. We added comments to the code. And we can now either save that or reset back to our original code before we move on. We don't have a lot more time. I'll speed through it. The next parts here are now, once you have everything converted, you can apply all of that code to your destination from the same exact console. So you choose. You can do it in an iterative way. You can go schema by schema or just apply everything in one go. That's, by the way, true across the entire DMS experience. So now you apply this to your destination. We go and create all of the tables, functions, views, everything on the destination. So it's ready to receive, you know, the data itself. Once everything is successfully applied, then we can create a migration job. Again, from the same console, from the same tool, we know exactly what database it is, what tables you've chosen, and so on. So it's a very easy and quick configuration. We choose the source and destination, basically, and which tables. And you just click go. We'll start a migration job for you, which will start doing the full dump or initial load. Once finished, it will switch to CDC. At any point in time, you can monitor, you know, the progress, the replication delay, right? And once you're happy with, you know, the replication delay is zero, you stop your application, you stop making changes, you can finally promote the job and complete the migration. So everything is a swift process in the same UI, in the same tool. Of course, there's programmatic access if you want to automate it, but that's the full experience with DMS. We have a few more minutes, so let me quickly welcome Shosheng from Wafer to give you a bit of a customer perspective on migrations, how these things looked before DMS, and how, you know, this can be leveraged to accelerate. Good morning, everyone. Thank you, Ares, and thank you, Kerry, for great insights into the database management service. Now let's move on to the million-dollar question. How does it help us as customers? Because we are the ones using it, and we are the ones who are going to be migrating it. I am Shosheng Srivastava. I am basically managing the data foundations teams in Wayfair, managing large-scale databases, powering our tech ecosystems. Before we do a deep dive into what this is, let me ask a few questions from the audience. How many of you are database administrator or database reliability engineers, if you can raise your hand? Nice. And how many have you dealt with heterogeneous database migrations before? Great. And how many are just for fun and learning? Everyone. Awesome. Thank you for that. A little bit before I go about Wayfair. Wayfair is a recognized brand in America and Europe, specializing in home furnishing, and we literally have a zillion things at home. Behind the scene, we basically serve our 22 million customers with a robust engine of supply chain, technology, pricing, all working together to provide you a seamless experience of shopping. We are always striving to be ahead of the curve and leveraging tools like Wayfair Muse, our AI-based app, which basically lets you see the furniture in your home, and basically you can buy it from there itself. How do we progress from there from a database perspective to evolve our customer needs, innovation? We began a tech transformation back three or four years back as we moved into the GCP cloud native ecosystem. It basically helps us into faster release cycles, better availability, and easy-to-manage systems. I think most of them have already been covered by Perez and Cary, but a major component in tech transformation was data decoupling and migration, application modernizations, and how do we enable our stakeholders for cloud. So, yeah, so why we need tech transformation, right? Reduce time to market, unlocking our business values to help our suppliers, help our customers, help our stakeholders, improve customer experience when you are making calls to a sales team or requiring an assistance with an order, cost-effectiveness, and basically improving our developer velocity. Wafer is a data story. We have been harnessing 20 years of our data, all primarily in SQL Server, now updating our tech stack to go into modern databases. But data is being used everywhere, from customer agents, sales, pricing, suppliers, anywhere we can think of. And if data is being used, the data has to persist somewhere, where the data is persisted in the Wafer database story. SQL Server, Mario DB, PostgreSQL, Spanner, Mongo DB. So, we have both relational and non-relational capabilities interacting with thousands of applications and database. I remember this picture that Eric showed before about the legacy database modernization. This is database with apps connecting to each other in real time. Now, what we did back in 2022, a lot of application teams were coming up and asking us, hey, I want to modernize my database, I want to move from SQL Server to PostgreSQL. But that time, there was no database migration service to assist us. But we were running in the GCP Cloud ecosystem, and we had to help our customers. Most of the issues that generally arise, which they have already encountered, is data type mismatch, data quality issues, application-level refactoring. Moving data is very easy. Data type issues and application refactoring was the major challenges that we were facing. We created this whole workflow, an automated system, but it still requires a lot of effort on our side. How do you convert your monolith database of a SQL Server going into a Postgres SPDB? This is a very simple example, but behind the scene, the engineer has to follow a 40-page runbook to actually achieve that. How much amount of time it took? Maybe a week or two weeks to actually do one migration. Again, the data migration is very simple. It took like two or three hours. Understanding how to do that took the engineer at least one or two weeks to actually do it, which is still a big lift for people who want to migrate like thousands and thousands of databases to Postgres. Fast forward to today, with the new preview available, we have everything encompassed into a database migration service. So we have streamlined the conversion. We have the AI-powered Gemini on top of it. And we also have the deterministic engine and a developer-centric mindset. Because instead of the manual work, the people in the company were kind of in the comfort zone of, say, like, I know SQL. I don't know Postgres. And I need to learn Postgres because I need to move to the new engine. So there was a learning curve and a cultural shift inside the organization also that they had to go through. And that all comes with a time. That basically impacts your developer velocity because people are now learning Postgres because you have to move. With Gemini, things are very easy because when you are moving that data, Gemini is actually telling why it is doing that. What is the reason behind that? So people learning is a little bit much more faster. It doesn't just migrate your data. It also educates the engineer also in the real time. So it is just an accelerator. It basically accelerates what we were doing in two weeks. We should be able to do it in, like, one or two days now because the learning curve has been much more faster. We have not tested this yet, but we will be looking forward to it because we are on a journey to modernization. And this is going to be a great accelerator for us. The tool also plays in accelerating our wayfair journey to tech transformation and allowing us, our customers, to basically serve most faster and efficient and less painful for our engineers so they can work on more productivity things rather than working out on the data migration stuff. With that, I want to wrap it up and say a big thank you for Ares and Kari to actually giving us this engine, which we really required. With this new capability and the toolkit, we are looking forward to what lies ahead. Thank you.