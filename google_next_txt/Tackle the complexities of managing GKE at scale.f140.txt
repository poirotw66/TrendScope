 Thanks for joining us. Let's put our mug shots there. So that's me, Anand Dimri. I'm the outbound PM in cloud runtimes. I'm joined with my three musketeers. We've got Cal and Donal over here. He'll be joining me a little bit later. So let's talk about why are we here. I want you guys to use that LLM in your head and imagine the time when you ran your first Kubernetes deployment in production. The excitement, the energy that you got, you playing around with kubectl, deleting parts, and it magically reappearing it again, all that fun times, and you thought you would scale heights. You would run application after application, and nothing will go wrong. And now you are here. So what went wrong? Well, we'll talk about some of those challenges and solutions. And then just so that it's just not all theoretical stuff, we're going to have our partners from Dunn and Backstreet come on over here and talk about how they did it in their production environment. So hopefully it's a good talk. So what's our strategy in a nutshell? And by our strategy, I mean we want to talk about cloud runtimes, how to run applications. And it's pretty simple, guys. We want to run managed services, and we want to run containers. And we want to run the best containers out there. And our strategy is to run containers for scale. We want to build for change. We want it to be fault tolerant. And we want it to be manageable. Pretty simple, right? So the one word that comes right next when you talk about scale for every VP out there is you need to scale, but you need to scale it cost effectively. So what are some of the complexities that come in? So there's complexity of running a heterogeneous deployment when you make a choice between do I scale for application A or do I scale for application B? And that makes you choose. There's a pressure to deliver price versus performance and do it in a competitively, economically viable, especially for AI solutions. And then, of course, the AI solutions, the model size and the training demand is pushing the limits of scalability and obtainability for any system out there, including Kubernetes. So how do we think about this? And what is the use cases we're getting from our customers? So when we hear our customers talk, the usage patterns application teams are coming to us and talking to us about is either customers have multiple clusters in different environments or these could be spread across many different projects. That's one pattern we are seeing, the one illustrated in red over there. And then we also have teams that actually deploy apps over multiple clusters, and these could be spread in multiple regions, multiple sites, and sometimes even on-prem. So these two are the common patterns that we have seen. So what we heard from customers is they want a single point to configure and observe multiple clusters in the same space. They want cross-project grouping so they can deploy the same settings, the same configuration to all of them. And then on the team side, the application developers are saying we want to define configurations that could be applied to our application and nobody's else. So how do we think about this? When we hear our customers say this, the way we are approaching this is to think about either for multiple clusters using fleets, and then you hear about teams' vocabulary in GKE, which is to manage and give your application teams ability to manage their application. So let's dive a little bit deeper on what are fleets. So I want you to think about a fleet is a collection of Kubernetes cluster. So think about your production fleet or your development fleet, and then it could be one cluster, like in the example over here for development, or it could be multiple clusters in your production fleet across different projects that you can manage. And then similarly for teams, so in the same fleet, you could have different teams deploying different applications. So in this case, let's look at team scope one, which is the scope of your one application. Let's say you are a retailer, so you could have your inventory application with its own RBACs deployed in cluster one and two, and then you could have another application that uses namespace four and five, let's say your pricing application, and it has a different team admin that manages its RBAC for that application. And you could spread it between cluster two and three and see how we have kind of distinguished between application one and application two, and this lets the application teams work in their scope without affecting other applications running in the same cluster. So we are kind of separating the role of a platform administrator from an application operator. So platform administrator can provision teams, can provision for different teams. They can set for tenant policies. They can view the team statistics at a holistic level. And then for application operator, they can do self-service onboarding. They don't have to go and talk to a platform administrator on how do I onboard a developer in my account. They can view their workload status and logs without running back to the platform administrator to give them access. And they can manage their own application cost and security and operational concerns. And then, of course, the benefit for the company is to simplify multi-cluster management and to apply consistent config either to a cluster or to an application. And it gives your team self-service application agility. Having said all of that, we have a curveball. And that's if you're training for AI ML. And Donal is going to talk a little bit more about their specific environment. But AI ML workloads are straining the limits in a very different way. GPU resources are becoming really hard to acquire. So one of the ways we're thinking about is portability gives you, and by portability, I mean ability to run in a, not in a singular GPU type, ability to run in multiple GPU types gives you the ability to scale more. Also, large-scale model requires efficient orchestration across thousands of GPUs. And the high cost, like we've talked about, drive the need for maximum performance for price, which introduces sharing concepts. So there are some core fundamental features we have at GKE. There is four-dimensional auto-scaling. So you have pod and node, and you can go up for size, or you can go up out for scale. So that gives you the four-dimensional auto-scaling. We build cost-optimized profiles out of the box. So if you use HPA, you can use the default profile that gives you the best, most aggressive way to turn down your resources. Also, you only pay for resources consumed, not what you've set out in your configuration. There are more capabilities we have introduced for spot VMs and T2D VMs, which can help you run some of your non-critical workloads, even at a lower cost. And then we also introduced multi-instance GPU. So the same GPU can be split into more virtual GPUs and can be shared across your clusters, even for more sharing and more utilization, so not a second goes by where a GPU is not being utilized. And the last one, I'm going to save it. It's the best one. Which is to actually scale, you have to have an engine that can scale really, really high. You can do all the tinkering with all the other settings, but we recently announced support for 65,000 nodes. And just to give you a clue of how awesome that is, without naming any clouds, these are the next two public cloud providers out there. Them combined do not even come close to the amount of node support that we can offer. So we remain invested in engineering and in contributing to the K8 space, which is Kubernetes open source. And then Kubernetes came out of Google, and it is where we run all our workloads. So we tested on ourselves before we released it to everybody. With that, I would like to invite Cal and Donal to come up over here and share their story. Thank you. Thank you. Thank you. Hello. Good afternoon, everybody. So my name is Cal Bufalino. I've been part of the Google family now seven years, always in the customer experience and Google Cloud consulting organization. And my goal and the goal of my organization is pretty simple. Ensure that customers like Dun & Brow Street, or all of you, have the best possible outcome and business value of their investment with the Google technology. Today, we are here with Donald. Thank you so much for being here from a long trip from Dublin, Ireland, to talk about the journey of Dun & Brow Street, in particular with some reference about AI, about how reference to also with scaling on GKE. So, Donald, do you want to introduce yourself? Thank you very much, Cal. So I'm here today to give you an overview of an exciting generative AI enabling platform that we are building in Dun & Bradstreet, what we're building, and why we think this is super important to look at it from a platform centralized perspective. So, Dun & Bradstreet are a data and analytics company. We help our customers make better business decisions by providing them the most trusted and clear data and analytics on companies worldwide. From a technology perspective, we power those data and analytics insights from our massive global data cloud of commercial information. The D&B data cloud is the single aggregation point of all of our commercial data. Over the years, we have collected over 600 million unique business entities, collected from over 250 countries and regions across the globe. On each one of those 600 million entities, we can have up to 11,500 unique attributes. That's 11,500 unique pieces of information on each one of those businesses. This information is obviously changing constantly in the real world. What our teams do is we scan and we monitor and we update our data cloud daily to the tune of something like 2.2 billion data points every day. So, this is a story about D&B, a data and analytics company, with the richest, largest commercial database in the world, driving our analytics innovation activities. But the one data point that I'd like to just focus on first is the one on the top left. D&B can trace its roots back to 1841. That's 184 years ago. So, as we all sit and stand here in this amazing venue with this incredible Google Next conference, we're all lucky enough to be here really celebrating modern technology. But for the next few minutes, if you will, I'd like to take us on a reflective journey just to try to explain how D&B have come from a company set up 184 years ago to one building a generative AI enabling platform today. Sorry. So, I really like this image. It talks about the concept of generational waves of innovation. Innovations replace outdated technologies and industries, and major innovations create long economic waves. As the waves overlap over the years, and more recently, as modern technology has taken over and has developed, these waves have become much shorter. We are in the conceptual sixth wave right now with AI right at the forefront of the disruptive technologies and innovations. And to the right, we look at some of those initiatives that have completely disrupted and influenced how we run our companies and live our lives today. I highlight again that date of 1841 in the bottom center of that wave. When you work for a company with a heritage and a history of D&B, you really feel as if you were part of something so much bigger, something that has evolved over the years and literally over generations to being what it is today. And you think of those technologies over the years that it has leveraged. What started off 184 years ago with D&B credit reporters on horseback riding out over this great continent from the East Coast to meet with prospector companies on the West that were setting up operations, think during the gold rush of 1849, because they were looking for credit and banking and finance from the East Coast banking institutions. It has evolved through all of these amazing innovations to where we are today. The telephone, the electric light bulb, the automobile, more further up the timeline, the first computers in the 1950s, mainframes in the 60s, microprocessors and personal computers in the 1970s and 80s, the public internet in the 90s, the smartphone and, of course, cloud in the mid-2000s, with the widespread use of powerful AI systems coming online around 2010. So now we find ourselves in the sixth wave, and together we are all moving together with what would seem an inevitability of human-level artificial intelligence. So D&B's digital journey started back in the 1960s. We were one of the first companies to digitalise and computerise business commercial records. We created the DUNS number in 1963, which is the persistent business identifier that I'm sure many of you are familiar with today. Our AI journey started the decade later in the 70s, with our data scientists developing our first commercial credit scores. In the 80s, we had proto-ML and early machine learning techniques. They were really driving our predictive analytics and scoring at a much larger scale. The internet and the big data era through the 90s and the 2000s was the start of us really leveraging this vast data universe that we had amassed. And then came cloud and its compute scale behind our analytics. And finally, in the last few years, generative AI and the wave that we are all surfing on today together. So AI has been around for a long time. D&B, as a data and analytics company, has been innovating in this space for several decades. But what the data is showing us in the more recent times, when we look at the wider business ecosystem, we saw the beginning of widespread business adoption of AI around 2017. This rose to about the 50% or 60% mark and stayed relatively flat for the next number of years. Then came mid-2023. And that was a time when us data nerds or anyone who could explain how an LLM worked suddenly found ourselves feeling very interesting to other people. This green trend line is the percentage of businesses now adopting generative AI. Personally, I don't see the pitch of this line reducing any over the coming years. Certainly not from a willingness to adopt perspective. The opportunity that generative AI presents to all of our businesses, either internal as internal efficiency plays, or building new products or client experiences, is simply too great. So how do we harness this business opportunity fast? How do we drive generative AI research and development velocity? How do we ensure that it is done in a secure and trustworthy way? How do we control the responsible use of AI and the development of responsible AI systems in a way that retains and upholds the compliance and ethics goals of our company? So we believe you must centralize. We believe you must put all of your efforts into enabling generative AI, but do it with control. And that's the foundational purpose behind the AI platform strategy. So thank you so much, Donald, for telling all this comprehensive story. You know, almost two centuries to get us here. So let's talk about AI platform. So which was the idea? Why? Which were the challenging that Dun & Brush Street was foreseeing of this adoption of the AI? So when you start needing to have this centralized platform to control it. Yeah. So the AI platform strategy really is, as I was talking about, it's enablement of data science, research, and development, plus doing it with a level of control where we knew that our data was being used in a compliant manner, that it was super secure and access, et cetera. So on this diagram, everything from the left-hand side in development is about enablement. We've got self-provisioning. We've got our data science team, ultimately, and that runs in this number of hundreds within Dun & Bradstreet. So really the main part was providing them as much self-provisioning capability as we could, and that's self-provisioning of infrastructure. It's access to any data, be that our full universe global data cloud or be that any external data set. It's access to any development framework. It's access to all models globally, and it's essentially providing engineering support along with that. So we've got two real groups of engineering support that support the data science teams. We've got our application engineering, who are tasked with essentially all of the non-AI development, and also with all of the pipelining, CICD, et cetera. I mean, I'm sure many in this audience kind of have experienced what you receive from a successful data science proof of concept is such a long way from a production-level software deployment. There's so much more that needs to go into it. So they're dealing a lot with test automation, kind of refactoring a lot of the code that we'd be getting from the data science group. On the platform engineering team, that's all about building in security, building in observability, building in scale, et cetera, that we need. Everything is automated. Everything is going through checkpoints that ensure secure compliance code. We've got a very strong focus as well internally on AI governance. So we've got a central AI governance council. They essentially create the metadata that we then put into policy as code as part of our CI pipelines, and that everything then is stored in the development artifact registry, which really is trying to bring traditional software discipline to the data science world as we try to bring these workflows to production. Yeah, that is... Thank you so much. It's a very interesting concept because Dun & Brassery is a data provider, so there's a high quality of data, compliant, right, with all the rule and regulation for each country, and also you create all this framework to ensure that also this wave of AI don't create any disruption, right? So question on this area. Which was the impact of, you know, because you are a senior director from research engineering. Is there any impact from the velocity of adapting the new technology in your team? I suppose it's, again, this is really trying to... What we're seeing internally is that there's quite a blurring happening between what would be traditional data science and an engineer. You know, when we're working in pods, I guess, together, there's a lot of kind of co-development happening, you know, potentially, some of the more traditional engineering work, you know, being developed on the data science side and more of the kind of LLM and RAG development that's happening traditionally more in the data science is now being taken care of by more of the LLM ops type engineers, you know? So I think it is all about enablement. It's about the central artifact registry is a really critical part as well, both from a control perspective in terms of ensuring, you know, nothing gets into production that hasn't gone through those pipelines. Nothing goes through that that hasn't gone through all of those security checkpoints, all of the legal compliance and ethics checkpoints. But also, once you've got it in there, it is ensuring reuse of these reusable components. Everything is treated as a software component, be that our prompts, our tuning, our RAG pipelines, everything kind of goes into that. And again, because what we find is on those hundreds of data scientists that are working in their own individual GKE cluster or whatever, previously there was a lot of reinventing of the CM wheel. There's an awful lot of similarities across a lot of these workflows. So we find that that's a really critical part. We also think from a talent perspective, when you're onboarding a new data scientist or a new engineer, having all of this centralized, you know, is a real, provides real velocity to the kind of ramp up of that person where they're going to get productive much, much quicker. That's interesting. And the other thing to highlight on this platform, when Donna engaged me and at that time was conceptual, I said, oh, everybody will need something like that. The other thing to highlight is the fact that it's also LLM independent, right? You want to explain more of this concept? I mean, that's something that was very important to us at the beginning. This world is changing so fast and there's different models being deployed and released every day, I mean, constantly. So our ability to be able to swap in, swap out was very, very important. And plus as well, just kind of, and this was an area that I found working with the Google Professional Services Organization, as I was going into that relationship, I was a little worried that there was going to be a constant push for Google services for everything. But I think, you know, I think all of us probably have a cloud agnostic ambition. We certainly want to be model agnostic. We want to be able, you know, because ultimately whilst we provide all of the access on the left-hand side and allow our data scientists to play with any models and to kind of, ultimately, that's down to our central ethics and compliance group as to whether that's something that we do want to be kind of working with or we do want our data to be associated with and all of that side of it, you know. So it's really important that we can swap in, swap out really quickly. Now, whether we can do that really, I mean, there is always a level of re-engineering and re-testing and re-calibration as you swap in models. But it's really important that as these develop with such speed that you can leverage that opportunity fast as well. Absolutely. Yeah, and for us, it was a very interesting project because we came from conceptual to start working together. And that's also like another testament of the kind of partnership we're able to build on Brass Street. Indeed, maybe you want to elaborate a little bit about how our team engaged together. Sure. So Carl and I started working on this probably 18 months ago now at this stage. And I think one of the most powerful parts of it, really, is having access to the experts on the ground. I mean, our GCP journey started, I suppose, maybe it's 24 months ago or a little bit longer, maybe. So there was quite a learning curve, you know, as we moved from in-house data center technologies or we moved from another partner. So really understanding at the design stage, you know, and at certain stages that we could give you a call, you would get the right people in the room that would kind of talk to us around the various options of the Google services and then more around the kind of platform scale aspect of things as well. So I think having the professional services guys on the ground with us in ideation sessions, in design sessions, and then leaving us alone to learn, you know, and it's kind of, I think that's a really important point as well. And there was a really nice balance of our engineers and architects getting their hands dirty on GCP and all the services and learning enough so we build our own competence and our own IP and there wasn't a full dependence on the professional services organization. So I think it had worked really well then. And then when we hit bumps in the road, and we certainly did, that, you know, that we were able to call, having Cal on speed dial helps, you know, kind of being able to raise the right resources at the right time so that we can get over those bumps quickly. Absolutely. For us also it was a challenging project because these are the projects where you don't know even what kind of resource at the beginning to staff. So one thing that was important from a Google point of view is to co-work together, right? So while we were developing this platform and we also help a little bit to train and also when we say train DMV, in reality we are learning also, basically, you know, because if we were not able to understand the core of the company, then we could not have been successful. So it was very critical to be, from a Google point of view, very flexible and bring the right resource at the right time. And same thing from DMV to be patient with us to staff the right person. And the other thing that I want to highlight is that the time we spend together in person, right? We kick off this project, I remember, in Dublin. Yeah. And a couple of days all team together because you create this kind of bonding. Yeah. The taps are, you know, even with the remote work, it works well. That's right. And I think the point I made earlier that I would reiterate based on experience, as we went into this relationship, I was fearful of, it must be GCP service for every time, you know, and there would be that hard sell. And that has never been there. And that's something that I'm very grateful of, you know, that it was, we obviously are always weighing our buy versus build decision. So it's, you know, again, we have our cloud agnostic ambitions. And there are, like all architecture decisions, there are trade-offs either way. So there are times we do leverage very strongly a lot of the Google services, but other times that it's better to stay and kind of build our own. So I was kind of appreciated of not only, you know, kind of at our level, kind of when we're talking about the relationship, but right down to the architects, you know, it was always looking at what the best solution was for that use case. Yeah. Thank you. I remember this conversation. Really? You're not trying to convince me that Gemini is the right one? But that's, at the end of the day, that for us is a good story about learning from one each other to do something that can be innovative and also can have a future, right, and then for the platform. Thank you for listening today and giving us your time. Thank you so much. Cheers. Cheers. Cheers. Cheers. Cheers. Cheers. Cheers. Cheers. Cheers. Cheers. Cheers. Cheers. Cheers. Cheers. Cheers. Cheers.