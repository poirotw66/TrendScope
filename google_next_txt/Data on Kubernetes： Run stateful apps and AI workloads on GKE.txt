 . All right, everyone. Welcome to Data on Kubernetes. I'm so appreciative of you all coming out on the last day at Google Next. I hope I did you all a favor. I changed the description of the session slightly and that we're still going to focus on Data on Kubernetes, but I'm going to really kind of emphasize the cost savings. I want to send you all home with some tools, some tips and tricks to realize some of the things I'm talking about today back home to really help you with not only performance gains but cost savings. I want to make this worth your while. You stayed with us to the end. I want to make sure this works out for you all. Before I go any further, I want to do a vibe check of the room. How many folks in the audience are currently running databases on Kubernetes today? If you just raise your hand. Okay, what about ETL pipelines, analytical workloads, that sort of thing? Okay, AI. How many folks are in AI? Wow, okay. Got a lot of people that this is going to really resonate with. So the Data on Kubernetes organization, they released a survey at the end of last year. They spoke with about 150 IT leaders and they asked them, you know, what workloads are they running on Kubernetes today? Databases was the largest. To address the elephant in the room, Kubernetes is a more than appropriate compute fabric to run your stateful applications. Right? That being said, let's kick things off. My name is Brian Kaufman. I'm a product manager at Google Cloud. In just a few moments, I'm going to be joined with Terry from Quadrant and Vulcan and Ogre from Codeway. Now the way we're going to spend the next 45 minutes, I'm going to first walk you through a storage strategy with GKE. I'm going to introduce some solutions focusing on cost savings and performance. I'm going to take that into some AI ML use cases. Again, some solutions around performance gains and cost savings. And then we're going to hear from Quadrant and Codeway about how they're using some of the solutions I'm talking about in their workloads today. Does that sound good? Ready to go? All right. So let's talk about a stateful application, Postgres, one of the most popular databases we see on GKE today. I want to run Postgres on GKE. What storage medium do I use? Well, there's block storage, traditional PDs or hyperdisk. I can do a parallel file system like ParallelStore or Lustre. Of course, I could do an NFS mount. That's our file store product. Well, my Postgres database in this example, it's a relatively small database. It needs to be performant. But I think the most appropriate thing to do would be to put it on our hyperdisk product. This is a network attached block storage disk. Now, why did I choose hyperdisk over maybe our persistent disk, which some of you all might be more familiar with PD SSD, PD balanced? Well, hyperdisk allows us to tune the IOPS and the throughput on the disk level independent of the capacity. So, some of you folks might be familiar that to get more performance out of a persistent disk today, you need to increase the capacity probably beyond what you actually need. Not the case with hyperdisk. You tune the IOPS and the throughput in the storage class. So, when the disk is created. If you want more granular control, you can use today the gcloud command to adjust the IOPS and the throughput of the hyperdisk. And then coming, well, it's actually targeted in Kubernetes 134. You'll be able to use the volume attribute class to directly modify the IOPS and the throughput of an individual disk. So, you have a storage class to create the hyperdisk with just a generic performance specification. And then you can use the volume attribute class should your application's performance needs grow. And again, you're not having to add more capacity to the disk. You're just right-sizing the disk. So, the takeaway here, again, you're only paying for what you need. You're able to right-size the disk to the exact performance that's appropriate for your application. All right, so I have my Postgres database on a hyperdisk. What's next? Well, Postgres is probably not the only persistent disk that's in my environment. I got boot disks, obviously, right? If you're running GKE, you have a node, it's got a boot disk. You're probably just not running Postgres on a cluster. There's probably all sorts of microservices that require disks. Some of them might require persistent storage like this. I got something else to show you here. You can put all these disks in what's called a storage pool, which allows you to share capacity and performance across all the disks in the pool. Now, the way this would work is you would over-provision your disks beyond what they would need, and you're only going to consume capacity or performance from the pool when it's needed. So, for instance, you're only going to consume capacity out of the pool for bytes written, not, you know, when you create the disk. The result is you can get up to 80% capacity utilization by the disks in the pool. At 80%, we're going to auto-scale the pool up to give you more capacity, right? So you're not paying for these large disks that you don't need that you're probably only using maybe 20%. I don't know what the exact number is. But it's fair to say a lot of you folks are probably paying for storage that, you know, is unconsumed, right? So here's a way you can optimize all your storage, put them in a storage pool, and then share the capacity, the performance, the throughput, the IOPS across all the disks. It makes it very easy to manage as well. You're managing on the pool level, not the disk level. All right, so I got my hyper disk. I put all my hyper disks in the storage pool, my boot disks, other block storage disks. It's a zonal resource. Everything in the same zone of the same disk type can go in the hyper disk storage pool. I got another problem here. The app that, you know, uses my Postgres database is doing really well, right? It's a problem for me because I'm on the, you know, platform team. There's problems with latency, unfortunately. And I know how to fix latency. I just put a bigger VM, right, more memory. The issue with doing that is memory is very expensive. So memory, let's say, is around $3 a gigabyte a month. That's expensive. If I want to store everything in memory, I can solve a latency problem, but I can't, you know, it's going to cost me a lot. So I have another solution for you. I could put a local SSD in line of the Piper disk to serve as a caching layer. So what this is is actually a new solution, and on behalf of the GKE stateful team, I want to introduce you all to GKE data cache. Using a storage class, we automatically position a local SSD as a cache in front of a persistent disk. Local SSDs are about $0.08 a gigabyte a month, much different than the $3 of memory. Local SSDs can still provide sub-millisecond latency, right? What we've seen with testing Postgres on data cache is 80% latency reduction and up to a 480% increase in transactions per second. So this is ideal for read-heavy workloads. You all heard Quadra is going to come on stage soon. That's a vector database, very read-heavy, right? So really nice fit for my Postgres application. I'm serving a lot of requests. It's going to come off of a local SSD when it can, and much cheaper solution than just plugging in a bigger VM. Now, the way the data cache works is in the storage class, I would pick my disk. It could be a hyper disk, or it could be a traditional persistent disk. It's not dependent upon hyper disk. And I would specify the amount of the local SSD that I want to use as the caching layer. My persistent volume claim, I just referenced the data cache storage class here. Very easy to set up this cache. You don't have to manually configure the local SSD. The storage class will do everything. We're very excited about this launch, and the documentation's live today if you want to test it out on Kubernetes 132 or later here. So here's what we built. We have a hyper disk to tune the IOPS and the throughput on the disk level should I need more performance on the disk. If that's not enough and I have some latency, I introduce the data cache, and that's just going to work for my, or it's just necessary really for my Postgres database. I'm going to have other disks. I have boot disks. You've heard about the 65,000 OGK clusters, right? That's a lot of boot disks. It could be very expensive, obviously, in the storage. I put it in a storage pool, okay? It's a very simple strategy to allow you to scale and then reel in the cost as you do. It's very optimal here. All right, so that's our traditional stateful application storage strategy. What about AIML workloads? Now, a lot of hands went up for AIML. Let's talk about AIML. It's kind of a loaded phrase here. At Google, we think about it two ways. There's training workloads and then there's inference workloads. And they're very different. It's really two sides of a coin. With training, node failures are very important. You're going to spin up a cluster ahead of time and you're going to run your training job. And if there's a failure, you're going to have to restart the whole job from a checkpoint. That's very disruptive. Node latency, not a huge deal because, again, you spin everything up ahead of time. You're not, like, waiting for customer requests to come in to do any auto-scaling or anything like that. But you are doing a lot of reads, usually small files from object storage. That's your training data. Some writes, that's your checkpointing. We'll talk about that in just a bit. These are kind of the fundamental characteristics of a training workload. Inference, very different. Reliability, of course, it's important, but it's not as critical because inference workloads are very similar to just regular web applications, right? Stateless applications, you scale up when there's demand, you scale them down when there's no demand. But startup time is a huge problem, right, for inference. And I'm going to talk about that in just a bit. Some of you in the room are probably very familiar with the startup problems for inference workloads. And then there's a few large files you download for inference workloads. They're usually the weights, right? And depending on the models, they can be very large. Read heavy, of course, you're downloading the models here. Now, when we talk about checkpointing, if you're doing a training workload, there's a few solutions we have for checkpointing. Of course, there's object storage. You're training your workload. You're taking snapshots of the state so you can restore if there's a problem. You can just beam it up to cloud storage if you're not taking too many checkpoints and you have a relatively small training job. Parallel file system you can use if you need a few more checkpoints, you know, one to five minutes frequency of checkpoints. And then a new solution we just launched was multi-tiered checkpointing. So if you're doing a, you know, massive training job and you need to take lots of checkpoints because it's so expensive if things fail, you need to, you know, get as close to where the failure happened as possible to do the restart. We have the multi-tiered checkpointing which does asynchronous checkpoints to memory and doesn't interrupt the training job. So it's a really performant way to do your checkpointing at scale for some of our largest customers. So that's training. Let's talk about the inference startup time that I alluded to that is really the, you know, kind of the issue with running inference in our industry. So if I have an application that's running an inference workload and I suddenly get a lot of demand, I need to spin up a new node to service all the requests. So I need Kubernetes to provision the machine, load the container, start the inference engine, and then of course load the model, whatever I'm using. You know, if you're not familiar with AI ML, you're like, okay, that's kind of like a regular app. I, you know, spin up a pod and, you know, Next.js server comes online and answers a bunch of requests, whatever. Well, it's very different in AI ML because of the amount of data you need. So a container image that's running, say, Llama 3, that could be up to 16 gigabytes. It's massive, right? That takes a while to load if you're pulling it from a container registry. But that's not the whole thing. If, you know, you're running one of these models here, and some examples, the Llama 4 that came out, if you all, you know, saw last week, the Maverick version of that's 800 gigabytes. Llama 3.1, which is a very popular open source model, that's 130 gigabytes on disk. Now, folks raise their hand that they're, you know, we're using AI on GKE. If you've tried to download one of these models, we'll pick on Llama 3.1 from Hugging Face, that could take like 20 minutes. So if you add up all the steps in this process, you could estimate maybe a half an hour to spin up more capacity to service your AI ML application if you're doing this all just, you know, natively, right? Just, you know, using all the, just the public APIs. So we have some tools that you can use to drastically accelerate this process. The first is called container preloading. And what this is, is essentially a secondary boot disk where you would preload the container image, that 16 gigabyte image, onto the boot disk. So when the node comes up, it doesn't have to go to the container registry to download the image. It's already there. We've seen a 29x improvement in load time using this strategy, and this is used by some of our largest users. So this solves the problem of loading those big container images to create your pods. What about the model sizes? Well, we have two products I want to introduce you to. Some of you all might be using them. Cloud Storage Fuse. This is how you directly connect an object storage bucket to a pod. I want to do somewhat of a PSA announcement here. A lot of folks use this product, but they don't have the proper tuning. This chart, and I'll move out of the way here. It's, it's an estimation here. But if you just install Fuse and you're downloading llama from object storage, it's going to take a lot of time. However, if you enable caching and parallel downloads, you drastically accelerate that process. Because we set up multiple threads, download multiple files at the same time. Right? And we utilize all the bandwidth to the host machine. So if you're using Cloud Storage Fuse for training or inference, you want to make sure that you have caching and parallel downloads enabled. Caching is really important for training because you're downloading a lot of small files, and if you're running through a lot of epochs, you know, you want the file there. You want to, don't want to have to keep downloading it every time you run through the process. Hyperdisk ML, that's a block storage device that you can attach up to 2,500 pods. So really helpful for inference workloads where you're scaling a lot of instances at the same time. It's a zonal resource, so you don't have to download the image from object storage every time the node starts. If you're using Fuse, for instance, it's just right in the zone on a block storage disk, and you can attach that to all your nodes. Some of our customers really like the Hyperdisk ML product because it's immutable. It's read-only, so it's not like an object storage where you could change, like, a file name and everyone still downloads the same file. The Hyperdisk ML, once you create it, you can kind of trust that it hasn't been tampered with. Now, we do have a private preview for folks interested in Hyperdisk ML. We do have a new solution called Volume Populators coming out. Please contact me after if you're interested in testing it. But this basically automatically hydrates the Hyperdisk ML block storage disk from data in object storage when you do that PVC creation. So you're creating PVC to use the Hyperdisk ML, and then you specify the storage bucket you want it populated with. So it really simplifies the operational burden of using, you know, a block storage device for data that you're potentially updating quite a bit if you're making a lot of changes to your models. All right, so I want to recap this because it's Friday and, you know, exposing you to a lot of technologies here. But this is, I guess, the one thing to remember here. Which one should you use? Well, if you're focused purely on performance, you can use Hyperdisk ML, the block storage disk, to host your models and weights. If you want operational simplicity, use Cloud Storage Fuse because you're pulling the data directly from your object storage bucket. So it doesn't, you know, you don't have to do any sort of pre-processing like I showed you with the Volume Populators product. Now, one more question for the audience here. How many of you all are using VLLM? Okay, a few folks. So there is a new capability in VLLM. So VLLM, for those that don't know, it's an inference engine. It's commonly used to run open source models. There is a, what's called Model Streamer, that's part of VLLM now. Run AI has donated it to VLLM, NVIDIA acquired Run AI. What we've seen out of the Model Streamer is really nice performance when downloading files from object storage. So very similar to Cloud Storage Fuse. We can download lots of files at the same time and we do it right into memory, CPU memory, and then into GPU memory. So it's a really fast way you can get started downloading models in the most efficient manner with VLLM if you happen to be using that. And the GKE team is working closely with the Run AI team and NVIDIA on making this even faster and easier to use. But as you can see, just a parameter you put after your VLLM command to get started. All right. I hope that was good on a Friday. I don't want to go too deep on the tech here from the GKE side. I want to introduce some of our customers now. I want to bring Quadrant to the stage. Terry, to talk a bit more about what they're doing and how they're using some of the solutions that I spoke about. Thank you so much for the intro, Brian. And thank you all for being out here with me. I'm really happy to be here. I'm a data scientist and developer advocate at Quadrant. I always start out my talks with a selfie with the audience. So if you wouldn't mind, make a funny face while I take this picture. And let's get started. So one of the things I really like about Google Next is talking with people in the audience. We've got people from all over the world. Every continent is represented except for Antarctica. And we'll give them a pass. And I think that's cool because Quadrant is actually a company that is from Berlin. So our founders are the two Andres, one from Russia, one from Ukraine. They met in Georgia and they were working on these massive AI problems. And they found that the databases and data management tools that existed simply weren't up to snuff. So they built Quadrant from scratch specifically to target this problem. So if you're not familiar with vector databases, a vector database is going to allow you to take unstructured data and do things like search, anomaly detection, recommendation, Q&A, etc., etc. So if you look up there, you can see some examples of the ways that Quadrant is currently being used. Some of our clients include companies like Johnson & Johnson, Perplexity, Disney, Bosch. So these companies with a lot of data and they want to get insight from their data, right? And you can see there some of the industries that we work in, online dating, law tech, fashion, enterprise. Obviously, we work with a lot of startups. The way that I like to describe it is whether you're in the garage or you're in the office, Quadrant is going to work for you because we allow you to move from local mode to cloud by just changing one piece of one line of code, right? It's just where you're storing it. All the other code would stay the same. So a quick walkthrough of embedding models or sorry, embedding space for people that aren't familiar. Here I have three images. I've got two cats and one dog. And the idea is an embedding model is going to take this data and it's going to actually give it numbers. And then once they have the numbers, we can apply the traditional machine learning techniques to get insight from this data. So here in my example, I have a cat and a dog. So we see we have these two cats, we have this dog. And the way that you could use this information is, for example, if you had a shopping site for pet food and you wanted to make recommendations, maybe you would do recommendations based off of the images, right? So here I'm using images of the cats and the dogs. But imagine this being any kind of data you can imagine, right? This could be video. It could be PDFs. It could be images. It could be whatever you want to put on Quadrant and get insight from, right? So the next part of it is RAG. If you're not familiar with RAG, a good way to think about this is if you were to go to ChatGPT today and say, what time do I need to leave to catch my flight? ChatGPT is going to say, what time is your flight? Where are you going? It's going to have a bunch of questions because it doesn't have that information. Now, the beauty with RAG is you could upload a PDF of your schedule and then you can build a model using something like Llama where all of a sudden you can get that insight from the model by combining what we saw here with the LLM. So the LLM would first take the query and let's say you make a query about cats. It would retrieve those relevant documents and then give it to the LLM. The LLM gives you a complete answer with not only the knowledge that it has but also the knowledge that you have in your document base. So when we talk about Quadrant, the key features are definitely going to be speed. Speed and scale. So I mentioned some of the people we worked with before, but we're looking at people who have billions of vectors. We want to make sure that they can get their search right away, fast, whether that's content recommendations, whether that's legal, whether that's medical, right? That's what we're focused on is just that speed. We are also fully open source. So if you have any questions about data privacy, data sovereignty, we're SOC2 compliant. We are HIPAA compliant. We work with medical, legal, banking, all of that stuff, right? Because we are open source, we can support government projects as well. All embeddings are supported out of the box. So we are embedding agnostic. We do have Fast Embed, our embedding library, but you can use any embedding that you want with Quadrant. We'll help you out with the indexing and retrieval. And then, of course, quick and easy to start, right? As I mentioned before, going from local mode to cloud is literally one line of code, and you can just get started. And on that note, we've got Quadrant Cloud, which is our cloud offering. So you can, of course, use Quadrant locally. It's fully open source. But we also have a cloud offering, and that has three different deployment options. So we've got the managed cloud, the hybrid cloud, and the private cloud. And those basically are three different tiers of the management that we'll give you. On the managed cloud, you can imagine it would be something like GCP. Thank you, Brian, for having us here. Hybrid cloud is going to be a combo, right? So maybe you want to run things on your own Docker container, but you also want to host that Docker container via GCP. That's where we'd come in and help you. And then finally, the private cloud where everything's private. That's where you're going to maintain your data sovereignty, data privacy, and all of that good stuff. So that's basically an overview. And here we've got an overview of the actual architecture. And you can see here, this is a simplified architecture, of course, but the key thing to take away is that the database is run in an isolated environment where it doesn't talk or speak anywhere with any other data. You control your data completely, so you have that data privacy and data sovereignty. And as I mentioned earlier, we do work with a lot of companies that have to deal with compliance, so we're familiar with that. But the main thing you want to take away here is your database is going to be isolated, your data is going to be private, you don't need to worry about it going anywhere else. So, when we talk about storage, going back to what Brian mentioned with GKE, that's really exciting for us, because obviously storage is really important for the performance of a vector database. I have some benchmarks up here on the screen, and of course, vectors and indices are cached and RAM. But it's not feasible at scale, especially when we start talking about billions of vectors, to store everything in RAM, right? You can't store 100% of your stuff in RAM, it's just not feasible at scale. That's why fast disks are really exciting for Quadrant, because you'll be able to still get that great performance on search without breaking the bank, right? So, here we have some actual numbers of GKE persistent disks with balanced data cache versus non, and you can see there, you, excuse me, with a new persistent data disk, you get 10 times faster speed than the standard disk, two times faster than premium SSDs, and I should also note that this is a worst case scenario test, where we are not using any of the RAM for caching. So, in reality, you'll probably get even better performance. So, that's a quick walkthrough of Quadrant. The big things I want you to take away are, we are a vector database, we're open source, we're built for scale, and when you use Quadrant with GKE, you can save money, but you can still go fast, baby. And then, on that note, I'm going to pull up a quick demo of the Quadrant UI. This is going to be where you can do all of your Quadrant-related things. So, I just want to show you guys how this UI works. Give me one second to connect here. Okay, so, I should be connecting. While that's coming up, I'll just mention that our web UI basically allows you to upload all of your data, and once again, you see some of our clients here. So, you can use your imagination. If you have unstructured data, just like these companies down here, what you can build is anything that one of these companies might want to build, right? So, we've got Deutsche Telekom, HubSpot, Disney, recommendation, anomaly detection, all that good stuff. So, this is the Quadrant UI. This is your dashboard, and if you open up the dashboard, I've already preloaded this with an H&M dataset here, and this is kind of how Quadrant works, right? You've got the image of the picture, or sorry, of the clothes, and then you've also got all of these columns kind of describing what the image is. And here I've uploaded about 25,000 pictures from H&M. And then what we can do is actually look at the graph of the items that we have inside of Quadrant. So, here, let me refresh this because it looks like it went left on us. Oh, sorry. So, here we have this dataset of images, right? And you can actually see how similar or dissimilar the images are. So, you can imagine, as our friends from Codeway are going to come up here, they might use something like this to be able to recommend images to users or to be able to find the quality of the image. But keep in mind, this data could be anything, right? So, here, for example, I have Twitch messages that I uploaded. That might be used for something where you're going to try to find Twitch messages that are similar. You could also upload PDFs. So, any kind of data that you have, basically, you could upload on Quadrant. And then not only can you do that, you can get info about your cluster and you can find information about the search quality. And then just quickly to show you how this is actually used in practice, we have this demo of e-commerce categorization. I'm going to type in iPhone. And then you're going to see, oh, iPhone lives in this area. And we can kind of see what is similar to the iPhone. Okay, accessories. So, this is one example of how Quadrant works with this clustering. But, as I mentioned earlier, some of our bigger clients are things like Perplexity, Johnson & Johnson, Bosch, Merck. These companies that have a bunch of data, they need to do search recommendation analysis. So, my name is Terry D'Amiba. Thank you so much. If you have any questions, feel free to reach out. That is Quadrant. Once again, two questions I would recommend you go back home and ask your team. First of all, what vector database are we using? Second, how are we storing these processes, right? Make sure that you're taking advantage of all the tools that Brian mentioned. Thank you very much. My name is Terry. It was great to meet you. Hi, everyone. At Codeway, we're passionate about transforming how AI interacts with user. And today, we would like to showcase how an advanced technology with Google Cloud help us to innovate, and AI avatars. So, without further ado, let me introduce who we are. I'm Volkan Eydengül, AI Research Engineer at Codeway. Join me today, Ur Arpaju, our lead SRE. So, let's start with the brief introduction of Codeway. At Codeway, we're leading a major mobile transformation on a global scale. And we become one of the top innovators in the non-gaming app space, and not just in Turkey, but worldwide. So, what exactly does Codeway do? We build digital products at the intersection of AI and mobile technology. And our approach is simple but bold. We aim to redefine industry standards, and we're doing it at a global level. And this approach has led to a significant growth. In just five years, we developed more than 60 apps, achieving more than 300 million downloads and engaging with over 10 million monthly active users. And today, we're honored to present our latest TalkingHead video generation platform, and our language learning app that is bundled with it. Can we increase the sound? Can we increase the sound? ...25. Our AI-driven avatars bring lifelike expressions and precise lip sync, making virtual communication seamless. From content creation to engagement, we help businesses and educators generate high-quality videos at scale. Let's shape the future of AI-generated content together. Thanks to our state-of-the-art platform, now we can create AI avatars that truly come to life. Our TalkingHead platform transforms simple audio to dynamic video output featuring lifelike facial expressions and synchronized lip movements. By automating video generation, this significantly enhances industries such as digital content creation and interactive communication, as in the case of language education. And businesses and educators can now effortlessly deliver personalized and high-quality video content without the hassle and the expense of the traditional workflows. So, how do we enable this platform? A method called Gaussian Splatting forms the backbone of our platform. Traditionally, rendering an image or video involves predicting a color value for each pixel. But in contrast, Gaussian Splatting renders static or dynamic scenes using compact units called Gaussians. And you can think of it as a digital and elliptical brush stroke. And we predict a pose, a color, and an opacity value for each Gaussian. Then we splat them on top of each other to create images. And actually, this is similar to painting a digital empty canvas with this digital elliptical brush strokes. And this process allows us to create detailed images efficiently. And it does not only produce high-quality images, but it also reduces training and rendering times, making it ideal for the real-time applications. And finally, I would like to mention how we harness the power of this method to enable our model training and rendering pipeline. So, we have an advanced model training and rendering procedure. First, we extract the speech features and the corresponding facial attributes. Then, we split these features into manageable chunks that are aligned with each other. In addition to these features, we have also our base Gaussians that represents the subject avatar's characteristics. And finally, and most importantly, we have our deformation network that modifies the attributes of each Gaussian to be able to reflect, to leap and jump movements for a given audio input. And through iterative optimization, our model learns nuanced mappings from audio input to the visual movements and ensuring that every generated frame is synchronized with the given audio. And the final step seamlessly combines all of these optimized frames, synthesizing realistic videos. And now, I will hand it over to Ur, and he will walk you through our model development hub and our also cyrus architecture. Thank you, Valkan. Thank you, Valkan. Thank you, Valkan. Thank you, Valkan. Thank you, Valkan. Thank you, Valkan. Thank you, Valkan. Thank you, Valkan. So, as Valkan mentioned, this project is an implementation of a rendering algorithm based on Gash and splitting to provide some real-time avatar generation. And to empower this product and platform, we needed to set up and use some sort of abstract platforms to basically abstract away the complex infrastructure and let our AI research team to work on what is important, which is like the building a high-quality platform on top of this. So, scaling from 100 to up to 3,000 FPS requires us to use many different cache mechanism, optimal GPU usage. So, there are lots of details in the back, but we need a very abstract way of API to provide these features to our AI research team. So, there are a bunch of steps involved. The data cache part is mainly for using different caching mechanism, as Brian mentioned earlier, to optimally load a high amount of data into GPUs. And after that, the kernel optimization involving the AI models to use and test different base images, like CUDA versions, also the frameworks they are using, so a bunch of different combinations in place. So, that means there is an iterative development also tick in this level. And as last part, the model compilation precision and the batch inference is basically testing different set of runtime parameters provided to Kubernetes pods. So, each of these experiments are pretty long iterative process, and they are basically conducted on an in-house platform, what we call the Codeways AI model development hub built on top of GCP. What we targeted here is that we still want to leverage very useful features on GCP, but we want to, you know, hide away all the things, and they will be basically consuming all these useful features. So, in order to make it possible, our DevOps and AI platform research team put great effort working together to abstract the FAD, abstract already underlying infrastructure components. So, basically, there are two different flows. One of them requires to provide an interactive development environment to our AI research team, which is completely based on notebooks. So, we basically provide individual notebooks to each of the person in the team, and they are able to use their personalized data access. They are basically able to select which base image they want to use. So, they specify their CUDA version, their PyTorch version. They combine and test with, you know, different combinations of these. And on top of that, they also have access to GCS in the back-end with different interfaces. So, obviously, they can use the SDK access, but if they want to do some parallel and fast reading, if they have such requirement, they can also use the GCS Fuse integration. So, we provide some sort of, like, marketplace for them to selectively enable the required access that they need. So, when they want to trigger a notebook, this basically lands a stateful Kubernetes spot on GKE, and they have entire file system access backed by person volumes, as well as GCS access by using the GCS Fuse as well. So, they can, as I said, on the runtime, they can select which one to use. And so, with the different access paths, depending on the read bandwidth, they can optimize their runtime. And when the initial development part is finished, they can now test different parameters at scale. So, for that, we are using an open source experiment tracking platform, which, again, aims to abstract away all the details on the GCS part. And eventually, they run things in parallel, testing different things, like hyperparameters, for example. And they collect all these feedbacks and compare them depending on the data they collect on a centralized dashboard. So, this was the platform that included the research part. But eventually, this platform will be provided as a service to our product, which is Lerna. And in order to make it compatible with our product, we used a seat of Google platform tools and services to ensure scalability and modularity. So, as seen here, there are different layers. We used, for example, different set of services to create a continuous feedback loop from speech-to-text service. And then we route it to an LLM service to create the actual reasoning coming from the user. And then we send it to text-to-text service. And eventually, we are sending the request to the talking at video generation service. And then this goes like a close feedback loop. So, we use, for example, the Firebase for front-hand integration and sync the data to the end users. Firestore to keep the user data and manage the user sessions. Cloud storage and CDN to store and provide the media files generated by our platform. And Cloud functions to orchestrate all the microservice logic in a serverless way. And Popsub to create, like, we use the Popsub as a centralized backend. And also, like, keep the state of the messages throughout these transactions. And eventually, we are also using a lot of GPUs for sure. And in order to empower the talking and video generation platform, we utilize a bunch of different GPUs, including, like, preemptive T4 instance type, H100 instances, and on-demand A100 instances. So, in order to, like, make them work in a harmonical way, we are using a custom autoscaler. And depending on the demand, it can select the most feasible and cost-effective GPU that we need. So, if we switch back to the, like, the product side, Lerna is the app that we integrated this feature. So, Lerna is our flagship language learning app used by millions globally as of now. And it is, like, designed to deliver a personalized learning journey, covering grammar, pronunciation, and vocabulary in all sense. So, with the support with the talking and generation platform integration, it allows Lerna to significantly provide more intuitive learning experience for our users. So, that means users doesn't need to just read or listen, but they can really interact with real-life, like, avatars to, you know, expand their skill set on language learning. So, before having, like, giving it back to Brian, I just have a very quick video showing that, how the interaction will actually look like in this product. Welcome to Google Cloud Next in Las Vegas. You're at a cafe. How do you order a latte? Um, give me one latte? Close! Try, can I have a latte please? Oh, got it. Can I have a latte please? Great! Luis, what do you do for work? I'm engineering Google. Almost! You should say, I'm an engineer at Google. Oh, got it. I'm an engineer at Google. Good job. Keep practicing every day with Lerna AI, and your English will be amazing in no time. Yeah, again, thanks for being us really here again on the last day, and I might give it back to Brian. E38-1 E ولا Eier Eier We'll be back in the next day, by your back. We look forward to getting into the five free fastest Rider model picture. In half, come across and say, you have?