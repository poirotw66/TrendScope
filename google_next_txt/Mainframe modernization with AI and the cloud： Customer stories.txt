 So, hi, everyone. Thanks for joining us today to talk mainframe modernization. My name is David Yalom. I lead product management for mainframe modernization at Google Cloud. I'm joined today by my colleague Rajesh, who leads our global delivery practice for mainframe, and two key customers who are going to share their success stories about how mainframe modernization was achieved. We have Claudio from Intesa, and we have Suman Deep and Retna from Ford Motor Credit. So, before we start, and I want to make sure that we give most of the time for our customers to speak and share their journey for modernizing mainframe workloads, I just want to talk, really, for just a few minutes about what is mainframe modernization in Google Cloud? What do we offer customers who want to embark on this journey? So, we know that mainframe modernization is not a one-size-fits-all solution. It's a multi-pattern problem. There are multiple patterns that need to be aligned with business priorities for velocity and modernization, and we offer several solutions and several products that are either first-party products, which are products that we built to help customers, or products from our incredible partners. Starting from our mainframe assessment tool, which uses generative AI, Gemini, customize with mainframe contacts to assess the mainframe, and basically reverse engineer applications, creating a knowledge base of the mainframe, business logic summaries, application specification, test cases, giving customers insights on what is currently running on their mainframe and paving the first step for a modernization journey. And then we have solutions for the different, most in-demand modernization patterns, again, some from our partners, some from Google Cloud ourselves, including solutions for re-hosting mainframe, augmenting mainframes with Google Cloud capabilities. Ford's success story today will focus on that, how they achieve augmentation by moving data warehouse workloads from on-premises to Google Cloud. Re-platforming, we have refactoring and rewriting. Refactoring is the algorithmic-based conversion, right? Like-to-like modernization, but you also have solutions leveraging customized Gemini models that allow complete reimagination of the mainframe applications. And also for customers who are looking to replace mainframe applications with a commercial off-the-shelf solution, we have solutions for replacing them. Claudia from Intesa is going to speak today about their journey for replacing their core banking with a commercial off-the-shelf solution. And also, very importantly, for testing, de-risking, validating, and certifying the migration, which is important, we have Durrun, which allows customers to run both applications, the mainframe application and the modernized application, in parallel for a period of time to certify that they're functionally equivalent and validated. And in Intesa's success story, also, they use Durrun to verify that their modern core banking in Google Cloud is certified by the regulators and valid and functionally equivalent to the mainframe solution. Just focusing on our AI solutions from mainframe for one minute. So, you know, at Google, we have a full AI stack, starting from best-of-class AI infrastructure, world-class research, models, Gemini, and tooling, right? And this is the foundations for the products and platform that we in the mainframe modernization group are building on top of, making them enterprise-grade products for mainframe customers. Notably, our mainframe assessment tool for assessments and our mainframe rewrite solution, which is an IDE plugin for actually modernizing mainframe code. We see generative AI for mainframe modernization as a two-step process. Again, starting with assessment to understand what the application does, inputs, output, data flows, generating application dependencies, understanding the business logic, peeling the layers of the onion, so to speak, right? And then from that, using all of those outputs with our mainframe rewrite solution to reimagine mainframe applications for the age of cloud. So it's a two-step process. And what we want to offer our customers is an holistic solution combining our products and partner products as well to help facilitate the end-to-end modernization journey. So for example, for assessment, customers can use our mainframe assessment tool. For modernization, they can use two of our modernization products for code transformation. Rewrite, which is Gen AI-based reimagination, and mainframe refactor, which is algorithmic-based, like-to-like modernization of, say, COBOL to Java. For testing and certifying, we offer dual run. You can use it throughout the modernization process to test certain modules, application workloads that are being modernized, and then certify it by running both systems in parallel for six months, a year, a year and a half, however long is needed to make sure they're functioning equivalent. And then, of course, go live with the modernized application in Google Cloud. And with that, I want to introduce Rajesh, who can speak more to some of our customer success stories. Thank you. Thanks, David. I'm glad to see some familiar faces here. So as David mentioned, we have a lot of solutions and a lot of different patterns. And what we are showing here is how we have a lot of different customers embarking on the mainframe modernization journey using all of our solutions or partner solutions. So if you see Ford Motor Credit, you will hear from them about using the mainframe connector to help them migrate data. You will hear from Intesa about how they're using dual run to de-risk the solutions. We've already had Santander present here before about how they use dual run to actually migrate workloads to micro focus. And Boavista, where we actually did a rewrite for them. And then we have other projects going on with a lot of other customers who are not yet references. Hopefully, in the next couple of years, you will see a lot more names and logos there. So that was the main intent of this to show that there are a lot of customers actually embarking and working with us as we go forward in this. The most important thing that I want to highlight in this is Google doesn't do this by ourselves. We work with a lot of partners. So if you see here, you have a lot of logos here with a lot of partners. This is not the entire list, but these are all the key partners that work with us. They provide excellent delivery capability in mainframe modernization. Mainframe modernization requires a lot of services, and the services industry is actually working very closely with us on this. Next is, along with that, we also have a lot of technology partners as we go along. The mainframe estate, for example, is very varied. A lot of technologies. You have Assembler, PL1, SAS, IDMS, Broadcom, Datacom, all of those products. So now, we don't solve all problems by ourselves. We actually work with technology partners who actually bring all of these solutions to the table. So here are some of them. Rocket, Mlogica, Precisely, Stadia, some of the partners we work very closely with. The next one is, this is a new offering that we have actually brought out. It's being announced at Next. This is a partnership with Accenture, EPAM, Kindle, and ThoughtWorks, where we are actually partnering with them to help do rewrite using generative AI. So we'll have some modernization solution offerings that are going to be announced at Next, and you can leverage that as part of your modernization journey. And lastly, you know, we are also launching Mechanical Archard's generative AI-based rewrite platform at Next. There's a breakout session tomorrow. You should actually go and attend that. I don't want to give you all the details about what they provide, but actually Rob, me from Mechanical Archard, is going to be presenting all the capabilities. Now, I don't want to take too much time, so I'm just going to quickly highlight some of the success stories and then hand it over to the customers with whom you are all actually more interested in hearing from. So first thing is from Dual Run. I just wanted to give you an idea of what Dual Run does, and then Intesa is going to be talking about Dual Run. Dual Run is you have a mainframe application on the left, you have your modernized application on the right side running on Google Cloud, and it can be a refactor, replatform, Cots product. We are able to handle protocol transformations because you have a lot of integrations in your mainframe space. We are able to handle all of that, and we are able to compare all the data. So now you have two systems running live, and Claudia will talk about how they have achieved this. The next one, quickly, before I hand it off to Claudio, is about the mainframe connector that Ford is being used. This is about taking data. Mainframe has a lot of data, a lot of data warehouses, a lot of proprietary data formats. Now, how do you get that off the platform quickly into Google Cloud? So we have a free solution. It's called the mainframe connector. A lot of customers have built it. It was actually built originally at the world's largest retailer, and now a lot of other customers are using it. Ford is going to talk about how they're using it. So without much ado, I'm going to hand it over to Claudio. Thank you, Rajesh, and good morning to everybody. In 2022, InterSan Palo announced the launch of the Easy program. Easy program consists in creating a new fully digital bank, whose name was Easy Bank, and to use that bank as an environment to create a new IT platform to leverage and to accelerate the transformation of the full Intesa San Palo group in Italy. The final results that were expected at the end of 2025, because the plan is four years plan, and it's ending this year, are those that you see there. So the bank was expecting overall 800 million savings per year, started for 2026. of course, not only in the IT, but for the full review or rethinking of the service model, to have a bank that was at a cost-incaration, which is below 30%, which is very aggressive for a retail bank, having a faster product generation, and also onboard additional 2,000 people in the IT with the program, and to onboard them and to strengthen the IT capability of the group. And actually, we hired today around 2,000 people. Easy Bank, well, it's designed, actually, it's live because it went live in the first quarter of 2023 with a mobile-first service model, which is very different from what Intesa San Paolo does in Italy today, which is a traditional incumbent bank. Simplified product offering. Redesign HEP, which is best in class, also in terms of speed of onboarding. Cloud Native technical backbone, and that's what I'm going to talk you about later. And then, a very fast delivery timeline because the bank went live in less than 12 months. A completely new bank with a new technology platform. The aim of the program is to change the technology platform of the group from a platform which is, that was, because today is already moved, mainly moved, heavily on-prem with more than 50% of the systems and the application of legacy applications with a relevant portion on the mainframe. Just to give you a scale, we have 250 million lines of code of COBOL, 50 million lines of code of PL1. So, that's the scale of the systems we had on the mainframe. And to move it to what you see here on the right side, to have 70 to 80% of the applications that are based on open technologies and mainly running on public cloud. And by the way, we selected, that goes, before it goes to 2020, Google Cloud is our preferred cloud provider and actually most of the workloads are currently running on Google Cloud. And Google Cloud also opened two regions in Italy and we are using them. So, we also did the repatriation because initially we installed everything on Frankfurt because in Italy there were no regions and then we moved them to Italy for sovereignty reasons. Now, how did we do it? So, we selected some platforms, cloud native platforms from the market, thought machine for deposits, quid for lending. We developed a set of additional functionality in-house in microservice architecture and with this we are touching the core banking and redesigning the core banking of the bank. As said, this new core banking went live with EasyBank and now we are on the journey to move it to Intesa Sao Paulo. And changing the core banking, of course, we had also to change a set of additional capabilities surrounding them because if you change the core banking, which was on the mainframe and you move it to the cloud, not all the other systems can stay as they are because you need to change also part of the surrounding systems. What has been the journey? So, 2023, 2022, 2023 has been used to launch, to create the platform, to launch EasyBank. It took around one year to do it. Additional nine months to scale it up in terms of product offering, we had the lending, we had the investments. And then, 2024 and 2025 have been used, we are still in the progress, to move and to do what we call the forward fit, that means to move the platform of EasyBank to serve in Teza San Paolo. And for next year, also the wealth management banks because Intel San Paolo has two wealth management banks in Italy, which are also them using the mainframe core banking. That's the journey where we are. When you think to legacy modernization, so you have a core banking based on the mainframe and you are taking it out, a relevant part, you have four, at least we face and I think they are pretty common, four key challenges. The first is latency. Hey, you know, on the mainframe everything is very fast, everything is very closed and the communication at the end is instantaneous. If you take out deposits, if you take out lending, there is a latency. It's the physics, there is no way you can avoid a latency. And you have to understand how to deal with the latency. What do we do? What we did is before starting the program, we tested a delay. So we said, okay, current account is moving out on the mainframe. Let's see what will be the impact on the system and how elastic is the system if there is a forced latency. And so we put a slip at the beginning of every call to the current account. Everybody who was calling current account was having a slip. And we saw the system was actually already very elastic. It was not true that latency was a big issue. It was a problem in some parts, and of course we had to manage them, and this gave us the opportunity to transform other pieces. But the system was elastic in a way that I could slow down current account on the mainframe Monday morning, which is the day before a bank is critical, and the system was still working. It was a bit slower, but if I give you 50 milliseconds of delay in the response, you don't see it. But the system was working, so the system was already elastic enough to end at that. Second point is transaction management. On the mainframe, everything is very nice. The transaction we're starting the operation is committing for everything. The coherence of the data is ensured by the platform, which is something that is lovely. When you have a piece of the system that is going out on the mainframe, you're losing this capability. There is no technology that can ensure that that can work. So what do we do? You have to change the way the applications communicate one with the other, and in our case, we introduced the pattern of the idempotency. So every application is using a transaction key that transaction key is managed across the different systems, and using that transaction key, you can ask the system what is the status of the application, what is the status of that specific transaction. And that's the way, so using the idempotency, we could change the surrounding application, breaking the concept of the logical unit of work that you lose when you go out of the mainframe. was a big cultural shift, because especially it was used to work on the mainframe, that was given by the platform. Third, we have around 400 systems that were connected to the core banking, and when you move out the core banking, they have to be changed. And we didn't want to do a big bank in a moment when we take out the core banking and we change for a 400 system, because it was too risky. The bank and the regulator will never have accepted that. So what we did, we created a layer of API, which are behaving like the new core banking cloud platform. They are running on the cloud, but they are still using the old system in the back end. And we are asking all the 400 systems to adapt. It takes 15 months. All the other systems are gradually adapting to this new pattern. And we are planning to finish and to complete it at the end of May. So at the end of May, we will have the full information system going through this abstraction layer. That's how we are managing it gradually. And the last is testing. There is no way you can ensure that doing user tests, you can test properly a system that is managing 15 million accounts and 10 million transactions per day. So what we said is we will do a parallel run with the production. And that's the way we are testing the new core banking. So what did we do? On the left side you see the production. Production is still the production. The systems, which are the red ones, which are all the surrounding systems, are going through the abstraction layer. And the abstraction layer is going to the main frame. That's what we have today for 90% of the workload of the bank. 10% is missing, we will close it by May. Then we created the parallel run, which is the right side of the slide. Everything that is going through the abstraction layer is replicated to the parallel run on the right. On the right we have the new core banking. Everything which is happening on the left and on the right of the picture is going to the do a run that Rajesh presented. So everything happening left and right is going down. Hundreds of millions of operations are going to Google Cloud Storage, storing Google Cloud Storage, and the do a run of Google is checking the coherence between what is happening on the left and what is happening on the right. The target is to be at 100%. When we are able to show that we can stay stable 100% for a period, we will be confident to switch the core banking. Every day we see a picture like this. It's a report which is extracted from the do a run that is showing that, for example, in that case, we have the do run was very far comparing 83 million of records, 82.1 were matching, and 19,000 at that date were not matching. So in that day we had 99.97% of match. So 99.97% of what was happening on the left was also happening the same on the right. There is a piece which is missing. in this moment, on the top part, which is not coming from the do a run, is how much is already going through the cycle, and in that case was 59%. 59% of the bank was going through the comparison. Today the number will be closer to 92, 93%. This is how we are managing, and as Rajya said, the do a run is today our risk mitigation testing validation that the parallel run is working. End of year we will probably do the real migration. There is time for questions later, right? Okay. I end over to the colleagues of Ford. Thank you. Claudio. Hello. Good morning, everybody. Thank you, Claudio. My name is Sumandeep Singh, and I am the VP of Engineering and head of billing and payments platform for Ford Credit, and today with me is my colleague, Ratna Valabineni. She is a senior data engineering manager on Google Cloud platform journey. For those of you who do not know Ford Credit, let me just give you a little background. We are a financial division of Ford Motor Company, and we have been serving our customers proudly for the last 65 years. We are a global presence, and as you can imagine, our systems are dated to have been supporting us for all of those years. It is our immense pleasure to be here and talk about a very important topic that is gaining a lot of momentum and attention in the market, how to migrate from the mainframe to cloud. Ratna and I will walk you through one of our journeys. I see the room is full. I am sure everybody is a mainframe experience engineer sitting here, so I don't have to tell the crowd that mainframe over the last several decades has become a backbone of a lot of industries, and ours is one of them. Because it is sole legacy, it is very risky to rework and refactor. I can hear some music too. Please ignore. Background music, yeah, our project feels like pretty much that sound. So, yeah, so re-engineering or re-working these pipelines is a very risky business. Also, mainframe solutions are so deeply integrated into our business operations, we really don't have time to stop everything and take a year, year and a half to do this migration. We are, just like a lot of other companies and our divisions, in the middle of a massive digital transformation journey. And our systems are not scalable enough to provide what we need to provide to our customers. And so, just like a lot of other organizations, this has become our strategic priority to move from mainframe to Google Cloud or other cloud options that we have. Primary is Google Cloud. And today, we will walk you through one of those journeys. It is just one of the many journeys that we are on. And we'll just give you a little bit insight, a little deeper look into what was the problem that we are facing and how we are going to solve that. Next. I'm just figuring out how this works. Okay, it works. One other thing I wanted to mention is this is not just a technology shift. This is definitely a strategic decision that drives the need of improving scalability, the agility, cost efficiency, and really accelerating innovation, which is so important in today's world. Not just digital transformation, but we are having AI all over the place. And we have tons and tons of data, but mainframes do not allow us to use that data to really make smart AI solutions. So the mission that we are going to talk about today is one of our pretty complex projects. It is, we have about 25 years or old data warehouses, which where mainframe plays a very interesting role. And the code is so old, we don't have any documentation on that. We don't have any kind of code versioning, and the subject matter expertise has been depleting in our organization. So we have the task of moving all of this into Google Cloud within 12 months, and making sure that we are not compromising data integrity or causing any kind of data loss, which can cause a lot of problems for our business as well as our customers, since we are a financial institution. And in several markets in Europe and China, we are actually a bank, so we are heavily regulated organization. So that was the mission. And key focus areas where mainframe connector help us is we have to consider really how is, how do we detangle this complexity that has grown over the last 25 years? And is there an automated way? The Gemini solution was not available back when we started this project. And so we had to come up with a very meticulously planned and tested, full-proof execution plan to make sure that when we reach the end, we are actually successful. So we focused on things that Ratna is going to cover in a few minutes, I don't want to take away her recognition. But I do want to talk about some of the constraints. The path of re-engineering, which is, for any technologies, the first path is, let me just rewrite this code because nobody understands what this is. We have a hard time editing it. That path was a very risky path. We could not afford to stop the business and re-engineer a code in a way there were gaps left or there were inaccuracies in the final result. So we could not take on that risk. We did not have time or engineers, enough resources available for us to actually even undertake that. So what we needed was a reusable conversion pattern that can give us a large scale of automatic conversion and then we could augment it with custom development where we had to close those gaps. And I would say mainframe connector was one good choice for us. Mainframe connector does come with multiple options. Radna will talk about what we used. And very proud to say where we stand. There was a lot of hard work that went in. I do see our Google partners right here in the front rows that played a significant role along with our engineering teams in helping us solve this problem and build a connector that is really helping the progress of this project. I'm also very proud to say that we are more than 50% done and the results are very promising. So we can't claim that everything is done, all problems are over, but we are making good headways in the right direction. With that, over to you, Ratna. Hey, good morning, everybody. I'm the technical lead for our legacy warehouse migration to Google Cloud platform. Thank you, Sumandeep, for giving us a great insight into why we had to embark on this journey. I'm sure you all know how complex it is to modernize a legacy system. When I was approached to take on this role, I was a bit apprehensive as I felt like I literally had to move a mountain that has been rooted in place for 25 years. As I had to form a new team, I reached out to some of our data engineers and they literally tried to run away as far as possible from me as they were working on cool technologies and did not want to come back to mainframe. I convinced them that solving the most complex problems also gives the most satisfaction. And that is how we started. This migration involves conversion of thousands of mainframe pipelines and more than 30,000 database objects. The focus of our conversation today is on mainframe conversion. But we also had to convert thousands of pipelines that were not running on mainframe. Due to the compressed timelines, we could not take the risk of converting all our pipelines into Google native solutions. So we started our conversion with the Google Cloud mainframe connector as we felt it would help us move faster to the Google Cloud platform. We had the choice of using the connector in the local mode or the remote mode. The most pragmatic approach for us was to use it in the local mode as it runs on the mainframe and we already had the infrastructure available for us to start the conversion. Okay. We had to build a reusable pattern, something similar to the Ford assembly lines, to convert our code faster at an accelerated speed. The pattern needed to include a solution for a variety of mainframe pipelines, data formats and dependencies. For the design strategy, we took a couple of things into consideration. Ensuring the data parity between both the systems while minimizing the data discrepancies. Ensuring the job run times for GCP met or exceeded that of the current pipelines, especially when we had to run thousands of GCP jobs. As a financial institution, our month ends, quadrants and year ends are crucial and they process larger volumes of data when compared to the daily runs. Yet, we had to make sure that all the data was available for business consumption by 7 a.m. EST. So let me give you a quick overview of how our current pipelines were built. On the mainframe side, we have JCLs or the job control language scripts, which extract data from DB2 and load it into mainframe files. And from the mainframe files, the data is loaded into our legacy warehouse. This is a very common pattern. And this is what we had to do to build our GCP pipelines. The data in our source systems is constantly changing. In order to make the data match between the two parallel systems, we had to reuse the same snapshot of data. So we leveraged the mainframe files to process both the systems. This introduced another layer of complexity. The mainframe files could not be read concurrently by both the processes that had to load the data into on-prem as well as the GCP system. So we had to introduce a copy step. We were dealing with pipelines that ran significant amount of time as they were loading data from multiple large number of DB2 tables. So we could not wait for our current pipelines to finish before we started off the GCP jobs. Hence, we came up with an approach where we could kick off the GCP process as soon as the copy files were created while we simultaneously ran out legacy processes. Okay? Sorry. Sorry. Give me one second. Okay. We thought we had a foolproof approach. But once we started our conversion, both our on-prem pipelines and GCP pipelines ran into performance issues. They ran much slower than what we expected them to. Some of our mainframe files were stored on tape. Copying data from the tape files was notoriously slower than copying data from disk files. The combination of transferring the mainframe files to Google Cloud Storage Bucket while converting it to ORC format caused additional bottlenecks and overheads to the GCP loads. We had similar performance issues when we were bringing data back from BigQuery into mainframe files. This was essential for us as some of our downstream applications were still consuming data from mainframe. So we had to generate the files. Okay? So we were partnered with the Google professional team and mainframe product team and we refined the design. We introduced parallelism which pushed multiple partitions of the mainframe file simultaneously into Google Cloud Storage Buckets. The change along with the usage of disk files instead of tape files improved our GCP loads by 40%. Coming to BigQuery export performance, we initially suspected mainframe CPU or network bottlenecks to be the root cause. We were quickly able to eliminate the network bandwidth as a mix from the mix by pushing a huge file from Google Cloud Storage into Linux server which ran in a couple of minutes. This helped us focus on the real issue. Data from the BigQuery tables was written to a temporary buffer which was in turn sent to a mainframe file. The speed at which the data was sent to the mainframe file was very slow due to which the buffer was always full. We now have a parameter that can be used to change the speed at which we write to the mainframe file. So the processes are now running, the BigQuery export processes are now running 90% faster. Resolution of all these performance issues was very important for us to start our implementation. I do want to mention one thing quickly that I shared a very simplistic version of our process because this is where we had to fix a lot of performance issues. But once the data comes from the source applications, it travels to multiple layers of transformations, around 20 plus layers. But once we fixed these front end issues, all the others started working seamlessly. So I'm not covering that part as much. Lastly, I wanted to share that once we complete our migration, the only mainframe CPU intensive process would be the exchange of files. Every other process would be using only GCP infrastructure. Okay. Thank you for listening to us. And I'll give it over to Samandeep to wrap it up. Thank you, Ratna. I know we tried to simplify this as much as we could, but there's still a lot of complexity. So I will quickly wrap it up. What was very important or the key takeaways for us is this journey is quite demanding. This journey can also break the confidence. But one of the things you have to keep in mind is, which is what our team did, is you need to keep moving forward and take a proactive approach and collaborating with the right partners. Like in this case, mainframe connector that Google team brought in initially lacked many features or many things that we needed for our migration. So we worked very closely with them. Engineering came in. The team sitting here, they helped us modify that mainframe connector so that we could actually use it and address the gaps that we had. So leveraging them. So just to wrap up the key things, I would say our learnings are leveraging migration accelerator is crucial for efficiency and as well as for speed. You cannot solve each problem in a different way. And we were able to achieve it. Proactively identifying your problem areas, especially the high-impact problem areas at the start, is very important. And that's a lesson learned that sometimes you think your first solution is going to work, but we saw that that didn't. So you had to iterate through finding what was the right solution. And then establishing a validation process and the right tools. Earlier, Claudia was mentioning there are parallel environments. We have to do exactly the same thing because data has to match 100%. So identifying what your testing strategy will be, what tools are there that can be used to create that will be very important. And then developing a comprehensive risk mitigation strategy, not saying that you'll not find new ones, but that is something that we invested time early on and that really helped the project move forward. And then last but not the least is really understanding what is the impact to your users. Although the intent was this should be seamless to the users, but there is still changes because inherently the technologies are very different. So really understanding, recognizing there is a whole change management effort that is needed and retraining of the customers is needed can also go a long way. So I think we are out of time. If there is time, David, for questions, we can take it. Otherwise, I'll just hand it over to David to wrap up the session. Thank you all for listening. Hope you got some good lessons from the session. Thank you. Thank you.