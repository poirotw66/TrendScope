 . Hi, everyone. Welcome to this database spotlight. I'm thrilled to share with you today the latest innovations in Google Cloud databases and how they're empowering you to build data-driven applications in this Gen.AI era. Today, I'll be joined on stage by Kat Coleman, UX Director at Google Cloud. We'll also hear from Anil Madan, Senior Vice President, Cloud and Data Platforms at Walmart. And also, we're going to talk about the Oracle partnership with Jay Evans, Global CIO and Executive Vice President at Oracle Cloud Infrastructure. And lastly, we're going to see some awesome demos by Gabe and Billy. So let's get started. We're witnessing the explosive growth of Gen.AI. Businesses are moving from experimentation to building real-world agents and apps. However, to build truly intelligent applications, AI models need access to high-quality, real-time data, the data that's stored in your operational databases. At Google Cloud, we offer a robust suite of industry-leading databases built on planet-scale infrastructure with AI at its core. And our momentum is undeniable. For the fifth consecutive year, Google was named a leader in the Gartner Magic Quadrant for cloud database management systems. And for the second year in a row, we were positioned furthest on vision. This is a testament to our commitment to delivering a truly innovative and unified data and AI foundation. But don't just take our word for it. Let's hear from our customers who are pushing the boundaries of what's possible. What's possible? Neuro's mission is to better everyday life with robotics. Bayer Crop Science is a pioneering force in the agriculture industry. Causal is the next step into what a spreadsheet is meant to be. Synergy Bank is going to create a new way to do banking. Character AI is a generative AI platform. Our move to AlloyDB, coupled with our use of BigQuery, has enabled a higher level of operational excellence in our collaboration with data scientists across Bayer. Having something like Cloud SQL, its managed solution, allows us to focus on the important things when it comes to databases, which is, is everything working in their optimal way for the business? I would position AlloyDB as the ultimate relational database. It has a heritage of Postgres, but all the genius of Google. AlloyDB's new scan index outperforms both IVF and HNSW. A single search can return over 20,000 results with high precision. Spanner allows us to ingest terabytes of data every day without any worry that we're going to bring the site down. With Gemini, it means that I have a sidekick that sifts through this enormous amount of data and just presents me the key results that I need. We're exploring where we can leverage generative AI to fast-track production of artifacts to help with that decision-making. Character AI's goal is getting to a billion users. And with Google Cloud, I know we can get that. Well, it's just awesome to hear from our customers. And as you saw, they're not just developing applications, they're reinventing their industries, tackling massive challenges, and driving groundbreaking innovation. So today, you're going to hear about three key areas of focus for us. First of all, how we're helping our customers build Gen.AI apps and agents. Second, how we're delivering more APIs to developers. And lastly, how we're helping organizations accelerate their migration and modernization to Google Cloud. So let's dive right in. Databases are a primary component of agentic workflows. And the best agents are going to be the ones that use real-time data to enable high-quality agentic decisions. Last year at Next, I talked about how we brought vector processing to all of our databases. And we've seen tremendous adoption across our customer base. We've also continued to enable seamless connections between AI agents and enterprise databases. And today, we're excited to announce support for model context protocol with MCP Toolbox for databases. MCP Toolbox for databases offers simplified development with reduced boilerplate code, enhanced security, and end-to-end observability. It supports Allodb, Spanner, Neo4j, Cloud SQL, DGraph, Postgres, and MySQL. And lots more coming soon. And we've actually open-sourced this framework. So if you wanted to add another database, you can easily add another database to it. And by implementing the MCP protocols, the tools you build can be used by any MCP-compatible application, including the agent-to-agent framework that Thomas talked about in the keynote today. And we didn't just stop there. We've continued to invest heavily in Allodb, our supercharged Postgres-compatible database that provides the best of Google with the best of open-source Postgres. We've been investing in Allodb capabilities to make it easier for developers to build these intelligent apps and agents. Let's walk through some of the latest AI innovations we're delivering in Allodb. As you heard from Thomas this morning, with Google Agent Space, we're putting AI agents into the hands of every worker. And today, we're excited to announce that we're enabling Agent Space to search structured data in Allodb. Agent Space brings together Gemini's advanced reasoning with Google quality search and enterprise data. And now you can activate all your data stored in Allodb. This empowers users to combine their real-time structured and unstructured data in creative ways. Second announcement, we know that database developers are looking to enable more flexible ways to interface with their database. while staying secure and accurate. Last year, we talked about natural language support inside of Allodb. We've been iterating on this, and today I'm super excited to announce the next generation of Allodb AI natural language. Compared to standard natural language to SQL, it goes beyond just interpreting database metadata, enabling you to query structured data, and to query structured data in Allodb securely and accurately. It does that by not only supplying context and having interactive intent clarification when querying the database, but it also supports multiple modalities when customers are actually querying their data. Third, building intelligent apps requires powerful vector search capabilities. We've seen Allodb's vector search adoption increase sevenfold between last year and today. And today I'm happy to announce that we now offer optimized SQL functionality spanning vector search, structured filters, and joins. With these innovations, Allodb scan index offers up to 10 times faster filtered vector search queries at higher accuracy compared to the HNSW index that comes standard in Postgres. Now that's not all. We've also partnered really closely with the Vertex AI team, with the Google DeepMind team, to bring three new AI models right into Allodb. First, a cross-attention-based Reranker model to improve relevant search. Second, a multi-model embeddings model that supports text, images, and videos. And a new state-of-the-art Gemini embeddings text model. As a result, you can now easily add intelligence into your apps across multiple modalities, such as images and text, with high efficiency and accuracy. And finally, we're helping infuse AI experiences directly into the apps. We're thrilled to announce today Allodb AI Query Engine. With the AI Query Engine, we enable developers to freely use natural language expressions and constructs right within the SQL queries in a very, very natural way. At the heart of this, our foundation model powered semantic operators that run side by side with traditional relational operators in the Allodb AI Query Engine. We're basically starting to see that original vision around SQL from 1974 of being this structured English query language come true. It only took us 50 years to do so, but now we can truly put English into SQL. And actually, it will also work with other languages, not only English. So all these innovations are helping customers quickly infuse AI into their apps. For example, Target used Allodb to improve their online search experience. They used the ability to combine their structured and unstructured data to enhance the accuracy of natural language querying on their online retail site by 20%. Think about what that can do to drive revenue. So just to recap, lots of announcements around Allodb. We integrated with Agent Space to make it easy for any worker to search data in Allodb. We're delivering the next generation of Allodb AI in natural language. We optimize SQL processing with vector search, supporting three new AI models inside of Allodb, and add a new AI Query Engine capability right within Allodb. And to bring these Allodb AI innovations to life, let's see a demo in action from Gabe. Thanks, Andy. Hi, folks. So I'm on stage quite a bit. And as you can see, these lights get really harsh. And I'm always on the lookout for products that can help me look my best. Here we have an online cosmetics and skin care store, Symbol Shops. So why should you care? Because this online shop happens to be powered by Allodb AI's natural language and web search. Let's go ahead and dive in, and I'll show you how Allodb can help you improve your web applications. I spent a lot of time in the sun. When I was younger, and while I can't turn back the clock, maybe I can help find some makeup to help with that. I'm going to go ahead and filter our store on preventative aging. So you're going to see I get stuff like an Express Smoothing eye mask. I've got Immortale Divine Cream. I don't know what that is. Don't ask. What I want to call out here, and the reason I'm calling out these products, is because they don't contain the words preventative or aging. A traditional keyword search, of course, would miss items like this. But with a vector search, we're capturing the semantic meaning behind what I'm searching for. Like, you know, I happen to know that smoothing eye masks help with fine lines, so they help make me look younger. So I want to show you behind the scenes what's going on behind these queries. What we have here is AlloyDB Studio. Studio is a GUI that lets you run SQL queries right against your instance from within the console. This is the query that I called on what you just saw. What I want to highlight here is this embedding call, if you haven't seen it yet. This call is generating an embedding, leveraging AlloyDB's model endpoint management feature, which means it's calling a text embedding model on the Vertex AI from inside MySQL. Let's go ahead and run this query. And you're going to see that I get the same results that we saw in the app. Here's my express smoothing eye mask that I mentioned. So now, back to the app, what I didn't show you before is that I've got a profile in the app. And I'm storing things like my type of skin, you know, I get shiny and oily. I love animals, so I want to be sure that anything I put on my face is going to be vegan. And if I go ahead and turn on personalized results in the store, you're going to see that now all of these match my preferences. So what does that actually do to our SQL? Our second query is the same as the first, but now you see I've got this where clause for my oily skin and vegan products. And if I run this one, these are the same results that you just saw. Great, these are really good results, but being good isn't enough. I also need to get results quickly because no one wants a spinning wheel in their app. Our scan vector algorithm brought inline filtering. This added the ability to apply filter conditions like my preferences with the vector search rather than having to pick before or after. And this is faster, but not for every single query. Today, I'd like to announce that we have adaptive filtering where the query optimizer is going to choose between applying the filter in line with the vector search or before based on the workload characteristics. Now, I know not everyone in here is an expert on query planners. I'm certainly not. So let me just show you what the performance looks like with and without the scan index. In the first query here, I want you to notice that we're telling it, do not use the scan index. So this is that query without it. And when I run it, it takes about 108 milliseconds to run. Now the next one, same exact query, but now we're telling it to actually use the scan index. And running this one, 6.5 milliseconds. Now, it's about 16 or 17 times faster with the LEDB scan index turn on. In this demo, the difference between, you know, 100 milliseconds and 6 milliseconds is because our data set's not that huge. It's about 5 gigabytes. But imagine now if you had a huge amount of data. This could take a query that takes 16 or 17 seconds down to a single second. I might not be able to notice 100 milliseconds to 5 or 6, but 16 seconds to 1, I will for sure notice. So now I want to take us back to the app and show you a little bit more about, behind the curtains, how our magic is working. We've already found a bunch of products to try. And I actually saw one that I've used before, the Symbol Prim Resurfacing Toner. It's a really good product. So I want to find more products in this line. So now I've got a chat built in here that I can actually ask it. And I'm going to use a clipboard because no one wants to watch me trying to type. Are there any products in the Symbol Prim line that fit my profile preferences? Here's where I hope testing an LLM on stage wasn't a bad idea. And sure enough, I find more to try out that fit my preferences. We even built in a button into the app so I can show you what the SQL ran that was executed by my statement. What I want to call out here is because AlluDB AI understands the schema of the database, it understands how to map my natural language question to column names. You can see in the query here, it's mapped Symbol Prim to a product name. This interface is like having the power of Google right from within my database, which is awesome. All right. So one area that I'm definitely not good in is making sure that I've got the right foundation under all of this stuff. I'm still hopeless. But last year, I gave my laptop to the makeup crew backstage. And I had them just, you know, told them, buy the right stuff. Just do this. Of course, that was a really long time ago. And I have the memory of a goldfish. But I can ask, what shade did I order of that skin tint? Our assistant can find the right shade what I ordered before so I can reorder it. It's awesome that it found it. And when we look at the SQL query it ran, it's able to go into my secure orders. The reason it's allowed to do that, it's able to use my order history because the app is authenticated to me. That's where AlloyDB AI's natural language security capabilities really shine. Now, while I'm here, sorry, while I've got you, I'm going to go ahead and see if I can order it. Because why not? I'm here. And sure enough, it's already got my credit card information, all of that. Great. Let's go ahead and buy it. And sure enough. So my AI assistant has managed to buy it for me. Oh, hi. Cool. I do have to add a disclaimer. We can't actually make your deliveries faster, only your queries. So thank you for letting me show you all of the great enhancements we've made to AlloyDB AI's natural language and vector search. While I don't have the time right now to show you the AI query engine, please go see the latest vector search and AI innovations in AlloyDB. That breakout session is on Friday and you can see all about it. Check it out. Thanks. Thank you so much, Gabe. What an awesome demo. You can all build these kind of applications now. So many of these capabilities are available in preview. Sign up today and let us know what you think. Now let's switch gears a bit and talk about developer freedom and APIs. For that, I'd like to welcome Kat Coleman, UX Director at Google Cloud. Thanks, Andy. It's great to be here. Now we know that many of you have come to Google Cloud because of our dedication to openness. That means the freedom to choose the technologies you want, whether they're open source, Google, or partner technologies, and a commitment to play nicely with other platforms. So let's see how our databases give you freedom as a developer. First, we offer a choice of database engines and data models. Databases are not one size fits all. That's why we provide the freedom to choose from popular engines in addition to our own cloud-first databases, Bigtable, Spanner, and Firestore. The next step is platform freedom. We launched AlloyDB Omni so you could run AlloyDB anywhere. On your laptop, on the edge, and in other clouds. Just last year, we partnered with Ivan to announce Ivan for AlloyDB Omni, a fully managed AlloyDB Omni service running on AWS, Azure, and Google Cloud. And today, we are excited to announce that this partner solution, Ivan for AlloyDB Omni, is generally available. It now offers marketplace availability across all major clouds, 4.9's uptime SLA, and consistent compliance and security. So you can build high-performance, AI-powered applications wherever you want. Freedom also means the ability to mix and match data models, whether in separate databases or within one database. That's why we added multi-model support in Spanner, our cloud-first database. Spanner brings together relational, graph, key value, full-text search, and vector search into a single database. And today, we're excited to announce that Spanner vector search is generally available. Designed to address the most demanding AI workloads at virtually unlimited scale. Customers such as Snap have adopted Spanner Graph to support their high-value use cases and applications. And just a few weeks ago, we announced Spanner Graph RAG, so you can ground foundation models with rich contextual information stored in a knowledge graph. But we didn't stop there. Today, we're announcing graph visualization for Spanner. It allows developers of varying technical expertise to extract valuable information from their Spanner database. You can simply drill down into specific nodes and relationships, filter and highlight relevant data subsets, and explore the graph through intuitive navigation. To hear more about AlloyDB Omni and Spanner in action, I'm pleased to welcome Andy back on stage, and to welcome Anil Madan, SVP Cloud and Data Platforms at Walmart, to chat about the road we've traveled together. Hey, Anil, thank you so much for joining us today. Thank you, Andy, for having me. Excited to be here. So it'd be great to just share with the audience, you know, what the Walmart journey has been and how Google fits into it. Absolutely. So from a context, I hope everybody uses Walmart, but we are the world's largest retailer in the world, and we serve millions of customers every day. We've built an incredible platform that powers everything from our e-commerce to in-store systems and enables omni-channel experiences. Now, what has that journey been like? We've built one of the most, the biggest, most complex hybrid multicloud in the world, where we call that the triplet. Now, what that triplet is really composed of, it's some interesting pieces of infrastructure. An open source cloud management platform, an open source-based container platform called a Walmart Cloud Native Platform. And then the third piece is a data abstraction layer. And that's why, what we do in that is we bring in three cloud providers, with Google being one of the public cloud providers with our private cloud infrastructure, and hence the triplet name. What it does is it allows us to point services in three clouds in three regions and seamlessly move workloads if we need to. And by doing this at scale, with bringing the computational power and data to 30,000 edge cloud locations, we've constructed the largest distributed cloud in the world. Well, we're proud to be one of the triplets. So tell us a bit more about how you're using Google Cloud databases and some of the use cases. Yeah, absolutely. So running a retail giant like Walmart means we're dealing with massive amounts of data. And on a given day, we are managing a large fleet of thousands of systems which power our distributed cloud. Now, how we do that? We built a data abstraction layer, which basically helps us to operate, manage, and move data across these regional clouds. It constitutes of two aspects. The first and foremost is the data access layer, which basically allows applications to interact with the data through a highly optimized API, so that the agnostic of whatever the data backend is. The second piece is the data acquisition framework, which again, with patterns like change data capture, lets us intelligently move data across these clouds. So by bringing these two interesting pieces of technology, we create the foundation for doing this at scale. Now, how does Google help in this and the Google Cloud databases specifically? So there are some three key use cases by which we use to power our business. First and foremost, all our e-commerce and our retail payments are on Google Cloud Spanner, where reliability is important. And we want to make sure that all shopping transactions are going smooth. The second piece is when our customers are searching in our catalog and discovering items, we want to make sure that they get accurate information. And that's where the Google Cloud, the AlloyDB comes in play. And third, we've partnered and we've worked in fact, co-developed with your team to deploy Omni AlloyDB, which is because of our private cloud infrastructure. We have that ability now, and that's following some interesting use cases like robotics and supply chain automation. So the key here again, you know, I think as practitioners, is getting the right tool for the right workload and making sure you're deploying that. And Google has been a big supporter in that journey. Yeah. And it's been great working with you on that and, you know, getting all the feedback and you've made us better for sure. Yeah. And how about AI? Like, how is AI transforming your industry and how are you thinking about it? Yeah. So AI is rapidly transforming, you know, our industry, in fact, every industry. And the benefits are extensive, right? Now, for us in the context of databases, there are again three areas where we have looked at AI. Let's start with traditional AI, right? So in traditional AI, what we've done is taken technologies like anomaly detection and deployed those models at scale. Now, what that gives us is the ability for now automatically triaging and remediating issues, minimizing downtime so that we can run our infrastructure reliability. And these are, I would just say, the foundational aspects for every big company. Second, a key component of our strategy is now the generative AI. And that's where I think we talked at this morning and even you covered that with vector databases. So we are leveraging technologies like vector databases to not only enhance our search capability, but also to improve product recommendations. So those two pieces become very critical in the generative AI space. Now, last but not the least in the space of agentic AI, because as we're moving more and more towards agents, it's about what we are starting to do is look at how we can automate every day's foundational yet critical tasks. I mean, database backups, maintainances, performance tuning. There's just tons and tons of things which a database practitioner goes through, right? So we are trying to make sure our developers can get more efficient as we manage thousands of databases with the fleet at scale, so that agentic AI can really help them accelerate their support. Now, again, here, AI, the way we look at it, it's not just a trend, but a paradigm shift to how we operate and run our databases. Great. Well, thank you so much, Anil. It's great having you. It's been great partnering with you. Thank you, Anil. I appreciate you sharing the vision with the audience. Thank you, everyone. Thank you. Back to Kat. Back to Kat. Thank you both so much. It was great to learn from Walmart's journey. Now, we know that developers love the agility of the popular MongoDB API and query language to store and query semi-structured JSON data. And that's why today I am so excited to announce Firestore with MongoDB compatibility, built from the ground up by Google, providing developers with additional choice for their demanding document database workloads. MongoDB API compatibility has been a highly requested capability from Firestore's existing community of over 600,000 monthly active developers. With this launch, Firestore developers can now take advantage of MongoDB's API portability, along with some of the Firestore capabilities they have come to depend on, including virtually unlimited scalability. Industry-leading high availability of up to five nines SLA, and single digit millisecond read latency performance. This is all without worrying about the underlying database infrastructure. In addition, developers can now use their existing MongoDB application code, drivers, and integrations with the Firestore service. Firestore with MongoDB compatibility offers a customer-friendly serverless pricing model with no upfront commitment required. And customers only pay for what they use. We've already seen a lot of interest from Firestore customers. For example, HighLevel, a marketing and sales technology company, migrated their platform to Firestore and improved developer productivity by 55%. They've observed better service reliability and are now liberated from database DevOps. Now, I'd love to welcome Billy on stage to show us all a demo of how this actually works. Welcome, Billy. Billy. Billy. Billy. First all we�apo is here. Did you learn about theיסiony conundrum switchboard slests that you can buy, but first, First will be the one to어�leb227th as well. Thank you so much for joining us. Hi, everybody. Thank you. and tools, so I love how we're making Firestore an easy choice. Let's say I'm a developer at this fictitious fashion forward footwear company and we're ready to launch our e-commerce site. We have a great front end built already, so I just need to take care of the data. In the Firestore console, I've already made a database and collections in my project with just a few clicks. And to use my favorite MongoDB tools with Firestore, I just need to grab this connection string down here. And now we can jump right into the good stuff, right into an IDE. I worked with the inventory team to get a JSON file of our product catalog, and I'm just going to import this to Firestore with the tools that I'm familiar with. Let me open up my console. And with the mongo import command, I'll create a new products collection, specify the data location of my Firestore URL, and point to that product's JSON file. You can run that. And the documents were imported successfully, so if I go back to my web URL or my e-commerce shop, let's refresh, and we'll see our inventory is there. We're ready to start accepting orders. I also want to launch this site and just start getting some load on it, an influx of generated purchases, so I'm just running this command right here. We'll come back to that in a bit. MongoDB has so many great tools, like the client libraries, the MongoDB shell, IDE extensions, and now you have the choice of Firestore as the database for those tools. With our shop up and running, we'll need a grand opening sale. So I'm going to do this using the MongoDB extension in VS Code. So I've got a file here right now, and I'm just going to do a simple query here to start, like, just getting all of the products in my database. So I'm running that, and I can look at them all, but I see that none of them have any discounts. So I'm going to apply a 75% discount to all the products that are on the front page. Let's go pull up my other query right here. So here I'm using a filter to get all the products on the front page and then setting that discount value to 75%. I can run that. And I'll see it matched five and modified five. So we'll go back to our shop, refresh, and get ready for this Labor Day sale. I love it. The front end team did a great job. We've got up a dynamic banner for the sale now. If I go hover over each of these shoes, we're getting that 75% off discount. Really a steal. And now orders are flying in. These prices are too good to not. So we're going to go to the monitoring dashboard and see how our transactions per second are doing. So we started out on zero. Now if I refresh this, we'll see in just a few minutes we're already over a thousand transactions per second. As a reminder, some of these purchases or all of these purchases are fake, but the load and scale are real. Now querying my data with the MongoDB tools is great, but querying my data with the MongoDB tools at scale is fantastic. Firestore is incredible for its ability to scale from zero to virtually unlimited. And being serverless, auto scaling is handled without needing to provision. Resources have downtime or shard data. As a developer, I can just focus on my code. And we can even refresh this more. We'll see the scale just continues to increase. And I love to see the way Firestore goes. All right. We go back to our sale. You know, from importing our data with MongoDB commands, from the ability to scale to handle thousands of sales a second, Firestore with MongoDB compatibility is a fantastic choice for developers looking to get the best of Firestore with the familiarity of Mongo. And back to you, Kat. Thank you. Awesome. Thank you, Billy. That was really impressive. Okay. Let's end this section with Bigtable, our key value and wide column database. We see developers increasingly looking to incorporate real-time data into their apps, like the number of views on a video page or the number of mentions in social media. Today, we're announcing Bigtable, continuous materialized views to greatly simplify integration of real-time metrics into your apps. They automatically handle updates, deletes, and late arriving data, so you can make every interaction and every signal count. Now that we've talked about how our databases give you freedom as a developer, I'd love to welcome Andy back on stage to talk about our third focus area, helping you migrate and modernize your database estate to the cloud. Welcome back, Andy. Thank you so much, Kat. So we all know that self-managing databases is a real drain, wasting your team's time and just exposing you to unnecessary risk. And that's why we're committing to providing the industry-leading managed services for the most popular open source and commercial engines, such as MySQL, Postgres, Valkey, Oracle, and SQL Server. By using managed services, you can focus your time on innovation and not on integration and management. Now, I know you're always looking for the best price performance for your workloads. Last year, Google Cloud introduced Google Axion processors, our first ARM-based CPUs custom-built for the cloud. Today, I'm excited to announce that Cloud SQL and AlloyDB are available on C4 instances, which are based on Google Axion processors. Cloud SQL and AlloyDB leveraging C4A provide nearly 50% better price performance compared to the N-series-based instances, and also provide up to two times better transactional throughput compared to Amazon's equivalent Graviton 4-based offerings. Now, we know that many of you have Oracle estates. Last year, we announced Oracle Database at Google Cloud in four global regions, enabling you to migrate and modernize your Oracle workloads with Google's industry data and AI services. In the last six months, we've added Oracle Autonomous Database Serverless, cross-region disaster recovery support, and are on track to deploy Oracle services in 20 Google Cloud locations. It's truly remarkable how fast we've made progress on this partnership. We also heard that many of you have been running your Oracle instances on-premises on VMs and want to bring them as is to cloud. And so that's why today we're excited to announce Oracle-based database service, offering a flexible and controllable way to run Oracle databases inside Google Cloud. Now, we're continuing to innovate rapidly together with Oracle, and today we're also announcing the general availability of Oracle Exadata X11M in Google Cloud. This brings together the latest generation of the Oracle Exadata platform to Google Cloud, providing significant performance, benefits, and increased capacity for the most intensive Oracle applications. It's been a tremendous year for the Oracle and Google Cloud partnership. And without further ado, I'd like to invite on stage Jay Evans, Global CIO and Executive Vice President for Oracle Cloud Infrastructure, to talk more about the journey we've been on together. Welcome, Jay. Thanks for having me. Jay, thank you so much for joining us today. It's a pleasure having you. Thank you for having me, Andy. So since this partnership announcement, we've been working hard, I think, as I just mentioned. What are some of the customer use cases you've been seeing? Yeah, you know, Andy, it's only been six months since we actually announced this, right? So lots have happened since then. We have customers, shared set of customers, who have workloads that they're running on Oracle database, and they also have workloads they want to run in Google Cloud. And so with this new offering, and what we've been doing for the past six months, is enabling our customers with Oracle database at Google Cloud, so that they get the enterprise capabilities that they want. It's the security, the flexibility, and the performance, and the expectations they need for their data, and integrating that data. So I think that that's a huge, huge unlock that we've experienced with this latest offering. With the partnership combining tech and innovation across both of our clouds, Andy, technology, this is what is the huge unlock for our customers. With Vector Search, using our 23 AI models, and leveraging your Vector AI, as well as the Gemini Foundation models, customers can now use their data in so many new ways that they hadn't been able to do before. So this is huge. I'm very excited about the partnership, and we're just getting started, right? Since we launched in fall, we have customers we're seeing great momentum and excitement where they're actually using it and adopting it, or they're in the process of learning more about it and starting to pilot with it. And so we're seeing a lot of momentum and progress. And an example is specifically, we have a retail customer. They're migrating their on-premise operational database onto Exadata, Oracle database at GCP. And so one of the big things there that we're seeing is that level of capability they didn't have before gets them to have their operational database, their analytics database, combining that and being able to do new things with the data for their retail business. So this is a huge, again, unlock for them and what they're able to do now with this new offering. So very excited to see that. And then enterprise customers, they also want to have fully managed capabilities. Autonomous database is something that we offer as part of our services with all the variations of Oracle databases that we can offer on GCP. And so we're super excited that this enables them to also have less overhead and be able to take those administrative tasks, leveraging our autonomous database capabilities in your secure cloud environment. That's awesome. And as I said before, we want our customers to focus time on innovation, not on integration and management. Maybe tell us a bit more about the cost efficiencies and just overall efficiencies customers can get. Absolutely, Andy. You know, customers, they want to have the seamless, frictionless, unified experience. Right. And so what we've been doing and working on for the past six months is really behind the scenes, taking out those complexities for our customers where we're enabling that seamless experience, where they get the networking capabilities, the authentication capabilities, and the billing systems are all tightly integrated. And we handled all of that behind the scenes to then make it easier for our customers to be able to adopt. So from an efficiency's perspective, what we're seeing is not only are they getting savings opportunities with that, commercially, we're making it simple for them to adopt. Right. So as they procure or provision their services on it, they're able to use the Google Cloud commitments and then also Oracle license benefits with bring your own license and Oracle support rewards. Right. So we're giving that real ease of use and again, tightly secure, getting the performance, but the flexibility for them to be able to use this service. So I think that that's a big unlock for them and enabling them to get that experience where faster provisioning, easy procurement for them, and it's all together. So we see this native integration and interoperability. It renders significant benefits for them and it reduces the operational overhead that we see a lot of customers are struggling with. And also again, to mention that the autonomous database capabilities really helps a lot of these customers who are really looking for how do they reduce those administrative tasks and be able to focus on higher level value for their data. And so we love seeing this and we're really excited. And you know, we're hard at work together. So what do you think should be most exciting to customers about the upcoming year? Yeah. Well, we talked about how this has only been for six months. And when we launched this six months ago, we hit some of the really strategic large regions, but we still have more to do, Andy. Right. So we've got more regions that we're launching throughout the world. So in places like Latin America and Canada and Europe, and enabling our customers to be able to have the data and this technology of services where they need us to be. And so this is a big thing we're focusing on is continue launching those regions. Right. So that's one big thing. And then from a products and services perspective, we see that you mentioned earlier as well, the Exadata X11M, it's a huge performance gain for customers for running their AI capabilities and doing an AI transformation. This will enable them to get much faster speeds and capabilities that they haven't had before. And so that's one big thing we're really excited about. And they also get the latest Oracle 23 AI database capabilities and functionalities with that. In addition to that, you mentioned as well, but I want to highlight the Oracle based database. That's another huge set of service offerings that we want to offer to customers, again, back to giving them that flexibility, because they can run those workloads on VM shapes that enables them to be able to have the capabilities as well as automated database lifecycle management and reduce again, the ops overhead, because that's another really key unlock that customers are looking for. And, you know, Andy, we probably have more things that we're going to continue working on, right? You think more innovation happening. And as well as I'm really excited about this, because I see this is we've just started this journey. And there's so much more we can do together. And we see and hear from our customers, what this is enabling them to do and being able to leverage their data in so many new ways with having the combination of both of our clouds together, really giving them that potential that they need for their enterprise. So super excited and look forward to the partnership. Yeah, it's been a great partnership. Thank you so much. Looking forward to more. Thank you. Thank you. Now, you know, we also hear from many of you that you're looking to reduce your dependency on Microsoft SQL service due to the inflexible licensing and high costs. We're getting tons of customers asking us for this. And we heard you. Today, we're excited to announce that our database migration service now supports SQL server to Postgres migrations for both Cloud SQL and AlloyDB. And this new migration experience offers online data migration, features a schema and code conversion engine that utilizes both algorithmic and Gemini AI based techniques. It's also designed to automate some the most difficult steps in the migrations, including converting T-SQL code to Postgres. We're looking forward to seeing how many of you migrate your SQL server databases to Postgres to really get the benefit of the open APIs and the open ecosystem. Now, finally, to help you manage your entire database estate on Google Cloud, we're excited to announce that Database Center, our AI powered unified fleet management solution is now generally available and supports every database in our portfolio. This release delivers rich metrics, actionable AI based recommendations, empowering users to optimize performance and deal with the reliability of their overall database fleet from a single pane of glass. So we're excited to keep on innovating on your behalf. At Google Cloud, we offer an intelligent, unified and open data platform critical to activating AI. Three things that I suggest you think about in the rest of this event. First, we have lots of great sessions in the database professionals track. Definitely, you can get the deep dive on each of the topics we talked about today. We have a demo booth on the showcase floor. We're going to be showing things like the MongoDB compatible API. And then just catch any of us Google Cloud experts, especially the database experts with your questions. We're very happy to take them and get you the right answer. Thank you so much for joining us today. Enjoy the rest of the show.