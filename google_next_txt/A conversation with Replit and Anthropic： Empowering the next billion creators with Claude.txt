 amazing really excited to be here thanks you guys all for for joining us and Scott and Jeff thanks so much for for taking the time to be here I know it's a busy conference there's a lot going on a lot of announcements so thank you all for for taking a couple minutes out of your day to to come and chat a little bit more about what we're doing here at Next and having that conversation with great partners such as Replit and how they're leveraging tools like Claude to build not only new functionality for their product for their customers but also share best practices how they got to where they are today what some of the decisions that they made because I know a lot of folks in the room are maybe at that earlier stage of their startup life cycle or are maybe a larger company trying to understand what are the best use cases to leverage LOMs so excited to share some of those best practices today and have a great conversation so again there was a quick introduction I'm Jamie Newerth I lead the startups team here at Anthropic and I'll pass it off for quick introductions hey I'm Scott I lead engineering at Replit I've been building things for builders for almost 20 years and before my three years at Replit I spent some time at Google so some former co-workers out there and I'm Jeff Burke I lead BDM partnerships at Replit I haven't been building for years but that's why this product is why I came to join the company and what we're talking about today awesome well I want to start off with maybe an easy one but a little bit before we get into the you know the nitty-gritty of how we leverage Claude and and leverage these tools in the workplace I'm curious how do you guys use Claude because I understand you guys are large users of it just at home what are some of the like assistant chatbot interfaces or use cases that you have and I'll share mine I have a preschooler who's a very picky eater and I'm sure no one has a picky eater in preschool so for for me just sharing my own story meal planning is really hard to do because a lot of times you look up what's a great meal for you know someone of this age and they give you examples and I just look at it and go oh my goodness I'm not she's not gonna eat any of this and so what do I do I work with Claude and say no Claude that's not going to work give me some other examples so that's the way that I love to use Claude on a weekly basis and we get in little arguments with you know Claude to make sure we get the right meal but but it's been working out so I'm curious yeah mine's also family related I have a four-year-old and he has picky parents and his parents are picky about schools and so a good way to use Claude or chatbots in general is give them a bunch of constraints and help them research something so it's like I want a school that has roughly this commute distance from my house I want them to have a lot of out-dicts. I want them to be focused on this or that and it can actually do a pretty great job coming back with a list and ranking it. It's awesome. Yeah I use it a lot for my kids but for a different example I recently moved to a house and there's a lot of DIY stuff that I was not prepared to handle so I'm often just asking Claude like what do I do in this situation so if you look back you can basically see all the problems I have around my house. Claude I think actually saved my house from burning down because it taught me how to use our fireplace because it was a little confusing and I couldn't figure out and I'm just taking photos saying no I don't think that's right what is this and oh you're right Jamie that is wrong like here's what it is so yeah it turns out for those projects it's really helpful. Cool let's get into the meat of things I want to you know share with the crowd and have you guys explain a little bit more about you know Replit in general and you guys have this great vision for the next billion software creators. Tell us a little bit more about the origin story of Replit as well as like what is that vision and how does AI fit into that. Yeah. I think Replit was always fundamentally a tool of empowerment so the idea is there's many people would be benefited by being able to create software whether to solve problems in their life maybe to upskill it could be your career but it doesn't have to be and it got its early start in schools actually because that's a place where people naturally are trying to look to learn to code. And it got you over the hurdle of all the pains of setting up your environment and installing Python correctly and then keeping up to date with what the correct way to install Python is which like frankly changes every two years and it's very frustrating. And so it just took care of all of that and so it got great adoption in that space. You could do it on a Chromebook that's a very unique value proposition. Over time we built up the platform to take care of more things that get in the way of your software so we could deploy for you. We could handle your database all of these things but fundamentally we were constrained by the difficulty of learning to code. Frankly like there was a time where we thought like everyone can learn to code this will be the way to empower and now no I don't think most people need to learn to code. I don't think that's important anymore. But I do think it's important to like learn some of the thinking for how you would spot a problem that could be solved by an app and then learn some skills of working with the AI to build it out for you. And as soon as we got over that barrier it was just it tied everything together all of a sudden it's it's not making the mistake of using SQLite instead of Postgres which is super frustrating for a beginner but just don't even think about that. And so now I think we always said a billion software creators and people thought it was kind of crazy. Now I see people stealing that tagline all the time. It's become like almost obvious it'll happen. Yeah, yeah, yeah, yeah, I first learned about replet a little over four years ago. I was in consulting and I was writing a newsletter on startups and every month I'd pick one that would do a deep dive into and actually the first one ever wrote about was replet. And what really got me was the mission and I was somebody who had never learned to code. I don't really know why it just never really was something people talked about growing up in my high school. And by the time I got to college and wanted to maybe start a startup and couldn't find a dev, it was I was kind of busy and a lot of other things. And every time it would be hard to carve out time to learn the code. So what really got me was this concept of getting a billion software creators and the implications from the economic mobility component, like what happens when everybody has access to this, like the original concept of the Internet was that. But then what ended up happening is a lot of the wealth and jobs are concentrated in the Bay Area. And so that was something that really struck me when I wrote about it. And ultimately why I ended up jumping to replet and spending 100 percent of my time on it. But it was still very unclear. Like, how do you get a billion people to learn the code? Even myself working there, it was my job to get more technical and learn the code. And it's still very challenging for all the reasons that Scott mentioned. And I think our founder, Amjad Massad, had talked early about AI being a component in that, but I'm not sure I totally got it even when I joined. Until we launched our first AI product. And like, it's funny now, that was only two years ago. It would only generate the right code maybe 10 percent of the time. Oh, interesting. For very simple apps. And you're like, oh, this is magic. But to see where we've come, it's now become quite clear that a billion people can do this. Yeah. Yeah, I'm curious too, and speaking of the Replet agent in general. And like, what were some of the challenges preventing non-technical individuals from creating software before, you know, the Replet agent existed? And how does it kind of jump that gap for them? Yeah. I probably failed. Jeff felt this as much as anybody. Yeah. Yeah, I probably failed to learn to code 30 times. I don't know. Like, every three months, you kind of... And then I finally did learn when I joined Replet, but that's because it was my job. But I think it was just, it takes a lot of time and a lot of the early steps. You're just spending time setting things up and you're not actually building anything yet. And it was, it would take a pretty long time until you kind of built like a thing that you could share with somebody. Replet was pretty clear to me because you could jump right into an environment. You'd have a link you could share with people. I got excited to share that with my family as I started building. I built stuff for my grandfather-in-law so we could track baseball scores. Like, that stuff created the motivation, but the aha moment is so far down that even when we launched some of our courses, you see it. Like, we were very good at driving sign-ups, but the retention throughout is very hard. And like the top questions you get people is like, okay, if I stick with this for 30 days, what can I build? Yeah. And at that time, it's like, well, you know, maybe you can build like a very simple Flask app. And it may do like one or two things, but to build what you want to build, we're talking about a much larger investment. And so AI collapsing that timeline, it's now super impactful. I mean, my shirt says, ask me to make an app for that, but you can make an app in minutes now, which is pretty extraordinary. Yeah. Yeah. It's been really cool to see. And it's a great example of like, I have not tried 30 times to learn how to code, but I've definitely failed every time that I've tried and a lot of the websites and like, this is my fault. Those are all great products, but I wasn't able to get it done. But it was interesting, a friend of mine, I think you kind of have that aha moment when it's something that's much more tangible, where there's this idea of like coding and it's the difficult nature of it. And what they're building is almost, you know, like you kind of envision the lines of code and how can I do that. A friend of mine who's not technical either used the agent actually to build a multiple choice game for their kids for like long like flights, like for Bluey, the cartoon. And he sent me that a couple of weeks ago and it was like, yeah, this thing took like a few hours and like I have no technical background in my daughters, like the whole flight they were doing this. So like you kind of have that moment of like, oh, I can actually do this. So it's really, really cool to have those tangible assets like that. Yeah, I think that aha moment is the right way to think of it. It used to be a pretty long journey to that. Some people get satisfaction from like the first time you print Hello World or something. Maybe those are those of us who go on to become software engineers, but most people want to see the app on the other side. Yeah, yeah, exactly. Like yeah, Hello World, that's cute. Yeah, yeah, give me the, I need Bluey now. Yeah, that's awesome. Well, thinking about AI in general, and I want to get into, especially for the crowd here, I think it might be helpful to some of the eval criteria and how you guys think about evaling going really deep into the LLM space. But before we get there, just your journey to leveraging LLMs and how you navigated the evaluation process to ultimately not only decide on on quad, but just in general how you wanted to leverage these tools. Yeah, so I think about the moment we decided to go all in on the AI agent, the Replit agent, because we recognized there was this problem, right? Like Amjad literally wrote down the goal as like make coding 10,000 times easier or something. Yeah. Right? And he put that on the board for us. And so he instructed, we were all going all in on this. And we believed, and it was a bit on faith. So at the time we were using the current state of the art model, and you could see like, it was starting to do the right thing. But if it even went down a trajectory for like 30 seconds or a minute, it would start to fail very regularly. And we didn't have any rigorous eval. It was just like, zero to one, is this going to work at all? And the moment that Claude Sonnet 3.5 came out was like one of the moments I'll remember most in Replit's journey. Because we all started trying, we wired it up and we're like, oh my god, we're going to launch this thing. Yeah. Like this is going to be real. This is it. This can happen now. And there was no eval. That was like what we would call vibes at the time. Yep. Right? It's like my idea worked. It actually did the whole thing end to end. And so that got us to, we had started this journey in kind of April 2024. That got us to the big launch in September. From there we got a little bit better about being able to test, you know, does it generate the first output correctly? Like how successful is it on edits? We had a few real criteria. But again, Sonnet 3.7 was such a leap forward. I had that feeling again, right? I have this app that I always test for myself, my favorite prompt, which is an app to split expenses with your family on a family trip. I won't say which app I'm cloning. But there's one that has some dark patterns on payments that I don't want to use anymore. And our first version of the agent never really got that far. And again, it happened. It's on at 3.7. We put it in, all of a sudden, it's building out multiple pages and off workflow, everything on the first try with a few iterations. It's now doing trajectories of multiple minutes and succeeding. And so you kind of have to project this is going to happen again. Yeah. And again. Yeah. And so while we've built up some evals, I still think what ultimately gets you is those initial internal vibes. And then when we put it out to users in an A-B test, that's like our ultimate criteria. Yeah. We got people to convert to the product at a much higher rate. We got eight times more likely to get an app that you actually deployed and shared with the world. And like, there's nothing you can do in product that moves your metric 8x. Yeah. Yeah. Yeah, exactly. Go ahead, Jeff. Yeah. The part I would add there is, I can't speak to the technical evals, but I really just think you just, I mean, it's a lucky part of being, I guess, at a smaller company, but just staying close to our customers is kind of the ultimate evaluation, which I think there have been many things since we launched these AI products that I expected that were totally wrong. And so one example is Cloud 3.7 runs a lot longer than 3.5. And before that, it was kind of, you just felt like you had to get the output really quickly and people wanted something that was very snappy. And since we launched V2, you know, we had initial good metrics there, but just talking to all sorts of customers, the quality of output is so much higher that there's actually a much larger tolerance to wait for the AI to work, sometimes for 10 to 15 minutes. And that's one of those things that if we just kind of looked at the first time output or the speed to response or things like that, I think we might have been scared to launch V2. Or I would have been fighting really hard to be like, we can't do this. All our customers, our partners are going to get mad at me. But we launched it and the feedback was quite the opposite. So it kind of forces you to refactor quickly. And the best thing I think you can do is just try and find your highest signal customers and stay as close to them as possible. Yeah. Yeah. The user feedback, the product metrics are really king. Because we all thought, yeah, time to first output correlates with user success. You got to get that down. It's like, no, quality matters more. Yeah. Yeah. How do you think about that with like in today's world? I mean, it's funny. It's one of those things like I think a lot of us joke about how fast AI is moving. And they say like there's a new model every day. But the reality is, it's like maybe not every day, but it's maybe every seven days like in practicality. So it's like really interesting to see that. So there's a lot of noise out there, but also a lot of ground truth of really high quality models coming from a lot of different angles all the time. So I'm curious how you guys think about your internal eval set and just what that looks like and you know, when there's something new that that testing period and what that process looks like. And I'll share some examples because I think you guys kind of fit in a variety of these is, you know, I talked to a lot of startups and a lot of them do the vibes, vibes check, which is great. Like that is one way to do it for sure. And then you look at maybe it's a more specific coding type of platform where effectively it's math. So it's like it's right or it's wrong. So there's almost some like, you know, easier aspect to those evals. But then there's that user experience. There's, you know, I've worked with platforms in like, you know, the lyric generation consumer space where they're effectively looking at two models and they'll actually blindly put out the outputs from other models to their customers. And they have a thumbs up, thumbs down. And whoever gets the most thumbs up, that's actually after a, you know, a few day trial period of doing this. That's actually how they'll convert to a new model. So I'm curious how you guys think about that just like internally that like more tactical LLM eval process. Yeah. So coding is a little bit like math. It's a reason it's great LLM use case, right? Because you can test it and you get the feedback. And so there's some aspect of that. Does the first app it created work? Did it, because it kind of knows, right? And it's testing and sometimes there's human in the loop, but it can figure some things out itself. But that's even still subjective. Like one reason we pick Claude is it just creates subjectively nicer UIs. And I don't know what you all do. I don't know what the special sauce is for that, but that's generally considered consensus in the industry right now. And so we even run these like bake offs where we'll run the same prompt through competitor models. And then we'll just put them all up in slack and let people vote which one you like. And you can see like who comes out on top. So it's a quick way to like turn ourselves into human raiders for an hour or so. But that's a very useful tactic too. Yeah. Yeah. I think that it's funny you say that about the UI. I mean, there's, and I think a lot of different models have a lot of different things that they're really good at maybe compared to others. And I think that will probably continue as this LM space continues to grow and adopt. But it's funny you say that about the UI. We actually have a team called Claude Character that specifically works on the character of Claude. And it was actually one of the original kind of like core research groups. So it's been there from day one. It isn't a new thing that we've, you know, added on in the last year or so. And a lot of it had to do with more of the pros and the outputs of how Claude engages from a, you know, writing perspective. But we're seeing it in the UI layer as well of just like for some reason this just looks better. So there's a bit of the vibe piece. But when you guys are designing something, you know, it's like, you know, we were talking earlier of just like what is fashion? Like what looks good? It's like, I think we all just agree that that looks good, but I can't really put my finger on it. So there's an interesting aspect from the eval process with these models that it's hard to really like button down and have very like hardcore quantitative metrics when sometimes that's not the right thing. Yeah. I'm curious to, we talked a lot about coding and with the Replit agent as well, but are there other aspects or capabilities of Claude beyond that that make the, like make it essential for the success of the Replit agent? Yeah, I do. I lean most on that subjective UI quality. I think right now it's ability to succeed on a longer trajectory is a big difference maker for us because that's essentially like your debugging loop and making the app work and especially for that agentic workflow, which we leaned into quite early. I think many people in this space are trying to one shot to success more than they are trying to run something truly agentic. And so the fact that the model is very optimized for that workflow makes a big difference to us getting the working app, which is really ultimately success. That's what downstream converts a user to pay, get them to deploy, get them to share. Yeah. Yeah. Is that like long horizon, just like tool, tool following or like, what do you mean by that more specifically? Yeah, both, uh, the tool falling and tool use, but also just being able to spot an error, go through the loop, debug, keep going. Um, not get stuck in the loop. Not get stuck. Yeah. I mean, that's kind of a three, five V one to three, seven change. That's been actually pretty significant. Right. Early on. You would talk about these death loops. Yeah. We would try to detect them. That's actually difficult. Like having it look back on its own trajectory and say like, am I in a death loop? Yeah. Yeah, exactly. Jeff, I'm curious you as well. Yeah. I think, uh, it's interesting to hear the Claude character team because I do think, uh, Claude. So I think one thing we were surprised with is how much people use the Replit agent, how like the depth of usage. Yeah. And so you'll see some trajectories that are, you know, five, six, seven hours of working time, maybe spread over weeks. And so it's quite literally for some people, especially if they're solopreneurs or individuals of businesses that are kind of on one person teams or one person projects, it's quite literally their coworker. Yeah. That's helping accelerate them. And I think the Claude character team clearly has done a great job, uh, with the output, even on the tech side. So on the non code generation side, if we're doing our jobs, right, no one's seeing the code it's generating. Uh, it just works, but the text is what they're seeing and they're engaging with the model. And, and you have people who are, you know, thousands of chat message messages quite literally. And so, um, I, I was a bit surprised how deep it went and like, you know, working on larger projects is, is certainly huge technical side. That's been great. But even just the output and how they, the model engages with the user has been quite good with Claude. Yeah, no, that's great. And one thing too, I think it's, it's helpful to share for, for folks in the room who are either leveraging Claude through, through Vertex today, um, and maybe aren't, you know, working as closely with our team. One thing, cause we'd love to do that. One thing that's been so valuable with what the Repli team has done is really give us that feedback really often. And sometimes the feedback is pretty harsh. And I think in a good way, as in like, this needs to be better for these reasons or, hey, we're seeing these errors in particular when you're running those evals, whether they're again, much more quantitative or more of the vibe check of like, is this the right thing? Because you can spot the, Hey, we went from the, we didn't call it version 3.6, 3.5 V2 to your feedback on your naming. Yes. Yeah, exactly. That was the best feedback you gave, um, you and everybody else. Um, going from, from, you know, V2 to seven, um, like, I mean, if you pointed out that it actually like kind of like ran a little bit longer and that's a change turned out actually not to be the, the, the bad change, but it was different. Um, and so, you know, an area that, that we'd love to partner with more folks, you know, here, of course, in the room and really throughout is just these types of close relationships that can help you all build your companies, but also, you know, like we have expertise on the LM side. You guys are the experts on the product side. You guys are building what, what folks are really grasping and leveraging on a day to day basis. So that type of feedback and that loop is really, really helpful. And I'm curious, you know, thinking of that as well, kind of foundation layer of the application, you know, vertex, as I mentioned, like, where does that fit in and just the availability of, you know, vertex in general in your not only like ability to deploy, but your decision making on, on leveraging Claude. Um, you know, you know, would love to hear more about that. And, and I can share a little bit after of how other startups are leveraging vertex today. Yeah. So, uh, Replit is already like a big GCP customer. And so having it available on vertex is a big convenience, but beyond that, um, it's important for us to have two providers. Yeah. If you want to run on a reliable system, like our product doesn't really work. Yeah. Without LLM and without inference. And so having two providers is obviously very key. And GCP, you know, lives up to their reputation on reliability and security. And so those are very important to us. Um, and then there's also a conversation that's coming up more and more is we'll have customers in Europe who would really prefer their inference runs in a European data center. And so vertex does have a Europe region. And, you know, for us to route traffic there for a user, that's like two hours of work. Yeah. That's a big request from them, but we can deliver it quickly. Yeah. Yeah. That's interesting. Jeff, I'm curious if you run into this when thinking of like, it could just be more scouts role, but just the ability to deploy things like that as it relates to just managing even quad for Replit agent. Like where does vertex fit in for you in that? Yeah, I think it's a, it's a huge boost. I mean, when we're talking to a lot of customers, uh, you know, how businesses are using Replit agent and stuff like that is a very new muscle. It's a new tool. It's not a direct replacement of another tool you're ripping out. So there's a lot of questions on how it runs and, you know, where do the workload sit? And I think having it run in Google Cloud and our close partnership with Google Cloud, but also in vertex is, is extremely helpful in just ensuring that we're doing this in a safe and secure way. Yeah. Yeah. It's interesting. I mean, a lot of folks, whether it's in the, you know, the enterprise space and, you know, you guys are, you know, approaching enterprise and do have consumers as well. But there's a lot of aspects, um, for folks deploying Cloud or really just elements in general. And like, how do you, how do you deploy that? What is the safety mechanism that, um, beyond just the model itself and the, you know, the guardrails and the safety aspect, like something like cloud and how we think about areas all the way down to constitutional AI and how we actually build the model, but also like where it gets deployed and how is so valuable. And it's been interesting to see over the last year or so as more and more startups actually are, you know, or excuse me, enterprises are leveraging startups, LOM based tooling to power a lot of what they're doing for their, you know, large established businesses. And having that partner like, like Google to be able to do that, it's been really impactful for us. And, you know, something we've seen, I think I mentioned yesterday, like thousands of startups and really large companies as well, leveraging, uh, you know, quad through Google to be able to maintain that kind of like business resiliency. So it's been like really interesting to kind of see that that is like the need of the market, even like AI is like the magic, but then I get the end of the day, like what's the ground truth of like, where is this and like, how can it be run? Yeah. Yeah. I'm curious to like, you know, replets, like the agent is just doing such like, honestly, like empowering, like really cool work. So I'm curious, like when you think of the ways that you're measuring success, some can be quantitative, some can be qualitative. I am curious, like how you guys have been measuring that and just like the impact that replet agents had. Yeah, that's, I think that's what gets the team excited. Yeah. Jeff has lots of good stories here. He's closer to those than me. Um, yeah, there's, we have a lot of people that are really excited about. There's, we have a lot of customer use cases. I think there are two that stand out that I can talk about briefly. One is, uh, with a private equity firm called HG capital, which we have a big partnership with. And that's more of a qualitative one so far, although we have some quantitative metrics, but I'll share those at a later date probably. Um, but basically they, you know, own, run a bunch of different companies and, um, they are trying to figure out how they bring AI into their portfolio companies and transform the business, obviously with like some financial mindset as well. Uh, and they view AI as a huge lever to make their companies like a lot more productive. And so what we did was we started with just actually a hackathon where we went there with the CEOs, uh, founders, CTOs of all the companies. And we got them in a room and we're like, we'll make everybody here build an app in 30 minutes. And, um, I think there were 35 people and we were able to get every single person to deploy an app in the 30 to 60 minutes, which, and some of them were technical. They coded or in the past, probably not in a while, but, and some were completely non-technical. And I think that was a really big eyeopening moment for all of them. So qualitatively, it was really, really great to be there and feel that. Uh, so much so that, uh, HD team told us one of the founders six hours later is still in the hotel lobby building his app. And, uh, I think pinging his team back home, which I, they may not have appreciated, but, uh, that was a really good one. But on the quantitative side, do you mind if I actually interrupt real quick? I'm curious on the agent, what was the business challenge they were trying to solve? Cause I think like, how did you even get to the hackathon? Yeah. So, I mean, I think they feel, uh, some of the pain points that a lot of other people feel, which is in all their companies, they are constrained on the engineering capacity side. You know, one thing we always tell people with Replit Agent is actually the majority of the people using Agent are non-developers. They're product managers, they're designers, they're operations people. And our whole point is like, there's a lot of other tools that are making your developers 20, 30, 40% more effective, which is great. We, we use a lot of those tools internally as well. But what if you could just cut the number of requests going to your engineers by half? Mm-hmm. And so there's plenty of tools that the rest of the business needs that are not core to the product roadmap of an HG Capital portfolio company. So an example, an example is actually another customer with a quantitative one, um, is a company called Zinus Mattresses. They're about like a 1,500 person, uh, mattress company. And they basically wanted a tool, uh, that allowed them to use LLMs to grade customer support interactions against a rubric that they've defined internally. Like this is what a good customer support interaction looks like. And when they kind of shopped around, they found a point solution that told them it was gonna be $100,000 a year for them to do this. And so they actually had two people on their operations team just build a tool with the Replit agent that uses LLMs, uh, goes through each chat, you paste the link, it brings the chat in, grades it, and then aggregates all the data into dashboard for their executive team. So in that case, you know, they basically probably cut their spend on software by an order of magnitude. They don't have an extra contract for a single point solution that otherwise would have. And that's how HG Capital is thinking about as well. They, they've already identified quite a few workloads. Maybe they can build for their portfolios a shared expense. Yeah. So each portfolio is not, um, portfolio companies paying for it, but they just build one in house. Yeah. I'm curious, what's that like, almost like that aha moment where like, I mean, I don't think it's gonna be with like a private equity firm, like a bluey app. Um, maybe you never know who the customer is, but I'm curious, like, what is that moment when you mentioned, like, I think was the CEO or one of the executives was like six hours later, still working on this. What is that thing that they're looking for? Because there is this like UI component to what folks are building. It isn't just, hey, we did a bunch of amazing stuff on the back end and now we're saving, you know, costs. And like, that's obviously there and that's really great, but you do have the ability with the rebel agent for people to go like, oh, I like, wow. I'm curious, like, like, how does that kind of, you know, present itself in the more like, you know, private equity type of experience or those types of use cases? Yeah. Well, I actually wouldn't undersell the bluey use case. Because we actually have a lot of instances of people who start with building fun consumer applications and that kind of gets them hooked. And then they start thinking about where can I use software in my day to day to make me more effective at my job to replace costs, whatever it might be. So we have tons of examples. Actually, with HG Capital, the person on the value creation team, I don't know if he'll appreciate me telling the story publicly because he was embarrassed by it a little bit. But he built an app that took Premier League scores in and then generated puns on that that he could send to his friends to mess with them. But that's an example of the beauty of the rebel agent is we have a mobile app. It's accessible in a lot of places. You just get your idea in there and just try and build it. And it can be pretty disposable, actually. Like, it doesn't, at the hackathon with founders, I wouldn't, of those 35, I don't know how many of those actually led to them cutting spend or bringing back to their business. But it's more just rebuilding this muscle of like, particularly for people who have never been able to code before. Like, if I wanted to go build something, I don't even think about the things I could build because in the past it would be like eight hours or a lot of spend on a developer. But actually rebuilding that muscle of like, oh, I think this would actually make me better at my job or this would be cool to have. Let me just go see if I can one or two shot this with a replicant agent and see how close I get. And then if you're close, you actually might spend a lot of time building that out a little bit more. Yeah. Yeah, it's been interesting over the last, honestly, really just a year or so. I mean, definitely the last two years, but it feels even more tangible in the last 12 months of just the questions you get, especially from folks that are either non-technical or, you know, maybe won't, aren't going to the, you know, Google Next conference. And they're not engaging in the way that a lot of us are with these tools is like, what is AI going to do? And like, I still don't, you know, I mean, probably a lot of people are using it for, you know, you know, date or, you know, for, for preschool, preschool meals. And so it's been really interesting to see something like what you guys have built that kind of shows them what the, like the actual capabilities are, but it can be this fun. I mean, I feel like whoever owns Bluey should be paying me at this point, but like, like a Bluey app for kids or something that a private equity firm is like, oh, this is the thing that I need. But they probably didn't wake up that morning saying, this is, this is the way I'm going to do it. So it's been really cool to see that, just that AI kind of like transformation and just be able to kind of become such a large part of our lives in a variety of different ways. And I'm curious, you know, Scott, thinking of a more, especially on the more engineering or technical front for the next, I mean, it's hard to project too far out in the future, but I'm thinking of like, you know, in the quest for like, you know, the next billion, you know, developers and billion users, what are some of the more technical, like, I don't know, blockers is the right term, but things that you're thinking about of like, what might actually slow this down? And how are you thinking about potentially like, you know, like ways that you can get past that to ensure that you're hitting those billion plus? Yeah, so, I mean, we're riding one of the greatest technical waves ever. And so that is going to solve a lot of problems for us, frankly. Yeah, I think there's, there's actually two sort of core product challenges you need to like unlock the aha moment. And Jeff actually kind of touched on a couple of them is first, you have to get people to realize like, there's like this fun, addictive thing about building. Yeah, I mean, the first thing I think of like my own learning to code journey. The first thing I did was hack up DOS scripts, so I could run this Mario game on a machine that shouldn't have supported it. But that's like, that was a magical moment, I realized like, I could mold the computer to do something. And then later that became my full time job. Way more people can experience that moment, like, oh, wow, I can do this. And then you start seeing these software shaped holes all over the place. Yeah, it's like, oh, I could actually build something that fixes that. And I think that's not the way most people think. Yeah, this is this is why right now we see someone like a product manager succeeding really well on repli because like, they know they're always thinking that way. And turns out they prompt really well. Yeah. But I think you have to get those two moments to realize it's like sort of fun, addictive and empowering. And then to start seeing the problems around you because, yeah, I think software is this perfectly elastic thing. Yeah, it's like there's kind of infinite demand, you'll just further customize and further enhance and further polish. Yep. Yeah. And Jeff, I'm curious, kind of taking that theme as well, just the AI kind of like assisted coding space and, you know, like where repli agent kind of fits in an interesting kind of like world within that. I'm curious, like, where do you see that going in the next couple of years? That's a great question. I have actually, even though I work here, I've been historically terrible at predicting these things. Like, I think it's happening much quicker than I expected. Even some of these customer examples where we've had some product managers build really cool tools. We have one large customer started with three or four early adopters, then they got to their 40, 50 person product team. And now they have 250 people at the company, all just getting their ideas as quickly to the agent as possible. You know, I think V2, cloud 3.7 and our V2 was a really big unlock because to me, I think that was a huge step change in the quality. And so I suspect we'll get a couple more of those this year. Yeah. With how fast you all are working. And if that's the case, then I think the ability for people to build really high quality applications by the end of the year is going to be pretty significant. I think my kind of, my perspective is if you don't have a plan to adopt some of these tools you're already behind, like some of the organizations I think that are doing this, even if they're not building things they can quantitatively point to yet, the direction's quite clear. And many of them are pushing it much further than I expected. Yeah. And I think there's a lot of stuff like, as it gets better with longer trajectories or reasoning across long context windows, that it's going to help us be able to build stuff that supports bigger projects that can maybe suggest how to maintain the application, solve a lot of the big things that are kind of limits now. Yeah. So I think you're going to see a lot of people build in-house tools. I think you might even see teams in large organizations solely designed to look at their software stack and say, what can we actually replace in-house? I know there have been public headlines on replacing some really massive software tools. I don't know that we're going to get there. That seems terrible. But I think people also underestimate how many smaller tools people are only using one feature of that's quite expensive. And I think there's a really big place within this year to replace some of those with software built in-house. Yeah. You said two years. That's kind of an eternity in this country right now. I know. It's impossible. I really think exponentially. I'll steal a line from Dario. It's like the percentage of code written by machines instead of humans will round to 100%. And people think like, oh, like the software engineers will all get automated. Or sure, a lot of theirs will be automated. But the denominator on that is growing exponentially, right? Yeah. Things like Replit. You could be working with a team of like 10 agents working in parallel on multiple projects, right? It's going to evolve in sort of ways you can't totally predict. Yeah. But if it's exponential, you kind of have to think like that. It's like two years is so hard to think right now. Yeah. Totally. I mean, I think about like, you know, I've talked to some folks where I'm like, hey, if you could look out into the future of like, man, if like, it won't be there tomorrow, but if quad five, you know, hypothetical could have this thing, that hits my product vision. And what I think we've started to see is that it's, this isn't two years away. First of all, it seems to be happening much faster than that. But also it's really hard to, everything is changing so quickly. And that kind of have that, like, I can't wait for a period of time for the technology to get that much different when it's one rapidly changing. And two, I can build better things every single time this happens. So that you don't need that like really far out goal, goal post, which I think has been just really interesting for, you know, you know, you guys are building with your up-to-agent, but just startups in general. And just being able to, I mean, with the launch, you said not too long ago, just see the users and just the like obsessiveness they have on the product. I'm curious when you guys did the launch, was it like, did you, what was, did you expect the success that quickly or like, was it like, oh my God, we did it or like, we knew this was going to happen? The launch was, yeah, we, we talked about all the good things up here. It's, it's certainly under the hood feels like pretty much everything's on fire all the time. Well, Anthropix is not like that at all. Yeah. I'm sure no one else's company. I mean, I think, I think we like the number one lesson we've learned is just like, you just have to move quickly and be open to refactoring all the time. I mean, at launch, we actually, we've tried to do big splashy launches in the past and some have been good and some have not been big and splashy. This one, we purposely were like, we're only going to post it on X in one place just to try and get some users. We weren't even charging for it. We were just going to give it to our paid users and try and keep it relatively quiet. We said internally. And so we could just iterate with like our paid customers and try and improve it as quick as possible while also being first to market. And it just kind of went viral. And people just started coming in with crazy expectations. And it was great because we got 10 X of feedback we originally expected. But under the hood, it was a little bumpy. But, but that's kind of one of the things you just kind of figure out as you go, you keep learning. And it really did allow us to know exactly what we had to build next. Yeah, we all know what a quiet launch means for Amjad, really. Yeah. I remember that. I remember it was like, there's no PR. We're just going to, it's like, okay, let's see how it goes. It's great. And it was like, oh, you're breaking anthropic right now. Yeah. Good, good things to break. But yeah. Amazing. Well, good. I mean, as we wrap up, I'm curious, again, we have a lot of folks here in the room who are building pretty closely today, you know, kind of like close to the rails, I call it with the LLMs themselves. And I'm curious, like, you know, one piece of advice you could offer the folks in this room as they continue to build on top of LLMs now. Yeah, I think this is a really good question because it's so fast moving. I think if you, if you're really aiming big, and you're really thinking ambitiously, what you have now should almost barely work. Yeah. Right? You should be aiming for what's coming in six months. Yeah. Or a year. And at the same time, that means you have to stay very nimble. So a good example is Cloud 3.5 to 3.7. Of course, we work very closely. So we got some early peek at how that was going to work. And it works very differently. 3.7 wants to run for longer. It wants to sort of be more ambitious and try new things. And so as software engineers, sometimes you get a little bit tied to what you've built up. You could make the mistake of really over building, but it's important to just sort of embrace that change. And so someone who's good at this on our AI team, James, has no qualms about this. I just watched him delete half the code base he poured the last year of life into. Yeah. Because, hey, 3.7 is different. Yeah. And he was right there. And he lives a little bit in the future, so he gets it. Yeah, yeah, yeah. And so, yeah, you have to be willing to let go and embrace change and stay nimble. I'm sure that's tough. But I imagine to be able to, seeing the success of your work, like, well, I still have to delete it, but look how much better it got. And look how much, you know, my work put into this. That's awesome. Yeah. Yeah. I agree with all that. I think the part I would add, too, is, I mean, I said it earlier, but just stay as close to customers or who you're building for as you can. I think it's really hard, too, in AI, because everyone has kind of a different set of expectations with whatever you're building in that space. And you really need to parse out, like, who are we building for, who are these customers that are our best customers, and then how do we get the feedback and make sure what we're building is unlocking value for them. And feed that back to the engineering team as much as possible, because I think it is changing, really, for everybody involved. Even when we launch something new, you know, we have customers immediately emailing me or partners, like, either this is awesome or, like, why'd you change that? And then I'm kind of scurrying to figure that out. And that's kind of the madness under the hood. But it's the part I like. It's not necessarily for everyone. It can be painful at times. But I think it's just kind of the nature of the world we're in, which is there's stuff launching that's making previous work obsolete. And there's, it's a lot of change for people to grasp. So you need to really try and parse out the signal for the noise and iterate on that. Yeah. That's awesome. Well, for the audience here, again, not only thank you so much for sticking around and coming to hear us chat, but also to talk about this change, to talk about these challenges, feel free to, one, come down to the expo floor and talk to the Anthropic team. We have a booth down there. And, two, please start building on the Replit Agent. Like, we'd love to hear, I mean, the team, I'm sure, would love to hear what you guys are building. And there's a way to scan and get access to it right away, just to see, like, you know, what you guys can do and go from there. So, Scott, Jeff, thank you guys so much. This was great. Great. Thank you. Thanks, everybody.