 We're going to talk about an amazing cloud transformation journey. One of the most fascinating ones that I've been involved with in the next 45 minutes. And what we are hoping is that we can share some of the learnings from this cloud transformation that can help you some real time and money in your own cloud transformations. So my name is Dana Soltani. I'm a product manager at Google Cloud. In the past few years, I had the pleasure of working with more than 50 global Fortune 2000 companies to migrate their analytics and AI infrastructure to the cloud. And one of the ones that stood out to me was Yahoo's. As the birthplace of Hadoop, Yahoo built one of the largest and most efficient data lakes on the planet. So when they decided Google Cloud as a new home for it, we were as excited as them, if not more, because we were hoping this partnership teach us as many new things as to them. To tell about that, this transformation and this cloud journey, I have with me Akshay Sharma and Ayapan Arasu, two of the most talented engineers that I've ever worked with. So when we talk to our customers, what we are hearing from them is that managing data lakes on-prem have increasingly become complicated and difficult. You are working with siloed data and you are working with fixed compute and storage infrastructure. And it's very difficult to respond quickly to the changes in the market conditions, deliver fast value to the business, and get competitive edge over your competitors. So a lot of our customers have used services and products like BigQuery, Dataproc, and Vertex AI to build unified lake houses on Google Cloud that are, first, cost efficient, which is very important for large businesses. Second, AI ready. And third, built on the best of Google and open source technologies together. So we've created an ecosystem of resources, tools, and partnerships based on open standards to help you with your cloud transformation and data lake migrations to cloud. But I know you are mostly here not to hear from me. So what I'm going to do, I'm going to hand it over to Akshay to talk about how they move Yahoo consumer data platform to Google Cloud. At the end of the session, I'll come back to talk a little bit more about these resources and talk about two exciting launches that we think is going to save you a lot of time and money when you are migrating data lakes to Google Cloud. So I'll hand it over to Akshay, and I'll be back soon. Thank you, Dana. Hi, I'm Akshay, and I'd like to start by introducing Yahoo. Although introducing Yahoo is a little bit like introducing someone to a bottle of ketchup, except did you know that 8 out of 10 Americans put ketchup on their burgers, but 9 out of 10 have been to a Yahoo site? By the way, this is how you know I work in data. I have stats about ketchup. Okay, so launched in 1995, Yahoo stays true to its mission of being a trusted guide for our users in the digital wilderness that is the Internet. We have a diverse portfolio of iconic products. With these, we enable our user base to achieve their daily goals, whether it be checking on the latest happenings in news, finance, sports, or whether it's just checking their email. For almost 30 years now, Yahoo has become second nature to its hundreds of millions of users. So a little bit about the brands within Yahoo and the data ecosystem, some of which I mentioned earlier, like Yahoo Finance, an industry-leading finance destination, Yahoo Mail, a world-class leader in emailing, Yahoo Sports for sports-related news and fantasy, Yahoo Search to solve problems and answer questions, Yahoo Ads to help businesses find the right audiences, and many, many more. I can stand here and talk about brands for 10 more minutes at least. So as you can imagine, these brands all have diverse needs at the business level for their varied user bases, and these are what we call the verticals within Yahoo. This translates to complex needs across the data landscape when it comes to things like latencies, data models, reliability or completion guarantees, and when you start throwing in applications like ETL, analytics, machine learning, or building user profiles across all these different businesses, with their diverging needs at the Yahoo scale, you can kind of see how quickly the challenges can pile up. There are two ways of looking at data within Yahoo, horizontally and vertically. There is data generated and owned by the vertical businesses. This data can then be processed and used by horizontal teams to produce data products that are then consumed by other horizontals or verticals. I work for one such horizontal team called Consumer Data that produces data products both around user engagement data, so things like analytics or experimentation, as well as advertising data catering to things like revenue or commerce reporting or building user profiles. So I'll be diving into the story of Consumer Data's migration to GCP from a horizontal point of view. Later, Ayyapun will be presenting the story of Yahoo Mail's journey to GCP from a vertical point of view. Speaking of stats, as you can imagine, as a horizontal, consumer data is a very large domain, and this slide kind of summarizes some of the key statistics around the volumes of data processed by our platform. Our platform provides solutions for verticals to instrument their data, which is then moved through Yahoo-wide custom data transport solutions into our data centers. And this primarily lands on HDFS on-premise for batch data and custom deployments of messaging queues like Apache Pulsar or Apache Kafka for streaming data. The products that surface insights from this, such as corporate KPIs or, you know, content performance or revenue reporting, they all use a variety of open source technologies, such as Hadoop, already mentioned before, or Apache Storm or Apache Druid. There are also a lot of custom support systems around this for scalability, observability, and security. By the numbers, consumer data ingests about 82 terabytes of data or 71 billion records per day in batch fashion. Similarly, for streaming, it's around 2.9 gigabytes per second or 1.2 million records per second. This data has a large fan-out and is stored in many different aggregated forms. With the long history of business that Yahoo has had, there is over 10-plus ID spaces with about 10 billion unique IDs across them. The main point here is that we have about 138 petabytes of data on-premise, and this data is accessed as a platform, as I mentioned, by both horizontals and other verticals. Not to mention, there's also a bunch of ad hoc use cases supported by over 10K dashboards fielding around 40,000 queries per day. So this is a large platform that comes with a lot of capabilities and some issues. So let me highlight some of the main challenges that we face on-premise. So in on-prem data centers, Yahoo runs shared processing platforms such as Hadoop or Storm clusters. These help amortize the cost as resources are shared across multiple workloads. There's also custom central solutions for things like data transport, governance, and observability. These are great. But on the flip side, there are significant operational expenses associated with running these platforms. You need to manage expertise, tech debt accrued as things evolve, for one. You need to be in touch with upgrading infrastructure. And horizontal teams such as ours build standardized data products for the various businesses at Yahoo. Now, this might seem to be beneficial in that it takes the onus of a small vertical to design, build, iterate, and maintain these data products. But it also forces them to adopt common data models that might not quite fit their data exactly or lose control entirely over when and how their data arrives or how it evolves. So what we actually need is for data producers to own and maintain their data while getting the benefits of the shared resources and expertise. Now, if that sounds familiar, that's exactly the data mesh architecture, and that's one of the things we addressed in our move to GCP. So if you then couple this complexity with the long history that Yahoo has had both on the business and technology side, you also get a couple of other issues. So when use cases evolve, a well-used data product might not quite fit the bill for either technology reasons or data limitations, and so a different but related product gets built. And when you have multiple products supporting similar use cases, you get fragmentation, duplication of data with all the issues that brings, and just general confusion amongst users on what to use and when to use it. Finally, with a lot of changes happening as our businesses rapidly innovate, we're spending far too much time unraveling all this complexity. With a complex set of dependencies between our data products, changes also require extensive buy-in or detailed rollout and mitigation strategies. All in all, this basically just causes a lot of time in getting things to go up. So how do we solve all this? Enter GCP. To address our pain points on-prem, we are using our digital transformation as a forcing function to both simplify our complex business logic accumulated over 30-plus years and also modernize our tech stacks at the same time. We're currently right in the midst of a multi-year strategy in moving Yahoo to the cloud. One of the most crucial aspects of GCP for us is the world-class support for open source software. With this, we can adopt a phased approach. For example, we can move some of our Hadoop solutions directly to Dataproc and GCS with minimal interface changes for our customers. While we're doing that, we can then go ahead and replace the underlying implementation of these data products with cloud-native solutions. And in GCP, we get the benefits of the shared platforms that we had on-prem by using things like committed usage discounts or sharing slots across the company in BigQuery. And this way, we get to have our cake and eat it, too. So another aspect of using GCP is that we also get to simplify quite dramatically. We use BigQuery as our unified querying and processing layer with Looker on top for business intelligence needs. We chose to reconcile many of our database and API layers with things like GKE, Cloud SQL, or Spanner. We get a unified layer for both batch and stream processing for advanced processing needs with Dataflow. And we use PubSub for universal messaging. And observability and governance are now right out of the box with things like cloud monitoring and Dataplex. So armed with all of these standardized solutions and moving away from building bespoke ones, we can now shift our focus to using our immense first-party data signal to build world-class user experiences and advertiser experiences. So this basically kind of summarizes pictorially what I was just saying. This is a simplified view of the consumer data platform on GCP, and I promise I did try to simplify this. So there are six main categories of systems. There's the sources, the ingestion, the storage, the processing, and the two kinds of things, the internal and external things. And if a box kind of flows across multiple domains, it just means that it participates in all of them. So you can see our core data platform on BigQuery, right, pretty much going through everything. And similarly for our governance, observability, and security going through everything. The phased transition approach that I was talking about is denoted with dashed boxes. So we have our legacy serving, our legacy transport, and our legacy processing systems shown here, which we are replacing with our North Star of BigQuery, Dataflow, PubSub, and GKE. So as for the data mesh architecture part, the output of our platform is naturally divided into GCP projects per business. So each vertical gets its own GCP project where their data gets delivered to them, and the horizontals end up consuming across them. This way, the verticals now have full control over their data. So to sum it up, in short, as a horizontal, consumer data is now focusing on delivering data products with signals that are needed at the cross Yahoo level. Verticals now have the agency to extend that core data model with their own metrics and dimensions. We have focused our efforts from moving out of building bespoke processing solutions into enabling our businesses and empowering them to activate their data using GCP in an effective manner. Thank you. Now over to Ayyepan for the other side of the story from a vertical point of view. Good evening, everyone. I would like to start out with a quick poll. Raise your hand if you have not used your inbox recently. Anyone? Anyone? Yes, indeed. Your inbox is an indispensable part of your day-to-day lives. It is your personal assistant that enables you to communicate with the world around us and stay organized with the business of life with tools like contacts, calendar, and to-do list. Launched in 1997, Yahoo Mail revolutionized the way people communicate online. Today, Yahoo Mail is one of the top consumer email providers in the world with several hundred million active users. We are reimagining your inbox and powering new AI experiences for our consumers, like reply suggestions, prioritized views of your inbox, and email summarization feature cordals, TLDR. And we are just getting started. AI today not only powers many of these user-facing features, but several back-end systems, like span detection and filtering, to keep your inbox safe. To provide you some context, Yahoo Mail is undergoing a multi-year digital transformation journey to Google Cloud. This is a massive undertaking, with tens and millions of lines of code spanning 150 projects. Mail attributes to a significant portion of the numbers you saw in Akshay's presentation. Just to put it in perspective, we are talking about more than 100 billion events a day, terabytes of data transfer, petabytes of data storage. On-premise infrastructure uses best-in-class open source software, originally built by Yahoo, like Apache Hadoop, Pig, Pulsar, and many CNCF projects coming from Yahoo, like Athens and Screwdiver. Re-hosting these services as is on the cloud comes with its own significant challenges. In the next few slides, I'm going to talk about three challenges in each of the category of data collection, building the data lake, and powering AI innovations, and how we are embarking on this cloud transformation journey. Let's first talk about some of the challenges in migrating the data collection stack. Traditional data lakes are built on the principle of storing the data in a one centralized location. This is further used for extractions, transformation, and aggregation to build data products and gain business insights. Given distinct systems used for serving and offline processing, needs two copies of the data, one on the serving store, another on offline block storage on HDFS. At male scale, migrating this solution as is to the cloud was not a cost-effective solution. Our second challenge, on-premise infrastructure uses a highly scalable proprietary service called as data hiving. This is used for even collection, transport, and delivery to a centralized location. Re-hosting or lift and shift of this service would have added significant complexity to the overall solution and would have increased the cost of ownership. We also realized that teams were building their stream processing workloads differently. Some teams leveraged open source software like Apache Strong, while others used, wrote custom stacks on top of bare metal servers for stateless applications. These stacks have become outdated over time and missed some of the key innovations in this area or domain, especially like Flink and Dataflow. Lastly, as these systems were assembled over time, we faced significant challenges in providing the end-to-end observability across the entire system. Now, let's take a look at how we are solving these challenges on Google Cloud using cloud-native solutions. We simplified our architecture by moving to Cloud PubSub as the engine of choice to do transport and delivery of the events. We leveraged a tiered enrichment strategy by moving the enrichment processing closer to the region of origin. This enabled us to use optimization techniques like micro-batching, reduce the data transfer, and also employ jurisdiction and compliance checks closer to the region of origin, which keeps changing all the time. Secondly, we standardized on cloud data flow as our stream processing engine of choice. This enabled teams to use latest stream processing semantics like stateful processing, exactly one consistency, and auto-scaling. Dataflow integrates with Vertex AI through Dataflow ML and also enables us to explore future use cases like structured queries on top of streams using Dataflow SQL. Lastly, Cloud Dataflow and Cloud PubSub integrate seamlessly with Cloud Operations Suite, providing us a single pane of glass for site reliability engineers to diagnose issues like data freshness, backlogs, and other performance bottlenecks. Now that we have looked at our data collection stack, let's look at some of the challenges in migrating the data lake. A significant portion of our processing is batch in nature. These pipelines have evolved over time and were built with various frameworks like PIG, Hive, Presto, and Spark. I bet everyone here would relate to that and about the whole complexity that comes with multiple frameworks. Building data products on-premise often led to fragmented solutions with increased complexity and a significant learning curve for our new developers. We also realized that without an enforcing platform or a business process, teams were making actually multiple copies of the data because one consumer would have fewer set of columns than another consumer. Yes, indeed, inefficient usage of the storage. Our second challenge, on-premise infrastructure runs long-running Hadoop clusters spanning more than thousands of machines, sharing the workloads across multiple Yahoo properties. With digital transformation and the movement towards business property-centric cost centers, the need to write sites and optimize these workloads was absolutely critical. Lastly, Yahoo Mail has hundreds of pipelines and several more data sets generated from these pipelines. Let's just pause for a second there. Anything of that magnitude is going to be highly complex. We face significant challenges in providing the cataloging solution and also deriving the end-to-end lineage across the entire system. Now, let's look at how we are solving these challenges on Google Cloud. We are adopting data mesh principles and standardizing on BigQuery as the interface for building data products. Authorized views on top of big lake tables and BQ data tables streamlines assets and serves as data contracts for our consumers. A single data set can power multiple authorized views resulting in significant cost-saving symptoms of storage. Secondly, we are leveraging ephemeral data prop clusters to help us optimize and fine-tune our workloads based on historical characteristics. This also gives us greater flexibility for us to choose the right compute shapes and attach storage, thereby enabling us to optimize the price performance on a workload level. Lastly, we are leveraging Dataplex, the data governance service, to centrally catalog all our data sets and enable discovery between teams. Data business glossaries allows teams to self-serve and have a common interpretation of the data across all teams. Dataplex has wide range of integration with various GCP services like PubSub, BigQuery, and Vertex AI providing us the end-to-end lineage across the entire system. Lastly, let's look at some of the challenges in innovating using AI. The rate of innovation in the AI industry is astonishing. Anyone here disagrees? Every other day, we see a large pre-trained foundation model being released trying to gain industry adoption. Keeping pace with these industry advancements needs significant investment in terms of infrastructure, and resources, often making on-premise infrastructure out of date. Secondly, we face significant challenges in providing an end-to-end ML ops for engineers, often resulting in fragmented solutions. Lastly, challenges in acquiring GPUs lead time to procure and ability to scale up by major entrances in many of our AI ML experiments. Google Cloud changed all of that. Mail teams are leveraging Vertex AI as a fully managed end-to-end platform for all their AI ML needs. Enterprises operating either in cross-cloud or hybrid cloud setups can enable the right security pusher on the cloud by using GCP services like VPC service control and security command center, copy their datasets to the cloud and start innovating using Vertex AI. We are using some of these patterns in some of our newer experiments. Secondly, Vertex AI provides us one cohesive platform for us to do for the ML ops teams to enable scientists to launch their work benches do exploratory data analysis, select the models from the model guard and train the models and deploy it as prediction endpoints anywhere on the planet using infrastructure as code through Terraform. Lastly, teams are able to provide consistent and predictable experiences for their consumers by using Vertex AI features like provision throughput and also have greater flexibility in GPU training using dynamic workload scheduler. Here are my last thoughts. Keep it simple. Rethink your existing systems in terms of cloud native solutions. Modernize. Modernize using the best in class that Google Cloud has to offer and stay adapt to industry trends. Optimize. Optimize for agility and efficiency by using one cohesive platform with end-to-end observability. Next, I'd like to pass it on to Diana to talk about some of the exciting tools and services being released which address some of these challenges that we talk in our use cases. Thank you. Thank you so much. All right. By a show of hands, who has used BigQuery migration services before? Okay. One person. He doesn't count. He's from Google. All right. Not too many people. That's our fault. So, BigQuery migration services actually help a lot of customers to migrate their SQL workloads to BigQuery. But now we are expanding it to facilitate data-like migrations, not only to BigQuery, but also to Dataproc. And there are two launch announcements that I'm going to share with you tonight. And these launch announcements can help you shave a significant portion of your data-like migration to Google Cloud cost and time. The first one is automated assessment for data-lakes. So, as you guys know, the first step of any data-like migrations or any kind of migration is actually understanding what is running in the source system. And depending on the complexity and the scale of your on-prem data-lakes, this could take weeks or months. Well, now you can install an agent and within a few hours, you get an inventory of all the databases and tables, all the jobs and workflows and all the users and permissions that are in that on-prem data-lakes. Not only you get it as a nice report, you get it as a BigQuery table where you can do all kinds of analysis on it. So, this, we are hoping that actually saves a lot of time to get data-like migration started and also plan for it based on the learnings and the feedback that we have from the customer. The second announcement that I have is an automated data migration for open source tables like Hive and Iceberg. So, if you have more than a few terabytes of data, migrating your tables and data from on-prem to cloud is a major headache. It actually, based on our conversation with our customers, up to 40% of the cost of a large-scale data-like migration is migrating the tables to cloud. And this, for example, a $50 million data-like migration project is a $20 million cost. And we think we can fully automate this process. So, the second feature actually helps you take a table with all these files and directories and metadata and the permissions and migrate it to the cloud. Not only it migrates the files that make up that table, it actually registers it with the right metastore in the cloud and actually automatically move all the permissions from on-prem to cloud. Both of these features are available in public preview. So, if you right now, if you guys need to start using them today, that's entirely possible, but we're going to make it publicly available in a couple of weeks. I wanted to end with one thing. We are here to help you guys with your cloud transformation project. there are a couple of resources on this slide that you can take advantage of. We have condensed our learnings from Yahoo's migrations and many other migrations like Yahoo's into a very concise white paper that you can download by scanning the leftmost barcode. If you are interested in learning more about the BigQuery migration services, scan the middle one. And we know that migrating data lakes to cloud is actually a costly endeavor. So, what we are offering is actually a number of incentives to help reduce the costs, reduce the complexity. And these are not only financial incentives, but engineering assistance and help. Everybody Principal The International Organize millimeters Music and A veteranesta Pier Hospital of C können Google understand their human Il And under音 voice