 How are we doing? All right, let's get started. We are super excited to show you something truly impactful for your everyday work. We're talking about accelerating your software development lifecycle, not just the coding parts of it, but your entire software development lifecycle using agents which will help you augment your workflows across the entire stage, every stage of your software development lifecycle. We've got a team of folks here today who are really passionate about making your lives easier. I am Pritpal. I am a product manager on the Gemini Code Assist team within Google Cloud. And joining me today are going to be Harsha and Jaroop, who will come on the stage in a short few minutes. We have all lived a developer life and understand the challenges. Let's take a look at what we plan to cover today, focusing on your needs. First, we're going to talk about developer productivity boost. We'll show you how agents automate the road tasks that slow you down across the whole development lifecycle so you can focus and be free on doing the work that you actually enjoy to do. Next, agent architecture. We will double-click a little bit into how do the agents work. We'll demystify how these agents come together to bring together all the advantages that you hope and expect from agents. After that, Harsha is going to come out here on the stage, and he's going to walk us through PayPal's journey on infusing AI through the entire software development lifecycle. And then last but not the least, we have an exciting set of demos lined up for you folks to take you through the journey of leveraging Gemini Code Assist agents to create apps from scratch based on a product specification, and then also taking it a step further and then enriching and enhancing the code base with new features. We plan to have some time towards the end for Q&A, so please hold on to your questions, and then all the speakers will come up on the stage and take questions towards the end. Okay, just a quick show of hands. How many developers in the audience today? Woo-hoo! Keep holding your hands up. Keep holding your hands up. How many in the audience have used any kind of AI coding assistant? All right, look around. That's a survey in real time. Excellent, you can put your hands down. We hope they return each and every one of you into a Gemini Code Assist fan by the time you leave the session. All right, so let's try that. Okay, let's start with a core issue. Developer productivity. We all want to be more productive, right? Okay. Our mission is simple. Cut down on the engineering toil and make you all, the developers, more productive. We want to augment your software development lifecycle workflows with AI agents handling all the tedious stuff so you can focus on the more creative, fun parts of development. Here's a recent survey done by Stack Overflow around the use of AI tools. Let's take a look at some of these numbers. Developers already see the benefits of using AI tools. About 81% say increased productivity. About 62% find that they are able to learn faster by using some of the AI tools. And then about 58% greater efficiency. These aren't just abstract benefits. These are tangible improvements to your daily grind so that you can work better. Right now, most of us are using AI mainly for coding. Makes sense. That is where AI assistants started from. Coding was one of the top use cases through many different surveys, followed by searching for answers, debugging, and getting help. But let's take a look at the remainder of the chart out here. Documentation, content generation, testing, project planning. These are all areas where AI adoption is low. And let's be honest, these are the most time-consuming but the least enjoyable aspects of our job. Do you folks agree? Okay. Now check this out. Developers also happen to be most interested in using AI for the exact same areas. Documentation, content generation, project planning, testing, see the inverse relationship. We are using AI for the fun coding parts that we like. But we want AI to take over the stuff that we don't enjoy. Right? That's exactly what we're trying to address in this session. Most AI tools focus on the coding stage. Code completion, code generation, all within your IDE. But the SDLC, your software development lifecycle, it's the loop. It is not a single point. Coding is important, but just the very first step. Our approach recognizes the bigger picture. We are building agents for every stage of the software development lifecycle, starting with coding, migration, testing, security, monitoring, deployment. This is about comprehensive AI support for your entire software development lifecycle, not just coding snippets. And to manage this ecosystem of agents, we are introducing the Gemini Code Assist agent. Think of this as your intelligent project manager. It's a central agent that coordinates all of these specialized agents so you don't have to. You interact with one agent, and it handles all the delegation and the orchestration of reaching to whatever agentic capabilities it needs to solve the problem that you throw at it. And to allow for a rich user experience, when you work with this agent, we are introducing the Gemini Code Assist, Kanban board. One surface that allows you to interact with the agent when the agent presents a plan or it needs certain kinds of inputs from the human developer, that's what you use. The second use case for this Kanban board is to observe the state of all the jobs that we ask the agent to do. and then lastly, to review all the generated artifacts by the agent, such as generating source code files and having you look at all of those changes and accept some of those changes. All right. So we spent a lot of time talking through a bunch of things. Let's take a look at how does this stuff actually work underneath. The term agent has evolved quite a bit. It has started from function calling, simple tools, all the way to having the agents be doing a lot more complex tasks, such as breaking down problems, also known as task decomposition, to all the way to doing things which we call as working on solutions iteratively, and then more importantly, self-learning and becoming better at executing the job thrown to it. To keep up with that, our approach to agents has evolved as well. We started with simple APIs, right? Simple APIs to provide specific behaviors. to static agents, which can pretty much try to solve many of the specific problems that we see every day. Next came dynamic agents, which really are able to solve slightly more broad and ambiguous problems. To a multi-agent platform that can solve even some of the novel problems, the kind that you all face every day. So what is this multi-agent system all about? Well, the analogy is very simple. Think about you getting a project, right? There's a project manager that spins up a team into action, gets people lined up, so that the deliverables of the project are met. In the same fashion, the multi-agent system consists of different agent personas, software engineer, a test engineer, architect, and a technical writer. They work together in unison, think the analogy of a chat room, where there's a shared goal, they collaborate together, taking turns, and critiquing each other in a loop. And we have seen that using this approach, we are able to get a higher quality response and do a much better job at trying to solve a complex problem. Okay. With this, I'm going to have Harsha come and talk about PayPal's journey with AI and the SDLC. Harsha, take it away. Good afternoon, everyone. It's a pleasure to be here at Google Cloud Next. Let me get my clicker. Yeah. I'm Harsha. I'm an architect with a developer platform engineering group at PayPal, and I'm here to talk about how PayPal has been embracing AI across all our SDLC stages. A couple of years ago, we embarked on an ambitious journey, one where we wanted to redefine how we do SDLC. Now, our goal wasn't just incremental improvements. We really wanted a fundamental shift in how we do things. Since then, we've been actively exploring and implementing AI-backed solutions. Like people have mentioned, we started with Gemini Code Assist, you know, just getting code assistance. We also experimented with pre-agentic tooling, like even before you had agent frameworks or MCP or any of that. How can we just use prompts to, like, solve problems? We also went ahead and implemented rag-based solutions. However, we quickly realized that we actually needed a multi-faceted approach. One, where we are actively embracing the AI solutions that are coming in our code solutions, and second, where we are implementing our own agentic frameworks into the internal developer platforms that we are providing. So, as we kept investing in this, we actually started identifying a whole bunch of friction points, more of them in the inner loop, you know, areas where your developers really care about. What you see over here are areas that you can actually relate to. In the planning phase, if your PRDs are not accurate, or if your documentation is outdated, your developers will give you a bad estimate. Likewise, in the development phase, we noticed that our developers were juggling tasks which are not really adding too much value to our business. Whether it was dealing with technical debt, or, you know, we keep telling them to do a migration from Java 17 to 21, or anything else that's over there. And adding more and more complex tooling to solve the problem wasn't really helping. We spend more time trying to train people how to use things. I mean, everybody relates to that. Let's also talk about the testing side of things. Our developers were finding it difficult to create good integration tests, to actually improve our test coverage, or even, you know, if you would ask them, why are things taking too long? The answer is we are over-testing. Can you get rid of things that you don't want? Very, very difficult to answer that. Right? So, I see a bunch of nods. I'm assuming that, you know, a lot of you feel the same pains. While our early adoption of AI assistants told us that, yes, you know, there's a lot of value in it, we really needed something different. like most of you over here, we've also gone down trying to explore agentic workflows and agents. Sorry. So, as part of trying to solve this problem, we crafted a vision. And, like Pritpal had already mentioned earlier, we want specialized AI agents. AI agents that can, in each of these phases, go solve a problem. developers don't want to write good PRDs. They don't want to write Jira stories. They do not want to, like, deal with things that don't add value. So, planning phase. We're thinking about agents that can create better PRDs out of natural language conversation. There's documentation. Can I have an agent that just ensures that it's up-to-date, accurate, something that I can use? Sorry. On the development side, I'm sure a lot of demos are talking about this. How do we get better feedback during our PR cycles? If there are open code issues, like sonar cube code smells, can we just get an agent to go fix it, instead of allocating time to do that? On the testing front, we're doing something different, because agents are still catching up with that space, so it's going to be a while. But we're looking at technologies that can allow us to do selective test execution. We've been talking about it. We're actually trying to implement it. We're trying to see, can we feed this data back into our agents to go make the right set of changes? On the large-scale code transformation side, this is either migrating from an entire tech stack or just moving from one technology version to another. We are experimenting with agents that can complete this exercise with a good amount of accuracy and very minimal manual input. Again, as I said, we are experimenting, so that means we're not yet there. Another interesting thing that came out of all this research was for all these transformative processes to happen, your agents need data. And I'm not just talking about an MCP server. Your agents actually need relevant data, not something that just shows up in a rack query. Take an example. Let's just tie all these things together. You want an agent that writes a good integration test. It's not going to be able to do that if it doesn't understand your PRD, if it doesn't understand your code base, and it doesn't understand, say, design decisions that you've made, which potentially change how your test data gets created. So what you end up is you get a half-pig solution and a developer that's frustrated. So we believe that this is an area where we want to improve, experiment, and do some more research. Oops, sorry. Let me go back. So that is where we stand, and as part of the audience, you might ask the question, how much work do you do? How much do you hand off to people? What we are doing is for anything that we believe is non-code-related stuff, those are areas that we are investing our own bandwidth to try and figure out how to solve them. For areas, like as part of this demo, we are relying on our partners to come and tell us how to solve it. With that, I'd like to invite Pritpala and Jirov back to the stage. Thanks. Thank you, Oshar. I'm back. And that's Jirov. You want to introduce yourself? Hey. I'm Jirov Kipruto, software engineer and tech lead in the Gemini Coda system. Okay. Who's ready for a demo? Can we try that once more? Yes. That's what I'm talking about. Awesome. So, we have something super exciting to show you folks. We're going to demonstrate two high-level use cases. You saw from Harsha's slides, he spoke about the role of AI agents and what enterprise and customers expect. So, we're going to show you some of those scenarios actually live in action. The first scenario is focused on creating a brand new application from scratch using a product specification in a very popular tool that we all love, I love, Google Docs. So, we'll walk you through how you can start from scratch in the blank repo and create an app. It's called Pixel Shop. So, we're hopeful that this application can help you capture more memories of your stint at Next this time. And then, we'll show you how we can use the Code Assist agent to modify that code base and make it even better. Okay. Okay. We good? Yes. Okay. Before we get started, I want to follow up on the raise or show of hands from earlier on. So, I saw many of you said that you've used AI coding assistance, but I'm curious, how many of you have used Gemini Code Assist before, if you can... Show of hands? Okay. A good number. I'm really honored to be here to introduce to you who are not familiar Gemini Code Assist and for those who are also really familiar to show you the new things that we've added to it to make you really productive while working on your software development tasks. So, if you could switch over to my laptop. Great. So, Pripa, it sounded like we have a product specification. Is that right? Typical product manager. What do you expect, right? I just uploaded the Google Doc. Why don't you go take a look at it? I have written everything what we want to implement. Well, personally, when I go into a Google Doc and I'm trying to look for the product specification, I easily get distracted by an email from my manager or a ping from a teammate or if you're being honest, maybe a cut video on YouTube or some memes. So, I'd like not to go to Google Drive to look for that. So, I want to do that right within the IDE. So, as I mentioned, this is Gemini Code Assist chat and when I type at, there's a pop-up that comes up. I want you to focus on agents and tools. So, under agents, you can see Code Assist, AI testing, and Code Documentation agent. The one we're going to focus on today is the Code Assist agent. But just to get started, we're going to start with tools. Tools are things that we launched at the end of last year. These are API connectors that allow you to bring all the data that you need right within the IDE so that you can stay in the zone, stay focused, and execute your tasks without having to jump between different surfaces, get distracted, and you can just be right in the IDE to accomplish the task at hand. So, because this product specification is in Google Docs, I'm going to start by selecting the Google Docs tool. And immediately, I see some suggested prompts. Conveniently, List My Docs is the first thing that I'm going to try out. so that I can actually see the document that Pripal is referring to and if it's available to me and I can find out more details so I can start executing on this. Okay. We have six documents. Pripal, it looks like the third one, Pixel Shop MVP is the right one. I told you, let's ship the MVP. Okay. Let me actually fetch the document, the contents of the document, and try to see what it's asking me to do. So, for this, I'll ask the Google Docs tool and then I'll get my prompt from earlier. So, I'm asking it to give me the full contents of Pixel Shop MVP. The way these tools work is that they take your natural language query, extract the parameters from it. So, in this case, it will see that I'm trying to look for a document with the title Pixel Shop MVP. And then it will identify which API operations need to be called to get that document. So, in this example, it will take that name of the document, go fetch the ID of the document, then make another API call to actually get the contents of the document and return that back to me, the IDE. So, right here, we have the product specification without me having to leave VS Code. Okay. Let's see what you're supposed to do here. It's an image editor that has tools to create and edit images. We have the first milestone which will be creating the layout and the canvas for this application. The second milestone is adding some editing tools, look like things like pencil to draw on that canvas. We also have some functionality about erasing and filling in any shapes that we draw. And then lastly, saving, loading, undoing and redoing. Amazing. So, at this point, I can now call our code assist agent and give it this product specification from Google Docs and ask it to execute it or to implement the application. The way I would do that is to type at code assist and I'll find my prompt and paste that in. As Brickle mentioned, these agents are long running. It's going to take a long time to get all the full context that it needs from reading the product specification, coming up with the application, continuing to iterate on it as it runs, tests, compiles the code, et cetera. Doing and going through that process is probably going to take minutes or up to an hour. So, for that reason, I won't run this right here because of the time that we have unless we're all willing to stay much longer. But we'll switch over to a recording where we can see this in action in a much briefer time. So, when I click this, we'll see the recording start up and immediately the Kanban board comes up. I'll pause it for a minute to just explain what's happening. Once the task starts, a card shows up for that task on the running column. In this column, you can observe what's happening behind the scenes and just have some observability into the system. If the agent has any question for you, for example, it's missing some parameters or it's come up with a plan and wants clarifications from you and approval from you or let's say it just wants to ask what are the requirements for the application. For example, is it a web application? Is it a mobile application? Then the card would move over to the action needed column where you can provide your inputs. This is commonly known as human in the loop and this is very important in improving the quality of response to make sure that it actually meets your needs. And then the agent would continue executing in the running column until when it's done, it would move to the completed column where you can actually view the changes, look at the diff and decide whether or not you want to apply the changes and accept the code. Okay. Let me resume this. At this point, we are setting up. At this point, we are creating the runtime environment that's needed by the agents. We are setting up all the agent personas that are needed like the programmer and code reviewer. We are getting all the tools needed for this task and then we start execution. We start by creating an initial plan for this task that has been assigned. We identify what are the tech stack that we need to use, what are the requirements and then we write the technical design documents. After this, these agents go on to break down the tasks that need to be executed and then start prototyping and implementing the application. At the very end, it does system integrations by running the tests, compiling the code and if there's anything that needs to be fixed, then it can continue iterating on the files until it's done. And now we can see that our task is done. It's moved on to the completed column where we can click on view changes and see the files that have been returned. When I click on the diff view, we can see that there were no files before, so it was blank before and everything that's being written is new. This is because we're building this app from the ground up. There's no existing files before. So I can select all the files and apply them and have them written to the workspace. When I open the file editor, now I can see all those files that we viewed and I can spot check some of them. I can see app.js is our main entry point. It's calling out to the canvas.js, drawing tools, history, and we also have the front-end components in the HTML and CSS. Okay, I'm listing the files and now I'm running the application. When I run the application, I'm going to open up an endpoint local host and try to see and try out the application that has been returned by the Code Assist agent. And immediately, we see a very simple pixel-sharp iteration or MVP and we can start playing around with the tools available here. I want to pause here and just call out that we were able to use the agent, provide a product specification from a Google Doc and kick it off and ask it to go accomplish the task for me and it was able to return an application back to us. I've tried a few functionalities, the pencil, drawing, etc., but that was too plain so I'm going to import a base image and write something on top of it so that we can create a postcard that we can send home after this conference and just write hello from Google Cloud. So I'm just going to pick up the pencil, pick up the colors and start drawing on top of that. But now that I'm looking at the fonts, I'm rethinking, maybe I should write this more carefully so I'm going to do the, I'm going to use the undo function and write in a more cursive, more interesting font. Briefel, I think we have a good postcard here and a decent MVP. What do you think? What can we do next? Hey, you know PNs, right? What have you done for me lately? Well, this is great and I think this will help us create good memories, but we want awesome memories. So, I have found that, hey, we can take the next step and improve this minimum viable prototype that we have by adding some layer management controls in it. And guess what? True to my PM nature, I've actually logged and created an issue exactly for you, Jero, on GitHub. So, with that, I'll let you kind of walk us through what you plan to do in making this application even better. Okay. Right, Toneek, briefel. So, going back to the IDE, this time, I can use the GitHub tool to get the task assigned to me. So, I'm going to ask the GitHub tool and see to list issues assigned to me in Symbol AI Pixel Shop, which is our repository. At this point, this GitHub tool is going to extract that the organization name is Symbol AI, the repository is Pixel Shop, construct API call, get the contents it needs, and then return that to me right here where I can see for sure I've been assigned to implement layers. Okay. I'm curious what's actually in this issue. So, I'll ask the GitHub tool to fetch the description of that specific issue. I want to call out that similarly to before where the code assist agent has access to the Google Docs tool, it should have access to the GitHub tool so that if you ask it to address or fix a specific issue assigned to you, whether it's to fix a bug or implement a new feature, it should be able to fetch the contents or the context that it needs from that issue. Okay. We can see here that I'm supposed to implement the layer management system. There's a back-end component to it and there's a front-end component as well and interestingly, it looks like there's also Google Doc that has more details about this particular feature. So, as before, I'm going to use the Google Docs tool and ask it to fetch the contents of Pizza Shop layers. So, I want to highlight just in the same way that I myself could look and see that in this issue there's a reference to a Google document and then decide to get the Google document with further details before I can start working on the task, the agent should be able to do the same thing because it doesn't have any predefined plan and it adjusts based on the content that it receives. So, I expect it behind the scenes when it's executing to be able to get the issue and be like, oh, there's a Google Doc that's been referenced. Then call the, use the Google Docs tool it has access to to get that specific document and get the full context that it needs to be able to do the task at hand. Okay, and looking at this document we see that the layer manager needs to be implemented. It's modifying the canvas objects. We have some details about API changes that need to be made and some changes to the editing tools, the data model, et cetera. Okay, at this point I can go to a trusted code assist agent and ask it to fetch issue title implement layers from symbol layer in GitHub and implement the feature. Just as before, I'm going to switch to a recording where we can see this within this time that we have in this session. So when I sent this recording, we're expecting the same Kanban board to come up as soon as we get a chat response telling us that the agent has started working on the task. And for sure that has happened. So we now have a card showing up that the task is running, we are starting up, we are setting up the runtime environment. The key difference this time is that the agent has access to the existing code base. So in the previous example, we were starting from the ground up, but this time it's using the existing files iterating on it to add the new feature based on the GitHub issue and the Google Docs that we just looked at. And then creates a plan, understands the code base and then implements the new feature, verifies it before returning the files back to us. Okay, now the task is completed and I can view the changes that we have here by clicking on the view changes button in the task card in the completed column. We can see that the app.js file has been modified because at this point there are existing files before, so we have a before state and a new state and it seems that there's some layer data being added on top of that. If we scroll down, we see that there's a brand new file that has been added called layers.js and this is the file that contains the logic that's being implemented for this particular layer management feature. As I scroll through it, I can see what's being added. We're getting layer count, we're deleting, we're updating the buttons. So all the other files are getting updated or have been updated by the agent to support the functionality that's been added in layers.js. So I'll click apply and now I can see the file has been applied to my workspace. I can look through the code, understand exactly what's doing in further detail. I can also click through and see how it's been using the other files that were added or updated within my code base. Now, when we go back to the terminal and list my files, I can see that that new file, layers.js, exists and the other files, of course, have been updated. Now, when I run the application once more, I can go back and refresh the Pixel Shop and now we see that there's a layers UI that I can use to manage the layers for my postcard. So I'm going to import the original postcard we had as a base image and I'm going to draw some squares on top of that and add more context that this is about Cloud Next 25 in Las Vegas and export it and now we have an even better postcard that we can use thanks to this new feature, layer management. As we're saving this, I'm now going to be able to upload the code changes that I've made, push them to GitHub, open a pull request and have my team review it so that when it's merged, then it can close the issue that people assigned me to work on. I'll pause here and just acknowledge or recap exactly what we've done here together. We started from scratch and built a fully functional app using Gemini Code Assist and the Google Docs tool which had our product specification. And then, we went and took that base MVP app that we had and we implemented a new feature on top of that using the GitHub tool, Google Docs tool and the Code Assist agent. I'm really excited to see what all of you will be able to build using the tools and the Code Assist agent that we've been able to use today. Thank you. Back to you, Pritpa. Thank you, Jerov, for an awesome demo. Can we switch back to the slides, please? Awesome. So, let's do a quick recap. We looked at how AI tools really provide some tangible improvements to your daily life. We also looked at how multi-agent systems can actually help you become more productive by augmenting your workflow so you can delegate more mundane tasks to it. Then, we spent some time looking at, you know, how PayPal is actually looking at infusing their SDLC with AI through different aspects, including agents. And then, you saw this comprehensive, awesome demo. Hopefully, it'll help you create some more memories. So, although we started in the IDE, today, you saw a demo scenario on a couple of use cases where we brought the power of the different developer tools that you use, Google Docs, their tools for Atlassian, Sentry, GitHub, GitLab. That's just a starting point, right? Developers, all of us, these are not the only tools that we use today, right? We all hang out on other surfaces. Chat applications, collaboration tools such as Slack, Google Chat, source control management system, GitHub, GitLab, et cetera, and even project management tools such as Atlassian. That's kind of where we're headed. We are super excited to take this journey and the next step on making the Code Assist agent available across all of these surfaces. That's kind of where we're headed next. Now, let's get your cameras out. We have some QR code for you folks to scan. The most exciting stuff. Okay. We are super excited to announce the private preview of Gemini Code Assist agents. Please follow the steps below, take pictures, sign up for the wait list, and then in the next few weeks, we plan to start accepting customers into that program. So, if this is something that you're eager and trying out, please do me a favor, sign up. The next announcement, we would love to have you folks help us define and shape the future of Gemini Code Assist agents. If you like what you saw today, scan this QR code. We also have Sarah in the audience today. Sarah, say hi to everyone. And, if you want some awesome swag, please stop by booth number 811 in the Expo. There's a UX research booth where Sarah is conducting research around this area, and she'll be happy to hand out some QR codes as well. Please stop by and do that. please extend your learning journey. There are multiple sessions around agents. The two that I'm going to call out, both of them happen to be tomorrow. Once at 2.30, there's a developer keynote. Check it out. We are super excited to show you not just what we have built today, but where this thing is headed in the future. Pretty exciting stuff. And then, another spotlight session tomorrow at 4 p.m. around using AI to drive the app quality across the entire software development lifecycle. So, please check this out. Also, we really appreciate your feedback. Please take a few minutes and just punch into your app what you like about this session, what you didn't like, speakers, demos, et cetera. And with that, we'll take questions. So, I know there are going to be some microphones that are going to be out here. All right. I would like to also invite Harsha and Jirob to come back on stage. And Sarah, if you want to join us as well. Can we turn on the microphone for Sarah? Microphone's here. Okay. Awesome. All right. Questions. So, let's wait up and see. All right. Let's take the first question here, please. I just have two questions. One question is, how does your agentic framework, how does it handle hallucinations? Thank you. Do you mind speaking up a little bit? How does your agentic framework, how does it handle hallucinations? How does your agentic... Handle hallucinations? Sure. Second question I have is, so this is great, thank you, but then real enterprise-latched code bases, which could be millions or maybe, I mean, tens of millions of lines of code. How do you fit that into a context window? The context window of Gemini 1.5 Pro is maybe two million. How do you fit that into it? Is there a rag mechanism that you have to use to enable or to ensure that you can actually pack more information? Yes. So, great questions. Let me replay them back for the audience just to make sure everyone heard. I think the first question was around how do we handle hallucinations? And the second one was around the context window may have some limitations. How do you get around that? Did I get them right? Okay. So, first question. I mentioned what we have built is a multi-agent system in which these agent personas do what we call self-critiquing. So, the output of one of the agent personas is critiqued by the other and this goes in a loop. And what we have seen is by doing that we are able to reduce the amount of noise or hallucination that happens just by the code LLM. There are checks and balances in place to be able to kind of look from that and then iteratively refine the response until we get to a specific point of quality, this continues in a loop. Which is why as Jerov mentioned that while the chat interface is good to be able to ask questions around like one or two files, these agents really kind of work in the background in that loop and sometimes can take time to execute. The second question that he asked was around context window and the limitations. Well, the way agents work, the good news is we have access to the entire code base, right? And so, that is not something that has to be sent over to the LLM all at once. The agent personas when they work in a loop, they're able to take a look at a code base and then selectively send whatever pieces that are needed to be sent to the LLM, chunking it down into smaller pieces if necessary. So, the agents handle and deal with the context window and get around that. Which, when you're interacting with an LLM directly, you may run into some of them, but with some of the increasing context windows, even with 2.5, some of those things are going to, you know, hopefully not be an issue. All right. We have a second question here. Yes. Yes. This may be a question for the gentleman from PayPal. You've been doing a lot of experimentation around different approaches for improving developer productivity. That's something that we're doing in my company as well. One of the issues that we're encountering is how do you actually measure developer productivity so you can determine which tools have a greater ROI for that. You know, some tools, maybe they don't cost that much, but they're actually reducing developer productivity, even though it seems like a neat idea at the time. I'm sure we can have an entire session about that. But the short answer of that is we are relying more on qualitative data points at this point. So we send out surveys to the team or the pilot group and ask them to make an estimate of how things would work. There are some amount of quantitative data points that we can look at, like, you know, the velocity of things but mostly in the experimentation phase, it is relying on qualitative data. Okay. I think we have room for maybe just one question. So, and we will be outside after the session is over to take a few more questions as well, but go ahead. Hey, my name is Anil. My name is Anil. I wanted to ask the demo. Code generation mostly depends on the product specification, the documentation which needs to be generated. Are there tools to, like, and rate the product specification or even to check if it is the mark where it can be used for the code generation? Yeah. So I think I heard parts of the question. I think the core question we had is, are there tools available to generate even the product specification? Right? In some instances, to be very honest, we actually use Gemini inside of Google Docs to actually write full specification. I've also used AI Studio personally quite a bit. you have to get a little bit more clever with prompting, but in some cases, some of the PRD that we generate, we've actually used some of the tools to be able to generate a decent level of specification as a starting point. Right? So these are getting mature. I think the more details you give them, they do a much better job, and so you have to just kind of experiment with that. And what was the second part of the question you had? Yeah. Yeah. Yeah. Yeah. That was still good. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. I see. Are there any? Yeah. Yeah. So I think there are some checks in place. I mean, we have integration with different kinds of things which can provide some kind of boundaries around some of these things. So yes, the short answer, yes, they are. But again, I think it comes down to like, you know, your use of those tools. With this, I think we'll step away. A few of us are going to be outside if they have more questions, happy to answer them. But hopefully, folks, like the session and enjoy the rest of your conference. Thanks. Thank you all. Thanks. Thank you.