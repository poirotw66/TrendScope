 Thank you for coming. Did you all go to the party last night? Was it fun? Yes. Love that. Enthusiasm. Well, it's a tough act to follow, but we did arrange a killer lineup. Between myself, Joe, I'm a product manager for BigQuery. I have Jamie, my peer, also on BigQuery, and then Camillo, who will be coming in and talking about what he's done in MercadoLibre using vector embeddings and multimodal analysis. So very exciting stuff. What we'll cover is I'll go very quickly. I want to leave a lot of time for demo. This is a technical talk, but I'll cover some general ideas around data foundations for AI, what we're trying to accomplish. I'll cover vector search in BigQuery. Jamie will come on and talk about BigQuery multimodal data. Camillo will talk about his accomplishments with MercadoLibre, and then we'll leave some time for Q&A. If we run over, we can talk in the hall. So data foundations for AI. Have you guys heard about AI? I'm sure it's a topic of many conversations here. And I think when I talk to a lot of the data platform owners, they're very excited. There's a lot of opportunity we see for AIs, agent recommendations. But we all realize that the way that these AIs will provide an excellent experience is if your data quality is high quality. And I think that is our opportunity as the data platform owners, those who work with BigQuery, to build really high-quality data sets that work with your AI platform. So where BigQuery has started to evolve and solutions that we have started to target more and more, it's no longer that the data that is having high value is in the structured tabular format with calculations and aggregations. And no longer is it just semi-structured log data, things that exist in JSON format so that you can analyze quickly. We see there's a massive opportunity for unstructured or multimodal data. So things like images, documents, video, all provide opportunity to help improve your data quality landscape. The challenge is these are often living off an entire other universe and cannot be combined with your structured data into your data platform. So what we've done over these past few years is started to provide a native solution across all these formats, your tabular data, your semi-structured data, and your unstructured data all within one unified interface from BigQuery. So why build with BigQuery? Many reasons, but the same reasons that I have talked about for the long time that I've been on BigQuery still ring true. So for developers, BigQuery provides a lot of simplicity. You put it in BigQuery and it just works. So many of my customers are just like, can you help me bring this workload into BigQuery? Because when it's in there, it just works. It also is very scalable. Like BigQuery was the first platform to scale to petabyte data sets and supports many, many workloads in the terabyte range as well. And then price performance. The way that we're able to scale out our compute layer on top of that, Dremel, has really been industry-leading in price performance at that scale and level of simplicity. And yes, for admins, you will get observability, governance. That sort of comes for free. But today, because this is a more technical conversation, we'll focus more so on the value propositions of scale, simplicity, price performance for the developers, particularly those for multimodal workloads. Okay, BigQuery vector search. So this is a concept that is near and dear to my heart, something that has lived in the AI space for a while, but we've brought to BigQuery as well. The general concept is that you are able to take any type of data, run it through an embedding generation model, which turns it into a numerical value that exists in vector space. And what this means is you can compare it to any other type of data object or record that you have to see if these records are similar. That's simply the concept there. But because it is a generally applicable approach to generalizing your data across many different data types and data workloads, we've seen this use case come up in many different areas. So initially, search, what Google is founded on, the vector embedding and vector search approach has helped to support Google search for keyword search and even image search. If you're able to find records, you can also start to do really interesting things for personalization. So we've seen customers use our vector search product for ads targeting, recommendation, and fuzzy matching, so combining data sets that are similar but not exact matches. And then that started to extend more into the analytic space, so focusing on anomaly detection and manufacturing, drug discovery and healthcare, or even customer support by deduplicating records. A lot of use cases that we've worked on. And then finally, vector databases became really popular from a framework called retrieval augmentation generation. If I can find some similar objects within my database, pass it back to an LLM, that actually grounds it on the objects in your database rather than hallucinate. So it helps drive accuracy of your AI systems as well. Okay, so I am going to switch, not yet, but very shortly, I'm setting up a demo, into a demo that shows you how to do product recommendations and product substitutions all within BigQuery. So I'll cover a lot of features, and it'll be better to show them, but in general, I'll take you through a workflow that allows you to explore multimodal data, extract meaningful insights using AI in your database, embed those insights so that you can make them searchable within your index, then actually do a similarity search in there to return similar records, and then augment, take action on that. In this case, what we'll do is use Gemini to show if your similar records are actually duplicates to actually provide a product substitutions approach all within BigQuery. And to demonstrate this, if you've listened to a BigQuery topic already, you might have already seen our demo dataset. We have Symbol Pets, which is an imaginary e-commerce website that helps to sell nice toys and food for your furry friends. And it has a lot of product categories, descriptions all within with BigQuery. So our goal is to come up with a better product recommendation system to sell to owners of pets. And to ground this, I want to introduce you to Mitten. So this is my cat. She's cute, but she's bad. But she can be sort of tamed and distracted with toys and food. So she is an excellent candidate to sort of ground this example here of what could we potentially market to animals like Mitten. So I'm going to go ahead and switch to demo now. Okay. So here I am in BigQuery, and you may recognize this, but I'm using Data Canvas as the exploration tool here. And Data Canvas is a really excellent tool to help mind map complex workflows and help you explain things. So really good for demos like this. And what I have here is a table here of products. And you'll see product name, brand, category, descriptions, inventory, so on and so forth. I'll go ahead and preview this because I do want to call out one specific thing. So in this data set, we do have an embeddings column. And for most of the queries, I will remove this embedding column from the analysis just because it's kind of tough to digest. It's not really human readable. It is an array of floats that can be hundreds or even thousands of dimensions here. That is the backbone of our vector searches. But I'll come up here and run a query here. I'm just going to query on top of this to show you what this data set looks like. And what you'll see here are product names. So here is some cat food that, you know, Mitten love, Jungle Chew Parrot toy, some aquariums, some shampoo. So wonderful products available from Symbol Pets. You'll also see some descriptions here, some price, supplier ID, and then a URI over here. So this URI is really important. So this is delivering on that first expectation of working with multimodal data sets. This is a link to an image that is stored on GCS. Let me go ahead and exit out this. So to show you what that looks like, I have an object table here. And an object table is a reference to an object that lives in GCS so that you can work with it all within BigQuery. And the source URI is where the data is actually stored. So no replication of data to BigQuery. And you'll see a sample of some of these different products here. I'll open up a few of them because they will come up later. You'll see an aquarium, a nice igloo that Mitten would find very cozy, and then a cat toy. That would be perfect for distracting her. And now what this gives us is the ability to interact with this all within BigQuery with SQL or Python. So I'll step down here, and I'm going to do a task of generating product metadata. So if you look back at this data set here from the query I ran, you may have noticed that there are specific columns that are completely null. So subcategory, animal type, and search keyword. And these are because, you know, we didn't set up an ETL pipeline to actually categorize this, but this is an excellent opportunity to use AI within your database to augment that metadata. So that's exactly what I'll do. I'll show you the query that I'll run here. So a couple concepts here. I'm using this new function that we have called AI generate table. This is really considered, I think, like an AI function that is tailor-made for the database because you can pass in a prompt and also specify specific structure of the output table there. So it all operates all on your tables and with your tables. So I'm referencing symbol pets Gemini as my model here. This is something that I pre-configured that will communicate with Gemini Flash to output my structured table. So I'll go ahead and run this. You'll see the prompt while it runs. For the image of the pet product, concisely generate the following metadata. One animal type, five SEO search keywords, three product category. And then I've structured the animal type, search keywords, and subcategory all within that output schema. Now, you'll see exactly what's output there. So based on that aquarium that you saw within that image, we identified, yes, that should be for a fish. Here's some example search keywords that we might want to match on and the subcategories aquarium. For the cat, those cozy cat beds, we have beds and mattress and search categories are output there. So just within a couple clicks there, what I was able to do is augment my metadata based on the images that were provided to help extend the usability of my data tables. But that's great, but I think we can do better. So you notice that, hey, we have our products and our image table that are separate. The object table and the native table are still considered distinct. And you could join them back, but what we've invested in is actually providing a native integration within our native tables and providing a new data type for objects within those data tables. So these new object types, now soon to be available in preview, are called object refs. So within here, I'll select my product table. I will look at the metadata that was pulled from my connection and set up a new object ref. And within this new object ref type, I can now interact with that image and the metadata directly side by side with the actual product numerical data and categorical data. So here's what it looks like. It's a little, let me zoom in a little bit. You can see the ref, URI, version, authorizer, ref details. Basically everything you saw in the object table, now available directly in the native table. So that's interesting, but what can I actually do with that? Okay, here's an example. I found this a very compelling example. I want to pass an array of the object refs and images and generate some brand descriptions. And what I want to do here is from that object array is actually build an aggregation over the images and product descriptions, group them by the brand, and from the images and product descriptions, generate a brand description, and then count how many fit within that brand all in one place. So my prompt that I'm passing into generate text is use the images and text to give one concise brand description for a website brand, return the description only, aggregate the refs, aggregate the descriptions, and then give me some sums over the total. So I'll go ahead and execute this. And it's doing two things. It's both aggregating the things within the same brand and summarizing the similarities of the products in that brand and then defining that with the brand description. So here, that AquaClear brand took a look at all those images of the aquariums and described it as a premium aquarium supplies and equipment creating and maintaining healthy, thriving underwater ecosystem. The cozy naps are stylish and comfortable cat furniture for toys and happy, healthy cat. And then finally, that aggregation here is really interesting. You'll have the aggregation of the total products that fit that brand description. The next use case that I want to check in on here is some row operations that we can use within traditional work clauses. So just really quickly, too, Canvas is really nice because when it sees that an upstream node has been updated, it will tell you to re-execute the downstream one. So I can just go ahead and re-execute this. And what this is going to do is take in a new AI function that we're launching in Experimental. This is part of our AI query engine efforts that takes in a prompt and generates a true or false based on the prompt so that you can use it for filtering. So I've done some filter here of, hey, take a look at everything with a higher rating, category toys, and the prompt, this item would be enjoyable for my cat Mitten to play with. And what's really interesting here is you'll see, yes, the toys that are matched and the high ratings are matched, and obviously the cozy cat beds are in there, but you'll also see some that weren't actually, you know, tailor-made for cats. But when you look at these toys themselves, they are actually toys that would be interesting to cats. So I think this is like an interesting fuzzy matching approach to extending your product category reach and providing them in recommendations in ways that you wouldn't necessarily think based on deterministic categories. So to round out that section, what we did here is combine both structured and unstructured data. We used AI to analyze it and aggregate it with new insights, all in one single table using these object refs. Great. So let's talk a little bit about search. So let's go back to this product table, and I talked initially about vector search, but I want to talk about two different kinds of search that are available in BigQuery, because I think it will make sense why vector search is a value add. First, what I have here is a search over a keyword over the description data set, and I'm looking at dog toys. And what this is doing is a keyword match just based on the description if it has dog, toy, or any combination there. But if I had something that was not listed as an exact keyword match, like puppy toy, this is going to return no data at all. And this is the value of semantic matching. Like, I want to return puppy toy or dog toy matches when I search for puppy toys. And this is where vector search comes in. So I've added in a vector search command here. I'm actually pointing to that embeddings column, and I'm searching for puppy toys and generating an embedding from that puppy toy text. So when I run this, it's going to return three semantic... Oh, I'll just do that. It's going to return three semantic matches here based on the product name and description. So you'll see in any of these descriptions, puppy toy is actually not listed anywhere. You'll see pup, you'll see dog, and you'll see other matches that are semantically similar but not a keyword match. So that improves your search quality all within BigQuery by using embeddings and vector search. So very simple approach. This is being used in many different applications. So one thing I want to show you is an end-to-end pipeline that we could create to use this approach to deduplicate some of our records within Symbol Toys. So I'm going to pop over to our Python notebook here. So within BigQuery Studio, we have a notebook solution, and that's backed by any Python library that's useful that Colab provides. But we see the synergies in working with Big Frames, which are native Python data science package that works directly on top of BigQuery at the scale and performance that you would expect. So in this notebook, I've set up some libraries. I've pointed it to the right data set, and I've set up a function here that is display product images. And this is a custom function that we'll soon externalize that allows you to start to visualize the object ref variables so that you can use your notebook to explore both the categorical data and the visualization data. So just to show you what that looks like, I'll rerun the puppy toy SQL from a SQL cell in the notebook and then visualize the outputs of the matches there. You can actually see the puppy toys within the notebook that are stored via the object ref. Another interesting application of this is, you know, what we did is looked at the descriptions and the images and used embeddings to do the keyword match. But you can also embed your images directly. So if you want to use a multimodal image model to embed your images and then use that same multimodal model to embed text, you can do text to search all within BigQuery as well. So here I search for fishtank over just the images, the multimodal image table, and I'll see some fishtank-looking objects within our database. Okay, so now let's start to apply this to deduplicate some of our product catalog. So I'm going to do a search for squeaking fire hydrant dog toy. And what it returns here are two things that seem pretty similar, right? Perfect pup squeaker, playful pup squeaky toy. And, you know, based on the descriptions, the price, the rating, they seem like they could be the same. So what I'll do is I'll actually add a data frame on this to start to analyze it in a Pythonic way. So this is employing a big frame's multimodal data frame that takes all the structure that we have for object ref and making that available within a data frame approach so that data scientists can work with this. So I am structuring this so that I can analyze it. And then I'm going to actually pass this to Gemini and ask Gemini, are these two images identical? So based on the descriptions, they look similar, but let's check the images and see if these are actually the same product. This will take just a moment to run. Allow me to catch my breath. Okay, cool. So it did run. I'll display the results. Okay, if you can see this, yes, the two images are identical. And I'm not asking you just to trust me based on that LLM response. I'll actually show you what that looks like. And if you look back at these two product images, remember now, because the object ref is available within that data frame, I can display that. So you should see in line an image of these puppy toys and these giant fire hydrants. And in fact, yes, they are the same product. Cool. So we have resolved that these are the same product. Now, what do we want to do? Now, there's a lot of actions you might want to do as an e-commerce platform to when you have two duplicate items. You might want to provide a substitute. You might want to provide recommendations. You might want to remove it from your data set. In this case, what we want to do is choose the one that is the lower price so that we can save our customers money. So here, it returns just the lower price, $599. I've chose this because this is particularly relevant in Camilo's session later today, and he'll talk about how they use a similar approach within their data platform. But all of this serves to build an end-to-end pipeline. And the way that you would do this to extend this is to start to build a pipeline that is based off of a large batch job that uses similarity search and Gemini to optimize your ecosystem. So that is where our vector index comes into play here. And what I have is a much larger data set for a million rows here that is product listings and embeddings. I'll go ahead and run this. So this new index type is the one that we'll be launching in GA. This is the scan index in BigQuery that uses asymmetric hatching to optimize your resources. And it's the one that was invented by Google and is backing a lot of our consumer-based products. And what it's particularly good at is batch jobs. So if I do an embedding search over a million similarity search over a million embeddings at a time, this JML is able to scale out and scale across the index most cost-effectively to get really solid. You actually couldn't see that. I'll run it again. Get really cost-effective performance here. So you'll see all the dissimilar embeddings that are returned there. So let's go ahead and switch back to the slides and I'll end up before I hand it over. Oh, you can skip past Mitten. Oh, that's me. Okay, so all in one place, I covered a lot of ground here. So you're able to explore, extract, embed, index, search, and augment your data all within BigQuery in one single platform for multimodal data sets. All of those features are coming or available now. So the ones available now are generate embedding, scan vector index, and vector search. The ones that are coming in preview soon are object ref. And the ones that are available in preview now are generate table and multimodal data frames. I'll include all these within the follow-up at the end so you can pick them up and get going on them. I mentioned the scan index. I just wanted to talk about this just very briefly again. It was invented at Google, supports Google Search, Google Play, YouTube, and the real value here is for batch workloads. We ran some performance benchmarks versus our existing IVF index, and when you do batch jobs of millions or more, we really see 100x performance both in storage and compute. We'll be launching this with an integration with Vertex Vector Search, and we will be previewing partitioning support for our vector index to further help you optimize around categorical spaces as well. So the BigQuery Vertex Vector Search connection here. So we have, you know, really awesome offline use cases that BigQuery supports, and Vertex Vector Search has a vector database that is completely optimized for online low-latency support. We're going to create a connection between these two. So if you have embeddings in BigQuery and want to do online serving, it is a full unified platform between BigQuery and Vertex. And then if you want to, you know, bring your own embedding model for streaming or for batch use cases, we do provide other data analytics services to help you for streaming. Dataflow is a great platform to bring your own embedding model and get your embeddings in, and then we'll help you index them once they're there. And Dataproc also is a great place to help you do batch support as well. So we've been really excited to see all the momentum. Vector Search is having increased six times in the last six months. So a lot of energy, particularly around this new scan index. And we're excited to have you on board and join this ride with us and see what problems we can accomplish together. So with that, I'll hand this over to Jamie, and she can talk more about multimodal analytics. Thanks, Joe. I'm just going to dive deeper into a couple of the concepts that we just reviewed. in the awesome demo. So going back in time, we started unifying unstructured and structured data in May of 2023 with the launch of object tables. And they were really a way to represent unstructured data in a tabular format. And the idea was to give a familiar SQL interface to discover and list your objects. And because object tables contain URIs, the tables could be an input to BQML functions. Here's an example. We're using the function ml.generateText. And the table argument specifies image object table as the input. And under the hood, BigQuery uses the URI to tell Gemini which rows of images in GCS to process. In this example, we're prompting Gemini to write one sentence to describe the product in the image. This week, we're announcing object ref. It's a new BigQuery type that builds on top of object tables to seamlessly integrate unstructured data into your existing BigQuery tables. It's a struct that contains the object storage and access control metadata. Going forward, when you create an object table, there will automatically be a ref column containing all of your object refs. And object refs improve composability by encompassing all of the pertinent data into one cell. You can move object refs around tables to store the unstructured data alongside structured data in the same row throughout different data engineering stages and govern both using a similar access control model. Object refs can also be created as the output of a transformation. So now unstructured data processing can be done with LLMs, ML models, and open source Python libraries in the same SQL or Python script that processes your tabular data. They can also be persisted in another table while keeping your data in place. So no breaking any of your data management policies. BigQuery with object ref unlocks unique platform capabilities across both data and AI. So firstly, multimodality. Here's an example of what it looks like to store structured and unstructured data side by side. So if you look at the pop-out here at the bottom, we have the structured data in the green columns and the red column represents your object refs. And doing this enables us to natively handle structured data, unstructured data, and a combination of the two in a single table. And also build multimodal data pipelines to process both structured and unstructured data. Our data platform is Gen AI ready, serverless, and autoscaled, enabling us to focus on building data pipelines instead of managing infrastructure. We can process unstructured data with LLMs or use serverless Python UDFs with your favorite open source library. We can also create embeddings, summarize data using a prompt, use BigQuery tables as inputs to Vertex AI jobs, and much more. We can use Python or SQL without worrying about interoperability. So if it works with SQL, it works with Python with BigQuery data frames and vice versa. Transforming objects, saving the transformed objects back to GCS, and any other aggregations or filtering can all be done in one SQL or Python script. And because BigQuery is our data foundation, we have unified governance and access control. We can use familiar BigQuery governance features such as fine-grained access control, data masking, and connection-delegated access on unstructured data. There's no need to manage siloed governance models anymore for your structured versus unstructured data. With this combined multimodal data, we can extract more value from our data and answer business questions such as fine electronics, fine electronics products with high return rates and customer photos showing signs of damage on arrival. To answer this question, we have to join structured data. In this case, the product category is electronics. Return rate is above some value. And to your unstructured data, customer photos in this case. And then we use AI ML to find photos showing damaged products. Some other examples. Generate summaries for all inspection reports related to machines in the western region that failed safety checks last quarter. Which marketing campaigns generated leads with sales calls that subsequently mentioned budget constraints and were lost? And the last example, which top 10 revenue customers complained about performance issues last month? And this is what the secret would look like for that first example. Our where clause has three arguments. Category equals electronics. Return rate greater than 0.25. And Joe already touched on our new AI query engine function, generate bool. And we're passing two columns, image ref and video ref as inputs. And we're prompting the LLM to check the image or video shows signs of damage. We have several new AI query engine functions that take object refs as inputs. There's a blog post coming out soon that goes into more detail for each of those. So please be on the lookout for that. Now I'll pass over to Camilo to talk about how he uses BigQuery and Gemini to improve data annotation quality for Mercado Libre. Thank you. APPLAUSE OK. So we are going to see what Joe and Jamie talk about, but in some clear example in the real life. First of all, to give us a little bit of context about Mercado Libre, our mission is to democratize e-commerce and financial services in Latin America. And to give some clue about and numbers about this statement, I want to mention that this company has 25 years in the market, but today is the most valuable company in Latin America. We operate in 18 countries, and we have 100 million customers in our e-commerce platform, but we also have 61 million customers in the ficture part that is called Mercado Pago. that might be similar to PayPal. Most important, I like the last statement, that this platform allows people to, I mean, is the main source of income for almost 2 million people in Latin America. Right? And part of the success is, I guess you can guess it, is data. And because we have all kind of data from images, video, audio, text, anything you can imagine is multimodal, multi-type, anything, we need to do an universal language to combine it and to create synergies between all this data. And the universal language to do that is embeddings, as you might guess. And embeddings is just part or almost each part of an e-commerce platform. It's starting for personalization, semantic search, content moderation, and even dynamic pricing. And I want to give you just a clear example of how it works. Imagine you are in the e-commerce, and you want to go back into your purchases and buy something back again. And what happens is that you don't find the product. I assure you this doesn't happen all the time, but when it happens, it's really frustrating. And in this moment, we have to face an alternative. The cost of opportunity of losing that sale, or looking for an alternative amongst our 450 million product catalog. And we have to do it fast and accurate, right? And you might think, okay, it's quite simple. This picture, it looks like the universe, and actually it is the universe of products of a category, where we can see in the middle, like, the embedding representation of those trending products. products. And we can look for a product, right? And for example, the product that we were looking for before, we apply cosine similarity to look what of the products that are closer to this product, right? And I am not a dermatologist, but I can assure you the similar product in terms of embedding is not exactly the same product, right? So it's not so trivial to do that. And just to give an example, and you believe me, I want to test you guys and say to me if these products are the same. What do you think? Yes? Maybe. Maybe. The pictures, as you can check, are not exactly the same. So we actually have to take a look, for example, for the description, the attributes, the titles, so to realize if they are a real match. And in order to do so, we create this architecture when we have all these data sources and we have this item that we're looking like the twin or the exact match. Then we apply open source embedding model to create the vectors and we can compare. And then we apply also a vector search that behind is just the ANN algorithm to look for candidates. Most of the use cases end here, but for some specific ones, like the one that I showed you before, we need the human input, the human in the loop to check which one is exactly the same. So we thought, why not to completely automate this process using Gemini? And actually we did that. And this is the input, all the information combined. And we did the prompt engineering, but we realized soon that trying to do product matching with a prompt is like trying to do sushi in one step. It doesn't work, right? And that person was me because at some point I thought maybe this was this kind of problems that we thought because the hype of LLMs, we need LLMs, but maybe we need another approach. But before giving up on this challenge, we try, like, why not doing this thing step by step, right? Like a well-made sushi. And to do that, we try to mimic what a person will do, right? And starting for the rigorosity level. If you realize that the prices are close, the probability that the same product might be higher. If not, maybe you are more, like, critical about the next information that is coming. Then you take a look of the pictures. We talk about if they are similar or not. But then why not enrich the information of the title with the description and make the comparison of the titles, too. Until this point, we have to make a decision. Can we say the products are the same? And if not, why not go deeper and look for the attributes? And right there, we have all these little boxes that you can see some of them are LLMs, but other ones are just Python nodes where we mix and transform some information in order to process it better. And what are the results? We reach human-level performance in this task. And you might see in this picture that the human performance actually is a range. Because when the human are doing repetitive tasks, maybe in the morning they do pretty good, but in the afternoon they are just a little bit tired. So the LLM, the combination of the system that we saw before, is more reliable and performs more consistently with the tasks that we are giving to it. And I would like to say this was like a straight path to success, but actually the path looks like this. It was like a roller coaster. Because in order to increase the precision that in this context means the quality of the match, sometimes we have to deal with the trade-off that increasing the cost, like bringing on an LLM that is more expensive, or decreasing the coverage, in this case the recall. right? So it was a roller coaster, but something that was important in the end is that we'd increase the quality of the matches by 150%, almost, and more than 400% of the coverage. And we did that decreasing the cost of the system 91%. And maybe you can look like the scale of the cost. It looked like 0.00, and it looked like, oh, so cheap, right? But because we need to do 70 million matches per month, this was like almost like a million task, a million dollar task. With this decrease, nowadays it costs just $70,000. As trivial as this process sounds, we use it in another place. Like, for example, in the page, we can see the dinosaur, and why not give the user like another alternative for like a new price, a different price, a different seller, a different delivery method of that exactly the same product. And also, Mercado Libre is an ad company. We promote items. And if we have more items to promote, then the business is bigger, right? So why not bring so relevant items to promote so the user is just in the right context to make the right decision? And with this algorithm, we populate this little box with 20% more candidates. Finally, I would like to mention that behind all the scenes is not just LLMs, it's not just Gen AI. I think for me, the lesson of this part of the presentation is that actually is the mix of everything, from embedding, machine learning, vector databases, to also LLMs, to really create something that creates value to customers, and for us to increase the experience in the platform. Thank you so much for coming. It's always tough on a Friday, so I appreciate your time. I wouldn't spend it anywhere else. I know you guys want to다. Thanks for moving. Yeah. Thank you, Anthony Dears. Thanks for getting irritated in the new world. Thank you. You could hear that in the네. And what what are you going to say